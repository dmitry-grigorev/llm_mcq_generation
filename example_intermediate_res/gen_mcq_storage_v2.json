{"ALL_TERMS": ["similar word", "discourse coherence", "seed word", "word error", "entity mention", "discourse structure", "negative word", "mention detection", "dependency parsing", "relation extraction", "word meaning", "dialogue act", "coherence relation", "target word", "temporal expression", "parse tree", "dialogue system", "edit distance", "semantic role", "text classification", "hidden layer", "antecedent", "speech recognition", "temporal", "large language", "neural network", "question answering", "regular expression", "naive bayes", "loss function", "coherence", "dependency", "coreference", "lexicon", "logistic regression", "grammar", "dialogue", "hidden", "translation"], "questions_data": {"similar word": {"max_id": 11, "Questions": [{"id": 0, "context": "Much of natural language processing is concerned with measuring how similar two strings are. For example in spelling correction, the user typed some erroneous stringlet`s say graffeand we want to know what the user meant. The user probably intended a word that is similar to graffe. Among candidate similar words, the word giraffe, which differs by only one letter from graffe, seems intuitively to be more similar than, say grail or graf, which differ in more letters. Another example comes from coreference, the task of deciding whether two strings such as the following refer to the same entity: ", "Bloom_type": "remember", "question": "In natural language processing, why is it important to measure similarity between strings?", "options": ["To find words that sound alike but mean different things", "To determine if a string contains grammatical errors", "To identify synonyms for a given word", "To improve the accuracy of spell checkers"], "complexity": 0}, {"id": 1, "context": "For documents, we saw that similar documents had similar vectors, because similar documents tend to have similar words. This same principle applies to words: similar words have similar vectors because they tend to occur in similar documents. The term-document matrix thus lets us represent the meaning of a word by the documents it tends to occur in. ", "Bloom_type": "remember", "question": "Why do similar documents tend to have similar vectors?", "options": ["Because similar documents are more likely to appear together in a document-term matrix.", "Because similar documents contain more keywords.", "Because similar documents have the same author.", "Because similar documents use different vocabulary."], "complexity": 0}, {"id": 2, "context": "Either the PPMI model or the tf-idf model can be used to compute word similarity, for tasks like finding word paraphrases, tracking changes in word meaning, or automatically discovering meanings of words in different corpora. For example, we can find the 10 most similar words to any target word w by computing the cosines between w and each of the V ", "Bloom_type": "remember", "question": "Which method is commonly used to calculate word similarity?", "options": ["tf-idf model", "PPMI model", "WordNet", "Latent Semantic Analysis (LSA)"], "complexity": 0}, {"id": 3, "context": "For example Levy and Goldberg (2014a) showed that using skip-gram with a 2, the most similar words to the word Hogwarts (from the Harry Potter window of series) were names of other fictional schools: Sunnydale (from Buffy the Vampire Slayer) or Evernight (from a vampire series). With a window of 5, the most similar words to Hogwarts were other words topically related to the Harry Potter series: Dumbledore, Malfoy, and half-blood. ", "Bloom_type": "remember", "question": "Which method did Levy and Goldberg use to find the most similar words to Hogwarts?", "options": ["Word Embeddings", "TF-IDF", "Text Mining", "Sentiment Analysis"], "complexity": 0}, {"id": 4, "context": "Much of natural language processing is concerned with measuring how similar two strings are. For example in spelling correction, the user typed some erroneous stringlet`s say graffeand we want to know what the user meant. The user probably intended a word that is similar to graffe. Among candidate similar words, the word giraffe, which differs by only one letter from graffe, seems intuitively to be more similar than, say grail or graf, which differ in more letters. Another example comes from coreference, the task of deciding whether two strings such as the following refer to the same entity: ", "Bloom_type": "comprehension", "question": "Which of the following best explains why 'giraffe' might be considered more similar to 'grafee' than 'grail' or 'graf'? ", "options": ["'Giraffe' has fewer differences compared to other words.", "'Grail' and 'graf' have more common letters with 'grafee'.", "'Giraffe' is spelled exactly the same as 'grafee'.", "The similarity between words depends solely on their length."], "complexity": 1}, {"id": 5, "context": "For documents, we saw that similar documents had similar vectors, because similar documents tend to have similar words. This same principle applies to words: similar words have similar vectors because they tend to occur in similar documents. The term-document matrix thus lets us represent the meaning of a word by the documents it tends to occur in. ", "Bloom_type": "comprehension", "question": "Explain how similar words are represented using the term-document matrix?", "options": ["Similar words are represented by their frequency in all documents.", "Similar words are represented by their presence in the most common documents.", "Similar words are represented by their occurrence in the least common documents.", "Similar words are represented by their absence from any document."], "complexity": 1}, {"id": 6, "context": "Either the PPMI model or the tf-idf model can be used to compute word similarity, for tasks like finding word paraphrases, tracking changes in word meaning, or automatically discovering meanings of words in different corpora. For example, we can find the 10 most similar words to any target word w by computing the cosines between w and each of the V ", "Bloom_type": "comprehension", "question": "Which method can be used to find the 10 most similar words to any target word w?", "options": ["Both PPMI model and tf-idf model", "PPMI model", "tf-idf model", "Neither PPMI model nor tf-idf model"], "complexity": 1}, {"id": 7, "context": "For example Levy and Goldberg (2014a) showed that using skip-gram with a 2, the most similar words to the word Hogwarts (from the Harry Potter window of series) were names of other fictional schools: Sunnydale (from Buffy the Vampire Slayer) or Evernight (from a vampire series). With a window of 5, the most similar words to Hogwarts were other words topically related to the Harry Potter series: Dumbledore, Malfoy, and half-blood. ", "Bloom_type": "comprehension", "question": "What was the method used by Levy and Goldberg to find the most similar words to Hogwarts?", "options": ["Skip-gram model with a 2-window size", "Skip-gram model with a 5-window size", "Word2Vec model with a 2-window size", "Word2Vec model with a 5-window size"], "complexity": 1}, {"id": 8, "context": "Much of natural language processing is concerned with measuring how similar two strings are. For example in spelling correction, the user typed some erroneous stringlet`s say graffeand we want to know what the user meant. The user probably intended a word that is similar to graffe. Among candidate similar words, the word giraffe, which differs by only one letter from graffe, seems intuitively to be more similar than, say grail or graf, which differ in more letters. Another example comes from coreference, the task of deciding whether two strings such as the following refer to the same entity: ", "Bloom_type": "application", "question": "Which method could be used to determine the similarity between two strings for tasks like spelling correction and coreference resolution?", "options": ["Calculate the Levenshtein distance between the strings.", "Use cosine similarity on the frequency vectors of the strings.", "Compute the Jaccard index based on set intersection over union.", "Measure the Hamming distance between the strings."], "complexity": 2}, {"id": 9, "context": "For documents, we saw that similar documents had similar vectors, because similar documents tend to have similar words. This same principle applies to words: similar words have similar vectors because they tend to occur in similar documents. The term-document matrix thus lets us represent the meaning of a word by the documents it tends to occur in. ", "Bloom_type": "application", "question": "What is used to represent the meaning of a word based on the documents it occurs in?", "options": ["Term-document matrix", "Word frequency count", "TF-IDF scores", "Cosine similarity"], "complexity": 2}, {"id": 10, "context": "Either the PPMI model or the tf-idf model can be used to compute word similarity, for tasks like finding word paraphrases, tracking changes in word meaning, or automatically discovering meanings of words in different corpora. For example, we can find the 10 most similar words to any target word w by computing the cosines between w and each of the V ", "Bloom_type": "application", "question": "Which method is commonly used to calculate word similarity?", "options": ["tf-idf model", "PPMI model", "WordNet", "Latent Semantic Analysis (LSA)"], "complexity": 2}, {"id": 11, "context": "For example Levy and Goldberg (2014a) showed that using skip-gram with a 2, the most similar words to the word Hogwarts (from the Harry Potter window of series) were names of other fictional schools: Sunnydale (from Buffy the Vampire Slayer) or Evernight (from a vampire series). With a window of 5, the most similar words to Hogwarts were other words topically related to the Harry Potter series: Dumbledore, Malfoy, and half-blood. ", "Bloom_type": "application", "question": "What is the method used to find the most similar words for Hogwarts?", "options": ["Word2Vec model with a 5-window size", "Skip-gram model with a 2-window size", "Skip-gram model with a 5-window size", "Word2Vec model with a 2-window size"], "complexity": 2}]}, "discourse coherence": {"max_id": 8, "Questions": [{"id": 0, "context": "The focus of mask-based learning is on predicting words from surrounding contexts with the goal of producing effective word-level representations. However, an important class of applications involves determining the relationship between pairs of sentences. These include tasks like paraphrase detection (detecting if two sentences have similar meanings), entailment (detecting if the meanings of two sentences entail or contradict each other) or discourse coherence (deciding if two neighboring sentences form a coherent discourse). ", "Bloom_type": "remember", "question": "In the field of machine learning, what type of task focuses on understanding the relationships between pairs of sentences?", "options": ["Paraphrase detection", "Sentiment analysis", "Topic modeling", "Entailment determination"], "complexity": 0}, {"id": 1, "context": "As mentioned in Section 11.2.2, an important type of problem involves the classification of pairs of input sequences. Practical applications that fall into this class include paraphrase detection (are the two sentences paraphrases of each other?), logical entailment (does sentence A logically entail sentence B?), and discourse coherence (how coherent is sentence B as a follow-on to sentence A?). ", "Bloom_type": "remember", "question": "In practical applications involving discourse coherence, what does it mean when one sentence follows another?", "options": ["It means the sentences are chronologically sequential.", "It means the sentences are unrelated.", "It means the sentences are semantically similar.", "It means the sentences are syntactically identical."], "complexity": 0}, {"id": 2, "context": "Training models to predict longer contexts than just consecutive pairs of sentences can result in even stronger discourse representations. For example a Transformer language model trained with a contrastive sentence objective to predict text up to a distance of 2 sentences improves performance on various discourse coherence tasks (Iter et al., 2020). ", "Bloom_type": "remember", "question": "What does training models to predict longer contexts improve?", "options": ["Discourse representation strength", "Model accuracy", "Sentence length prediction", "Word frequency analysis"], "complexity": 0}, {"id": 3, "context": "The focus of mask-based learning is on predicting words from surrounding contexts with the goal of producing effective word-level representations. However, an important class of applications involves determining the relationship between pairs of sentences. These include tasks like paraphrase detection (detecting if two sentences have similar meanings), entailment (detecting if the meanings of two sentences entail or contradict each other) or discourse coherence (deciding if two neighboring sentences form a coherent discourse). ", "Bloom_type": "comprehension", "question": "Explain how discourse coherence differs from other sentence pair tasks like paraphrase detection and entailment?", "options": ["Discourse coherence is concerned with the logical flow and consistency of ideas across sentences, unlike paraphrase detection which compares word-for-word similarities.", "Paraphrase detection focuses on understanding the meaning of individual words, whereas discourse coherence looks at the overall structure of sentences.", "Entailment detection aims to determine if one sentence implies another, while discourse coherence assesses whether sentences are logically connected within a paragraph.", "Paraphrase detection requires identifying synonyms, whereas discourse coherence evaluates the semantic relationships between sentences."], "complexity": 1}, {"id": 4, "context": "As mentioned in Section 11.2.2, an important type of problem involves the classification of pairs of input sequences. Practical applications that fall into this class include paraphrase detection (are the two sentences paraphrases of each other?), logical entailment (does sentence A logically entail sentence B?), and discourse coherence (how coherent is sentence B as a follow-on to sentence A?). ", "Bloom_type": "comprehension", "question": "What practical application does discourse coherence have in relation to pair classification problems?", "options": ["Paraphrase detection", "Logical entailment", "Text summarization", "Sentiment analysis"], "complexity": 1}, {"id": 5, "context": "Training models to predict longer contexts than just consecutive pairs of sentences can result in even stronger discourse representations. For example a Transformer language model trained with a contrastive sentence objective to predict text up to a distance of 2 sentences improves performance on various discourse coherence tasks (Iter et al., 2020). ", "Bloom_type": "comprehension", "question": "Explain how training models to predict longer contexts enhances discourse representation?", "options": ["It improves the ability to understand relationships between sentences.", "It allows for more accurate predictions within single sentences.", "It enables better understanding of individual word meanings.", "It increases the complexity of sentence structures."], "complexity": 1}, {"id": 6, "context": "The focus of mask-based learning is on predicting words from surrounding contexts with the goal of producing effective word-level representations. However, an important class of applications involves determining the relationship between pairs of sentences. These include tasks like paraphrase detection (detecting if two sentences have similar meanings), entailment (detecting if the meanings of two sentences entail or contradict each other) or discourse coherence (deciding if two neighboring sentences form a coherent discourse). ", "Bloom_type": "application", "question": "In the application of mask-based learning for discourse coherence, what is the primary task?", "options": ["Determining sentence relationships", "Predicting individual words", "Classifying image features", "Identifying object types"], "complexity": 2}, {"id": 7, "context": "As mentioned in Section 11.2.2, an important type of problem involves the classification of pairs of input sequences. Practical applications that fall into this class include paraphrase detection (are the two sentences paraphrases of each other?), logical entailment (does sentence A logically entail sentence B?), and discourse coherence (how coherent is sentence B as a follow-on to sentence A?). ", "Bloom_type": "application", "question": "What is another practical application of the classification of pairs of input sequences?", "options": ["Detecting plagiarism between documents", "Identifying synonyms in a paragraph", "Determining if a word is used correctly in a sentence", "Predicting future events based on past data"], "complexity": 2}, {"id": 8, "context": "Training models to predict longer contexts than just consecutive pairs of sentences can result in even stronger discourse representations. For example a Transformer language model trained with a contrastive sentence objective to predict text up to a distance of 2 sentences improves performance on various discourse coherence tasks (Iter et al., 2020). ", "Bloom_type": "application", "question": "What is an effective way to improve the performance of discourse coherence tasks?", "options": ["Train a model using a contrastive sentence objective to predict text up to a distance of 2 sentences", "Increase the length of the input sequences only", "Use a larger dataset for training", "Implement more complex neural network architectures"], "complexity": 2}]}, "seed word": {"max_id": 7, "Questions": [{"id": 0, "context": "In summary, semisupervised algorithms use a human-defined set of seed words for the two poles of a dimension, and use similarity metrics like embedding cosine, coordination, morphology, or thesaurus structure to score words by how similar they are to the positive seeds and how dissimilar to the negative seeds. ", "Bloom_type": "remember", "question": "In semisupervised algorithms, what is used as a starting point for defining the two poles of a dimension?", "options": ["human-defined sets", "positive seeds", "negative seeds", "similarity metrics"], "complexity": 0}, {"id": 1, "context": " Lexicons can be built with semi-supervised, bootstrapping from seed words ", "Bloom_type": "remember", "question": "What is a key component used in building lexicons through semi-supervised learning?", "options": ["Bootstrapping techniques", "Supervised learning algorithms", "Randomized algorithms", "Feature selection methods"], "complexity": 0}, {"id": 2, "context": "Another common way to learn sentiment lexicons is to start from a set of seed words that define two poles of a semantic axis (words like good or bad), and then find ways to label each word w by its similarity to the two seed sets. Here we summarize two families of seed-based semi-supervised lexicon induction algorithms, axis-based and graph-based. ", "Bloom_type": "comprehension", "question": "What are two types of seed-based semi-supervised lexicon induction algorithms mentioned in the response?", "options": ["Graph-based and word-based", "Axis-based and sentence-based", "Sentence-based and graph-based", "Word-based and sentence-based"], "complexity": 1}, {"id": 3, "context": "In summary, semisupervised algorithms use a human-defined set of seed words for the two poles of a dimension, and use similarity metrics like embedding cosine, coordination, morphology, or thesaurus structure to score words by how similar they are to the positive seeds and how dissimilar to the negative seeds. ", "Bloom_type": "comprehension", "question": "What does the seed word method involve in semisupervised algorithms?", "options": ["It combines human-defined seeds with unsupervised learning techniques.", "It uses a machine learning model to define the positive and negative seeds.", "It relies solely on user input for defining the seeds.", "It assigns random weights to all words."], "complexity": 1}, {"id": 4, "context": " Lexicons can be built with semi-supervised, bootstrapping from seed words ", "Bloom_type": "comprehension", "question": "What method involves using existing knowledge (seed words) to build lexicons?", "options": ["Bootstrapping", "Semi-supervised learning", "Random sampling", "Manual creation"], "complexity": 1}, {"id": 5, "context": "Another common way to learn sentiment lexicons is to start from a set of seed words that define two poles of a semantic axis (words like good or bad), and then find ways to label each word w by its similarity to the two seed sets. Here we summarize two families of seed-based semi-supervised lexicon induction algorithms, axis-based and graph-based. ", "Bloom_type": "application", "question": "Which type of seed-based semi-supervised lexicon induction algorithm uses an axis to define the two poles?", "options": ["Axis-based", "Graph-based", "Both Axis-based and Graph-based", "None of the above"], "complexity": 2}, {"id": 6, "context": "In summary, semisupervised algorithms use a human-defined set of seed words for the two poles of a dimension, and use similarity metrics like embedding cosine, coordination, morphology, or thesaurus structure to score words by how similar they are to the positive seeds and how dissimilar to the negative seeds. ", "Bloom_type": "application", "question": "What is the first step in using seed words for semisupervised learning?", "options": ["Define the positive and negative seeds", "Choose random words as seeds", "Calculate the distance between all pairs of words", "Identify synonyms and antonyms"], "complexity": 2}, {"id": 7, "context": " Lexicons can be built with semi-supervised, bootstrapping from seed words ", "Bloom_type": "application", "question": "What is an essential step in building a lexicon using semi-supervised learning?", "options": ["Starting with a small set of seed words as initial vocabulary", "Collecting large amounts of data randomly", "Using only labeled data for training", "Implementing unsupervised algorithms exclusively"], "complexity": 2}]}, "word error": {"max_id": 2, "Questions": [{"id": 0, "context": "Alignment Knowing the minimum edit distance is useful for algorithms like finding potential spelling error corrections. But the edit distance algorithm is important in another way; with a small change, it can also provide the minimum cost alignment between two strings. Aligning two strings is useful throughout speech and language processing. In speech recognition, minimum edit distance alignment is used to compute the word error rate (Chapter 16). Alignment plays a role in machine translation, in which sentences in a parallel corpus (a corpus with a text in two languages) need to be matched to each other. ", "Bloom_type": "remember", "question": "In what application of string alignment does the minimum edit distance help determine the best match between two strings?", "options": ["Speech recognition", "Machine learning", "Data compression", "Image recognition"], "complexity": 0}, {"id": 1, "context": "Alignment Knowing the minimum edit distance is useful for algorithms like finding potential spelling error corrections. But the edit distance algorithm is important in another way; with a small change, it can also provide the minimum cost alignment between two strings. Aligning two strings is useful throughout speech and language processing. In speech recognition, minimum edit distance alignment is used to compute the word error rate (Chapter 16). Alignment plays a role in machine translation, in which sentences in a parallel corpus (a corpus with a text in two languages) need to be matched to each other. ", "Bloom_type": "comprehension", "question": "Explain how alignment plays a role in machine translation?", "options": ["Alignment helps in matching words from different languages by minimizing the number of changes needed.", "Alignment ensures that all words are translated into their exact equivalents in the target language.", "Alignment focuses solely on correcting spelling errors in the source text.", "Alignment does not play any significant role in machine translation."], "complexity": 1}, {"id": 2, "context": "Alignment Knowing the minimum edit distance is useful for algorithms like finding potential spelling error corrections. But the edit distance algorithm is important in another way; with a small change, it can also provide the minimum cost alignment between two strings. Aligning two strings is useful throughout speech and language processing. In speech recognition, minimum edit distance alignment is used to compute the word error rate (Chapter 16). Alignment plays a role in machine translation, in which sentences in a parallel corpus (a corpus with a text in two languages) need to be matched to each other. ", "Bloom_type": "application", "question": "In speech recognition, what metric is used to measure the quality of word errors?", "options": ["Minimum edit distance", "Hamming distance", "Jaccard similarity", "Levenshtein distance"], "complexity": 2}]}, "entity mention": {"max_id": 5, "Questions": [{"id": 0, "context": "Integrating entity linking into coreference can help draw encyclopedic knowledge (like the fact that Donald Tsang is a president) to help disambiguate the mention the President. Ponzetto and Strube (2006) 2007 and Ratinov and Roth (2012) showed that such attributes extracted from Wikipedia pages could be used to build richer models of entity mentions in coreference. More recent research shows how to do linking and coreference jointly (Hajishirzi et al. 2013, Zheng et al. 2013) or even jointly with named entity tagging as well (Durrett and Klein 2014). ", "Bloom_type": "remember", "question": "In what way does integrating entity linking into coreference improve the ability to distinguish between different meanings of an entity mentioned?", "options": ["It enhances the accuracy of disambiguating mentions by incorporating encyclopedic knowledge.", "It helps identify synonyms.", "It simplifies the process of tagging named entities.", "It reduces the need for further data annotation."], "complexity": 0}, {"id": 1, "context": "The entity grid model of Barzilay and Lapata (2008) is an alternative way to capture entity-based coherence: instead of having a top-down theory, the entity-grid model using machine learning to induce the patterns of entity mentioning that make a discourse more coherent. ", "Bloom_type": "remember", "question": "In the entity grid model of Barzilay and Lapata (2008), what does the use of machine learning primarily enable?", "options": ["To enhance the coherence of a discourse by understanding how entities are mentioned", "To identify the most important entities", "To predict future events based on past mentions", "To reduce the complexity of language models"], "complexity": 0}, {"id": 2, "context": "Integrating entity linking into coreference can help draw encyclopedic knowledge (like the fact that Donald Tsang is a president) to help disambiguate the mention the President. Ponzetto and Strube (2006) 2007 and Ratinov and Roth (2012) showed that such attributes extracted from Wikipedia pages could be used to build richer models of entity mentions in coreference. More recent research shows how to do linking and coreference jointly (Hajishirzi et al. 2013, Zheng et al. 2013) or even jointly with named entity tagging as well (Durrett and Klein 2014). ", "Bloom_type": "comprehension", "question": "How does integrating entity linking into coreference improve the understanding of entity mentions?", "options": ["It enhances the accuracy of disambiguating ambiguous mentions by incorporating additional encyclopedic knowledge.", "It helps identify specific entities within a sentence.", "It simplifies the task of identifying coreferences by reducing the need for external data.", "It decreases the importance of named entity tagging."], "complexity": 1}, {"id": 3, "context": "The entity grid model of Barzilay and Lapata (2008) is an alternative way to capture entity-based coherence: instead of having a top-down theory, the entity-grid model using machine learning to induce the patterns of entity mentioning that make a discourse more coherent. ", "Bloom_type": "comprehension", "question": "What does the entity grid model differ from in capturing entity-based coherence?", "options": ["It uses a bottom-up approach rather than a top-down one.", "It relies solely on human interpretation for pattern recognition.", "It focuses on sentence-level analysis rather than discourse-level coherence.", "It requires manual annotation for its implementation."], "complexity": 1}, {"id": 4, "context": "Integrating entity linking into coreference can help draw encyclopedic knowledge (like the fact that Donald Tsang is a president) to help disambiguate the mention the President. Ponzetto and Strube (2006) 2007 and Ratinov and Roth (2012) showed that such attributes extracted from Wikipedia pages could be used to build richer models of entity mentions in coreference. More recent research shows how to do linking and coreference jointly (Hajishirzi et al. 2013, Zheng et al. 2013) or even jointly with named entity tagging as well (Durrett and Klein 2014). ", "Bloom_type": "application", "question": "What technique involves integrating entity linking into coreference resolution?", "options": ["Both entity linking and coreference resolution", "Entity extraction only", "Coreference resolution only", "Named entity tagging"], "complexity": 2}, {"id": 5, "context": "The entity grid model of Barzilay and Lapata (2008) is an alternative way to capture entity-based coherence: instead of having a top-down theory, the entity-grid model using machine learning to induce the patterns of entity mentioning that make a discourse more coherent. ", "Bloom_type": "application", "question": "What does the entity grid model aim to achieve?", "options": ["To improve the coherence of a discourse through machine learning", "To create a bottom-up theory for entity mentions", "To predict future entities based on past mentions", "To enhance the accuracy of named entity recognition"], "complexity": 2}]}, "discourse structure": {"max_id": 14, "Questions": [{"id": 0, "context": "Thus when an entity is first introduced into a discourse its mentions are likely to have full names, titles or roles, or appositive or restrictive relative clauses, as in the introduction of our protagonist in (23.1): Victoria Chen, CFO of Megabucks Banking. As an entity is discussed over a discourse, it becomes more salient to the hearer and its mentions on average typically becomes shorter and less informative, for example with a shortened name (for example Ms. Chen), a definite description (the 38-year-old), or a pronoun (she or her) (Hawkins 1978). However, this change in length is not monotonic, and is sensitive to discourse structure (Grosz 1977b, Reichman 1985, Fox 1993). ", "Bloom_type": "remember", "question": "In what way does the length of mentions of entities change during a discourse?", "options": ["The length increases monotonically.", "The length decreases monotonically.", "The length changes unpredictably.", "The length remains constant."], "complexity": 0}, {"id": 1, "context": "In addition to the local coherence between adjacent or nearby sentences, discourses also exhibit global coherence. Many genres of text are associated with particular conventional discourse structures. Academic articles might have sections describing the Methodology or Results. Stories might follow conventional plotlines or motifs. Persuasive essays have a particular claim they are trying to argue for, and an essay might express this claim together with a structured set of premises that support the argument and demolish potential counterarguments. We`ll introduce versions of each of these kinds of global coherence. ", "Bloom_type": "remember", "question": "In academic writing, what is typically included at the end of a section?", "options": ["Global coherence", "Local coherence", "Sentence structure", "Paragraph flow"], "complexity": 0}, {"id": 2, "context": "In this section we introduce two kinds of such global discourse structure that have been widely studied computationally. The first is the structure of arguments: the way people attempt to convince each other in persuasive essays by offering claims and supporting premises. The second is somewhat related: the structure of scientific papers, and the way authors present their goals, results, and relationship to prior work in their papers. ", "Bloom_type": "remember", "question": "In what kind of documents are the structures discussed as part of computational studies introduced?", "options": ["Scientific papers", "Technical reports", "Legal agreements", "Persuasive essays"], "complexity": 0}, {"id": 3, "context": "The first type of global discourse structure is the structure of arguments. Analyzing people`s argumentation computationally is often called argumentation mining. ", "Bloom_type": "remember", "question": "What does the first type of global discourse structure refer to?", "options": ["The structure of arguments", "The structure of sentences", "The structure of paragraphs", "The structure of texts"], "complexity": 0}, {"id": 4, "context": "Another important line of research is studying how these argument structure (or other features) are associated with the success or persuasiveness of an argument (Habernal and Gurevych 2016, Tan et al. 2016, Hidey et al. 2017. Indeed, while it is Aristotle`s logos that is most related to discourse structure, Aristotle`s ethos and pathos techniques are particularly relevant in the detection of mechanisms of this sort of persuasion. For example scholars have investigated the linguistic realization of features studied by social scientists like reciprocity (people return favors), social proof (people follow others` choices), authority (people are influenced by those with power), and scarcity (people value things that are scarce), all of which can be brought up in a persuasive argument (Cialdini, 1984). Rosenthal and McKeown (2017) showed that these features could be combined with argumentation structure to predict who influences whom on social media, Althoff et al. (2014) found that linguistic models of reciprocity and authority predicted success in online requests, while the semisupervised model of Yang et al. (2019) detected mentions of scarcity, commitment, and social identity to predict the success of peer-to-peer lending platforms. ", "Bloom_type": "remember", "question": "Which feature of discourse structure is most closely linked to Aristotle's logos?", "options": ["Logos", "Ethos", "Pathos", "Kathexis"], "complexity": 0}, {"id": 5, "context": "Thus when an entity is first introduced into a discourse its mentions are likely to have full names, titles or roles, or appositive or restrictive relative clauses, as in the introduction of our protagonist in (23.1): Victoria Chen, CFO of Megabucks Banking. As an entity is discussed over a discourse, it becomes more salient to the hearer and its mentions on average typically becomes shorter and less informative, for example with a shortened name (for example Ms. Chen), a definite description (the 38-year-old), or a pronoun (she or her) (Hawkins 1978). However, this change in length is not monotonic, and is sensitive to discourse structure (Grosz 1977b, Reichman 1985, Fox 1993). ", "Bloom_type": "comprehension", "question": "How does the length of mentions of entities change throughout a discourse?", "options": ["The length changes but is not monotonic; it depends on the discourse structure.", "Length increases monotonically from short to long.", "Length decreases monotonically from long to short.", "Entities always remain at their initial lengths."], "complexity": 1}, {"id": 6, "context": "In addition to the local coherence between adjacent or nearby sentences, discourses also exhibit global coherence. Many genres of text are associated with particular conventional discourse structures. Academic articles might have sections describing the Methodology or Results. Stories might follow conventional plotlines or motifs. Persuasive essays have a particular claim they are trying to argue for, and an essay might express this claim together with a structured set of premises that support the argument and demolish potential counterarguments. We`ll introduce versions of each of these kinds of global coherence. ", "Bloom_type": "comprehension", "question": "What type of global coherence can be found in academic articles?", "options": ["Global coherence between paragraphs", "Local coherence within sentences", "Global coherence between chapters", "Global coherence between different genres"], "complexity": 1}, {"id": 7, "context": "In this section we introduce two kinds of such global discourse structure that have been widely studied computationally. The first is the structure of arguments: the way people attempt to convince each other in persuasive essays by offering claims and supporting premises. The second is somewhat related: the structure of scientific papers, and the way authors present their goals, results, and relationship to prior work in their papers. ", "Bloom_type": "comprehension", "question": "What are the two types of global discourse structures discussed in the context?", "options": ["Argumentative essay structure and scientific paper structure", "Persuasive argumentation and scientific research presentation", "Claim-based reasoning and evidence-based explanation", "Narrative storytelling and data visualization"], "complexity": 1}, {"id": 8, "context": "The first type of global discourse structure is the structure of arguments. Analyzing people`s argumentation computationally is often called argumentation mining. ", "Bloom_type": "comprehension", "question": "What does analyzing people\u2019s argumentation computationally refer to?", "options": ["The structure of arguments", "The sequence of events", "The flow of ideas", "The tone of communication"], "complexity": 1}, {"id": 9, "context": "Another important line of research is studying how these argument structure (or other features) are associated with the success or persuasiveness of an argument (Habernal and Gurevych 2016, Tan et al. 2016, Hidey et al. 2017. Indeed, while it is Aristotle`s logos that is most related to discourse structure, Aristotle`s ethos and pathos techniques are particularly relevant in the detection of mechanisms of this sort of persuasion. For example scholars have investigated the linguistic realization of features studied by social scientists like reciprocity (people return favors), social proof (people follow others` choices), authority (people are influenced by those with power), and scarcity (people value things that are scarce), all of which can be brought up in a persuasive argument (Cialdini, 1984). Rosenthal and McKeown (2017) showed that these features could be combined with argumentation structure to predict who influences whom on social media, Althoff et al. (2014) found that linguistic models of reciprocity and authority predicted success in online requests, while the semisupervised model of Yang et al. (2019) detected mentions of scarcity, commitment, and social identity to predict the success of peer-to-peer lending platforms. ", "Bloom_type": "comprehension", "question": "What aspect of discourse structure is most closely linked to Aristotle's ethos?", "options": ["Linguistic realization of features", "Argument structure", "Reciprocity", "Scarcity"], "complexity": 1}, {"id": 10, "context": "Thus when an entity is first introduced into a discourse its mentions are likely to have full names, titles or roles, or appositive or restrictive relative clauses, as in the introduction of our protagonist in (23.1): Victoria Chen, CFO of Megabucks Banking. As an entity is discussed over a discourse, it becomes more salient to the hearer and its mentions on average typically becomes shorter and less informative, for example with a shortened name (for example Ms. Chen), a definite description (the 38-year-old), or a pronoun (she or her) (Hawkins 1978). However, this change in length is not monotonic, and is sensitive to discourse structure (Grosz 1977b, Reichman 1985, Fox 1993). ", "Bloom_type": "application", "question": "In which type of discourse does the mention of an entity become longer and more detailed?", "options": ["When an entity is first introduced into a discourse", "As an entity is discussed over a discourse", "After the entity has been mentioned several times", "During the final summary of the discourse"], "complexity": 2}, {"id": 11, "context": "In addition to the local coherence between adjacent or nearby sentences, discourses also exhibit global coherence. Many genres of text are associated with particular conventional discourse structures. Academic articles might have sections describing the Methodology or Results. Stories might follow conventional plotlines or motifs. Persuasive essays have a particular claim they are trying to argue for, and an essay might express this claim together with a structured set of premises that support the argument and demolish potential counterarguments. We`ll introduce versions of each of these kinds of global coherence. ", "Bloom_type": "application", "question": "What is another way to describe the global coherence found in academic articles?", "options": ["Conventional discourse structures", "Local coherence within individual paragraphs", "Global coherence across different genres", "Structured sets of premises"], "complexity": 2}, {"id": 12, "context": "In this section we introduce two kinds of such global discourse structure that have been widely studied computationally. The first is the structure of arguments: the way people attempt to convince each other in persuasive essays by offering claims and supporting premises. The second is somewhat related: the structure of scientific papers, and the way authors present their goals, results, and relationship to prior work in their papers. ", "Bloom_type": "application", "question": "What is the main difference between the discourse structures discussed in the passage?", "options": ["The first kind focuses on argumentative techniques, while the second deals with the presentation of research findings.", "The first kind involves presenting evidence, while the second emphasizes the logical flow of ideas.", "The first kind is about persuasion, while the second is about summarizing previous works.", "The first kind is about authorship, while the second is about audience engagement."], "complexity": 2}, {"id": 13, "context": "The first type of global discourse structure is the structure of arguments. Analyzing people`s argumentation computationally is often called argumentation mining. ", "Bloom_type": "application", "question": "What is the next step after analyzing people's argumentation computationally?", "options": ["Organize data for further analysis", "Develop an algorithm for argumentation mining", "Build a model to classify arguments", "Plan the methodology for argument analysis"], "complexity": 2}, {"id": 14, "context": "Another important line of research is studying how these argument structure (or other features) are associated with the success or persuasiveness of an argument (Habernal and Gurevych 2016, Tan et al. 2016, Hidey et al. 2017. Indeed, while it is Aristotle`s logos that is most related to discourse structure, Aristotle`s ethos and pathos techniques are particularly relevant in the detection of mechanisms of this sort of persuasion. For example scholars have investigated the linguistic realization of features studied by social scientists like reciprocity (people return favors), social proof (people follow others` choices), authority (people are influenced by those with power), and scarcity (people value things that are scarce), all of which can be brought up in a persuasive argument (Cialdini, 1984). Rosenthal and McKeown (2017) showed that these features could be combined with argumentation structure to predict who influences whom on social media, Althoff et al. (2014) found that linguistic models of reciprocity and authority predicted success in online requests, while the semisupervised model of Yang et al. (2019) detected mentions of scarcity, commitment, and social identity to predict the success of peer-to-peer lending platforms. ", "Bloom_type": "application", "question": "Which method should be used first when analyzing the discourse structure for successful arguments?", "options": ["Identify the linguistic features mentioned by Habernal and Gurevych", "Analyze the argument structure as described by Tan et al.", "Combine linguistic features with argument structure as suggested by Rosenthal and McKeown", "Detect mentions of reciprocity, authority, and scarcity using a semisupervised model"], "complexity": 2}]}, "negative word": {"max_id": 2, "Questions": [{"id": 0, "context": "A second important addition commonly made when doing text classification for sentiment is to deal with negation. Consider the difference between I really like this movie (positive) and I didn`t like this movie (negative). The negation expressed by didn`t completely alters the inferences we draw from the predicate like. Similarly, negation can modify a negative word to produce a positive review (don`t dismiss this film, doesn`t let us get bored). ", "Bloom_type": "remember", "question": "In what way does negation affect the interpretation of a negative word?", "options": ["It changes the meaning of the negative word into something positive.", "It makes the negative word stronger.", "It has no effect on the interpretation of a negative word.", "It intensifies the negative emotion associated with the word."], "complexity": 0}, {"id": 1, "context": "A second important addition commonly made when doing text classification for sentiment is to deal with negation. Consider the difference between I really like this movie (positive) and I didn`t like this movie (negative). The negation expressed by didn`t completely alters the inferences we draw from the predicate like. Similarly, negation can modify a negative word to produce a positive review (don`t dismiss this film, doesn`t let us get bored). ", "Bloom_type": "comprehension", "question": "Explain how negation affects the interpretation of negative words in sentiment analysis?", "options": ["Negation turns negative words into positive ones.", "Negation changes the meaning of negative words into neutral ones.", "Negation has no effect on the interpretation of negative words.", "Negation only affects the interpretation of positive words."], "complexity": 1}, {"id": 2, "context": "A second important addition commonly made when doing text classification for sentiment is to deal with negation. Consider the difference between I really like this movie (positive) and I didn`t like this movie (negative). The negation expressed by didn`t completely alters the inferences we draw from the predicate like. Similarly, negation can modify a negative word to produce a positive review (don`t dismiss this film, doesn`t let us get bored). ", "Bloom_type": "application", "question": "What is an example of how negation can change the meaning of a negative word?", "options": ["Don't watch this movie.", "The movie was boring.", "I love this movie.", "This movie is not worth watching."], "complexity": 2}]}, "mention detection": {"max_id": 14, "Questions": [{"id": 0, "context": "Referentiality or anaphoricity detectors can be run as filters, in which only mentions that are classified as anaphoric or referential are passed on to the coreference system. The end result of such a filtering mention detection system on our example above might be the following filtered set of 9 potential mentions: ", "Bloom_type": "remember", "question": "In the mentioned context, what is the purpose of using anaphoric or referential classification for mentions?", "options": ["To increase computational efficiency by reducing unnecessary processing", "To enhance the accuracy of coreference resolution", "To improve the readability of the document", "To focus solely on semantic analysis"], "complexity": 0}, {"id": 1, "context": "The modern approach is instead to perform mention detection, anaphoricity, and coreference jointly in a single end-to-end model (Ng 2005b, Denis and Baldridge 2007, Rahman and Ng 2009). For example mention detection in the Lee et al. (2017b),2018 system is based on a single end-to-end neural network that computes a score for each mention being referential, a score for two mentions being coreference, and combines them to make a decision, training all these scores with a single end-to-end loss. We`ll describe this method in detail in Section 23.6. 7 ", "Bloom_type": "remember", "question": "In what way does the modern approach differ from previous methods?", "options": ["It separates mention detection, anaphoricity, and coreference into different models", "It focuses solely on sentence-level analysis", "It uses traditional rule-based systems", "It relies entirely on manual annotation"], "complexity": 0}, {"id": 2, "context": "Since the earliest systems (Mihalcea and Csomai 2007, Cucerzan 2007, Milne and Witten 2008), entity linking is done in (roughly) two stages: mention detection and mention disambiguation. We`ll give two algorithms, one simple classic baseline that uses anchor dictionaries and information from the Wikipedia graph structure (Ferragina and Scaiella, 2011) and one modern neural algorithm (Li et al., 2020). We`ll focus here mainly on the application of entity linking to questions, since a lot of the literature has been in that context. ", "Bloom_type": "remember", "question": "In what stage does entity linking typically begin?", "options": ["Mention detection", "Mention disambiguation", "Both mention detection and mention disambiguation", "None of the above"], "complexity": 0}, {"id": 3, "context": " Mention detection can start with all nouns and named entities and then use anaphoricity classifiers or referentiality classifiers to filter out non-mentions.  Three common architectures for coreference are mention-pair, mention-rank, and entity-based, each of which can make use of feature-based or neural classifiers. ", "Bloom_type": "remember", "question": "In the process of mentioning detection, what is the first step?", "options": ["Identifying all nouns", "Using anaphoricity classifiers", "Filtering out non-mentions", "Applying neural classifiers"], "complexity": 0}, {"id": 4, "context": "The move from mention-pair to mention-ranking approaches was pioneered by Yang et al. (2003) and Iida et al. (2003) who proposed pairwise ranking methods, then extended by Denis and Baldridge (2008) who proposed to do ranking via a softmax over all prior mentions. The idea of doing mention detection, anaphoricity, and coreference jointly in a single end-to-end model grew out of the early proposal of Ng (2005b) to use a dummy antecedent for mention-ranking, allowing non-referential` to be a choice for coreference classifiers, Denis and Baldridge`s 2007 joint system combining anaphoricity classifier probabilities with coreference probabilities, the Denis and Baldridge (2008) ranking model, and the Rahman and Ng (2009) proposal to train the two models jointly with a single objective. ", "Bloom_type": "remember", "question": "In what year did the concept of mentioning detection emerge as part of the development of mention-ranking approaches?", "options": ["2005", "1995", "2000", "2010"], "complexity": 0}, {"id": 5, "context": "Referentiality or anaphoricity detectors can be run as filters, in which only mentions that are classified as anaphoric or referential are passed on to the coreference system. The end result of such a filtering mention detection system on our example above might be the following filtered set of 9 potential mentions: ", "Bloom_type": "comprehension", "question": "What does the response indicate about the filtering process for mention detection?", "options": ["The system removes all non-anaphoric and non-referential mentions.", "Only mentions that are explicitly stated are kept.", "Mentions are filtered based on their syntactic structure.", "The system focuses solely on semantic analysis."], "complexity": 1}, {"id": 6, "context": "The modern approach is instead to perform mention detection, anaphoricity, and coreference jointly in a single end-to-end model (Ng 2005b, Denis and Baldridge 2007, Rahman and Ng 2009). For example mention detection in the Lee et al. (2017b),2018 system is based on a single end-to-end neural network that computes a score for each mention being referential, a score for two mentions being coreference, and combines them to make a decision, training all these scores with a single end-to-end loss. We`ll describe this method in detail in Section 23.6. 7 ", "Bloom_type": "comprehension", "question": "What does the modern approach involve when it comes to detecting mentions?", "options": ["Mention detection, anaphoricity, and coreference together", "Performing mention detection alone", "Anaphoricity and coreference separately", "Not mentioned in the context"], "complexity": 1}, {"id": 7, "context": "Since the earliest systems (Mihalcea and Csomai 2007, Cucerzan 2007, Milne and Witten 2008), entity linking is done in (roughly) two stages: mention detection and mention disambiguation. We`ll give two algorithms, one simple classic baseline that uses anchor dictionaries and information from the Wikipedia graph structure (Ferragina and Scaiella, 2011) and one modern neural algorithm (Li et al., 2020). We`ll focus here mainly on the application of entity linking to questions, since a lot of the literature has been in that context. ", "Bloom_type": "comprehension", "question": "What are the primary applications of entity linking mentioned in the context?", "options": ["Entity linking is focused on improving question answering systems.", "Entity linking is applied to improve search engine results.", "Entity linking is used for natural language processing tasks.", "Entity linking is utilized in medical research."], "complexity": 1}, {"id": 8, "context": " Mention detection can start with all nouns and named entities and then use anaphoricity classifiers or referentiality classifiers to filter out non-mentions.  Three common architectures for coreference are mention-pair, mention-rank, and entity-based, each of which can make use of feature-based or neural classifiers. ", "Bloom_type": "comprehension", "question": "What are some common architectures used for mention detection?", "options": ["Mention-pair, mention-rank, and entity-based", "Coreference resolution and dependency parsing", "Dependency parsing and syntactic analysis", "Syntactic analysis and semantic role labeling"], "complexity": 1}, {"id": 9, "context": "The move from mention-pair to mention-ranking approaches was pioneered by Yang et al. (2003) and Iida et al. (2003) who proposed pairwise ranking methods, then extended by Denis and Baldridge (2008) who proposed to do ranking via a softmax over all prior mentions. The idea of doing mention detection, anaphoricity, and coreference jointly in a single end-to-end model grew out of the early proposal of Ng (2005b) to use a dummy antecedent for mention-ranking, allowing non-referential` to be a choice for coreference classifiers, Denis and Baldridge`s 2007 joint system combining anaphoricity classifier probabilities with coreference probabilities, the Denis and Baldridge (2008) ranking model, and the Rahman and Ng (2009) proposal to train the two models jointly with a single objective. ", "Bloom_type": "comprehension", "question": "What concept did Ng propose to use a dummy antecedent for mention-ranking to allow non-referential mentions to be chosen for coreference classifiers?", "options": ["Anaphoricity", "Coreference", "Mention-detection", "Antecedent selection"], "complexity": 1}, {"id": 10, "context": "Referentiality or anaphoricity detectors can be run as filters, in which only mentions that are classified as anaphoric or referential are passed on to the coreference system. The end result of such a filtering mention detection system on our example above might be the following filtered set of 9 potential mentions: ", "Bloom_type": "application", "question": "What is the final output after applying the filter mentioned in the context?", "options": ["The filtered set of 9 mentions undergoes further processing before being sent to the coreference system.", "All 9 potential mentions remain unchanged.", "Only the non-anaphoric and non-referential mentions are kept.", "None of the options are accurate."], "complexity": 2}, {"id": 11, "context": "The modern approach is instead to perform mention detection, anaphoricity, and coreference jointly in a single end-to-end model (Ng 2005b, Denis and Baldridge 2007, Rahman and Ng 2009). For example mention detection in the Lee et al. (2017b),2018 system is based on a single end-to-end neural network that computes a score for each mention being referential, a score for two mentions being coreference, and combines them to make a decision, training all these scores with a single end-to-end loss. We`ll describe this method in detail in Section 23.6. 7 ", "Bloom_type": "application", "question": "What is the first step in performing mention detection?", "options": ["Compute a score for each mention being referential", "Analyze the sentence structure", "Identify named entities", "Combine scores of referential and coreference"], "complexity": 2}, {"id": 12, "context": "Since the earliest systems (Mihalcea and Csomai 2007, Cucerzan 2007, Milne and Witten 2008), entity linking is done in (roughly) two stages: mention detection and mention disambiguation. We`ll give two algorithms, one simple classic baseline that uses anchor dictionaries and information from the Wikipedia graph structure (Ferragina and Scaiella, 2011) and one modern neural algorithm (Li et al., 2020). We`ll focus here mainly on the application of entity linking to questions, since a lot of the literature has been in that context. ", "Bloom_type": "application", "question": "Which stage does not involve any external knowledge sources?", "options": ["Mention Detection", "Mention Disambiguation", "Anchor Dictionary Use", "Wikipedia Graph Structure"], "complexity": 2}, {"id": 13, "context": " Mention detection can start with all nouns and named entities and then use anaphoricity classifiers or referentiality classifiers to filter out non-mentions.  Three common architectures for coreference are mention-pair, mention-rank, and entity-based, each of which can make use of feature-based or neural classifiers. ", "Bloom_type": "application", "question": "Which architecture is not commonly used for mention detection?", "options": ["entity-based", "mention-pair", "mention-rank", "neural"], "complexity": 2}, {"id": 14, "context": "The move from mention-pair to mention-ranking approaches was pioneered by Yang et al. (2003) and Iida et al. (2003) who proposed pairwise ranking methods, then extended by Denis and Baldridge (2008) who proposed to do ranking via a softmax over all prior mentions. The idea of doing mention detection, anaphoricity, and coreference jointly in a single end-to-end model grew out of the early proposal of Ng (2005b) to use a dummy antecedent for mention-ranking, allowing non-referential` to be a choice for coreference classifiers, Denis and Baldridge`s 2007 joint system combining anaphoricity classifier probabilities with coreference probabilities, the Denis and Baldridge (2008) ranking model, and the Rahman and Ng (2009) proposal to train the two models jointly with a single objective. ", "Bloom_type": "application", "question": "What is the initial step before developing a method for mention detection?", "options": ["Identify potential antecedents", "Collect annotated data", "Design the architecture", "Train the neural network"], "complexity": 2}]}, "dependency parsing": {"max_id": 14, "Questions": [{"id": 0, "context": "The earliest disambiguation algorithms for parsing were based on probabilistic context-free grammars, first worked out by Booth (1969) and Salomaa (1969); see Appendix C for more history. Neural methods were first applied to parsing at around the same time as statistical parsing methods were developed (Henderson, 1994). In the earliest work neural networks were used to estimate some of the probabilities for statistical constituency parsers (Henderson, 2003, 2004; Emami and Jelinek, 2005) . The next decades saw a wide variety of neural parsing algorithms, including recursive neural architectures (Socher et al., 2011, 2013), encoder-decoder models (Vinyals et al., 2015; Choe and Charniak, 2016), and the idea of focusing on spans (Cross and Huang, 2016). For more on the span-based self-attention approach we describe in this chapter see Stern et al. (2017), Gaddy et al. (2018), Kitaev and Klein (2018), and Kitaev et al. (2019). See Chapter 20 for the parallel history of neural dependency parsing. ", "Bloom_type": "remember", "question": "In which decade did the development of neural parsing algorithms begin?", "options": ["The early 1990s", "The early 1970s", "The late 1960s", "The mid-1980s"], "complexity": 0}, {"id": 1, "context": "Automatic parsing using dependency grammars was first introduced into computational linguistics by early work on machine translation at the RAND Corporation led by David Hays. This work on dependency parsing closely paralleled work on constituent parsing and made explicit use of grammars to guide the parsing process. After this early period, computational work on dependency parsing remained intermittent over the following decades. Notable implementations of dependency parsers for English during this period include Link Grammar (Sleator and Temperley, 1993), Constraint Grammar (Karlsson et al., 1995), and MINIPAR (Lin, 2003). ", "Bloom_type": "remember", "question": "In what year did the RAND Corporation lead work on machine translation introduce automatic parsing using dependency grammars?", "options": ["1970", "1980", "1960", "1990"], "complexity": 0}, {"id": 2, "context": "Dependency parsing saw a major resurgence in the late 1990`s with the appearance of large dependency-based treebanks and the associated advent of data driven approaches described in this chapter. Eisner (1996) developed an efficient dynamic programming approach to dependency parsing based on bilexical grammars derived from the Penn Treebank. Covington (2001) introduced the deterministic word by word approach underlying current transition-based approaches. Yamada and Matsumoto (2003) and Kudo and Matsumoto (2002) introduced both the shift-reduce paradigm and the use of supervised machine learning in the form of support vector machines to dependency parsing. ", "Bloom_type": "remember", "question": "In which year did Eisner develop an efficient dynamic programming approach to dependency parsing?", "options": ["1996", "1985", "1990", "2000"], "complexity": 0}, {"id": 3, "context": "Transition-based parsing is based on the shift-reduce parsing algorithm originally developed for analyzing programming languages (Aho and Ullman, 1972). Shift-reduce parsing also makes use of a context-free grammar. Input tokens are successively shifted onto the stack and the top two elements of the stack are matched against the right-hand side of the rules in the grammar; when a match is found the matched elements are replaced on the stack (reduced) by the non-terminal from the left-hand side of the rule being matched. In transition-based dependency parsing we skip the grammar, and alter the reduce operation to add a dependency relation between a word and its head. ", "Bloom_type": "remember", "question": "In transition-based dependency parsing, what is altered about the reduce operation compared to traditional shift-reduce parsing?", "options": ["The reduction process is skipped entirely.", "The dependency relations are removed before reducing.", "The input tokens are processed differently.", "The grammar rules are modified."], "complexity": 0}, {"id": 4, "context": "The Conference on Natural Language Learning (CoNLL) has conducted an influential series of shared tasks related to dependency parsing over the years (Buchholz and Marsi 2006, Nivre et al. 2007a, Surdeanu et al. 2008, Hajic et al. 2009). More recent evaluations have focused on parser robustness with respect to morphologically rich languages (Seddah et al., 2013), and non-canonical language forms such as social media, texts, and spoken language (Petrov and McDonald, 2012). Choi et al. (2015) presents a performance analysis of 10 dependency parsers across a range of metrics, as well as DEPENDABLE, a robust parser evaluation tool. ", "Bloom_type": "remember", "question": "In which conference did CoNLL conduct its influential series of shared tasks related to dependency parsing?", "options": ["Conference on Empirical Methods in Natural Language Processing", "International Conference on Machine Learning", "Conference on Artificial Intelligence", "Conference on Neural Information Processing Systems"], "complexity": 0}, {"id": 5, "context": "The earliest disambiguation algorithms for parsing were based on probabilistic context-free grammars, first worked out by Booth (1969) and Salomaa (1969); see Appendix C for more history. Neural methods were first applied to parsing at around the same time as statistical parsing methods were developed (Henderson, 1994). In the earliest work neural networks were used to estimate some of the probabilities for statistical constituency parsers (Henderson, 2003, 2004; Emami and Jelinek, 2005) . The next decades saw a wide variety of neural parsing algorithms, including recursive neural architectures (Socher et al., 2011, 2013), encoder-decoder models (Vinyals et al., 2015; Choe and Charniak, 2016), and the idea of focusing on spans (Cross and Huang, 2016). For more on the span-based self-attention approach we describe in this chapter see Stern et al. (2017), Gaddy et al. (2018), Kitaev and Klein (2018), and Kitaev et al. (2019). See Chapter 20 for the parallel history of neural dependency parsing. ", "Bloom_type": "comprehension", "question": "What was one of the early applications of neural methods in dependency parsing?", "options": ["Estimating probabilities for constituency parsers", "Parsing sentences using recursive neural architectures", "Using neural networks to focus on spans", "Applying neural methods to all types of parsing tasks"], "complexity": 1}, {"id": 6, "context": "Automatic parsing using dependency grammars was first introduced into computational linguistics by early work on machine translation at the RAND Corporation led by David Hays. This work on dependency parsing closely paralleled work on constituent parsing and made explicit use of grammars to guide the parsing process. After this early period, computational work on dependency parsing remained intermittent over the following decades. Notable implementations of dependency parsers for English during this period include Link Grammar (Sleator and Temperley, 1993), Constraint Grammar (Karlsson et al., 1995), and MINIPAR (Lin, 2003). ", "Bloom_type": "comprehension", "question": "What aspect of dependency parsing research did NOT see significant development after the initial work mentioned in the context?", "options": ["Dependency grammar theory", "Implementation techniques", "Machine learning algorithms", "Theoretical foundations"], "complexity": 1}, {"id": 7, "context": "Dependency parsing saw a major resurgence in the late 1990`s with the appearance of large dependency-based treebanks and the associated advent of data driven approaches described in this chapter. Eisner (1996) developed an efficient dynamic programming approach to dependency parsing based on bilexical grammars derived from the Penn Treebank. Covington (2001) introduced the deterministic word by word approach underlying current transition-based approaches. Yamada and Matsumoto (2003) and Kudo and Matsumoto (2002) introduced both the shift-reduce paradigm and the use of supervised machine learning in the form of support vector machines to dependency parsing. ", "Bloom_type": "comprehension", "question": "What are some key methods for dependency parsing mentioned in the context?", "options": ["Dynamic programming, bilexical grammars, and support vector machines", "Efficient algorithms, deterministic approaches, and supervised learning", "Penn Treebank, shift-reduce paradigms, and SVMs", "Dependency trees, lexical analysis, and rule-based systems"], "complexity": 1}, {"id": 8, "context": "Transition-based parsing is based on the shift-reduce parsing algorithm originally developed for analyzing programming languages (Aho and Ullman, 1972). Shift-reduce parsing also makes use of a context-free grammar. Input tokens are successively shifted onto the stack and the top two elements of the stack are matched against the right-hand side of the rules in the grammar; when a match is found the matched elements are replaced on the stack (reduced) by the non-terminal from the left-hand side of the rule being matched. In transition-based dependency parsing we skip the grammar, and alter the reduce operation to add a dependency relation between a word and its head. ", "Bloom_type": "comprehension", "question": "What is the primary difference between traditional shift-reduce parsing algorithms and transition-based dependency parsing?", "options": ["Shift-reduce parsing requires a context-free grammar, but transition-based parsing can operate without one.", "Traditional parsing uses context-free grammars, while transition-based parsing does not.", "Transition-based parsing relies solely on input tokens, whereas traditional parsing considers the entire grammar.", "Both methods use the same reduction strategy."], "complexity": 1}, {"id": 9, "context": "The Conference on Natural Language Learning (CoNLL) has conducted an influential series of shared tasks related to dependency parsing over the years (Buchholz and Marsi 2006, Nivre et al. 2007a, Surdeanu et al. 2008, Hajic et al. 2009). More recent evaluations have focused on parser robustness with respect to morphologically rich languages (Seddah et al., 2013), and non-canonical language forms such as social media, texts, and spoken language (Petrov and McDonald, 2012). Choi et al. (2015) presents a performance analysis of 10 dependency parsers across a range of metrics, as well as DEPENDABLE, a robust parser evaluation tool. ", "Bloom_type": "comprehension", "question": "What aspect of dependency parsing has been emphasized more recently by CoNLL?", "options": ["all of the above", "parser robustness with respect to morphologically rich languages", "non-canonical language forms such as social media, texts, and spoken language", "performance analysis of dependency parsers"], "complexity": 1}, {"id": 10, "context": "The earliest disambiguation algorithms for parsing were based on probabilistic context-free grammars, first worked out by Booth (1969) and Salomaa (1969); see Appendix C for more history. Neural methods were first applied to parsing at around the same time as statistical parsing methods were developed (Henderson, 1994). In the earliest work neural networks were used to estimate some of the probabilities for statistical constituency parsers (Henderson, 2003, 2004; Emami and Jelinek, 2005) . The next decades saw a wide variety of neural parsing algorithms, including recursive neural architectures (Socher et al., 2011, 2013), encoder-decoder models (Vinyals et al., 2015; Choe and Charniak, 2016), and the idea of focusing on spans (Cross and Huang, 2016). For more on the span-based self-attention approach we describe in this chapter see Stern et al. (2017), Gaddy et al. (2018), Kitaev and Klein (2018), and Kitaev et al. (2019). See Chapter 20 for the parallel history of neural dependency parsing. ", "Bloom_type": "application", "question": "Which method was primarily used for dependency parsing before the development of neural methods?", "options": ["Statistical parsing methods", "Recursive neural architectures", "Encoder-decoder models", "Span-based self-attention approach"], "complexity": 2}, {"id": 11, "context": "Automatic parsing using dependency grammars was first introduced into computational linguistics by early work on machine translation at the RAND Corporation led by David Hays. This work on dependency parsing closely paralleled work on constituent parsing and made explicit use of grammars to guide the parsing process. After this early period, computational work on dependency parsing remained intermittent over the following decades. Notable implementations of dependency parsers for English during this period include Link Grammar (Sleator and Temperley, 1993), Constraint Grammar (Karlsson et al., 1995), and MINIPAR (Lin, 2003). ", "Bloom_type": "application", "question": "What is the next step after applying dependency parsing?", "options": ["Combine dependency parsing with other linguistic techniques", "Use dependency trees directly as output", "Generate constituency parses instead", "Refine the grammar used for dependency parsing"], "complexity": 2}, {"id": 12, "context": "Dependency parsing saw a major resurgence in the late 1990`s with the appearance of large dependency-based treebanks and the associated advent of data driven approaches described in this chapter. Eisner (1996) developed an efficient dynamic programming approach to dependency parsing based on bilexical grammars derived from the Penn Treebank. Covington (2001) introduced the deterministic word by word approach underlying current transition-based approaches. Yamada and Matsumoto (2003) and Kudo and Matsumoto (2002) introduced both the shift-reduce paradigm and the use of supervised machine learning in the form of support vector machines to dependency parsing. ", "Bloom_type": "application", "question": "Which method did Yamada and Matsumoto introduce for dependency parsing?", "options": ["Supervised Machine Learning using Support Vector Machines", "Dynamic Programming Approach", "Deterministic Word-by-Word Approach", "Shift-Reduce Paradigm"], "complexity": 2}, {"id": 13, "context": "Transition-based parsing is based on the shift-reduce parsing algorithm originally developed for analyzing programming languages (Aho and Ullman, 1972). Shift-reduce parsing also makes use of a context-free grammar. Input tokens are successively shifted onto the stack and the top two elements of the stack are matched against the right-hand side of the rules in the grammar; when a match is found the matched elements are replaced on the stack (reduced) by the non-terminal from the left-hand side of the rule being matched. In transition-based dependency parsing we skip the grammar, and alter the reduce operation to add a dependency relation between a word and its head. ", "Bloom_type": "application", "question": "What is a key difference between traditional shift-reduce parsing and transition-based dependency parsing?", "options": ["In traditional parsing, dependencies are determined by matching the top two elements of the stack with the right-hand side of the grammar, while in transition-based parsing, dependencies are added directly.", "Traditional parsing uses context-free grammars, while transition-based parsing does not.", "Transition-based parsing relies solely on the input tokens, whereas traditional parsing considers the entire sentence structure.", "Shift-reduce parsing requires a context-free grammar, but transition-based parsing can operate without it."], "complexity": 2}, {"id": 14, "context": "The Conference on Natural Language Learning (CoNLL) has conducted an influential series of shared tasks related to dependency parsing over the years (Buchholz and Marsi 2006, Nivre et al. 2007a, Surdeanu et al. 2008, Hajic et al. 2009). More recent evaluations have focused on parser robustness with respect to morphologically rich languages (Seddah et al., 2013), and non-canonical language forms such as social media, texts, and spoken language (Petrov and McDonald, 2012). Choi et al. (2015) presents a performance analysis of 10 dependency parsers across a range of metrics, as well as DEPENDABLE, a robust parser evaluation tool. ", "Bloom_type": "application", "question": "What is the main focus of the CoNLL conference regarding dependency parsing?", "options": ["To evaluate the robustness of dependency parsers", "To develop new algorithms for dependency parsing", "To analyze the performance of various dependency parsers", "To study the impact of morphological richness on dependency parsing"], "complexity": 2}]}, "relation extraction": {"max_id": 41, "Questions": [{"id": 0, "context": "We begin with the task of relation extraction: finding and classifying semantic relations among entities mentioned in a text, like child-of (X is the child-of Y), or part-whole or geospatial relations. Relation extraction has close links to populating a relational database, and knowledge graphs, datasets of structured relational knowledge, are a useful way for search engines to present information to users. ", "Bloom_type": "remember", "question": "In what type of databases does relation extraction primarily occur?", "options": ["Relational Databases", "NoSQL Databases", "Graph Databases", "Key-Value Stores"], "complexity": 0}, {"id": 1, "context": "The text tells us, for example, that Tim Wagner is a spokesman for American Airlines, that United is a unit of UAL Corp., and that American is a unit of AMR. These binary relations are instances of more generic relations such as part-of or employs that are fairly frequent in news-style texts. Figure 20.1 lists the 17 relations used in the ACE relation extraction evaluations and Fig. 20.2 shows some sample relations. We might also extract more domain-specific relations such as the notion of an airline route. For example from this text we can conclude that United has routes to Chicago, Dallas, Denver, and San Francisco. ", "Bloom_type": "remember", "question": "What type of text does the passage describe?", "options": ["News article", "Science fiction", "Historical document", "Technical report"], "complexity": 0}, {"id": 2, "context": "There are five main classes of algorithms for relation extraction: handwritten patterns, supervised machine learning, semi-supervised (via bootstrapping or distant supervision), and unsupervised. We`ll introduce each of these in the next sections. ", "Bloom_type": "remember", "question": "Which type of algorithm is not mentioned as a method for relation extraction?", "options": ["handwritten patterns", "supervised machine learning", "unsupervised", "semi-supervised"], "complexity": 0}, {"id": 3, "context": "Supervised machine learning approaches to relation extraction follow a scheme that should be familiar by now. A fixed set of relations and entities is chosen, a training corpus is hand-annotated with the relations and entities, and the annotated texts are then used to train classifiers to annotate an unseen test set. ", "Bloom_type": "remember", "question": "In supervised machine learning for relation extraction, what does the process involve?", "options": ["Hand-annotating texts with predefined relations", "Annotating unseen texts with predefined relations", "Training classifiers on randomly selected data", "Choosing random relations from a database"], "complexity": 0}, {"id": 4, "context": "Neural supervised relation classifiers Neural models for relation extraction similarly treat the task as supervised classification. Let`s consider a typical system applied to the TACRED relation extraction dataset and task (Zhang et al., 2017). In TACRED we are given a sentence and two spans within it: a subject, which is a person or organization, and an object, which is any other entity. The task is to assign a relation from the 42 TAC relations, or no relation. ", "Bloom_type": "remember", "question": "In the context of relation extraction using neural models, what does the task involve?", "options": ["Assigning relations between subjects and objects", "Identifying entities within sentences", "Classifying objects based on their attributes", "Determining the sentiment of sentences"], "complexity": 0}, {"id": 5, "context": "A typical Transformer-encoder algorithm, shown in Fig. 20.6, simply takes a pretrained encoder like BERT and adds a linear layer on top of the sentence representation (for example the BERT [CLS] token), a linear layer that is finetuned as a 1-of-N classifier to assign one of the 43 labels. The input to the BERT encoder is partially de-lexified; the subject and object entities are replaced in the input by their NER tags. This helps keep the system from overfitting to the individual lexical items (Zhang et al., 2017). When using BERT-type Transformers for relation extraction, it helps to use versions of BERT like RoBERTa (Liu et al., 2019) or spanBERT (Joshi et al., 2020) that don`t have two sequences separated by a [SEP] token, but instead form the input from a single long sequence of sentences. ", "Bloom_type": "remember", "question": "In relation extraction, what technique is used to replace subject and object entities with their NER tags?", "options": ["Sentence splitting", "Tokenization", "Dependency parsing", "Part-of-speech tagging"], "complexity": 0}, {"id": 6, "context": "In general, if the test set is similar enough to the training set, and if there is enough hand-labeled data, supervised relation extraction systems can get high ac", "Bloom_type": "remember", "question": "In relation extraction, what aspect of the system determines its performance?", "options": ["Both A) and B)", "The similarity between the test and training sets", "The amount of labeled data available", "Neither A) nor B)"], "complexity": 0}, {"id": 7, "context": "curacies. But labeling a large training set is extremely expensive and supervised models are brittle: they don`t generalize well to different text genres. For this reason, much research in relation extraction has focused on the semi-supervised and unsupervised approaches we turn to next. ", "Bloom_type": "remember", "question": "Which approach do researchers primarily focus on for improving relation extraction?", "options": ["Unsupervised learning", "Supervised learning", "Rule-based methods", "Hybrid approaches"], "complexity": 0}, {"id": 8, "context": "The goal of unsupervised relation extraction is to extract relations from the web when we have no labeled training data, and not even any list of relations. This task is often called open information extraction or Open IE. In Open IE, the relations ", "Bloom_type": "remember", "question": "In the context of unsupervised relation extraction, what does the task involve?", "options": ["Finding new connections between entities on the web", "Extracting known relationships", "Identifying patterns in existing datasets", "Creating a list of all possible relations"], "complexity": 0}, {"id": 9, "context": "The lexical constraints are based on a dictionary D that is used to prune very rare, long relation strings. The intuition is to eliminate candidate relations that don`t occur with sufficient number of distinct argument types and so are likely to be bad examples. The system first runs the above relation extraction algorithm offline on 500 million web sentences and extracts a list of all the relations that occur after normalizing them (removing inflection, auxiliary verbs, adjectives, and adverbs). Each relation r is added to the dictionary if it occurs with at least 20 different arguments. Fader et al. (2011) used a dictionary of 1.7 million normalized relations. ", "Bloom_type": "remember", "question": "What does the system do before running the relation extraction algorithm?", "options": ["It eliminates candidate relations that occur with insufficient number of distinct argument types.", "It prunes very rare, long relation strings.", "It adds each relation to the dictionary only if it occurs with more than 20 different arguments.", "None of the above."], "complexity": 0}, {"id": 10, "context": "The great advantage of unsupervised relation extraction is its ability to handle a huge number of relations without having to specify them in advance. The disadvantage is the need to map all the strings into some canonical form for adding to databases or knowledge graphs. Current methods focus heavily on relations expressed with verbs, and so will miss many relations that are expressed nominally. ", "Bloom_type": "remember", "question": "What is one significant benefit of using unsupervised relation extraction?", "options": ["It handles an unlimited number of possible relations automatically.", "It can easily identify complex relationships.", "It requires less computational resources than supervised methods.", "It does not require any mapping of string data."], "complexity": 0}, {"id": 11, "context": "Supervised relation extraction systems are evaluated by using test sets with humanannotated, gold-standard relations and computing precision, recall, and F-measure. Labeled precision and recall require the system to classify the relation correctly, whereas unlabeled methods simply measure a system`s ability to detect entities that are related. ", "Bloom_type": "remember", "question": "In supervised relation extraction, what is required for evaluating the performance of a system?", "options": ["The system must classify all relations correctly.", "The system must only identify related entities.", "The system must use labeled data exclusively.", "The system must perform both classification and detection equally."], "complexity": 0}, {"id": 12, "context": "Progress in this area continues to be stimulated by formal evaluations with shared benchmark datasets, including the Automatic Content Extraction (ACE) evaluations of 2000-2007 on named entity recognition, relation extraction, and temporal expressions1, the KBP (Knowledge Base Population) evaluations (Ji et al. 2010, Surdeanu 2013) of relation extraction tasks like slot filling (extracting attributes (slots`) like age, birthplace, and spouse for a given entity) and a series of SemEval workshops (Hendrickx et al., 2009). ", "Bloom_type": "remember", "question": "In what way has progress in relation extraction been stimulated?", "options": ["By sharing results from previous studies", "By informal evaluations", "Through collaboration between researchers", "With no external stimuli"], "complexity": 0}, {"id": 13, "context": "Semisupervised relation extraction was first proposed by Hearst (1992b), and extended by systems like AutoSlog-TS (Riloff, 1996), DIPRE (Brin, 1998), SNOWBALL (Agichtein and Gravano, 2000), and Jones et al. (1999). The distant supervision algorithm we describe was drawn from Mintz et al. (2009), who first used the term distant supervision` (which was suggested to them by Chris Manning) but similar ideas had occurred in earlier systems like Craven and Kumlien (1999) and Morgan et al. (2004) under the name weakly labeled data, as well as in Snow et al. (2005) and Wu and Weld (2007). Among the many extensions are Wu and Weld (2010), Riedel et al. (2010), and Ritter et al. (2013). Open IE systems include KNOWITALL Etzioni et al. (2005), TextRunner (Banko et al., 2007), and REVERB (Fader et al., 2011). See Riedel et al. (2013) for a universal schema that combines the advantages of distant supervision and Open IE. ", "Bloom_type": "remember", "question": "In what year did the concept of distant supervision originate?", "options": ["1999", "1992", "2005", "2010"], "complexity": 0}, {"id": 14, "context": "We begin with the task of relation extraction: finding and classifying semantic relations among entities mentioned in a text, like child-of (X is the child-of Y), or part-whole or geospatial relations. Relation extraction has close links to populating a relational database, and knowledge graphs, datasets of structured relational knowledge, are a useful way for search engines to present information to users. ", "Bloom_type": "comprehension", "question": "What does relation extraction involve?", "options": ["Identifying relationships within a single sentence", "Finding and classifying semantic relations between sentences", "Extracting numerical data from texts", "Generating new paragraphs based on existing ones"], "complexity": 1}, {"id": 15, "context": "The text tells us, for example, that Tim Wagner is a spokesman for American Airlines, that United is a unit of UAL Corp., and that American is a unit of AMR. These binary relations are instances of more generic relations such as part-of or employs that are fairly frequent in news-style texts. Figure 20.1 lists the 17 relations used in the ACE relation extraction evaluations and Fig. 20.2 shows some sample relations. We might also extract more domain-specific relations such as the notion of an airline route. For example from this text we can conclude that United has routes to Chicago, Dallas, Denver, and San Francisco. ", "Bloom_type": "comprehension", "question": "What type of textual analysis does the passage describe?", "options": ["Relation Extraction", "Sentiment Analysis", "Entity Recognition", "Topic Classification"], "complexity": 1}, {"id": 16, "context": "There are five main classes of algorithms for relation extraction: handwritten patterns, supervised machine learning, semi-supervised (via bootstrapping or distant supervision), and unsupervised. We`ll introduce each of these in the next sections. ", "Bloom_type": "comprehension", "question": "Which type of algorithm is NOT typically considered when extracting relations from text data?", "options": ["Handwritten patterns", "Supervised machine learning", "Semi-supervised (via bootstrapping or distant supervision)", "Unsupervised"], "complexity": 1}, {"id": 17, "context": "Supervised machine learning approaches to relation extraction follow a scheme that should be familiar by now. A fixed set of relations and entities is chosen, a training corpus is hand-annotated with the relations and entities, and the annotated texts are then used to train classifiers to annotate an unseen test set. ", "Bloom_type": "comprehension", "question": "What is the typical approach for supervised machine learning in relation extraction?", "options": ["Training models based on labeled examples from a specific dataset", "Using unsupervised methods to identify patterns in data", "Analyzing natural language using neural networks alone", "Combining all three methods equally"], "complexity": 1}, {"id": 18, "context": "Neural supervised relation classifiers Neural models for relation extraction similarly treat the task as supervised classification. Let`s consider a typical system applied to the TACRED relation extraction dataset and task (Zhang et al., 2017). In TACRED we are given a sentence and two spans within it: a subject, which is a person or organization, and an object, which is any other entity. The task is to assign a relation from the 42 TAC relations, or no relation. ", "Bloom_type": "comprehension", "question": "What does the neural supervised relation classifier typically do when processing the TACRED dataset?", "options": ["Identify the type of relation between two entities", "Classify sentences into positive and negative examples", "Extract entities from sentences", "Generate new sentences based on existing ones"], "complexity": 1}, {"id": 19, "context": "A typical Transformer-encoder algorithm, shown in Fig. 20.6, simply takes a pretrained encoder like BERT and adds a linear layer on top of the sentence representation (for example the BERT [CLS] token), a linear layer that is finetuned as a 1-of-N classifier to assign one of the 43 labels. The input to the BERT encoder is partially de-lexified; the subject and object entities are replaced in the input by their NER tags. This helps keep the system from overfitting to the individual lexical items (Zhang et al., 2017). When using BERT-type Transformers for relation extraction, it helps to use versions of BERT like RoBERTa (Liu et al., 2019) or spanBERT (Joshi et al., 2020) that don`t have two sequences separated by a [SEP] token, but instead form the input from a single long sequence of sentences. ", "Bloom_type": "comprehension", "question": "Explain how the Transformer-encoder algorithm incorporates relation extraction?", "options": ["It replaces subject and object entities with NER tags before feeding into the model.", "The algorithm uses pre-trained models like BERT and fine-tunes them.", "Relation extraction is done after the model has been trained.", "The algorithm only extracts relations within a single sentence."], "complexity": 1}, {"id": 20, "context": "In general, if the test set is similar enough to the training set, and if there is enough hand-labeled data, supervised relation extraction systems can get high ac", "Bloom_type": "comprehension", "question": "What does the response indicate about the quality of the test set compared to the training set?", "options": ["The test set is less diverse than the training set.", "The test set is more diverse than the training set.", "The test set is identical to the training set.", "The relationship between the test set and the training set cannot be determined."], "complexity": 1}, {"id": 21, "context": "curacies. But labeling a large training set is extremely expensive and supervised models are brittle: they don`t generalize well to different text genres. For this reason, much research in relation extraction has focused on the semi-supervised and unsupervised approaches we turn to next. ", "Bloom_type": "comprehension", "question": "Explain how relation extraction differs between supervised and unsupervised methods?", "options": ["Supervised methods require more labeled data than unsupervised methods.", "Unsupervised methods use unlabeled data for better generalization.", "Supervised methods can handle any genre of text naturally.", "Unsupervised methods do not need labeled data at all."], "complexity": 1}, {"id": 22, "context": "The goal of unsupervised relation extraction is to extract relations from the web when we have no labeled training data, and not even any list of relations. This task is often called open information extraction or Open IE. In Open IE, the relations ", "Bloom_type": "comprehension", "question": "What does the term'relation extraction' refer to in the context of unsupervised learning?", "options": ["Identifying relationships between entities based on unlabeled data", "Extracting patterns from labeled data sets", "Creating a database of known relations for future use", "Training models using pre-existing datasets"], "complexity": 1}, {"id": 23, "context": "The lexical constraints are based on a dictionary D that is used to prune very rare, long relation strings. The intuition is to eliminate candidate relations that don`t occur with sufficient number of distinct argument types and so are likely to be bad examples. The system first runs the above relation extraction algorithm offline on 500 million web sentences and extracts a list of all the relations that occur after normalizing them (removing inflection, auxiliary verbs, adjectives, and adverbs). Each relation r is added to the dictionary if it occurs with at least 20 different arguments. Fader et al. (2011) used a dictionary of 1.7 million normalized relations. ", "Bloom_type": "comprehension", "question": "What is the primary goal of pruning very rare, long relation strings in the context of relation extraction?", "options": ["To ensure that only frequent and short relation strings remain for further analysis.", "To increase the complexity of the relation extraction algorithms.", "To reduce the computational resources required by the system.", "To focus solely on the most common relations."], "complexity": 1}, {"id": 24, "context": "The great advantage of unsupervised relation extraction is its ability to handle a huge number of relations without having to specify them in advance. The disadvantage is the need to map all the strings into some canonical form for adding to databases or knowledge graphs. Current methods focus heavily on relations expressed with verbs, and so will miss many relations that are expressed nominally. ", "Bloom_type": "comprehension", "question": "What is one of the primary advantages of using unsupervised relation extraction?", "options": ["It can automatically identify any type of relation.", "It requires less computational resources than supervised methods.", "It does not require mapping strings into canonical forms.", "It focuses more on expressing relations through nouns."], "complexity": 1}, {"id": 25, "context": "Supervised relation extraction systems are evaluated by using test sets with humanannotated, gold-standard relations and computing precision, recall, and F-measure. Labeled precision and recall require the system to classify the relation correctly, whereas unlabeled methods simply measure a system`s ability to detect entities that are related. ", "Bloom_type": "comprehension", "question": "What distinguishes labeled precision from unlabeled methods in supervised relation extraction?", "options": ["Both A) and C)", "Labeled precision requires the system to classify the relation correctly while unlabeled methods do not.", "Unlabeled methods need to compute F-measure for evaluation, whereas labeled precision does not.", "Labeled precision measures detection accuracy, while unlabeled methods focus on classification correctness."], "complexity": 1}, {"id": 26, "context": "Progress in this area continues to be stimulated by formal evaluations with shared benchmark datasets, including the Automatic Content Extraction (ACE) evaluations of 2000-2007 on named entity recognition, relation extraction, and temporal expressions1, the KBP (Knowledge Base Population) evaluations (Ji et al. 2010, Surdeanu 2013) of relation extraction tasks like slot filling (extracting attributes (slots`) like age, birthplace, and spouse for a given entity) and a series of SemEval workshops (Hendrickx et al., 2009). ", "Bloom_type": "comprehension", "question": "What are some examples of formal evaluations mentioned in the context regarding relation extraction?", "options": ["Both A) and B)", "Automatic Content Extraction (ACE) evaluations from 2000-2007 on named entity recognition, relation extraction, and temporal expressions.", "KBP (Knowledge Base Population) evaluations on relation extraction tasks like slot filling (extracting attributes like age, birthplace, and spouse for a given entity), and a series of SemEval workshops.", "Neither A) nor B)"], "complexity": 1}, {"id": 27, "context": "Semisupervised relation extraction was first proposed by Hearst (1992b), and extended by systems like AutoSlog-TS (Riloff, 1996), DIPRE (Brin, 1998), SNOWBALL (Agichtein and Gravano, 2000), and Jones et al. (1999). The distant supervision algorithm we describe was drawn from Mintz et al. (2009), who first used the term distant supervision` (which was suggested to them by Chris Manning) but similar ideas had occurred in earlier systems like Craven and Kumlien (1999) and Morgan et al. (2004) under the name weakly labeled data, as well as in Snow et al. (2005) and Wu and Weld (2007). Among the many extensions are Wu and Weld (2010), Riedel et al. (2010), and Ritter et al. (2013). Open IE systems include KNOWITALL Etzioni et al. (2005), TextRunner (Banko et al., 2007), and REVERB (Fader et al., 2011). See Riedel et al. (2013) for a universal schema that combines the advantages of distant supervision and Open IE. ", "Bloom_type": "comprehension", "question": "What is the origin of the term `distant supervision'?", "options": ["Mintz et al.", "Chris Manning", "Snow et al.", "Wu and Weld"], "complexity": 1}, {"id": 28, "context": "We begin with the task of relation extraction: finding and classifying semantic relations among entities mentioned in a text, like child-of (X is the child-of Y), or part-whole or geospatial relations. Relation extraction has close links to populating a relational database, and knowledge graphs, datasets of structured relational knowledge, are a useful way for search engines to present information to users. ", "Bloom_type": "application", "question": "What is the first step in relation extraction?", "options": ["Find entities mentioned in a text", "Populate a relational database", "Classify semantic relations", "Use its result in another method"], "complexity": 2}, {"id": 29, "context": "The text tells us, for example, that Tim Wagner is a spokesman for American Airlines, that United is a unit of UAL Corp., and that American is a unit of AMR. These binary relations are instances of more generic relations such as part-of or employs that are fairly frequent in news-style texts. Figure 20.1 lists the 17 relations used in the ACE relation extraction evaluations and Fig. 20.2 shows some sample relations. We might also extract more domain-specific relations such as the notion of an airline route. For example from this text we can conclude that United has routes to Chicago, Dallas, Denver, and San Francisco. ", "Bloom_type": "application", "question": "What type of relation extraction task involves identifying specific relationships between entities mentioned in a text?", "options": ["Finding domain-specific relations", "Identifying common nouns", "Extracting generic relations", "Determining part-of relations"], "complexity": 2}, {"id": 30, "context": "There are five main classes of algorithms for relation extraction: handwritten patterns, supervised machine learning, semi-supervised (via bootstrapping or distant supervision), and unsupervised. We`ll introduce each of these in the next sections. ", "Bloom_type": "application", "question": "Which type of algorithm is used when there is no labeled data available?", "options": ["Unsupervised", "Handwritten patterns", "Supervised machine learning", "Semi-supervised (via bootstrapping or distant supervision)"], "complexity": 2}, {"id": 31, "context": "Supervised machine learning approaches to relation extraction follow a scheme that should be familiar by now. A fixed set of relations and entities is chosen, a training corpus is hand-annotated with the relations and entities, and the annotated texts are then used to train classifiers to annotate an unseen test set. ", "Bloom_type": "application", "question": "What is the first step in developing a supervised machine learning approach for relation extraction?", "options": ["Choose a fixed set of relations and entities", "Annotate the training corpus with the relations and entities", "Train classifiers on the annotated texts", "Develop a new dataset"], "complexity": 2}, {"id": 32, "context": "Neural supervised relation classifiers Neural models for relation extraction similarly treat the task as supervised classification. Let`s consider a typical system applied to the TACRED relation extraction dataset and task (Zhang et al., 2017). In TACRED we are given a sentence and two spans within it: a subject, which is a person or organization, and an object, which is any other entity. The task is to assign a relation from the 42 TAC relations, or no relation. ", "Bloom_type": "application", "question": "What is the first step in applying neural models for relation extraction?", "options": ["Identify the subjects and objects in the sentence", "Choose the appropriate relation from the list of 42 options", "Train the model on the TACRED dataset", "Process the results obtained after applying methods"], "complexity": 2}, {"id": 33, "context": "A typical Transformer-encoder algorithm, shown in Fig. 20.6, simply takes a pretrained encoder like BERT and adds a linear layer on top of the sentence representation (for example the BERT [CLS] token), a linear layer that is finetuned as a 1-of-N classifier to assign one of the 43 labels. The input to the BERT encoder is partially de-lexified; the subject and object entities are replaced in the input by their NER tags. This helps keep the system from overfitting to the individual lexical items (Zhang et al., 2017). When using BERT-type Transformers for relation extraction, it helps to use versions of BERT like RoBERTa (Liu et al., 2019) or spanBERT (Joshi et al., 2020) that don`t have two sequences separated by a [SEP] token, but instead form the input from a single long sequence of sentences. ", "Bloom_type": "application", "question": "What technique can help prevent overfitting when using BERT-type Transformers for relation extraction?", "options": ["Replace subject and object entities with their NER tags", "Use different pre-trained models", "Add more layers after the BERT encoder", "Remove the [SEP] token separator"], "complexity": 2}, {"id": 34, "context": "In general, if the test set is similar enough to the training set, and if there is enough hand-labeled data, supervised relation extraction systems can get high ac", "Bloom_type": "application", "question": "What is the primary goal of developing a system for supervised relation extraction?", "options": ["To improve the accuracy of predicting relations between entities", "To increase the number of labeled examples", "To reduce computational resources needed for processing", "To automate the labeling process"], "complexity": 2}, {"id": 35, "context": "curacies. But labeling a large training set is extremely expensive and supervised models are brittle: they don`t generalize well to different text genres. For this reason, much research in relation extraction has focused on the semi-supervised and unsupervised approaches we turn to next. ", "Bloom_type": "application", "question": "What approach does the response suggest for dealing with the problem of labeling a large training set?", "options": ["Semi-supervised learning", "Manual annotation", "Supervised learning", "Unsupervised learning"], "complexity": 2}, {"id": 36, "context": "The goal of unsupervised relation extraction is to extract relations from the web when we have no labeled training data, and not even any list of relations. This task is often called open information extraction or Open IE. In Open IE, the relations ", "Bloom_type": "application", "question": "What does unsupervised relation extraction aim to achieve?", "options": ["To automatically discover relationships between entities on the web", "To label all possible relations in the dataset", "To find patterns in existing labeled data", "To create a comprehensive dictionary of known relations"], "complexity": 2}, {"id": 37, "context": "The lexical constraints are based on a dictionary D that is used to prune very rare, long relation strings. The intuition is to eliminate candidate relations that don`t occur with sufficient number of distinct argument types and so are likely to be bad examples. The system first runs the above relation extraction algorithm offline on 500 million web sentences and extracts a list of all the relations that occur after normalizing them (removing inflection, auxiliary verbs, adjectives, and adverbs). Each relation r is added to the dictionary if it occurs with at least 20 different arguments. Fader et al. (2011) used a dictionary of 1.7 million normalized relations. ", "Bloom_type": "application", "question": "What is the next step before pruning very rare, long relation strings?", "options": ["Use the existing dictionary for offline processing", "Run the relation extraction algorithm online on a smaller dataset", "Prune the relation strings manually", "Extract all possible relations from the web sentences"], "complexity": 2}, {"id": 38, "context": "The great advantage of unsupervised relation extraction is its ability to handle a huge number of relations without having to specify them in advance. The disadvantage is the need to map all the strings into some canonical form for adding to databases or knowledge graphs. Current methods focus heavily on relations expressed with verbs, and so will miss many relations that are expressed nominally. ", "Bloom_type": "application", "question": "What challenge does unsupervised relation extraction face when dealing with nominal expressions?", "options": ["It misses many relations expressed nominally.", "It cannot handle any relations.", "It requires explicit mapping of strings.", "It focuses too much on verb-based relations."], "complexity": 2}, {"id": 39, "context": "Supervised relation extraction systems are evaluated by using test sets with humanannotated, gold-standard relations and computing precision, recall, and F-measure. Labeled precision and recall require the system to classify the relation correctly, whereas unlabeled methods simply measure a system`s ability to detect entities that are related. ", "Bloom_type": "application", "question": "What is the difference between labeled and unlabeled methods in supervised relation extraction?", "options": ["Labeled methods evaluate the accuracy of relation classification, while unlabeled methods assess the detection rate of related entities.", "Labeled methods require human annotation for each relation, while unlabeled methods do not.", "Labeled methods focus on detecting unrelated pairs of entities, while unlabeled methods aim at extracting relevant relations.", "Labeled methods need to identify specific types of relations, while unlabeled methods can handle any type of entity pair."], "complexity": 2}, {"id": 40, "context": "Progress in this area continues to be stimulated by formal evaluations with shared benchmark datasets, including the Automatic Content Extraction (ACE) evaluations of 2000-2007 on named entity recognition, relation extraction, and temporal expressions1, the KBP (Knowledge Base Population) evaluations (Ji et al. 2010, Surdeanu 2013) of relation extraction tasks like slot filling (extracting attributes (slots`) like age, birthplace, and spouse for a given entity) and a series of SemEval workshops (Hendrickx et al., 2009). ", "Bloom_type": "application", "question": "What is an essential step in developing a system for relation extraction?", "options": ["Develop a robust framework that can handle various types of relations.", "Collect large amounts of data randomly.", "Ensure high accuracy in all types of relations.", "Implement complex machine learning algorithms immediately."], "complexity": 2}, {"id": 41, "context": "Semisupervised relation extraction was first proposed by Hearst (1992b), and extended by systems like AutoSlog-TS (Riloff, 1996), DIPRE (Brin, 1998), SNOWBALL (Agichtein and Gravano, 2000), and Jones et al. (1999). The distant supervision algorithm we describe was drawn from Mintz et al. (2009), who first used the term distant supervision` (which was suggested to them by Chris Manning) but similar ideas had occurred in earlier systems like Craven and Kumlien (1999) and Morgan et al. (2004) under the name weakly labeled data, as well as in Snow et al. (2005) and Wu and Weld (2007). Among the many extensions are Wu and Weld (2010), Riedel et al. (2010), and Ritter et al. (2013). Open IE systems include KNOWITALL Etzioni et al. (2005), TextRunner (Banko et al., 2007), and REVERB (Fader et al., 2011). See Riedel et al. (2013) for a universal schema that combines the advantages of distant supervision and Open IE. ", "Bloom_type": "application", "question": "What is the main difference between distant supervision and other methods mentioned?", "options": ["It uses unlabeled data.", "It requires more computational resources.", "It involves direct labeling of training examples.", "It focuses on extracting relations between entities."], "complexity": 2}]}, "word meaning": {"max_id": 23, "Questions": [{"id": 0, "context": "In the first part of the book we introduce the fundamental suite of algorithmic tools that make up the modern neural language model that is the heart of end-to-end NLP systems. We begin with tokenization and preprocessing, as well as useful algorithms like computing edit distance, and then proceed to the tasks of classification, logistic regression, neural networks, proceeding through feedforward networks, recurrent networks, and then transformers. We`ll also see the role of embeddings as a model of word meaning. ", "Bloom_type": "remember", "question": "In the context provided, what does the author mention about the role of embeddings in relation to word meaning?", "options": ["The author explains how embeddings represent word meanings using numerical vectors.", "The author mentions that embeddings are used for sentiment analysis.", "The author states that embeddings help improve the accuracy of image recognition models.", "The author suggests that embeddings replace traditional dictionaries entirely."], "complexity": 0}, {"id": 1, "context": "Surely we can do better than this! After all, we`ll want a model of word meaning to do all sorts of things for us. It should tell us that some words have similar meanings (cat is similar to dog), others are antonyms (cold is the opposite of hot), some have positive connotations (happy) while others have negative connotations (sad). It should represent the fact that the meanings of buy, sell, and pay offer differing perspectives on the same underlying purchasing event. (If I buy something from you, you`ve probably sold it to me, and I likely paid you.) More generally, a model of word meaning should allow us to draw inferences to address meaning-related tasks like question-answering or dialogue. ", "Bloom_type": "remember", "question": "What does the speaker suggest about models of word meaning?", "options": ["They will enable complex reasoning tasks.", "They will replace traditional dictionaries.", "They will help understand synonyms and antonyms.", "They will focus solely on grammatical aspects."], "complexity": 0}, {"id": 2, "context": "Either the PPMI model or the tf-idf model can be used to compute word similarity, for tasks like finding word paraphrases, tracking changes in word meaning, or automatically discovering meanings of words in different corpora. For example, we can find the 10 most similar words to any target word w by computing the cosines between w and each of the V ", "Bloom_type": "remember", "question": "Which method is used to calculate word similarity?", "options": ["tf-idf model", "PPMI model", "WordNet", "Latent Semantic Analysis"], "complexity": 0}, {"id": 3, "context": "The use of dense vectors to model word meaning, and indeed the term embedding, grew out of the latent semantic indexing (LSI) model (Deerwester et al., 1988) recast as LSA (latent semantic analysis) (Deerwester et al., 1990). In LSA singular value decompositionSVD is applied to a term-document matrix (each cell weighted by log frequency and normalized by entropy), and then the first 300 dimensions are used as the LSA embedding. Singular Value Decomposition (SVD) is a method for finding the most important dimensions of a data set, those dimensions along which the data varies the most. LSA was then quickly widely applied: as a cognitive model Landauer and Dumais (1997), and for tasks like spell checking (Jones and Martin, 1997), language modeling (Bellegarda 1997, Coccaro and Jurafsky 1998, Bellegarda 2000), morphology induction (Schone and Jurafsky 2000, Schone and Jurafsky 2001b), multiword expressions (MWEs) (Schone and Jurafsky, 2001a), and essay grading (Rehder et al., 1998). Related models were simultaneously developed and applied to word sense disambiguation by Schutze (1992b). LSA also led to the earliest use of embeddings to represent words in a probabilistic classifier, in the logistic regression document router of Schutze et al. (1995). The idea of SVD on the term-term matrix (rather than the term-document matrix) as a model of meaning for NLP was proposed soon after LSA by Schutze (1992b). Schutze applied the low-rank (97-dimensional) embeddings produced by SVD to the task of word sense disambiguation, analyzed the resulting semantic space, and also suggested possible techniques like dropping high-order dimensions. See Schutze (1997). ", "Bloom_type": "remember", "question": "In what way did the application of SVD lead to advancements in natural language processing?", "options": ["It facilitated better understanding of word meanings.", "It improved the accuracy of spelling correction.", "It enhanced the ability to identify similar documents.", "It increased the speed of machine learning algorithms."], "complexity": 0}, {"id": 4, "context": "By the next decade, Bengio et al. (2003) and Bengio et al. (2006) showed that neural language models could also be used to develop embeddings as part of the task of word prediction. Collobert and Weston (2007), Collobert and Weston (2008), and Collobert et al. (2011) then demonstrated that embeddings could be used to represent word meanings for a number of NLP tasks. Turian et al. (2010) compared the value of different kinds of embeddings for different NLP tasks. Mikolov et al. (2011) showed that recurrent neural nets could be used as language models. The idea of simplifying the hidden layer of these neural net language models to create the skipgram (and also CBOW) algorithms was proposed by Mikolov et al. (2013a). The negative sampling training algorithm was proposed in Mikolov et al. (2013b). There are numerous surveys of static embeddings and their parameterizations (Bullinaria and Levy 2007, Bullinaria and Levy 2012, Lapesa and Evert 2014, Kiela and Clark 2014, Levy et al. 2015). ", "Bloom_type": "remember", "question": "In what year did Bengio et al. first demonstrate the use of embeddings for representing word meanings?", "options": ["2003", "2006", "2007", "2008"], "complexity": 0}, {"id": 5, "context": "The point of all these examples is that these contextual words that help us compute the meaning of words in context can be quite far away in the sentence or paragraph. Transformers can build contextual representations of word meaning, contextual embeddings, by integrating the meaning of these helpful contextual words. In a transformer, layer by layer, we build up richer and richer contextualized representations of the meanings of input tokens. At each layer, we compute the representation of a token i by combining information about i from the previous layer with information about the neighboring tokens to produce a contextualized representation for each word at each position. ", "Bloom_type": "remember", "question": "In what way do transformers integrate the meaning of contextual words to compute the meaning of words?", "options": ["By integrating the meaning of contextual words through layer-wise computation", "By ignoring the meaning of contextual words", "By directly computing the meaning of each word independently", "By using only the first contextual word as reference"], "complexity": 0}, {"id": 6, "context": "We also introduced finetuning in the prior chapter. Here we describe a new kind of finetuning, in which we take the transformer network learned by these pretrained models, add a neural net classifier after the top layer of the network, and train it on some additional labeled data to perform some downstream task like named entity tagging or natural language inference. As before, the intuition is that the pretraining phase learns a language model that instantiates rich representations of word meaning, that thus enables the model to more easily learn (be finetuned to`) the requirements of a downstream language understanding task. This aspect of the pretrain-finetune paradigm is an instance of what is called transfer learning in machine learning: the method of acquiring knowledge from one task or domain, and then applying it (transferring it) to solve a new task. ", "Bloom_type": "remember", "question": "In the described finetuning process, why is the pre-trained transformer network important for learning word meanings?", "options": ["To enhance the representation of word meanings through extensive training", "To improve the accuracy of the neural net classifier", "To reduce computational resources required for training", "To increase the speed of the fine-tuning process"], "complexity": 0}, {"id": 7, "context": "Just as we used static embeddings like word2vec in Chapter 6 to represent the meaning of words, we can use contextual embeddings as representations of word meanings in context for any task that might require a model of word meaning. Where static embeddings represent the meaning of word types (vocabulary entries), contextual embeddings represent the meaning of word instances: instances of a particular word type in a particular context. Thus where word2vec had a single vector for each word type, contextual embeddings provide a single vector for each instance of that word type in its sentential context. Contextual embeddings can thus be used for tasks like measuring the semantic similarity of two words in context, and are useful in linguistic tasks that require models of word meaning. ", "Bloom_type": "remember", "question": "In what way do contextual embeddings differ from static embeddings?", "options": ["Contextual embeddings offer a more detailed representation of word meanings by considering their usage in specific contexts.", "Contextual embeddings only represent the meaning of individual words.", "Static embeddings provide a single vector for each word type in its sentence context.", "Static embeddings focus on representing the overall vocabulary rather than individual word instances."], "complexity": 0}, {"id": 8, "context": "In the first part of the book we introduce the fundamental suite of algorithmic tools that make up the modern neural language model that is the heart of end-to-end NLP systems. We begin with tokenization and preprocessing, as well as useful algorithms like computing edit distance, and then proceed to the tasks of classification, logistic regression, neural networks, proceeding through feedforward networks, recurrent networks, and then transformers. We`ll also see the role of embeddings as a model of word meaning. ", "Bloom_type": "comprehension", "question": "What concept does the author discuss as an important aspect of understanding word meanings within the framework of neural language models?", "options": ["Tokenization and preprocessing techniques", "Classification methods", "Logistic regression algorithms", "Feedforward and recurrent network architectures"], "complexity": 1}, {"id": 9, "context": "Surely we can do better than this! After all, we`ll want a model of word meaning to do all sorts of things for us. It should tell us that some words have similar meanings (cat is similar to dog), others are antonyms (cold is the opposite of hot), some have positive connotations (happy) while others have negative connotations (sad). It should represent the fact that the meanings of buy, sell, and pay offer differing perspectives on the same underlying purchasing event. (If I buy something from you, you`ve probably sold it to me, and I likely paid you.) More generally, a model of word meaning should allow us to draw inferences to address meaning-related tasks like question-answering or dialogue. ", "Bloom_type": "comprehension", "question": "What does a model of word meaning help us achieve?", "options": ["It aids in drawing inferences about the relationships between different words.", "It helps us understand synonyms and antonyms.", "It assists in creating new words based on existing ones.", "It enables us to translate languages more accurately."], "complexity": 1}, {"id": 10, "context": "Either the PPMI model or the tf-idf model can be used to compute word similarity, for tasks like finding word paraphrases, tracking changes in word meaning, or automatically discovering meanings of words in different corpora. For example, we can find the 10 most similar words to any target word w by computing the cosines between w and each of the V ", "Bloom_type": "comprehension", "question": "Which method involves calculating cosine similarities between a target word and all other words in a corpus to identify its top synonyms?", "options": ["tf-idf model", "PPMI model", "WordNet", "TextRank algorithm"], "complexity": 1}, {"id": 11, "context": "The use of dense vectors to model word meaning, and indeed the term embedding, grew out of the latent semantic indexing (LSI) model (Deerwester et al., 1988) recast as LSA (latent semantic analysis) (Deerwester et al., 1990). In LSA singular value decompositionSVD is applied to a term-document matrix (each cell weighted by log frequency and normalized by entropy), and then the first 300 dimensions are used as the LSA embedding. Singular Value Decomposition (SVD) is a method for finding the most important dimensions of a data set, those dimensions along which the data varies the most. LSA was then quickly widely applied: as a cognitive model Landauer and Dumais (1997), and for tasks like spell checking (Jones and Martin, 1997), language modeling (Bellegarda 1997, Coccaro and Jurafsky 1998, Bellegarda 2000), morphology induction (Schone and Jurafsky 2000, Schone and Jurafsky 2001b), multiword expressions (MWEs) (Schone and Jurafsky, 2001a), and essay grading (Rehder et al., 1998). Related models were simultaneously developed and applied to word sense disambiguation by Schutze (1992b). LSA also led to the earliest use of embeddings to represent words in a probabilistic classifier, in the logistic regression document router of Schutze et al. (1995). The idea of SVD on the term-term matrix (rather than the term-document matrix) as a model of meaning for NLP was proposed soon after LSA by Schutze (1992b). Schutze applied the low-rank (97-dimensional) embeddings produced by SVD to the task of word sense disambiguation, analyzed the resulting semantic space, and also suggested possible techniques like dropping high-order dimensions. See Schutze (1997). ", "Bloom_type": "comprehension", "question": "What did the development of LSA lead to in the field of natural language processing?", "options": ["Both A and B", "The creation of dense vectors to model word meaning", "The invention of the logistic regression document router", "The introduction of embeddings to represent words in a probabilistic classifier"], "complexity": 1}, {"id": 12, "context": "By the next decade, Bengio et al. (2003) and Bengio et al. (2006) showed that neural language models could also be used to develop embeddings as part of the task of word prediction. Collobert and Weston (2007), Collobert and Weston (2008), and Collobert et al. (2011) then demonstrated that embeddings could be used to represent word meanings for a number of NLP tasks. Turian et al. (2010) compared the value of different kinds of embeddings for different NLP tasks. Mikolov et al. (2011) showed that recurrent neural nets could be used as language models. The idea of simplifying the hidden layer of these neural net language models to create the skipgram (and also CBOW) algorithms was proposed by Mikolov et al. (2013a). The negative sampling training algorithm was proposed in Mikolov et al. (2013b). There are numerous surveys of static embeddings and their parameterizations (Bullinaria and Levy 2007, Bullinaria and Levy 2012, Lapesa and Evert 2014, Kiela and Clark 2014, Levy et al. 2015). ", "Bloom_type": "comprehension", "question": "What did Bengio et al. show about using neural language models for word prediction?", "options": ["They proved that neural language models can improve word prediction accuracy.", "They found that neural language models cannot predict words.", "They discovered that neural language models can only predict nouns.", "They concluded that neural language models should never be used for predicting words."], "complexity": 1}, {"id": 13, "context": "The point of all these examples is that these contextual words that help us compute the meaning of words in context can be quite far away in the sentence or paragraph. Transformers can build contextual representations of word meaning, contextual embeddings, by integrating the meaning of these helpful contextual words. In a transformer, layer by layer, we build up richer and richer contextualized representations of the meanings of input tokens. At each layer, we compute the representation of a token i by combining information about i from the previous layer with information about the neighboring tokens to produce a contextualized representation for each word at each position. ", "Bloom_type": "comprehension", "question": "In the context of computing word meaning using transformers, how do they integrate the meaning of contextual words?", "options": ["By calculating weighted averages of the meanings of surrounding words", "By ignoring the contextual words entirely", "By directly assigning meanings based on their positions alone", "By focusing solely on the central word without regard to its neighbors"], "complexity": 1}, {"id": 14, "context": "We also introduced finetuning in the prior chapter. Here we describe a new kind of finetuning, in which we take the transformer network learned by these pretrained models, add a neural net classifier after the top layer of the network, and train it on some additional labeled data to perform some downstream task like named entity tagging or natural language inference. As before, the intuition is that the pretraining phase learns a language model that instantiates rich representations of word meaning, that thus enables the model to more easily learn (be finetuned to`) the requirements of a downstream language understanding task. This aspect of the pretrain-finetune paradigm is an instance of what is called transfer learning in machine learning: the method of acquiring knowledge from one task or domain, and then applying it (transferring it) to solve a new task. ", "Bloom_type": "comprehension", "question": "What does the addition of a neural net classifier after the top layer of the transformer network enable?", "options": ["The ability to fine-tune the model for specific tasks", "The enhancement of the existing language model", "The improvement of the transformer network's performance", "The reduction of computational complexity"], "complexity": 1}, {"id": 15, "context": "Just as we used static embeddings like word2vec in Chapter 6 to represent the meaning of words, we can use contextual embeddings as representations of word meanings in context for any task that might require a model of word meaning. Where static embeddings represent the meaning of word types (vocabulary entries), contextual embeddings represent the meaning of word instances: instances of a particular word type in a particular context. Thus where word2vec had a single vector for each word type, contextual embeddings provide a single vector for each instance of that word type in its sentential context. Contextual embeddings can thus be used for tasks like measuring the semantic similarity of two words in context, and are useful in linguistic tasks that require models of word meaning. ", "Bloom_type": "comprehension", "question": "What do contextual embeddings represent differently from static embeddings?", "options": ["Contextual embeddings represent the meaning of individual words.", "Static embeddings represent the meaning of entire sentences.", "Static embeddings focus on the overall vocabulary usage.", "Both A and B are correct."], "complexity": 1}, {"id": 16, "context": "In the first part of the book we introduce the fundamental suite of algorithmic tools that make up the modern neural language model that is the heart of end-to-end NLP systems. We begin with tokenization and preprocessing, as well as useful algorithms like computing edit distance, and then proceed to the tasks of classification, logistic regression, neural networks, proceeding through feedforward networks, recurrent networks, and then transformers. We`ll also see the role of embeddings as a model of word meaning. ", "Bloom_type": "application", "question": "What is the next step after introducing the fundamental suite of algorithmic tools for the modern neural language model?", "options": ["We'll discuss the role of embeddings in modeling word meaning.", "We'll start with tokenization and preprocessing.", "We'll move on to classification and logistic regression.", "We'll focus on neural networks and their types."], "complexity": 2}, {"id": 17, "context": "Surely we can do better than this! After all, we`ll want a model of word meaning to do all sorts of things for us. It should tell us that some words have similar meanings (cat is similar to dog), others are antonyms (cold is the opposite of hot), some have positive connotations (happy) while others have negative connotations (sad). It should represent the fact that the meanings of buy, sell, and pay offer differing perspectives on the same underlying purchasing event. (If I buy something from you, you`ve probably sold it to me, and I likely paid you.) More generally, a model of word meaning should allow us to draw inferences to address meaning-related tasks like question-answering or dialogue. ", "Bloom_type": "application", "question": "What kind of model would help understand how different words relate to each other based on their meanings?", "options": ["A semantic vector space", "A dictionary", "An analogy network", "A syntactic parser"], "complexity": 2}, {"id": 18, "context": "Either the PPMI model or the tf-idf model can be used to compute word similarity, for tasks like finding word paraphrases, tracking changes in word meaning, or automatically discovering meanings of words in different corpora. For example, we can find the 10 most similar words to any target word w by computing the cosines between w and each of the V ", "Bloom_type": "application", "question": "Which method is commonly used to determine the cosine similarity between two words?", "options": ["tf-idf model", "PPMI model", "WordNet", "Latent Semantic Analysis (LSA)"], "complexity": 2}, {"id": 19, "context": "The use of dense vectors to model word meaning, and indeed the term embedding, grew out of the latent semantic indexing (LSI) model (Deerwester et al., 1988) recast as LSA (latent semantic analysis) (Deerwester et al., 1990). In LSA singular value decompositionSVD is applied to a term-document matrix (each cell weighted by log frequency and normalized by entropy), and then the first 300 dimensions are used as the LSA embedding. Singular Value Decomposition (SVD) is a method for finding the most important dimensions of a data set, those dimensions along which the data varies the most. LSA was then quickly widely applied: as a cognitive model Landauer and Dumais (1997), and for tasks like spell checking (Jones and Martin, 1997), language modeling (Bellegarda 1997, Coccaro and Jurafsky 1998, Bellegarda 2000), morphology induction (Schone and Jurafsky 2000, Schone and Jurafsky 2001b), multiword expressions (MWEs) (Schone and Jurafsky, 2001a), and essay grading (Rehder et al., 1998). Related models were simultaneously developed and applied to word sense disambiguation by Schutze (1992b). LSA also led to the earliest use of embeddings to represent words in a probabilistic classifier, in the logistic regression document router of Schutze et al. (1995). The idea of SVD on the term-term matrix (rather than the term-document matrix) as a model of meaning for NLP was proposed soon after LSA by Schutze (1992b). Schutze applied the low-rank (97-dimensional) embeddings produced by SVD to the task of word sense disambiguation, analyzed the resulting semantic space, and also suggested possible techniques like dropping high-order dimensions. See Schutze (1997). ", "Bloom_type": "application", "question": "What is the primary application of SVD in the field of Natural Language Processing (NLP)?", "options": ["All of the above", "To find the most important dimensions of a data set", "To analyze the semantic space of words", "To develop probabilistic classifiers for word sense disambiguation"], "complexity": 2}, {"id": 20, "context": "By the next decade, Bengio et al. (2003) and Bengio et al. (2006) showed that neural language models could also be used to develop embeddings as part of the task of word prediction. Collobert and Weston (2007), Collobert and Weston (2008), and Collobert et al. (2011) then demonstrated that embeddings could be used to represent word meanings for a number of NLP tasks. Turian et al. (2010) compared the value of different kinds of embeddings for different NLP tasks. Mikolov et al. (2011) showed that recurrent neural nets could be used as language models. The idea of simplifying the hidden layer of these neural net language models to create the skipgram (and also CBOW) algorithms was proposed by Mikolov et al. (2013a). The negative sampling training algorithm was proposed in Mikolov et al. (2013b). There are numerous surveys of static embeddings and their parameterizations (Bullinaria and Levy 2007, Bullinaria and Levy 2012, Lapesa and Evert 2014, Kiela and Clark 2014, Levy et al. 2015). ", "Bloom_type": "application", "question": "What is the first step in developing embeddings for word meanings?", "options": ["Implement neural language models using Bengio et al. (2003) and Bengio et al. (2006)", "Read Bengio et al. (2003) and Bengio et al. (2006)", "Develop embeddings based on Collobert and Weston (2007) and Collobert and Weston (2008)", "Create embeddings using Mikolov et al. (2011)"], "complexity": 2}, {"id": 21, "context": "The point of all these examples is that these contextual words that help us compute the meaning of words in context can be quite far away in the sentence or paragraph. Transformers can build contextual representations of word meaning, contextual embeddings, by integrating the meaning of these helpful contextual words. In a transformer, layer by layer, we build up richer and richer contextualized representations of the meanings of input tokens. At each layer, we compute the representation of a token i by combining information about i from the previous layer with information about the neighboring tokens to produce a contextualized representation for each word at each position. ", "Bloom_type": "application", "question": "In a transformer model, how are contextualized representations of word meanings built?", "options": ["By integrating the meaning of contextual words through layers", "By ignoring the surrounding context", "By focusing solely on the immediate preceding word", "By processing only the current word without considering its neighbors"], "complexity": 2}, {"id": 22, "context": "We also introduced finetuning in the prior chapter. Here we describe a new kind of finetuning, in which we take the transformer network learned by these pretrained models, add a neural net classifier after the top layer of the network, and train it on some additional labeled data to perform some downstream task like named entity tagging or natural language inference. As before, the intuition is that the pretraining phase learns a language model that instantiates rich representations of word meaning, that thus enables the model to more easily learn (be finetuned to`) the requirements of a downstream language understanding task. This aspect of the pretrain-finetune paradigm is an instance of what is called transfer learning in machine learning: the method of acquiring knowledge from one task or domain, and then applying it (transferring it) to solve a new task. ", "Bloom_type": "application", "question": "What does adding a neural net classifier after the top layer of the transformer network enable?", "options": ["It helps in improving the accuracy of the downstream tasks.", "It allows for fine-tuning the model parameters.", "It enhances the representation of word meanings.", "It increases the computational complexity of the model."], "complexity": 2}, {"id": 23, "context": "Just as we used static embeddings like word2vec in Chapter 6 to represent the meaning of words, we can use contextual embeddings as representations of word meanings in context for any task that might require a model of word meaning. Where static embeddings represent the meaning of word types (vocabulary entries), contextual embeddings represent the meaning of word instances: instances of a particular word type in a particular context. Thus where word2vec had a single vector for each word type, contextual embeddings provide a single vector for each instance of that word type in its sentential context. Contextual embeddings can thus be used for tasks like measuring the semantic similarity of two words in context, and are useful in linguistic tasks that require models of word meaning. ", "Bloom_type": "application", "question": "What is the difference between static embeddings and contextual embeddings?", "options": ["Static embeddings represent the meaning of word types while contextual embeddings represent the meaning of word instances.", "Static embeddings provide a single vector for each word type, whereas contextual embeddings provide a single vector for each word instance.", "Static embeddings measure semantic similarity, while contextual embeddings are not useful in linguistic tasks.", "Static embeddings are used for all tasks, while contextual embeddings are only useful for measuring semantic similarity."], "complexity": 2}]}, "dialogue act": {"max_id": 14, "Questions": [{"id": 0, "context": "While the naive slot-extractor system described above can handle simple dialogues, often we want more complex interactions. For example, we might want to confirm that we`ve understand the user, or ask them to repeat themselves. We can build a more sophisticated system using dialogue acts and dialogue state. ", "Bloom_type": "remember", "question": "In what way does the naive slot-extractor system fail when dealing with complex interactions?", "options": ["It lacks the ability to recognize different types of dialogue acts.", "It cannot handle any form of interaction.", "It struggles with understanding the user's intent.", "It is too simplistic to detect repetition."], "complexity": 0}, {"id": 1, "context": "In early commercial frame-based systems, the dialogue policy is simple: ask questions until all the slots are full, do a database query, then report back to the user. A more sophisticated dialogue policy can help a system decide when to answer the user`s questions, when to instead ask the user a clarification question, and so on. A dialogue policy thus decides what dialogue act to generate. Choosing a dialogue act to generate, along with its arguments, is sometimes called content planning. ", "Bloom_type": "remember", "question": "In early commercial frame-based systems, how does the dialogue policy determine what dialogue act to generate?", "options": ["Choosing the most appropriate act based on the current conversation state", "By randomly selecting from predefined acts", "Based solely on the number of available slots", "Using a predefined set of rules for each type of question asked"], "complexity": 0}, {"id": 2, "context": " In human dialogue, speaking is a kind of action; these acts are referred to as speech acts or dialogue acts. Speakers also attempt to achieve common ground by acknowledging that they have understand each other. Conversation also is characterized by turn structure and dialogue structure. ", "Bloom_type": "remember", "question": "In human dialogue, what do speakers aim to accomplish through their actions?", "options": ["To fulfill social obligations", "To establish personal identity", "To share personal experiences", "To maintain emotional connection"], "complexity": 0}, {"id": 3, "context": " The dialogue-state architecture augments the GUS frame-and-slot architecture with richer representations and more sophisticated algorithms for keeping track of user`s dialogue acts, policies for generating its own dialogue acts, and a natural language component. ", "Bloom_type": "remember", "question": "What does the dialogue-state architecture do?", "options": ["It enhances the GUS frame-and-slot architecture by adding more complex algorithms.", "It simplifies the GUS frame-and-slot architecture by reducing complexity.", "It focuses on improving the natural language processing capabilities.", "It decreases the memory usage of the system."], "complexity": 0}, {"id": 4, "context": "In the 1990s, machine learning models that had first been applied to natural language processing began to be applied to dialogue tasks like slot filling (Miller et al. 1994, Pieraccini et al. 1991). This period also saw lots of analytic work on the linguistic properties of dialogue acts and on machine-learning-based methods for their detection. (Sag and Liberman 1975, Hinkelman and Allen 1989, Nagata and Morimoto 1994, Goodwin 1996, Chu-Carroll 1998, Shriberg et al. 1998, Stolcke et al. 2000, Gravano et al. 2012. This work strongly informed the development of the dialogue-state model (Larsson and Traum, 2000). Dialogue state tracking quickly became an important problem for task-oriented dialogue, and there has been an influential annual evaluation of state-tracking algorithms (Williams et al., 2016). The turn of the century saw a line of work on applying reinforcement learning to dialogue, which first came out of AT&T and Bell Laboratories with work on MDP dialogue systems (Walker 2000, Levin et al. 2000, Singh et al. 2002) along with work on cue phrases, prosody, and rejection and confirmation. Reinforcement learning research turned quickly to the more sophisticated POMDP models (Roy et al. 2000, Lemon et al. 2006, Williams and Young 2007) applied to small slotfilling dialogue tasks. Neural reinforcement learning models have been used both for chatbot systems, for example simulating dialogues between two dialogue systems, rewarding good conversational properties like coherence and ease of answering (Li et al., 2016a), and for task-oriented dialogue (Williams et al., 2017). ", "Bloom_type": "remember", "question": "In what decade did machine learning models begin to be applied to dialogue tasks?", "options": ["The 1990s", "The 1980s", "The 1970s", "The 1960s"], "complexity": 0}, {"id": 5, "context": "While the naive slot-extractor system described above can handle simple dialogues, often we want more complex interactions. For example, we might want to confirm that we`ve understand the user, or ask them to repeat themselves. We can build a more sophisticated system using dialogue acts and dialogue state. ", "Bloom_type": "comprehension", "question": "What are dialogue acts and how do they relate to building a more sophisticated system for handling complex interactions?", "options": ["Dialogue acts refer to the different types of actions taken during a conversation, while a sophisticated system uses these acts to manage and track the ongoing interaction.", "Dialogue acts describe the understanding of the user, whereas a sophisticated system focuses on repeating their statements.", "Dialogue acts are irrelevant to managing complex interactions, so a sophisticated system does not use them.", "None/All of the above"], "complexity": 1}, {"id": 6, "context": "In early commercial frame-based systems, the dialogue policy is simple: ask questions until all the slots are full, do a database query, then report back to the user. A more sophisticated dialogue policy can help a system decide when to answer the user`s questions, when to instead ask the user a clarification question, and so on. A dialogue policy thus decides what dialogue act to generate. Choosing a dialogue act to generate, along with its arguments, is sometimes called content planning. ", "Bloom_type": "comprehension", "question": "What does choosing a dialogue act to generate involve?", "options": ["Selecting from predefined sets of responses for common scenarios", "Deciding on the type of question to ask", "Determining how to respond based on previous interactions", "Choosing between different types of clarifying questions"], "complexity": 1}, {"id": 7, "context": " In human dialogue, speaking is a kind of action; these acts are referred to as speech acts or dialogue acts. Speakers also attempt to achieve common ground by acknowledging that they have understand each other. Conversation also is characterized by turn structure and dialogue structure. ", "Bloom_type": "comprehension", "question": "What distinguishes speakers from listeners in human dialogue?", "options": ["Speakers acknowledge mutual understanding through dialogue acts.", "Listeners respond to speakers' actions.", "Speakers aim for common ground while listening.", "Both A) and C)"], "complexity": 1}, {"id": 8, "context": " The dialogue-state architecture augments the GUS frame-and-slot architecture with richer representations and more sophisticated algorithms for keeping track of user`s dialogue acts, policies for generating its own dialogue acts, and a natural language component. ", "Bloom_type": "comprehension", "question": "What does the dialogue-state architecture do by augmenting the GUS frame-and-slot architecture?", "options": ["It enhances the tracking of user's dialogue states.", "It simplifies the representation of users' dialogue acts.", "It reduces the complexity of generating dialogue acts.", "It integrates a natural language processing module."], "complexity": 1}, {"id": 9, "context": "In the 1990s, machine learning models that had first been applied to natural language processing began to be applied to dialogue tasks like slot filling (Miller et al. 1994, Pieraccini et al. 1991). This period also saw lots of analytic work on the linguistic properties of dialogue acts and on machine-learning-based methods for their detection. (Sag and Liberman 1975, Hinkelman and Allen 1989, Nagata and Morimoto 1994, Goodwin 1996, Chu-Carroll 1998, Shriberg et al. 1998, Stolcke et al. 2000, Gravano et al. 2012. This work strongly informed the development of the dialogue-state model (Larsson and Traum, 2000). Dialogue state tracking quickly became an important problem for task-oriented dialogue, and there has been an influential annual evaluation of state-tracking algorithms (Williams et al., 2016). The turn of the century saw a line of work on applying reinforcement learning to dialogue, which first came out of AT&T and Bell Laboratories with work on MDP dialogue systems (Walker 2000, Levin et al. 2000, Singh et al. 2002) along with work on cue phrases, prosody, and rejection and confirmation. Reinforcement learning research turned quickly to the more sophisticated POMDP models (Roy et al. 2000, Lemon et al. 2006, Williams and Young 2007) applied to small slotfilling dialogue tasks. Neural reinforcement learning models have been used both for chatbot systems, for example simulating dialogues between two dialogue systems, rewarding good conversational properties like coherence and ease of answering (Li et al., 2016a), and for task-oriented dialogue (Williams et al., 2017). ", "Bloom_type": "comprehension", "question": "What was one significant area of focus during the early applications of machine learning to dialogue tasks?", "options": ["Dialogue act analysis", "Natural language generation", "Reinforcement learning", "Task-oriented dialogue state tracking"], "complexity": 1}, {"id": 10, "context": "While the naive slot-extractor system described above can handle simple dialogues, often we want more complex interactions. For example, we might want to confirm that we`ve understand the user, or ask them to repeat themselves. We can build a more sophisticated system using dialogue acts and dialogue state. ", "Bloom_type": "application", "question": "What is an essential component for building a more sophisticated system when dealing with complex interactions?", "options": ["Dialogue acts and dialogue state", "Naive slot-extraction", "Simple dialogues", "User understanding"], "complexity": 2}, {"id": 11, "context": "In early commercial frame-based systems, the dialogue policy is simple: ask questions until all the slots are full, do a database query, then report back to the user. A more sophisticated dialogue policy can help a system decide when to answer the user`s questions, when to instead ask the user a clarification question, and so on. A dialogue policy thus decides what dialogue act to generate. Choosing a dialogue act to generate, along with its arguments, is sometimes called content planning. ", "Bloom_type": "application", "question": "What does choosing a dialogue act to generate involve?", "options": ["Planning the sequence of actions before generating the response", "Deciding on the type of question to ask", "Determining the next action based on user input", "Choosing between different types of responses for clarification"], "complexity": 2}, {"id": 12, "context": " In human dialogue, speaking is a kind of action; these acts are referred to as speech acts or dialogue acts. Speakers also attempt to achieve common ground by acknowledging that they have understand each other. Conversation also is characterized by turn structure and dialogue structure. ", "Bloom_type": "application", "question": "What is an example of a dialogue act?", "options": ["An acknowledgment of understanding between speakers.", "A statement made during a conversation.", "The beginning of a new sentence in a conversation.", "The end of a conversation."], "complexity": 2}, {"id": 13, "context": " The dialogue-state architecture augments the GUS frame-and-slot architecture with richer representations and more sophisticated algorithms for keeping track of user`s dialogue acts, policies for generating its own dialogue acts, and a natural language component. ", "Bloom_type": "application", "question": "What is an example of a dialogue act?", "options": ["A statement made during a conversation.", "The speaker's intention behind their words.", "The response generated by the system based on user input.", "The rules governing how the system should respond."], "complexity": 2}, {"id": 14, "context": "In the 1990s, machine learning models that had first been applied to natural language processing began to be applied to dialogue tasks like slot filling (Miller et al. 1994, Pieraccini et al. 1991). This period also saw lots of analytic work on the linguistic properties of dialogue acts and on machine-learning-based methods for their detection. (Sag and Liberman 1975, Hinkelman and Allen 1989, Nagata and Morimoto 1994, Goodwin 1996, Chu-Carroll 1998, Shriberg et al. 1998, Stolcke et al. 2000, Gravano et al. 2012. This work strongly informed the development of the dialogue-state model (Larsson and Traum, 2000). Dialogue state tracking quickly became an important problem for task-oriented dialogue, and there has been an influential annual evaluation of state-tracking algorithms (Williams et al., 2016). The turn of the century saw a line of work on applying reinforcement learning to dialogue, which first came out of AT&T and Bell Laboratories with work on MDP dialogue systems (Walker 2000, Levin et al. 2000, Singh et al. 2002) along with work on cue phrases, prosody, and rejection and confirmation. Reinforcement learning research turned quickly to the more sophisticated POMDP models (Roy et al. 2000, Lemon et al. 2006, Williams and Young 2007) applied to small slotfilling dialogue tasks. Neural reinforcement learning models have been used both for chatbot systems, for example simulating dialogues between two dialogue systems, rewarding good conversational properties like coherence and ease of answering (Li et al., 2016a), and for task-oriented dialogue (Williams et al., 2017). ", "Bloom_type": "application", "question": "What was a significant application of machine learning in dialogue tasks during the early 2000s?", "options": ["Applied reinforcement learning to dialogue", "Developed dialogue state tracking algorithms", "Informed the development of the dialogue-state model", "Created neural reinforcement learning models"], "complexity": 2}]}, "coherence relation": {"max_id": 26, "Questions": [{"id": 0, "context": "the second sentence gives a REASON for Jane`s action in the first sentence. Structured relationships like REASON that hold between text units are called coherence relations, and coherent discourses are structured by many such coherence relations. Coherence relations are introduced in Section 24.1. ", "Bloom_type": "remember", "question": "In which section of the text is the concept of coherence relations introduced?", "options": ["Section 24", "Section 23", "Section 25", "Section 26"], "complexity": 0}, {"id": 1, "context": "task that requires measuring the quality of a text. For example coherence can help in pedagogical tasks like essay grading or essay quality measurement that are trying to grade how well-written a human essay is (Somasundaran et al. 2014, Feng et al. 2014, Lai and Tetreault 2018). Coherence can also help for summarization; knowing the coherence relationship between sentences can help know how to select information from them. Finally, detecting incoherent text may even play a role in mental health tasks like measuring symptoms of schizophrenia or other kinds of disordered language (Ditman and Kuperberg 2010, Elvevag et al. 2007, Bedi et al. 2015, Iter et al. 2018). ", "Bloom_type": "remember", "question": "In what way can coherence relations improve the accuracy of text analysis?", "options": ["It aids in understanding the structure and flow of ideas within a text.", "It helps in identifying the author's writing style.", "It assists in predicting future events based on past data.", "It enhances the ability to detect plagiarism."], "complexity": 0}, {"id": 2, "context": "Now that we`ve seen examples of coherence, we can see more clearly how a coherence relation can play a role in summarization or information extraction. For example, the nuclei of a text presumably express more important information than the satellites, which might be dropped in a summary. ", "Bloom_type": "remember", "question": "In the context provided, what does the coherence relation help identify within a text?", "options": ["The most important sentences", "The least significant sentences", "The overall structure of the paragraph", "The main ideas expressed by the author"], "complexity": 0}, {"id": 3, "context": "Given a sequence of sentences, how can we automatically determine the coherence relations between them? This task is often called discourse parsing (even though for PDTB we are only assigning labels to leaf spans and not building a full parse ", "Bloom_type": "remember", "question": "In discourse parsing, what label is assigned to each sentence?", "options": ["Leaf Span", "Coherence Relation", "Sentence Label", "Parse Tree"], "complexity": 0}, {"id": 4, "context": " Modern neural representation-learning coherence models, beginning with Li et al. (2014), draw on the intuitions of these early unsupervised models for learning sentence representations and measuring how they change between neighboring sentences. But the new models also draw on the idea pioneered by Barzilay and Lapata (2005) of self-supervision. That is, unlike say coherence relation models, which train on hand-labeled representations for RST or PDTB, these models are trained to distinguish natural discourses from unnatural discourses formed by scrambling the order of sentences, thus using representation learning to discover the features that matter for at least the ordering aspect of coherence. ", "Bloom_type": "remember", "question": "In modern neural representation-learning coherence models, what technique do researchers use to measure how sentence representations change between neighboring sentences?", "options": ["Self-supervision", "Coherence relation", "Hand-labeled representations", "Representation learning"], "complexity": 0}, {"id": 5, "context": "Language-model style models are generally evaluated by the methods of Section 24.3.3, although they can also be evaluated on the RST and PDTB coherence relation tasks. ", "Bloom_type": "remember", "question": "In language modeling, what evaluation method is commonly used alongside other coherence relations?", "options": ["Section 24.3.3", "RST", "PDTB", "None of the above"], "complexity": 0}, {"id": 6, "context": " Discourses are not arbitrary collections of sentences; they must be coherent. Among the factors that make a discourse coherent are coherence relations between the sentences, entity-based coherence, and topical coherence. ", "Bloom_type": "remember", "question": "In discourses, what is essential for making them coherent?", "options": ["coherence relations between sentences", "arbitrary collections of sentences", "sentences with no meaning", "random sentence structures"], "complexity": 0}, {"id": 7, "context": "Coherence relations arose from the independent development of a number of scholars, including Hobbs (1979) idea that coherence relations play an inferential role for the hearer, and the investigations by Mann and Thompson (1987) of the discourse structure of large texts. Other approaches to coherence relations and their extraction include Segmented Discourse Representation Theory (SDRT) (Asher and Lascarides 2003, Baldridge et al. 2007) and the Linguistic Discourse Model (Polanyi 1988, Scha and Polanyi 1988, Polanyi et al. 2004). Wolf and Gibson (2005) argue that coherence structure includes crossed bracketings, which make it impossible to represent as a tree, and propose a graph representation instead. A compendium of over 350 relations that have been proposed in the literature can be found in Hovy (1990). ", "Bloom_type": "remember", "question": "In what way did Hobbs contribute to the understanding of coherence relations?", "options": ["He introduced the concept of coherence relations.", "He developed a new method for extracting coherence relations.", "He argued against the use of coherence relations in discourse analysis.", "He focused solely on the structural aspects of coherence relations."], "complexity": 0}, {"id": 8, "context": "Theories of discourse coherence have also been used in algorithms for interpreting discourse-level linguistic phenomena, including verb phrase ellipsis and gapping (Asher 1993, Kehler 1993), and tense interpretation (Lascarides and Asher 1993, Kehler 1994, Kehler 2000). An extensive investigation into the relationship between coherence relations and discourse connectives can be found in Knott and Dale (1994). ", "Bloom_type": "remember", "question": "In which theories of discourse coherence are algorithms primarily applied?", "options": ["Coherence relation theories", "Syntax-based models", "Dependency parsing techniques", "Semantic role labeling methods"], "complexity": 0}, {"id": 9, "context": "the second sentence gives a REASON for Jane`s action in the first sentence. Structured relationships like REASON that hold between text units are called coherence relations, and coherent discourses are structured by many such coherence relations. Coherence relations are introduced in Section 24.1. ", "Bloom_type": "comprehension", "question": "What do coherence relations refer to in the context?", "options": ["The logical connection between ideas expressed in different parts of a discourse.", "The relationship between two sentences within a paragraph.", "The structure of a single sentence.", "The grammatical rules governing sentence formation."], "complexity": 1}, {"id": 10, "context": "task that requires measuring the quality of a text. For example coherence can help in pedagogical tasks like essay grading or essay quality measurement that are trying to grade how well-written a human essay is (Somasundaran et al. 2014, Feng et al. 2014, Lai and Tetreault 2018). Coherence can also help for summarization; knowing the coherence relationship between sentences can help know how to select information from them. Finally, detecting incoherent text may even play a role in mental health tasks like measuring symptoms of schizophrenia or other kinds of disordered language (Ditman and Kuperberg 2010, Elvevag et al. 2007, Bedi et al. 2015, Iter et al. 2018). ", "Bloom_type": "comprehension", "question": "What aspect of coherence relations does it serve in educational contexts?", "options": ["Determining the relevance of sentences within a paragraph", "Measuring the overall structure of paragraphs", "Identifying logical connections between ideas", "Assessing the emotional tone of the text"], "complexity": 1}, {"id": 11, "context": "Now that we`ve seen examples of coherence, we can see more clearly how a coherence relation can play a role in summarization or information extraction. For example, the nuclei of a text presumably express more important information than the satellites, which might be dropped in a summary. ", "Bloom_type": "comprehension", "question": "Explain how coherence relations are utilized in summarization or information extraction?", "options": ["Coherence relations help identify the most important parts of a text by focusing on its nucleus.", "Coherence relations are irrelevant to summarization and information extraction processes.", "Coherence relations determine the order of sentences within a paragraph.", "Coherence relations only apply to narrative texts."], "complexity": 1}, {"id": 12, "context": "Given a sequence of sentences, how can we automatically determine the coherence relations between them? This task is often called discourse parsing (even though for PDTB we are only assigning labels to leaf spans and not building a full parse ", "Bloom_type": "comprehension", "question": "What is the primary goal of discourse parsing when applied to sequences of sentences?", "options": ["To identify logical connections between sentences", "To assign semantic roles to each sentence", "To build a complete syntactic structure", "To classify the genre of the text"], "complexity": 1}, {"id": 13, "context": " Modern neural representation-learning coherence models, beginning with Li et al. (2014), draw on the intuitions of these early unsupervised models for learning sentence representations and measuring how they change between neighboring sentences. But the new models also draw on the idea pioneered by Barzilay and Lapata (2005) of self-supervision. That is, unlike say coherence relation models, which train on hand-labeled representations for RST or PDTB, these models are trained to distinguish natural discourses from unnatural discourses formed by scrambling the order of sentences, thus using representation learning to discover the features that matter for at least the ordering aspect of coherence. ", "Bloom_type": "comprehension", "question": "What distinguishes modern neural representation-learning coherence models from earlier unsupervised models?", "options": ["They use self-supervision techniques.", "They focus solely on RST or PDTB.", "They require hand-labeled data for training.", "They do not learn the ordering aspect of coherence."], "complexity": 1}, {"id": 14, "context": "Language-model style models are generally evaluated by the methods of Section 24.3.3, although they can also be evaluated on the RST and PDTB coherence relation tasks. ", "Bloom_type": "comprehension", "question": "Which evaluation method for language model style models does not explicitly mention coherence relations?", "options": ["Section 24.3.3", "RST", "PDTB", "Both A) and B)"], "complexity": 1}, {"id": 15, "context": " Discourses are not arbitrary collections of sentences; they must be coherent. Among the factors that make a discourse coherent are coherence relations between the sentences, entity-based coherence, and topical coherence. ", "Bloom_type": "comprehension", "question": "What aspect of discourses makes them coherent?", "options": ["Both B) and C)", "Entity-based coherence", "Coherence relations between sentences", "Topical coherence"], "complexity": 1}, {"id": 16, "context": "Coherence relations arose from the independent development of a number of scholars, including Hobbs (1979) idea that coherence relations play an inferential role for the hearer, and the investigations by Mann and Thompson (1987) of the discourse structure of large texts. Other approaches to coherence relations and their extraction include Segmented Discourse Representation Theory (SDRT) (Asher and Lascarides 2003, Baldridge et al. 2007) and the Linguistic Discourse Model (Polanyi 1988, Scha and Polanyi 1988, Polanyi et al. 2004). Wolf and Gibson (2005) argue that coherence structure includes crossed bracketings, which make it impossible to represent as a tree, and propose a graph representation instead. A compendium of over 350 relations that have been proposed in the literature can be found in Hovy (1990). ", "Bloom_type": "comprehension", "question": "What are some methods other than SDRT and LDModel used to extract coherence relations?", "options": ["SDRT and Linguistic Discourse Model", "LDModel and SDRT", "LDModel and Linguistic Discourse Model", "None of the above"], "complexity": 1}, {"id": 17, "context": "Theories of discourse coherence have also been used in algorithms for interpreting discourse-level linguistic phenomena, including verb phrase ellipsis and gapping (Asher 1993, Kehler 1993), and tense interpretation (Lascarides and Asher 1993, Kehler 1994, Kehler 2000). An extensive investigation into the relationship between coherence relations and discourse connectives can be found in Knott and Dale (1994). ", "Bloom_type": "comprehension", "question": "What aspect of discourse analysis does the theory of coherence relations focus on?", "options": ["Coherence relations and discourse connectives", "Verb phrase ellipsis and gapping", "Tense interpretation", "Both A) and B)"], "complexity": 1}, {"id": 18, "context": "the second sentence gives a REASON for Jane`s action in the first sentence. Structured relationships like REASON that hold between text units are called coherence relations, and coherent discourses are structured by many such coherence relations. Coherence relations are introduced in Section 24.1. ", "Bloom_type": "application", "question": "What is the name of the relationship used to connect sentences within a discourse?", "options": ["Coherence Relation", "Logical Reasoning", "Semantic Linkage", "Syntactic Connection"], "complexity": 2}, {"id": 19, "context": "task that requires measuring the quality of a text. For example coherence can help in pedagogical tasks like essay grading or essay quality measurement that are trying to grade how well-written a human essay is (Somasundaran et al. 2014, Feng et al. 2014, Lai and Tetreault 2018). Coherence can also help for summarization; knowing the coherence relationship between sentences can help know how to select information from them. Finally, detecting incoherent text may even play a role in mental health tasks like measuring symptoms of schizophrenia or other kinds of disordered language (Ditman and Kuperberg 2010, Elvevag et al. 2007, Bedi et al. 2015, Iter et al. 2018). ", "Bloom_type": "application", "question": "What aspect of text analysis does coherence relation primarily focus on?", "options": ["Identifying logical connections within the text", "Determining the author's intent", "Counting the number of sentences", "Measuring the emotional tone"], "complexity": 2}, {"id": 20, "context": "Now that we`ve seen examples of coherence, we can see more clearly how a coherence relation can play a role in summarization or information extraction. For example, the nuclei of a text presumably express more important information than the satellites, which might be dropped in a summary. ", "Bloom_type": "application", "question": "In the context of coherence relations, what is typically considered more important for a coherent text?", "options": ["Nuclei", "Satellites", "Both nuclei and satellites equally", "Neither nuclei nor satellites"], "complexity": 2}, {"id": 21, "context": "Given a sequence of sentences, how can we automatically determine the coherence relations between them? This task is often called discourse parsing (even though for PDTB we are only assigning labels to leaf spans and not building a full parse ", "Bloom_type": "application", "question": "What is the first step in determining the coherence relations between sentences?", "options": ["Identify the main topic sentence", "Assign labels to each span", "Build a full parse tree", "Combine all sentences into one long paragraph"], "complexity": 2}, {"id": 22, "context": " Modern neural representation-learning coherence models, beginning with Li et al. (2014), draw on the intuitions of these early unsupervised models for learning sentence representations and measuring how they change between neighboring sentences. But the new models also draw on the idea pioneered by Barzilay and Lapata (2005) of self-supervision. That is, unlike say coherence relation models, which train on hand-labeled representations for RST or PDTB, these models are trained to distinguish natural discourses from unnatural discourses formed by scrambling the order of sentences, thus using representation learning to discover the features that matter for at least the ordering aspect of coherence. ", "Bloom_type": "application", "question": "What distinguishes modern neural representation-learning coherence models from earlier unsupervised models?", "options": ["They learn features specific to the ordering aspect of coherence.", "They rely solely on hand-labeled data.", "They focus only on the semantic content of sentences.", "They ignore the syntactic structure of sentences."], "complexity": 2}, {"id": 23, "context": "Language-model style models are generally evaluated by the methods of Section 24.3.3, although they can also be evaluated on the RST and PDTB coherence relation tasks. ", "Bloom_type": "application", "question": "Which evaluation method is commonly used for language model style models?", "options": ["Both Section 24.3.3 methods and RST and PDTB coherence relation tasks", "Section 24.3.3 methods only", "RST and PDTB coherence relation tasks", "None of the above"], "complexity": 2}, {"id": 24, "context": " Discourses are not arbitrary collections of sentences; they must be coherent. Among the factors that make a discourse coherent are coherence relations between the sentences, entity-based coherence, and topical coherence. ", "Bloom_type": "application", "question": "What is an example of a coherence relation?", "options": ["Temporal coherence", "Entity-based coherence", "Topical coherence", "Logical coherence"], "complexity": 2}, {"id": 25, "context": "Coherence relations arose from the independent development of a number of scholars, including Hobbs (1979) idea that coherence relations play an inferential role for the hearer, and the investigations by Mann and Thompson (1987) of the discourse structure of large texts. Other approaches to coherence relations and their extraction include Segmented Discourse Representation Theory (SDRT) (Asher and Lascarides 2003, Baldridge et al. 2007) and the Linguistic Discourse Model (Polanyi 1988, Scha and Polanyi 1988, Polanyi et al. 2004). Wolf and Gibson (2005) argue that coherence structure includes crossed bracketings, which make it impossible to represent as a tree, and propose a graph representation instead. A compendium of over 350 relations that have been proposed in the literature can be found in Hovy (1990). ", "Bloom_type": "application", "question": "Which approach does not directly involve the extraction of coherence relations?", "options": ["Segmented Discourse Representation Theory (SDRT)", "Hobbs' idea about coherence relations playing an inferential role", "Mann and Thompson's investigation of discourse structure", "Wolf and Gibson's argument on crossing bracketings"], "complexity": 2}, {"id": 26, "context": "Theories of discourse coherence have also been used in algorithms for interpreting discourse-level linguistic phenomena, including verb phrase ellipsis and gapping (Asher 1993, Kehler 1993), and tense interpretation (Lascarides and Asher 1993, Kehler 1994, Kehler 2000). An extensive investigation into the relationship between coherence relations and discourse connectives can be found in Knott and Dale (1994). ", "Bloom_type": "application", "question": "What is an important aspect of understanding discourse coherence?", "options": ["The examination of coherence relations and their connection with discourse connectives.", "The study of verb phrase ellipsis and gapping only.", "The analysis of tense interpretation alone.", "The research on theories of discourse coherence exclusively."], "complexity": 2}]}, "target word": {"max_id": 20, "Questions": [{"id": 0, "context": "To measure similarity between two target words v and w, we need a metric that takes two vectors (of the same dimensionality, either both with words as dimensions, ) and gives hence of length | a measure of their similarity. By far the most common similarity metric is the cosine of the angle between the vectors. ", "Bloom_type": "remember", "question": "In measuring the similarity between two target words v and w, what type of vector operation is primarily used?", "options": ["Angle measurement", "Dot product", "Cross product", "Magnitude calculation"], "complexity": 0}, {"id": 1, "context": "Either the PPMI model or the tf-idf model can be used to compute word similarity, for tasks like finding word paraphrases, tracking changes in word meaning, or automatically discovering meanings of words in different corpora. For example, we can find the 10 most similar words to any target word w by computing the cosines between w and each of the V ", "Bloom_type": "remember", "question": "Which method is commonly used to calculate word similarity?", "options": ["tf-idf model", "PPMI model", "WordNet", "Latent Semantic Analysis"], "complexity": 0}, {"id": 2, "context": "1. Treat the target word and a neighboring context word as positive examples. 2. Randomly sample other words in the lexicon to get negative samples. 3. Use logistic regression to train a classifier to distinguish those two cases. 4. Use the learned weights as the embeddings. ", "Bloom_type": "remember", "question": "In the process of training an embedding model, what is used to classify whether a word should be considered positive or negative example?", "options": ["Logistic regression", "Neural networks", "Random sampling", "Lexicon"], "complexity": 0}, {"id": 3, "context": "In summary, skip-gram trains a probabilistic classifier that, given a test target word w and its context window of L words c1:L, assigns a probability based on how similar this context window is to the target word. The probability is based on applying the logistic (sigmoid) function to the dot product of the embeddings of the target word with each context word. To compute this probability, we just need embeddings for each target word and context word in the vocabulary. ", "Bloom_type": "remember", "question": "In skip-gram training, what does the logistic (sigmoid) function do?", "options": ["It calculates the similarity between two words.", "It converts probabilities into binary values.", "It normalizes the embedding vectors.", "It computes the cosine similarity between vectors."], "complexity": 0}, {"id": 4, "context": "For training a binary classifier we also need negative examples. In fact skipgram with negative sampling (SGNS) uses more negative examples than positive examples (with the ratio between them set by a parameter k). So for each of these (w, cpos) training instances we`ll create k negative samples, each consisting of the target w plus a noise word` cneg. A noise word is a random word from the lexicon, constrained not to be the target word w. The right above shows the setting where k = 2, so we`ll have 2 negative examples in the negative training set for each positive example w, cpos. ", "Bloom_type": "remember", "question": "In SGNS, how many negative examples are created for each positive example?", "options": ["2", "1", "3", "4"], "complexity": 0}, {"id": 5, "context": "Nonetheless it is useful to have intrinsic evaluations. The most common metric is to test their performance on similarity, computing the correlation between an algorithm`s word similarity scores and word similarity ratings assigned by humans. WordSim-353 (Finkelstein et al., 2002) is a commonly used set of ratings from 0 to 10 for 353 noun pairs; for example (plane, car) had an average score of 5.77. SimLex-999 (Hill et al., 2015) is a more complex dataset that quantifies similarity (cup, mug) rather than relatedness (cup, coffee), and includes concrete and abstract adjective, noun and verb pairs. The TOEFL dataset is a set of 80 questions, each consisting of a target word with 4 additional word choices; the task is to choose which is the correct synonym, as in the example: Levied is closest in meaning to: imposed, believed, requested, correlated (Landauer and Dumais, 1997). All of these datasets present words without context. ", "Bloom_type": "remember", "question": "In what way are word similarity metrics typically evaluated?", "options": ["By comparing algorithms' word similarity scores against human ratings", "By measuring the distance between two words", "By analyzing the frequency of word usage", "By determining synonyms based on part-of-speech tagging"], "complexity": 0}, {"id": 6, "context": "The decoder is essentially a conditional language model that attends to the encoder representation and generates the target words one by one, at each timestep conditioning on the source sentence and the previously generated target language words to generate a token. Decoding can use any of the decoding methods discussed in Chapter 9 like greedy, or temperature or nucleus sampling. But the most common decoding algorithm for MT is the beam search algorithm that we`ll introduce in Section 13.4. ", "Bloom_type": "remember", "question": "In machine translation (MT), what method is commonly used for decoding?", "options": ["Beam search", "Greedy decoding", "Temperature sampling", "Nucleus sampling"], "complexity": 0}, {"id": 7, "context": "To measure similarity between two target words v and w, we need a metric that takes two vectors (of the same dimensionality, either both with words as dimensions, ) and gives hence of length | a measure of their similarity. By far the most common similarity metric is the cosine of the angle between the vectors. ", "Bloom_type": "comprehension", "question": "What is the most commonly used similarity metric for measuring the similarity between two target words?", "options": ["Cosine similarity", "Euclidean distance", "Jaccard index", "Hamming distance"], "complexity": 1}, {"id": 8, "context": "Either the PPMI model or the tf-idf model can be used to compute word similarity, for tasks like finding word paraphrases, tracking changes in word meaning, or automatically discovering meanings of words in different corpora. For example, we can find the 10 most similar words to any target word w by computing the cosines between w and each of the V ", "Bloom_type": "comprehension", "question": "Which method can be used to determine the similarity between two words?", "options": ["Both A) and B)", "PPMI model", "tf-idf model", "Neither A) nor B)"], "complexity": 1}, {"id": 9, "context": "1. Treat the target word and a neighboring context word as positive examples. 2. Randomly sample other words in the lexicon to get negative samples. 3. Use logistic regression to train a classifier to distinguish those two cases. 4. Use the learned weights as the embeddings. ", "Bloom_type": "comprehension", "question": "What method is used to create embeddings for words based on their usage patterns?", "options": ["Logistic Regression", "TF-IDF", "Word2Vec", "Neural Networks"], "complexity": 1}, {"id": 10, "context": "In summary, skip-gram trains a probabilistic classifier that, given a test target word w and its context window of L words c1:L, assigns a probability based on how similar this context window is to the target word. The probability is based on applying the logistic (sigmoid) function to the dot product of the embeddings of the target word with each context word. To compute this probability, we just need embeddings for each target word and context word in the vocabulary. ", "Bloom_type": "comprehension", "question": "What does the skip-gram model use to assign probabilities to target words?", "options": ["The softmax function applied to the dot product of the target word embedding with the context word embeddings.", "The cosine similarity between the target word embedding and all other word embeddings.", "The dot product of the target word embedding with the sum of all context word embeddings.", "The Euclidean distance between the target word embedding and the mean of all context word embeddings."], "complexity": 1}, {"id": 11, "context": "For training a binary classifier we also need negative examples. In fact skipgram with negative sampling (SGNS) uses more negative examples than positive examples (with the ratio between them set by a parameter k). So for each of these (w, cpos) training instances we`ll create k negative samples, each consisting of the target w plus a noise word` cneg. A noise word is a random word from the lexicon, constrained not to be the target word w. The right above shows the setting where k = 2, so we`ll have 2 negative examples in the negative training set for each positive example w, cpos. ", "Bloom_type": "comprehension", "question": "Explain how SGNS creates negative examples for training a binary classifier?", "options": ["SGNS creates k negative examples for each positive example using noise words.", "SGNS only uses positive examples because they are easier to find.", "SGNS randomly selects one negative example for each positive example.", "SGNS does not use any negative examples."], "complexity": 1}, {"id": 12, "context": "Nonetheless it is useful to have intrinsic evaluations. The most common metric is to test their performance on similarity, computing the correlation between an algorithm`s word similarity scores and word similarity ratings assigned by humans. WordSim-353 (Finkelstein et al., 2002) is a commonly used set of ratings from 0 to 10 for 353 noun pairs; for example (plane, car) had an average score of 5.77. SimLex-999 (Hill et al., 2015) is a more complex dataset that quantifies similarity (cup, mug) rather than relatedness (cup, coffee), and includes concrete and abstract adjective, noun and verb pairs. The TOEFL dataset is a set of 80 questions, each consisting of a target word with 4 additional word choices; the task is to choose which is the correct synonym, as in the example: Levied is closest in meaning to: imposed, believed, requested, correlated (Landauer and Dumais, 1997). All of these datasets present words without context. ", "Bloom_type": "comprehension", "question": "What are some examples of datasets used for testing word similarity?", "options": ["WordSim-353, SimLex-999, TOEFL, other datasets", "WordSim-353, SimLex-999, TOEFL", "WordSim-353, SimLex-999, other datasets", "WordSim-353, SimLex-999, TOEFL, other datasets, but NOT all of them"], "complexity": 1}, {"id": 13, "context": "The decoder is essentially a conditional language model that attends to the encoder representation and generates the target words one by one, at each timestep conditioning on the source sentence and the previously generated target language words to generate a token. Decoding can use any of the decoding methods discussed in Chapter 9 like greedy, or temperature or nucleus sampling. But the most common decoding algorithm for MT is the beam search algorithm that we`ll introduce in Section 13.4. ", "Bloom_type": "comprehension", "question": "What is the primary method used for machine translation (MT) decoding?", "options": ["Beam search algorithm", "Greedy decoding", "Temperature sampling", "Nucleus sampling"], "complexity": 1}, {"id": 14, "context": "To measure similarity between two target words v and w, we need a metric that takes two vectors (of the same dimensionality, either both with words as dimensions, ) and gives hence of length | a measure of their similarity. By far the most common similarity metric is the cosine of the angle between the vectors. ", "Bloom_type": "application", "question": "What does the cosine of the angle between two vectors represent?", "options": ["The ratio of the dot product of the vectors to the product of their magnitudes", "The absolute difference between the vectors", "The sum of the elements of the vectors", "The maximum value of the vectors"], "complexity": 2}, {"id": 15, "context": "Either the PPMI model or the tf-idf model can be used to compute word similarity, for tasks like finding word paraphrases, tracking changes in word meaning, or automatically discovering meanings of words in different corpora. For example, we can find the 10 most similar words to any target word w by computing the cosines between w and each of the V ", "Bloom_type": "application", "question": "Which method is commonly used to calculate word similarity?", "options": ["Both PPMI and tf-idf models are used", "PPMI model only", "tf-idf model only", "Neither PPMI nor tf-idf models are used"], "complexity": 2}, {"id": 16, "context": "1. Treat the target word and a neighboring context word as positive examples. 2. Randomly sample other words in the lexicon to get negative samples. 3. Use logistic regression to train a classifier to distinguish those two cases. 4. Use the learned weights as the embeddings. ", "Bloom_type": "application", "question": "Which step involves creating a model to differentiate between positive and negative examples?", "options": ["Use logistic regression to train a classifier to distinguish those two cases.", "Treat the target word and a neighboring context word as positive examples.", "Randomly sample other words in the lexicon to get negative samples.", "Use the learned weights as the embeddings."], "complexity": 2}, {"id": 17, "context": "In summary, skip-gram trains a probabilistic classifier that, given a test target word w and its context window of L words c1:L, assigns a probability based on how similar this context window is to the target word. The probability is based on applying the logistic (sigmoid) function to the dot product of the embeddings of the target word with each context word. To compute this probability, we just need embeddings for each target word and context word in the vocabulary. ", "Bloom_type": "application", "question": "What mathematical operation is used to calculate the similarity between the target word and the context words?", "options": ["Dot Product", "Addition", "Subtraction", "Multiplication"], "complexity": 2}, {"id": 18, "context": "For training a binary classifier we also need negative examples. In fact skipgram with negative sampling (SGNS) uses more negative examples than positive examples (with the ratio between them set by a parameter k). So for each of these (w, cpos) training instances we`ll create k negative samples, each consisting of the target w plus a noise word` cneg. A noise word is a random word from the lexicon, constrained not to be the target word w. The right above shows the setting where k = 2, so we`ll have 2 negative examples in the negative training set for each positive example w, cpos. ", "Bloom_type": "application", "question": "In SGNS, how many negative examples are created for each positive example?", "options": ["k=3", "k=4", "k=5", "k=6"], "complexity": 2}, {"id": 19, "context": "Nonetheless it is useful to have intrinsic evaluations. The most common metric is to test their performance on similarity, computing the correlation between an algorithm`s word similarity scores and word similarity ratings assigned by humans. WordSim-353 (Finkelstein et al., 2002) is a commonly used set of ratings from 0 to 10 for 353 noun pairs; for example (plane, car) had an average score of 5.77. SimLex-999 (Hill et al., 2015) is a more complex dataset that quantifies similarity (cup, mug) rather than relatedness (cup, coffee), and includes concrete and abstract adjective, noun and verb pairs. The TOEFL dataset is a set of 80 questions, each consisting of a target word with 4 additional word choices; the task is to choose which is the correct synonym, as in the example: Levied is closest in meaning to: imposed, believed, requested, correlated (Landauer and Dumais, 1997). All of these datasets present words without context. ", "Bloom_type": "application", "question": "Which method should be applied first when evaluating word similarity?", "options": ["Identify synonyms using algorithms", "Compute the correlation between word similarity scores and human ratings", "Assign human ratings to word pairs", "Choose the correct synonym based on context"], "complexity": 2}, {"id": 20, "context": "The decoder is essentially a conditional language model that attends to the encoder representation and generates the target words one by one, at each timestep conditioning on the source sentence and the previously generated target language words to generate a token. Decoding can use any of the decoding methods discussed in Chapter 9 like greedy, or temperature or nucleus sampling. But the most common decoding algorithm for MT is the beam search algorithm that we`ll introduce in Section 13.4. ", "Bloom_type": "application", "question": "What is the primary method used for machine translation (MT) decoding?", "options": ["Beam search", "Greedy decoding", "Temperature sampling", "Nucleus sampling"], "complexity": 2}]}, "temporal expression": {"max_id": 20, "Questions": [{"id": 0, "context": "Next, we discuss event extraction, the task of finding events in which these entities participate, like, in our sample text, the fare increases by United and American and the reporting events said and cite. Events are also situated in time, occurring at a particular date or time, and events can be related temporally, happening before or after or simultaneously with each other. We`ll need to recognize temporal expressions like Friday, Thursday or two days from now and times such as 3:30 P.M., and normalize them onto specific calendar dates or times. We`ll need to link Friday to the time of United`s announcement, Thursday to the previous day`s fare increase, and we`ll need to produce a timeline in which United`s announcement follows the fare increase and American`s announcement follows both of those events. ", "Bloom_type": "remember", "question": "In the discussion about event extraction, what is one way to identify when an event occurs?", "options": ["Recognizing temporal expressions", "Identifying the entity involved", "Determining the cause-effect relationship", "Analyzing the sequence of actions"], "complexity": 0}, {"id": 1, "context": "1. Extracting temporal expressions 2. Normalizing these expressions, by converting them to a standard format. 3. Linking events to times and extracting time graphs and timelines ", "Bloom_type": "remember", "question": "What is the first step in processing temporal expressions?", "options": ["Extracting temporal expressions", "Normalizing these expressions", "Linking events to times", "Extraction of time graphs and timelines"], "complexity": 0}, {"id": 2, "context": "to as the document`s temporal anchor. The values of temporal expressions such as today, yesterday, or tomorrow can all be computed with respect to this temporal anchor. The semantic procedure for today simply assigns the anchor, and the attachments for tomorrow and yesterday add a day and subtract a day from the anchor, respectively. Of course, given the cyclic nature of our representations for months, weeks, days, and times of day, our temporal arithmetic procedures must use modulo arithmetic appropriate to the time unit being used. ", "Bloom_type": "remember", "question": "In what way are today, yesterday, and tomorrow represented within the system?", "options": ["They are derived using modulo arithmetic.", "They are directly stored in the database.", "They are calculated based on the current date.", "They are predefined constants."], "complexity": 0}, {"id": 3, "context": "Relative temporal expressions are handled with temporal arithmetic similar to that used for today and yesterday. The document date indicates that our example article is ISO week 27, so the expression last week normalizes to the current week minus 1. To resolve ambiguous next and last expressions we consider the distance from the anchoring date to the nearest unit. Next Friday can refer either to the immediately next Friday or to the Friday following that, but the closer the document date is to a Friday, the more likely it is that the phrase will skip the nearest one. Such ambiguities are handled by encoding language and domain-specific heuristics into the temporal attachments. ", "Bloom_type": "remember", "question": "In what way do relative temporal expressions differ from absolute ones?", "options": ["Relative temporal expressions require more complex mathematical calculations.", "Relative temporal expressions use different time units.", "Relative temporal expressions are less precise than absolute ones.", "Relative temporal expressions cannot be resolved ambiguously."], "complexity": 0}, {"id": 4, "context": "The goal of temporal analysis, is to link times to events and then fit all these events into a complete timeline. This ambitious task is the subject of considerable current research but solving it with a high level of accuracy is beyond the capabilities of current systems. A somewhat simpler, but still useful, task is to impose a partial ordering on the events and temporal expressions mentioned in a text. Such an ordering can provide many of the same benefits as a true timeline. An example of such a partial ordering is the determination that the fare increase by American Airlines came after the fare increase by United in our sample text. Determining such an ordering can be viewed as a binary relation detection and classification task. ", "Bloom_type": "remember", "question": "In temporal analysis, what does imposing a partial ordering on events and temporal expressions achieve?", "options": ["It helps in understanding the sequence of events.", "It provides a detailed timeline of events.", "It simplifies complex timelines for easier comprehension.", "It allows for accurate prediction of future events."], "complexity": 0}, {"id": 5, "context": "Progress in this area continues to be stimulated by formal evaluations with shared benchmark datasets, including the Automatic Content Extraction (ACE) evaluations of 2000-2007 on named entity recognition, relation extraction, and temporal expressions1, the KBP (Knowledge Base Population) evaluations (Ji et al. 2010, Surdeanu 2013) of relation extraction tasks like slot filling (extracting attributes (slots`) like age, birthplace, and spouse for a given entity) and a series of SemEval workshops (Hendrickx et al., 2009). ", "Bloom_type": "remember", "question": "In which year did the first KBP evaluation on relation extraction tasks start?", "options": ["2010", "2000", "2005", "2013"], "complexity": 0}, {"id": 6, "context": "20.3 A useful functionality in newer email and calendar applications is the ability to associate temporal expressions connected with events in email (doctor`s appointments, meeting planning, party invitations, etc.) with specific calendar entries. Collect a corpus of email containing temporal expressions related to event planning. How do these expressions compare to the kinds of expressions commonly found in news text that we`ve been discussing in this chapter? ", "Bloom_type": "remember", "question": "In what type of applications are temporal expressions most commonly used?", "options": ["Email and calendar applications", "Social media platforms", "Financial reports", "Scientific research papers"], "complexity": 0}, {"id": 7, "context": "Next, we discuss event extraction, the task of finding events in which these entities participate, like, in our sample text, the fare increases by United and American and the reporting events said and cite. Events are also situated in time, occurring at a particular date or time, and events can be related temporally, happening before or after or simultaneously with each other. We`ll need to recognize temporal expressions like Friday, Thursday or two days from now and times such as 3:30 P.M., and normalize them onto specific calendar dates or times. We`ll need to link Friday to the time of United`s announcement, Thursday to the previous day`s fare increase, and we`ll need to produce a timeline in which United`s announcement follows the fare increase and American`s announcement follows both of those events. ", "Bloom_type": "comprehension", "question": "What does recognizing temporal expressions entail in the context of event extraction?", "options": ["Identifying when an event occurs relative to another event", "Determining the exact location of an event within a document", "Finding all occurrences of a single entity in a text", "Counting the frequency of different types of events"], "complexity": 1}, {"id": 8, "context": "1. Extracting temporal expressions 2. Normalizing these expressions, by converting them to a standard format. 3. Linking events to times and extracting time graphs and timelines ", "Bloom_type": "comprehension", "question": "What are the three primary steps involved in processing temporal expressions?", "options": ["Extracting temporal expressions, normalizing these expressions, linking events to times", "Normalizing temporal expressions, linking events to times, creating time graphs and timelines", "Linking events to times, extracting temporal expressions, creating time graphs and timelines", "Creating time graphs and timelines, extracting temporal expressions, normalizing temporal expressions"], "complexity": 1}, {"id": 9, "context": "to as the document`s temporal anchor. The values of temporal expressions such as today, yesterday, or tomorrow can all be computed with respect to this temporal anchor. The semantic procedure for today simply assigns the anchor, and the attachments for tomorrow and yesterday add a day and subtract a day from the anchor, respectively. Of course, given the cyclic nature of our representations for months, weeks, days, and times of day, our temporal arithmetic procedures must use modulo arithmetic appropriate to the time unit being used. ", "Bloom_type": "comprehension", "question": "What does the document's temporal anchor do?", "options": ["It provides a reference point for calculating the value of temporal expressions.", "It determines the specific date and time within the document.", "It specifies the exact location where the document was created.", "It sets the starting point for all calculations involving dates."], "complexity": 1}, {"id": 10, "context": "Relative temporal expressions are handled with temporal arithmetic similar to that used for today and yesterday. The document date indicates that our example article is ISO week 27, so the expression last week normalizes to the current week minus 1. To resolve ambiguous next and last expressions we consider the distance from the anchoring date to the nearest unit. Next Friday can refer either to the immediately next Friday or to the Friday following that, but the closer the document date is to a Friday, the more likely it is that the phrase will skip the nearest one. Such ambiguities are handled by encoding language and domain-specific heuristics into the temporal attachments. ", "Bloom_type": "comprehension", "question": "How do relative temporal expressions like 'last week' and 'next Friday' get resolved when dealing with different dates?", "options": ["The closest possible meaning is chosen based on the proximity of the document date to Fridays.", "Relative temporal expressions are always interpreted literally.", "Ambiguities are ignored entirely.", "Temporal expressions are never ambiguous."], "complexity": 1}, {"id": 11, "context": "The goal of temporal analysis, is to link times to events and then fit all these events into a complete timeline. This ambitious task is the subject of considerable current research but solving it with a high level of accuracy is beyond the capabilities of current systems. A somewhat simpler, but still useful, task is to impose a partial ordering on the events and temporal expressions mentioned in a text. Such an ordering can provide many of the same benefits as a true timeline. An example of such a partial ordering is the determination that the fare increase by American Airlines came after the fare increase by United in our sample text. Determining such an ordering can be viewed as a binary relation detection and classification task. ", "Bloom_type": "comprehension", "question": "What is one simple task related to imposing a partial ordering on events and temporal expressions?", "options": ["Determining if a time event occurs before another", "Identifying the exact duration of each event", "Creating a detailed timeline for every text", "Finding the most common occurrence of events"], "complexity": 1}, {"id": 12, "context": "Progress in this area continues to be stimulated by formal evaluations with shared benchmark datasets, including the Automatic Content Extraction (ACE) evaluations of 2000-2007 on named entity recognition, relation extraction, and temporal expressions1, the KBP (Knowledge Base Population) evaluations (Ji et al. 2010, Surdeanu 2013) of relation extraction tasks like slot filling (extracting attributes (slots`) like age, birthplace, and spouse for a given entity) and a series of SemEval workshops (Hendrickx et al., 2009). ", "Bloom_type": "comprehension", "question": "What type of evaluation has been most frequently conducted in the field of temporal expressions?", "options": ["Automatic content extraction evaluations", "Knowledge base population evaluations", "SemEval workshops", "Named entity recognition evaluations"], "complexity": 1}, {"id": 13, "context": "20.3 A useful functionality in newer email and calendar applications is the ability to associate temporal expressions connected with events in email (doctor`s appointments, meeting planning, party invitations, etc.) with specific calendar entries. Collect a corpus of email containing temporal expressions related to event planning. How do these expressions compare to the kinds of expressions commonly found in news text that we`ve been discussing in this chapter? ", "Bloom_type": "comprehension", "question": "How are temporal expressions different from common expressions found in news text?", "options": ["Temporal expressions are more frequently used in news text.", "Temporal expressions are more complex than news text expressions.", "News text expressions are more formal than temporal expressions.", "Temporal expressions are less frequent in news text."], "complexity": 1}, {"id": 14, "context": "Next, we discuss event extraction, the task of finding events in which these entities participate, like, in our sample text, the fare increases by United and American and the reporting events said and cite. Events are also situated in time, occurring at a particular date or time, and events can be related temporally, happening before or after or simultaneously with each other. We`ll need to recognize temporal expressions like Friday, Thursday or two days from now and times such as 3:30 P.M., and normalize them onto specific calendar dates or times. We`ll need to link Friday to the time of United`s announcement, Thursday to the previous day`s fare increase, and we`ll need to produce a timeline in which United`s announcement follows the fare increase and American`s announcement follows both of those events. ", "Bloom_type": "application", "question": "What is the first step in recognizing temporal expressions in a text?", "options": ["Find the occurrences of specific times and dates", "Identify the entities involved in the events", "Link different events together based on their timing", "Normalize all temporal expressions to specific calendar dates"], "complexity": 2}, {"id": 15, "context": "1. Extracting temporal expressions 2. Normalizing these expressions, by converting them to a standard format. 3. Linking events to times and extracting time graphs and timelines ", "Bloom_type": "application", "question": "What is the first step in processing temporal expressions?", "options": ["Extracting temporal expressions", "Normalizing these expressions", "Linking events to times", "Extraction of time graphs and timelines"], "complexity": 2}, {"id": 16, "context": "to as the document`s temporal anchor. The values of temporal expressions such as today, yesterday, or tomorrow can all be computed with respect to this temporal anchor. The semantic procedure for today simply assigns the anchor, and the attachments for tomorrow and yesterday add a day and subtract a day from the anchor, respectively. Of course, given the cyclic nature of our representations for months, weeks, days, and times of day, our temporal arithmetic procedures must use modulo arithmetic appropriate to the time unit being used. ", "Bloom_type": "application", "question": "What is the first step in computing the value of a temporal expression?", "options": ["Assign the temporal anchor directly to the expression", "Add a day to the temporal anchor", "Subtract a day from the temporal anchor", "Compute the modulo arithmetic based on the time unit"], "complexity": 2}, {"id": 17, "context": "Relative temporal expressions are handled with temporal arithmetic similar to that used for today and yesterday. The document date indicates that our example article is ISO week 27, so the expression last week normalizes to the current week minus 1. To resolve ambiguous next and last expressions we consider the distance from the anchoring date to the nearest unit. Next Friday can refer either to the immediately next Friday or to the Friday following that, but the closer the document date is to a Friday, the more likely it is that the phrase will skip the nearest one. Such ambiguities are handled by encoding language and domain-specific heuristics into the temporal attachments. ", "Bloom_type": "application", "question": "In which way does the document handle ambiguous next and last expressions?", "options": ["It applies domain-specific heuristics.", "It considers the closest Friday as the reference point.", "It uses absolute dates instead of relative ones.", "It relies solely on linguistic rules."], "complexity": 2}, {"id": 18, "context": "The goal of temporal analysis, is to link times to events and then fit all these events into a complete timeline. This ambitious task is the subject of considerable current research but solving it with a high level of accuracy is beyond the capabilities of current systems. A somewhat simpler, but still useful, task is to impose a partial ordering on the events and temporal expressions mentioned in a text. Such an ordering can provide many of the same benefits as a true timeline. An example of such a partial ordering is the determination that the fare increase by American Airlines came after the fare increase by United in our sample text. Determining such an ordering can be viewed as a binary relation detection and classification task. ", "Bloom_type": "application", "question": "What is the first step in imposing a partial ordering on events and temporal expressions?", "options": ["Identify the time periods when each event occurs", "Classify the type of temporal expression used in the text", "Find the common elements between different temporal expressions", "Determine which temporal expressions come before others"], "complexity": 2}, {"id": 19, "context": "Progress in this area continues to be stimulated by formal evaluations with shared benchmark datasets, including the Automatic Content Extraction (ACE) evaluations of 2000-2007 on named entity recognition, relation extraction, and temporal expressions1, the KBP (Knowledge Base Population) evaluations (Ji et al. 2010, Surdeanu 2013) of relation extraction tasks like slot filling (extracting attributes (slots`) like age, birthplace, and spouse for a given entity) and a series of SemEval workshops (Hendrickx et al., 2009). ", "Bloom_type": "application", "question": "What is an example of a formal evaluation used to stimulate progress in the field?", "options": ["All of the above", "KBP evaluations on slot filling tasks", "Automatic Content Extraction (ACE) evaluations on named entity recognition", "SemEval workshops on relation extraction"], "complexity": 2}, {"id": 20, "context": "20.3 A useful functionality in newer email and calendar applications is the ability to associate temporal expressions connected with events in email (doctor`s appointments, meeting planning, party invitations, etc.) with specific calendar entries. Collect a corpus of email containing temporal expressions related to event planning. How do these expressions compare to the kinds of expressions commonly found in news text that we`ve been discussing in this chapter? ", "Bloom_type": "application", "question": "What is an example of a temporal expression used in email for event planning?", "options": ["I will meet John at 3 PM on Friday.", "The weather forecast for tomorrow.", "It was raining heavily last night.", "We are expecting a snowstorm next week."], "complexity": 2}]}, "parse tree": {"max_id": 41, "Questions": [{"id": 0, "context": "In the early years, the space of MT architectures spanned three general models. In direct translation, the system proceeds word-by-word through the sourcelanguage text, translating each word incrementally. Direct translation uses a large bilingual dictionary, each of whose entries is a small program with the job of translating one word. In transfer approaches, we first parse the input text and then apply rules to transform the source-language parse into a target language parse. We then generate the target language sentence from the parse tree. In interlingua approaches, we analyze the source language text into some abstract meaning representation, called an interlingua. We then generate into the target language from this interlingual representation. A common way to visualize these three early approaches was the Vauquois triangle shown in Fig. 13.13. The triangle shows the increasing depth of analysis required (on both the analysis and generation end) as we move from the direct approach through transfer approaches to interlingual approaches. In addition, it shows the decreasing amount of transfer knowledge needed as we move up the triangle, from huge amounts of transfer at the direct level (almost all knowledge is transfer knowledge for each word) through transfer (transfer rules only for parse trees or thematic roles) through interlingua (no specific transfer knowledge). We can view the encoder-decoder network as an interlingual approach, with attention acting as an integration of direct and transfer, allowing words or their representations to be directly accessed by the decoder. ", "Bloom_type": "remember", "question": "In which type of approach do we first parse the input text before applying rules to transform the source-language parse into a target language parse?", "options": ["Transfer approaches", "Direct translation", "Interlingua approaches", "Encoder-decoder networks"], "complexity": 0}, {"id": 1, "context": "Our focus in this chapter is context-free grammars and the CKY algorithm for parsing them. Context-free grammars are the backbone of many formal models of the syntax of natural language (and, for that matter, of computer languages). Syntactic parsing is the task of assigning a syntactic structure to a sentence. Parse trees (whether for context-free grammars or for the dependency or CCG formalisms we introduce in following chapters) can be used in applications such as grammar checking: sentence that cannot be parsed may have grammatical errors (or at least be hard to read). Parse trees can be an intermediate stage of representation for formal semantic analysis. And parsers and the grammatical structure they assign a sentence are a useful text analysis tool for text data science applications that require modeling the relationship of elements in sentences. ", "Bloom_type": "remember", "question": "In the context of formal models of syntax, what does the term \"parse tree\" refer to?", "options": ["A diagram showing the hierarchical structure of a sentence based on rules of grammar", "A visual representation of a sentence\u2019s parts of speech", "The process of breaking down a sentence into its constituent parts", "An algorithm used to check the grammaticality of sentences"], "complexity": 0}, {"id": 2, "context": "We say the string a flight can be derived from the non-terminal NP. Thus, a CFG can be used to generate a set of strings. This sequence of rule expansions is called a derivation of the string of words. It is common to represent a derivation by a parse tree (commonly shown inverted with the root at the top). Figure 18.1 shows the tree representation of this derivation. ", "Bloom_type": "remember", "question": "In the context provided, what does the term \"parse tree\" refer to?", "options": ["A diagram showing the structure of a sentence", "A visual representation of a grammar rules", "A method for generating sentences using a computer program", "A chart indicating how a word breaks down into smaller parts"], "complexity": 0}, {"id": 3, "context": "In the parse tree shown in Fig. 18.1, we can say that the node NP dominates all the nodes in the tree (Det, Nom, Noun, a, flight). We can say further that it immediately dominates the nodes Det and Nom. ", "Bloom_type": "remember", "question": "In the parse tree shown in Fig. 18.1, what does the node NP dominate?", "options": ["Nouns", "Verbs", "Adjectives", "Prepositions"], "complexity": 0}, {"id": 4, "context": "I), and a random expansion of VP (let`s say, to Verb NP), and so on until we generate the string I prefer a morning flight. Figure 18.4 shows a parse tree that represents a complete derivation of I prefer a morning flight. ", "Bloom_type": "remember", "question": "In the context provided, what is represented by a parse tree?", "options": ["Sentence structure", "Word order", "Phonetic transcription", "Grammar rules"], "complexity": 0}, {"id": 5, "context": "A corpus in which every sentence is annotated with a parse tree is called a treebank. ", "Bloom_type": "remember", "question": "What is a collection of sentences where each one has been tagged with a parse tree?", "options": ["Annotated corpus", "Sentence list", "Text bank", "Word dictionary"], "complexity": 0}, {"id": 6, "context": "Dynamic programming provides a powerful framework for addressing the problems caused by ambiguity in grammars. Recall that a dynamic programming approach systematically fills in a table of solutions to subproblems. The complete table has the solution to all the subproblems needed to solve the problem as a whole. In the case of syntactic parsing, these subproblems represent parse trees for all the constituents detected in the input. ", "Bloom_type": "remember", "question": "In syntactic parsing, what do the subproblems represented by parse trees correspond to?", "options": ["Subsentences within the sentence", "The entire sentence", "All possible sentences", "Words in the sentence"], "complexity": 0}, {"id": 7, "context": "While the CKY parsing algorithm we`ve seen so far does great at enumerating all the possible parse trees for a sentence, it has a large problem: it doesn`t tell us which parse is the correct one! That is, it doesn`t disambiguate among the possible parses. To solve the disambiguation problem we`ll use a simple neural extension of the CKY algorithm. The intuition of such parsing algorithms (often called span-based constituency parsing, or neural CKY), is to train a neural classifier to assign a score to each constituent, and then use a modified version of CKY to combine these constituent scores to find the best-scoring parse tree. ", "Bloom_type": "remember", "question": "In neural CKY parsing, what is used to determine the most likely parse tree?", "options": ["The score assigned by the neural classifier", "The length of the sentence", "The complexity of the grammar", "The accuracy of the neural network model"], "complexity": 0}, {"id": 8, "context": "The standard tool for evaluating parsers that assign a single parse tree to a sentence is the PARSEVAL metrics (Black et al., 1991). The PARSEVAL metric measures how much the constituents in the hypothesis parse tree look like the constituents in a hand-labeled, reference parse. PARSEVAL thus requires a human-labeled reference (or gold standard) parse tree for each sentence in the test set; we generally draw these reference parses from a treebank like the Penn Treebank. ", "Bloom_type": "remember", "question": "In the context of parsing sentences, what does the PARSEVAL metric primarily evaluate?", "options": ["The syntactic structure of the sentence", "The grammatical correctness of the sentence", "The semantic meaning of the sentence", "The readability of the sentence"], "complexity": 0}, {"id": 9, "context": "In one simple model of lexical heads, each context-free rule is associated with a head (Charniak 1997, Collins 1999). The head is the word in the phrase that is grammatically the most important. Heads are passed up the parse tree; thus, each non-terminal in a parse tree is annotated with a single word, which is its lexical head. Figure 18.16 shows an example of such a tree from Collins (1999), in which each non-terminal is annotated with its head. ", "Bloom_type": "remember", "question": "In Charniak's model of lexical heads, what does each non-terminal in a parse tree represent?", "options": ["The most important word in the phrase", "The entire sentence", "The root of the tree", "The final output of parsing"], "complexity": 0}, {"id": 10, "context": " Span-based neural constituency parses train a neural classifier to assign a score to each constituent, and then use a modified version of CKY to combine these constituent scores to find the best-scoring parse tree. ", "Bloom_type": "remember", "question": "In span-based neural constituency parsing, what is used to determine the best-scoring parse tree?", "options": ["The accuracy of the constituent scores", "The length of the sentence", "The number of constituents", "The complexity of the sentence"], "complexity": 0}, {"id": 11, "context": "The main reason computational systems use semantic roles is to act as a shallow meaning representation that can let us make simple inferences that aren`t possible from the pure surface string of words, or even from the parse tree. To extend the earlier examples, if a document says that Company A acquired Company B, we`d like to know that this answers the query Was Company B acquired? despite the fact that the two sentences have very different surface syntax. Similarly, this shallow semantics might act as a useful intermediate language in machine translation. ", "Bloom_type": "remember", "question": "In what way does using semantic roles help computational systems?", "options": ["It helps computers infer meanings from raw data.", "It simplifies the process of parsing complex sentences.", "It allows for deeper understanding of sentence structure beyond just word order.", "It enables machines to translate languages more accurately."], "complexity": 0}, {"id": 12, "context": "For each of these predicates, the algorithm examines each node in the parse tree and uses supervised classification to decide the semantic role (if any) it plays for this predicate. Given a labeled training set such as PropBank or FrameNet, a feature vector is extracted for each node, using feature templates described in the next subsection. A 1-of-N classifier is then trained to predict a semantic role for each constituent given these features, where N is the number of potential semantic roles plus an extra NONE role for non-role constituents. Any standard classification algorithms can be used. Finally, for each test sentence to be labeled, the classifier is run on each relevant constituent. ", "Bloom_type": "remember", "question": "In the process of labeling sentences with semantic roles, what type of machine learning model is primarily used?", "options": ["Support Vector Machines", "Neural networks", "Decision Trees", "Random Forests"], "complexity": 0}, {"id": 13, "context": "Although the idea of semantic roles dates back to Pan. ini, they were re-introduced into modern linguistics by Gruber (1965), Fillmore (1966) and Fillmore (1968). Fillmore had become interested in argument structure by studying Lucien Tesni`ere`s groundbreaking Elements de Syntaxe Structurale (Tesni`ere, 1959) in which the term dependency` was introduced and the foundations were laid for dependency grammar. Following Tesni`ere`s terminology, Fillmore first referred to argument roles as actants (Fillmore, 1966) but quickly switched to the term case, (see Fillmore (2003)) and proposed a universal list of semantic roles or cases (Agent, Patient, Instrument, etc.), that could be taken on by the arguments of predicates. Verbs would be listed in the lexicon with their case frame, the list of obligatory (or optional) case arguments. The idea that semantic roles could provide an intermediate level of semantic representation that could help map from syntactic parse structures to deeper, more fully-specified representations of meaning was quickly adopted in natural language processing, and systems for extracting case frames were created for machine translation (Wilks, 1973), question-answering (Hendrix et al., 1973), spoken-language processing (Nash-Webber, 1975), and dialogue systems (Bobrow et al., 1977). Generalpurpose semantic role labelers were developed. The earliest ones (Simmons, 1973) first parsed a sentence by means of an ATN (Augmented Transition Network) parser. Each verb then had a set of rules specifying how the parse should be mapped to semantic roles. These rules mainly made reference to grammatical functions (subject, object, complement of specific prepositions) but also checked constituent internal features such as the animacy of head nouns. Later systems assigned roles from prebuilt parse trees, again by using dictionaries with verb-specific case frames (Levin 1977, Marcus 1980). ", "Bloom_type": "remember", "question": "In what year did Fillmore introduce the concept of semantic roles?", "options": ["1966", "1965", "1967", "1968"], "complexity": 0}, {"id": 14, "context": "In the early years, the space of MT architectures spanned three general models. In direct translation, the system proceeds word-by-word through the sourcelanguage text, translating each word incrementally. Direct translation uses a large bilingual dictionary, each of whose entries is a small program with the job of translating one word. In transfer approaches, we first parse the input text and then apply rules to transform the source-language parse into a target language parse. We then generate the target language sentence from the parse tree. In interlingua approaches, we analyze the source language text into some abstract meaning representation, called an interlingua. We then generate into the target language from this interlingual representation. A common way to visualize these three early approaches was the Vauquois triangle shown in Fig. 13.13. The triangle shows the increasing depth of analysis required (on both the analysis and generation end) as we move from the direct approach through transfer approaches to interlingual approaches. In addition, it shows the decreasing amount of transfer knowledge needed as we move up the triangle, from huge amounts of transfer at the direct level (almost all knowledge is transfer knowledge for each word) through transfer (transfer rules only for parse trees or thematic roles) through interlingua (no specific transfer knowledge). We can view the encoder-decoder network as an interlingual approach, with attention acting as an integration of direct and transfer, allowing words or their representations to be directly accessed by the decoder. ", "Bloom_type": "comprehension", "question": "In which type of approach does the system focus more on generating the target language sentence directly from the parse tree?", "options": ["Encoder-Decoder networks", "Direct translation", "Transfer approaches", "Interlingua approaches"], "complexity": 1}, {"id": 15, "context": "Our focus in this chapter is context-free grammars and the CKY algorithm for parsing them. Context-free grammars are the backbone of many formal models of the syntax of natural language (and, for that matter, of computer languages). Syntactic parsing is the task of assigning a syntactic structure to a sentence. Parse trees (whether for context-free grammars or for the dependency or CCG formalisms we introduce in following chapters) can be used in applications such as grammar checking: sentence that cannot be parsed may have grammatical errors (or at least be hard to read). Parse trees can be an intermediate stage of representation for formal semantic analysis. And parsers and the grammatical structure they assign a sentence are a useful text analysis tool for text data science applications that require modeling the relationship of elements in sentences. ", "Bloom_type": "comprehension", "question": "What is the primary purpose of parse trees in the context of formal models of syntax?", "options": ["To represent the syntactic structure of a sentence", "To check the readability of a sentence", "To analyze the relationship between elements in sentences", "To model the grammatical errors in a sentence"], "complexity": 1}, {"id": 16, "context": "We say the string a flight can be derived from the non-terminal NP. Thus, a CFG can be used to generate a set of strings. This sequence of rule expansions is called a derivation of the string of words. It is common to represent a derivation by a parse tree (commonly shown inverted with the root at the top). Figure 18.1 shows the tree representation of this derivation. ", "Bloom_type": "comprehension", "question": "What does the phrase 'a CFG can be used to generate a set of strings' imply about the relationship between a CFG and its derivations?", "options": ["The CFG only generates specific strings based on the grammar rules.", "The CFG generates all possible strings.", "The CFG cannot generate any strings.", "The CFG generates no strings."], "complexity": 1}, {"id": 17, "context": "In the parse tree shown in Fig. 18.1, we can say that the node NP dominates all the nodes in the tree (Det, Nom, Noun, a, flight). We can say further that it immediately dominates the nodes Det and Nom. ", "Bloom_type": "comprehension", "question": "What does the node NP dominate in the parse tree shown in Fig. 18.1?", "options": ["Nodes Det and Nom", "The entire tree", "Node Noun", "Node flight"], "complexity": 1}, {"id": 18, "context": "I), and a random expansion of VP (let`s say, to Verb NP), and so on until we generate the string I prefer a morning flight. Figure 18.4 shows a parse tree that represents a complete derivation of I prefer a morning flight. ", "Bloom_type": "comprehension", "question": "What does the parse tree represent in this sentence?", "options": ["The structure of the sentence", "The meaning of the sentence", "The grammatical rules", "The pronunciation of the sentence"], "complexity": 1}, {"id": 19, "context": "A corpus in which every sentence is annotated with a parse tree is called a treebank. ", "Bloom_type": "comprehension", "question": "What type of annotation involves assigning a parse tree to each sentence in a corpus?", "options": ["Part-of-speech tagging", "Dependency parsing", "Named entity recognition", "Sentence segmentation"], "complexity": 1}, {"id": 20, "context": "Dynamic programming provides a powerful framework for addressing the problems caused by ambiguity in grammars. Recall that a dynamic programming approach systematically fills in a table of solutions to subproblems. The complete table has the solution to all the subproblems needed to solve the problem as a whole. In the case of syntactic parsing, these subproblems represent parse trees for all the constituents detected in the input. ", "Bloom_type": "comprehension", "question": "What are the subproblems in syntactic parsing that correspond to parse trees?", "options": ["Parse trees for all possible combinations of constituent types", "Parse trees for individual sentences", "Parse trees for each word in the sentence", "Parse trees for every substring within the sentence"], "complexity": 1}, {"id": 21, "context": "While the CKY parsing algorithm we`ve seen so far does great at enumerating all the possible parse trees for a sentence, it has a large problem: it doesn`t tell us which parse is the correct one! That is, it doesn`t disambiguate among the possible parses. To solve the disambiguation problem we`ll use a simple neural extension of the CKY algorithm. The intuition of such parsing algorithms (often called span-based constituency parsing, or neural CKY), is to train a neural classifier to assign a score to each constituent, and then use a modified version of CKY to combine these constituent scores to find the best-scoring parse tree. ", "Bloom_type": "comprehension", "question": "How do span-based constituency parsers address the issue of ambiguity in sentence parsing?", "options": ["By combining constituent scores from a modified CKY algorithm to identify the most probable parse", "By using a neural network to predict the next word in a sentence", "By applying a traditional rule-based approach to sentence structure", "By training a model to classify sentences into different parts of speech"], "complexity": 1}, {"id": 22, "context": "The standard tool for evaluating parsers that assign a single parse tree to a sentence is the PARSEVAL metrics (Black et al., 1991). The PARSEVAL metric measures how much the constituents in the hypothesis parse tree look like the constituents in a hand-labeled, reference parse. PARSEVAL thus requires a human-labeled reference (or gold standard) parse tree for each sentence in the test set; we generally draw these reference parses from a treebank like the Penn Treebank. ", "Bloom_type": "comprehension", "question": "What does the PARSEVAL metric measure when comparing a hypothesis parse tree to a reference parse tree?", "options": ["How well the constituents in the hypothesis match those in the reference", "The length of the sentences", "The complexity of the grammar rules", "The accuracy of the parser implementation"], "complexity": 1}, {"id": 23, "context": "In one simple model of lexical heads, each context-free rule is associated with a head (Charniak 1997, Collins 1999). The head is the word in the phrase that is grammatically the most important. Heads are passed up the parse tree; thus, each non-terminal in a parse tree is annotated with a single word, which is its lexical head. Figure 18.16 shows an example of such a tree from Collins (1999), in which each non-terminal is annotated with its head. ", "Bloom_type": "comprehension", "question": "In the context of parsing, how do heads function within a parse tree?", "options": ["They point to the most grammatically significant word in each phrase.", "They represent the syntactic structure of the sentence.", "They indicate the semantic meaning of the sentence.", "They serve as placeholders for future expansions."], "complexity": 1}, {"id": 24, "context": " Span-based neural constituency parses train a neural classifier to assign a score to each constituent, and then use a modified version of CKY to combine these constituent scores to find the best-scoring parse tree. ", "Bloom_type": "comprehension", "question": "What does the span-based neural constituency parse aim to achieve?", "options": ["To identify the most probable constituent structure for a sentence", "To classify sentences into different categories based on their content", "To extract named entities from the sentence", "To predict the next word in the sentence"], "complexity": 1}, {"id": 25, "context": "The main reason computational systems use semantic roles is to act as a shallow meaning representation that can let us make simple inferences that aren`t possible from the pure surface string of words, or even from the parse tree. To extend the earlier examples, if a document says that Company A acquired Company B, we`d like to know that this answers the query Was Company B acquired? despite the fact that the two sentences have very different surface syntax. Similarly, this shallow semantics might act as a useful intermediate language in machine translation. ", "Bloom_type": "comprehension", "question": "Explain how shallow semantics using parse trees help in making inferences about sentence meanings?", "options": ["Shallow semantics using parse trees simplify sentence analysis by focusing only on syntactic structure.", "Shallow semantics using parse trees allow for deeper understanding of complex sentence structures.", "Parse trees provide a detailed breakdown of word order within sentences.", "Parse trees are irrelevant to making inferences about sentence meanings."], "complexity": 1}, {"id": 26, "context": "For each of these predicates, the algorithm examines each node in the parse tree and uses supervised classification to decide the semantic role (if any) it plays for this predicate. Given a labeled training set such as PropBank or FrameNet, a feature vector is extracted for each node, using feature templates described in the next subsection. A 1-of-N classifier is then trained to predict a semantic role for each constituent given these features, where N is the number of potential semantic roles plus an extra NONE role for non-role constituents. Any standard classification algorithms can be used. Finally, for each test sentence to be labeled, the classifier is run on each relevant constituent. ", "Bloom_type": "comprehension", "question": "What does the algorithm do when examining nodes in a parse tree?", "options": ["It predicts a semantic role based on the features.", "It decides if the node represents a predicate.", "It extracts features from the node.", "It classifies the node into one of two categories."], "complexity": 1}, {"id": 27, "context": "Although the idea of semantic roles dates back to Pan. ini, they were re-introduced into modern linguistics by Gruber (1965), Fillmore (1966) and Fillmore (1968). Fillmore had become interested in argument structure by studying Lucien Tesni`ere`s groundbreaking Elements de Syntaxe Structurale (Tesni`ere, 1959) in which the term dependency` was introduced and the foundations were laid for dependency grammar. Following Tesni`ere`s terminology, Fillmore first referred to argument roles as actants (Fillmore, 1966) but quickly switched to the term case, (see Fillmore (2003)) and proposed a universal list of semantic roles or cases (Agent, Patient, Instrument, etc.), that could be taken on by the arguments of predicates. Verbs would be listed in the lexicon with their case frame, the list of obligatory (or optional) case arguments. The idea that semantic roles could provide an intermediate level of semantic representation that could help map from syntactic parse structures to deeper, more fully-specified representations of meaning was quickly adopted in natural language processing, and systems for extracting case frames were created for machine translation (Wilks, 1973), question-answering (Hendrix et al., 1973), spoken-language processing (Nash-Webber, 1975), and dialogue systems (Bobrow et al., 1977). Generalpurpose semantic role labelers were developed. The earliest ones (Simmons, 1973) first parsed a sentence by means of an ATN (Augmented Transition Network) parser. Each verb then had a set of rules specifying how the parse should be mapped to semantic roles. These rules mainly made reference to grammatical functions (subject, object, complement of specific prepositions) but also checked constituent internal features such as the animacy of head nouns. Later systems assigned roles from prebuilt parse trees, again by using dictionaries with verb-specific case frames (Levin 1977, Marcus 1980). ", "Bloom_type": "comprehension", "question": "What did Fillmore propose regarding semantic roles?", "options": ["Semantic roles can be categorized into Agent, Patient, Instrument, etc.", "Semantic roles are only applicable in syntax.", "Semantic roles cannot be represented in natural language processing.", "Semantic roles do not exist in modern linguistics."], "complexity": 1}, {"id": 28, "context": "In the early years, the space of MT architectures spanned three general models. In direct translation, the system proceeds word-by-word through the sourcelanguage text, translating each word incrementally. Direct translation uses a large bilingual dictionary, each of whose entries is a small program with the job of translating one word. In transfer approaches, we first parse the input text and then apply rules to transform the source-language parse into a target language parse. We then generate the target language sentence from the parse tree. In interlingua approaches, we analyze the source language text into some abstract meaning representation, called an interlingua. We then generate into the target language from this interlingual representation. A common way to visualize these three early approaches was the Vauquois triangle shown in Fig. 13.13. The triangle shows the increasing depth of analysis required (on both the analysis and generation end) as we move from the direct approach through transfer approaches to interlingual approaches. In addition, it shows the decreasing amount of transfer knowledge needed as we move up the triangle, from huge amounts of transfer at the direct level (almost all knowledge is transfer knowledge for each word) through transfer (transfer rules only for parse trees or thematic roles) through interlingua (no specific transfer knowledge). We can view the encoder-decoder network as an interlingual approach, with attention acting as an integration of direct and transfer, allowing words or their representations to be directly accessed by the decoder. ", "Bloom_type": "application", "question": "Which method involves parsing the input text before applying rules to transform the source-language parse into a target language parse?", "options": ["Transfer approaches", "Direct translation", "Interlingua approaches", "Encoder-decoder network"], "complexity": 2}, {"id": 29, "context": "Our focus in this chapter is context-free grammars and the CKY algorithm for parsing them. Context-free grammars are the backbone of many formal models of the syntax of natural language (and, for that matter, of computer languages). Syntactic parsing is the task of assigning a syntactic structure to a sentence. Parse trees (whether for context-free grammars or for the dependency or CCG formalisms we introduce in following chapters) can be used in applications such as grammar checking: sentence that cannot be parsed may have grammatical errors (or at least be hard to read). Parse trees can be an intermediate stage of representation for formal semantic analysis. And parsers and the grammatical structure they assign a sentence are a useful text analysis tool for text data science applications that require modeling the relationship of elements in sentences. ", "Bloom_type": "application", "question": "What is the primary purpose of parse trees in the context of natural language processing?", "options": ["To represent the syntactic structure of a sentence", "To check the readability of a sentence", "To analyze the sentiment of a sentence", "To predict future events based on past patterns"], "complexity": 2}, {"id": 30, "context": "We say the string a flight can be derived from the non-terminal NP. Thus, a CFG can be used to generate a set of strings. This sequence of rule expansions is called a derivation of the string of words. It is common to represent a derivation by a parse tree (commonly shown inverted with the root at the top). Figure 18.1 shows the tree representation of this derivation. ", "Bloom_type": "application", "question": "What does the parse tree show?", "options": ["The steps taken to derive the string", "The structure of the original sentence", "The rules applied during parsing", "The final parsed output"], "complexity": 2}, {"id": 31, "context": "In the parse tree shown in Fig. 18.1, we can say that the node NP dominates all the nodes in the tree (Det, Nom, Noun, a, flight). We can say further that it immediately dominates the nodes Det and Nom. ", "Bloom_type": "application", "question": "Which of the following statements is true regarding the dominance relationship in the parse tree?", "options": ["The node NP immediately dominates both the nodes Det and Nom.", "The node NP does not dominate any other nodes.", "The node NP immediately dominates only the node Nom.", "The node NP immediately dominates the nodes Det and Noun."], "complexity": 2}, {"id": 32, "context": "I), and a random expansion of VP (let`s say, to Verb NP), and so on until we generate the string I prefer a morning flight. Figure 18.4 shows a parse tree that represents a complete derivation of I prefer a morning flight. ", "Bloom_type": "application", "question": "What is the purpose of a parse tree?", "options": ["To show the syntactic structure of a sentence", "To represent the physical layout of a building", "To illustrate the grammatical rules of a language", "To display the weather conditions"], "complexity": 2}, {"id": 33, "context": "A corpus in which every sentence is annotated with a parse tree is called a treebank. ", "Bloom_type": "application", "question": "What type of data structure is used for annotating sentences with parse trees?", "options": ["Trees", "Graphs", "Arrays", "Lists"], "complexity": 2}, {"id": 34, "context": "Dynamic programming provides a powerful framework for addressing the problems caused by ambiguity in grammars. Recall that a dynamic programming approach systematically fills in a table of solutions to subproblems. The complete table has the solution to all the subproblems needed to solve the problem as a whole. In the case of syntactic parsing, these subproblems represent parse trees for all the constituents detected in the input. ", "Bloom_type": "application", "question": "What is the first step in constructing a parse tree?", "options": ["Identify the root node of the sentence.", "Choose an appropriate grammar rule for each constituent.", "Fill in the parse tree with nodes representing different parts of speech.", "Combine the parse trees of all constituents into a single structure."], "complexity": 2}, {"id": 35, "context": "While the CKY parsing algorithm we`ve seen so far does great at enumerating all the possible parse trees for a sentence, it has a large problem: it doesn`t tell us which parse is the correct one! That is, it doesn`t disambiguate among the possible parses. To solve the disambiguation problem we`ll use a simple neural extension of the CKY algorithm. The intuition of such parsing algorithms (often called span-based constituency parsing, or neural CKY), is to train a neural classifier to assign a score to each constituent, and then use a modified version of CKY to combine these constituent scores to find the best-scoring parse tree. ", "Bloom_type": "application", "question": "What technique will be used to resolve ambiguity in the neural extension of the CKY algorithm?", "options": ["Training a neural model to assign probabilities to different parse structures", "Applying a neural network to predict the next word in a sentence", "Using a greedy approach to select the most probable parse", "Implementing a backtracking method to explore all possibilities"], "complexity": 2}, {"id": 36, "context": "The standard tool for evaluating parsers that assign a single parse tree to a sentence is the PARSEVAL metrics (Black et al., 1991). The PARSEVAL metric measures how much the constituents in the hypothesis parse tree look like the constituents in a hand-labeled, reference parse. PARSEVAL thus requires a human-labeled reference (or gold standard) parse tree for each sentence in the test set; we generally draw these reference parses from a treebank like the Penn Treebank. ", "Bloom_type": "application", "question": "What does the PARSEVAL metric measure?", "options": ["The similarity between the constituent structure of the hypothesis parse tree and the reference parse tree.", "The length of the parse trees generated by the parser.", "The accuracy of the parser's output on unseen data.", "The complexity of the sentences being parsed."], "complexity": 2}, {"id": 37, "context": "In one simple model of lexical heads, each context-free rule is associated with a head (Charniak 1997, Collins 1999). The head is the word in the phrase that is grammatically the most important. Heads are passed up the parse tree; thus, each non-terminal in a parse tree is annotated with a single word, which is its lexical head. Figure 18.16 shows an example of such a tree from Collins (1999), in which each non-terminal is annotated with its head. ", "Bloom_type": "application", "question": "What does it mean for a non-terminal in a parse tree to be annotated with its lexical head?", "options": ["The non-terminal is assigned the word that best describes its function within the sentence.", "The non-terminal represents the entire sentence.", "The non-terminal contains only the root word.", "The non-terminal is the first word encountered in the sentence."], "complexity": 2}, {"id": 38, "context": " Span-based neural constituency parses train a neural classifier to assign a score to each constituent, and then use a modified version of CKY to combine these constituent scores to find the best-scoring parse tree. ", "Bloom_type": "application", "question": "What is the next step after training a span-based neural constituency parser?", "options": ["Generate a parse tree based on the highest scoring configuration.", "Use the modified CKY algorithm directly on the parsed sentences.", "Train another model for part-of-speech tagging.", "Combine the constituent scores using the original CKY algorithm."], "complexity": 2}, {"id": 39, "context": "The main reason computational systems use semantic roles is to act as a shallow meaning representation that can let us make simple inferences that aren`t possible from the pure surface string of words, or even from the parse tree. To extend the earlier examples, if a document says that Company A acquired Company B, we`d like to know that this answers the query Was Company B acquired? despite the fact that the two sentences have very different surface syntax. Similarly, this shallow semantics might act as a useful intermediate language in machine translation. ", "Bloom_type": "application", "question": "What does the parse tree help with when dealing with documents?", "options": ["It provides a deeper understanding of the semantic role.", "It helps understand the deep structure of the sentence.", "It simplifies the surface syntax for easier processing.", "It acts as an intermediary language for machine translation."], "complexity": 2}, {"id": 40, "context": "For each of these predicates, the algorithm examines each node in the parse tree and uses supervised classification to decide the semantic role (if any) it plays for this predicate. Given a labeled training set such as PropBank or FrameNet, a feature vector is extracted for each node, using feature templates described in the next subsection. A 1-of-N classifier is then trained to predict a semantic role for each constituent given these features, where N is the number of potential semantic roles plus an extra NONE role for non-role constituents. Any standard classification algorithms can be used. Finally, for each test sentence to be labeled, the classifier is run on each relevant constituent. ", "Bloom_type": "application", "question": "Which step involves creating a feature vector for each node in the parse tree?", "options": ["Extracting features", "Training classifiers", "Running classifiers", "Labeling sentences"], "complexity": 2}, {"id": 41, "context": "Although the idea of semantic roles dates back to Pan. ini, they were re-introduced into modern linguistics by Gruber (1965), Fillmore (1966) and Fillmore (1968). Fillmore had become interested in argument structure by studying Lucien Tesni`ere`s groundbreaking Elements de Syntaxe Structurale (Tesni`ere, 1959) in which the term dependency` was introduced and the foundations were laid for dependency grammar. Following Tesni`ere`s terminology, Fillmore first referred to argument roles as actants (Fillmore, 1966) but quickly switched to the term case, (see Fillmore (2003)) and proposed a universal list of semantic roles or cases (Agent, Patient, Instrument, etc.), that could be taken on by the arguments of predicates. Verbs would be listed in the lexicon with their case frame, the list of obligatory (or optional) case arguments. The idea that semantic roles could provide an intermediate level of semantic representation that could help map from syntactic parse structures to deeper, more fully-specified representations of meaning was quickly adopted in natural language processing, and systems for extracting case frames were created for machine translation (Wilks, 1973), question-answering (Hendrix et al., 1973), spoken-language processing (Nash-Webber, 1975), and dialogue systems (Bobrow et al., 1977). Generalpurpose semantic role labelers were developed. The earliest ones (Simmons, 1973) first parsed a sentence by means of an ATN (Augmented Transition Network) parser. Each verb then had a set of rules specifying how the parse should be mapped to semantic roles. These rules mainly made reference to grammatical functions (subject, object, complement of specific prepositions) but also checked constituent internal features such as the animacy of head nouns. Later systems assigned roles from prebuilt parse trees, again by using dictionaries with verb-specific case frames (Levin 1977, Marcus 1980). ", "Bloom_type": "application", "question": "What is the initial step in developing a system for extracting case frames?", "options": ["Parse the sentence using an ATN parser", "Identify verbs in the sentence", "Create a dictionary with verb-specific case frames", "Map the parse to semantic roles based on grammatical functions"], "complexity": 2}]}, "dialogue system": {"max_id": 67, "Questions": [{"id": 0, "context": "In this second part of the book we introduce fundamental NLP applications: machine translation, information retrieval, question answering, dialogue systems, and speech recognition. ", "Bloom_type": "remember", "question": "Which application is not discussed in the context?", "options": ["dialogue systems", "machine translation", "information retrieval", "question answering"], "complexity": 0}, {"id": 1, "context": "This chapter introduces the fundamental algorithms of programs that use conversation to interact with users. We often distinguish between two kinds of architectures. Task-oriented dialogue systems converse with users to accomplish fixed tasks like controlling appliances or finding restaurants, relying on a data structure called the frame, which represents the knowledge a system needs to acquire from the user (like the time to set an alarm clock). Chatbots, by contrast, are designed to mimic the longer and more unstructured conversations or chats` characteristic of human-human interaction. Modern systems incorporate aspects of both; industrial chatbots like ChatGPT can carry on longer unstructured conversations; industrial digital assistants like Siri or Alexa are generally frame-based dialogue systems. ", "Bloom_type": "remember", "question": "What distinguishes task-oriented dialogue systems from chatbots?", "options": ["Task-oriented dialogue systems rely on structured data for user interactions.", "Chatbots use a frame structure to represent user knowledge.", "Industrial chatbots are typically frame-based while industrial digital assistants are not.", "Modern systems blend elements of both types equally."], "complexity": 0}, {"id": 2, "context": "The fact that chatbots and dialogue systems are designed for human-computer interaction has strong implications for their design and use. Many of these implications already became clear in one of the earliest chatbots, ELIZA (Weizenbaum, 1966). ELIZA was designed to simulate a Rogerian psychologist, based on a branch of clinical psychology whose methods involve drawing the patient out by reflecting patient`s statements back at them. Rogerian interactions are the rare type of conversation in which, as Weizenbaum points out, one can assume the pose of knowing almost nothing of the real world. If a patient says I went for a long boat ride and the psychiatrist says Tell me about boats, you don`t assume she didn`t know what ", "Bloom_type": "remember", "question": "In the early days of chatbot development, why did many designers choose to create dialogues that mimic Rogerian therapy?", "options": ["To make the conversations more realistic", "To avoid direct communication with humans", "To explore new forms of artificial intelligence", "To test the limits of computer processing speed"], "complexity": 0}, {"id": 3, "context": "Turn structure has important implications for spoken dialogue. A human has to know when to stop talking; the client interrupts (in A16 and C17), so a system that was performing this role must know to stop talking (and that the user might be making a correction). A system also has to know when to start talking. For example, most of the time in conversation, speakers start their turns almost immediately after the other speaker finishes, without a long pause, because people are can usually predict when the other person is about to finish talking. Spoken dialogue systems must also detect whether a user is done speaking, so they can process the utterance and respond. This taskcalled endpointing or endpoint detection can be quite challenging because of noise and because people often pause in the middle of turns. ", "Bloom_type": "remember", "question": "In spoken dialogue systems, what does it mean if a user pauses mid-turn?", "options": ["The system needs to detect the end of the turn.", "The user is likely to continue speaking.", "The system should ignore the pause.", "The system should wait for more input from the user."], "complexity": 0}, {"id": 4, "context": "Full mixed initiative, while the norm for human-human conversations, can be difficult for dialogue systems. The most primitive dialogue systems tend to use system-initiative, where the system asks a question and the user can`t do anything until they answer it, or user-initiative like simple search engines, where the user specifies a query and the system passively responds. Even modern large language model-based dialogue systems, which come much closer to using full mixed initiative, often don`t have completely natural initiative switching. Getting this right is an important goal for modern systems. ", "Bloom_type": "remember", "question": "In what way does getting the dialogue system right as mentioned in the response?", "options": ["It allows users to control the conversation more actively.", "It helps users understand the system better.", "It makes the dialogue system simpler to design.", "It improves the efficiency of data processing."], "complexity": 0}, {"id": 5, "context": "These subtle characteristics of human conversations (turns, speech acts, grounding, dialogue structure, initiative, and implicature) are among the reasons it is difficult to build dialogue systems that can carry on natural conversations with humans. Many of these challenges are active areas of dialogue systems research. ", "Bloom_type": "remember", "question": "Which aspect of human conversations makes it challenging for dialogue systems to mimic natural conversation?", "options": ["The subtleties involved in turn-taking, speech acts, and implicature", "The use of complex vocabulary", "The presence of background music", "The variety of accents spoken"], "complexity": 0}, {"id": 6, "context": "The frame and its slots in a task-based dialogue system specify what the system needs to know to perform its task. A hotel reservation system needs dates and locations. An alarm clock system needs a time. The system`s goal is to fill the slots in the frame with the fillers the user intends, and then perform the relevant action for the user (answering a question, or booking a flight). ", "Bloom_type": "remember", "question": "In a task-based dialogue system, what do the slots in the frame represent?", "options": ["The data needed by the system", "The specific actions the system can take", "The tasks the system performs", "The questions the system asks"], "complexity": 0}, {"id": 7, "context": "We can make a very simple frame-based dialogue system by wrapping a small amount of code around this slot extractor. Mainly we just need to ask the user questions until all the slots are full, do a database query, then report back to the user, using hand-built templates for generating sentences. ", "Bloom_type": "remember", "question": "In creating a simple frame-based dialogue system, what is primarily required?", "options": ["Implementing a series of user interactions", "Designing complex sentence structures", "Building an intricate database", "Coding a basic set of functions"], "complexity": 0}, {"id": 8, "context": "It is a common practice for dialogue systems to use further labeled data for finetuning. One function of this fine-tuning step is to improve the quality of the dialogue, training the system to produce responses that are sensible and interesting. Another function might be to improve safety, keeping a dialogue system from suggesting harmful actions (like financial fraud, medical harm, inciting hatred, or abusing the user or other people). ", "Bloom_type": "remember", "question": "What additional purpose can fine-tuning a dialogue system serve besides improving its response quality?", "options": ["To prevent the system from making inappropriate suggestions", "To enhance the system's ability to understand natural language", "To increase the speed of the dialogue process", "To make the dialogue more entertaining"], "complexity": 0}, {"id": 9, "context": "In the simplest method for improving quality and safety, speakers of the language are given an initial prompt and instructions to have high-quality, safe dialogues. They then interact with an initial dialogue system and their responses are used to finetune the model, usually as part of the instruct tuning step we introduced in Chapter 12. Thus a dialogue system learns to answer questions, follow other instructions, and also carry on high-quality, safe dialogues, in a single multi-task learning format. ", "Bloom_type": "remember", "question": "In what way does a dialogue system improve the interaction between speakers?", "options": ["It enhances understanding through repeated interactions.", "It simplifies complex tasks by breaking them down into smaller parts.", "It provides personalized feedback based on individual speaker performance.", "It ensures all participants engage equally in every conversation."], "complexity": 0}, {"id": 10, "context": "3. Iteratively test the design on users: An iterative design cycle with embedded user testing is essential in system design (Nielsen 1992, Cole et al. 1997, Yankelovich et al. 1995, Landauer 1995). For example in a well-known incident, an early dialogue system required the user to press a key to interrupt the system (Stifelman et al., 1993). But user testing showed users barged in (interrupted, talking over the system), which led to a redesign of the system to recognize overlapped speech. It`s also important to incorporate value sensitive design, in which we carefully consider during the design process the benefits, harms and possible stakeholders of the resulting system (Friedman et al. 2017, Friedman and Hendry 2019). ", "Bloom_type": "remember", "question": "In what way did user testing lead to a redesign of the dialogue system?", "options": ["The system was redesigned to automatically detect when the user wants to speak.", "The system was redesigned to require more complex input methods.", "The system was redesigned to use voice commands exclusively.", "The system was redesigned to include visual feedback for better understanding."], "complexity": 0}, {"id": 11, "context": "These ethical issues are an important area of investigation, including finding ways to mitigate problems of abuse and toxicity, like detecting and responding appropriately to toxic contexts (Wolf et al. 2017, Dinan et al. 2020, Xu et al. 2020). Value sensitive design, carefully considering possible harms in advance (Friedman et al. 2017, Friedman and Hendry 2019) is also important; (Dinan et al., 2021) give a number of suggestions for best practices in dialogue system design. For example getting informed consent from participants, whether they are used for training, or whether they are interacting with a deployed system is important. Because dialogue systems by definition involve human participants, researchers also work on these issues with the Institutional Review Boards (IRB) at their institutions, who help protect the safety of experimental subjects. ", "Bloom_type": "remember", "question": "What aspect of dialogue system research involves careful consideration of potential harms before implementation?", "options": ["Value-sensitive design", "Ethical guidelines", "User privacy protection", "Data security measures"], "complexity": 0}, {"id": 12, "context": "Chatbots and dialogue systems are crucial speech and language processing applications that are already widely used commercially. ", "Bloom_type": "remember", "question": "What is a key application of dialogue systems mentioned in the context?", "options": ["Natural Language Processing", "Speech recognition", "Machine Learning Algorithms", "Data Encryption"], "complexity": 0}, {"id": 13, "context": " Dialogue systems are a kind of human-computer interaction, and general HCI principles apply in their design, including the role of the user, simulations such as Wizard-of-Oz systems, and the importance of iterative design and testing on real users. ", "Bloom_type": "remember", "question": "In what way do dialogue systems differ from traditional computer interfaces?", "options": ["They require less training for users.", "They use more complex algorithms.", "They focus solely on visual feedback.", "They prioritize voice recognition over text input."], "complexity": 0}, {"id": 14, "context": "Computational-implemented theories of dialogue blossomed in the 1970. That period saw the very influential GUS system (Bobrow et al., 1977), which in the late 1970s established the frame-based paradigm that became the dominant industrial paradigm for dialogue systems for over 30 years. ", "Bloom_type": "remember", "question": "In what decade did computational-implemented theories of dialogue begin to flourish?", "options": ["The 1960s", "The 1980s", "The 1950s", "The 1940s"], "complexity": 0}, {"id": 15, "context": "In the 1990s, machine learning models that had first been applied to natural language processing began to be applied to dialogue tasks like slot filling (Miller et al. 1994, Pieraccini et al. 1991). This period also saw lots of analytic work on the linguistic properties of dialogue acts and on machine-learning-based methods for their detection. (Sag and Liberman 1975, Hinkelman and Allen 1989, Nagata and Morimoto 1994, Goodwin 1996, Chu-Carroll 1998, Shriberg et al. 1998, Stolcke et al. 2000, Gravano et al. 2012. This work strongly informed the development of the dialogue-state model (Larsson and Traum, 2000). Dialogue state tracking quickly became an important problem for task-oriented dialogue, and there has been an influential annual evaluation of state-tracking algorithms (Williams et al., 2016). The turn of the century saw a line of work on applying reinforcement learning to dialogue, which first came out of AT&T and Bell Laboratories with work on MDP dialogue systems (Walker 2000, Levin et al. 2000, Singh et al. 2002) along with work on cue phrases, prosody, and rejection and confirmation. Reinforcement learning research turned quickly to the more sophisticated POMDP models (Roy et al. 2000, Lemon et al. 2006, Williams and Young 2007) applied to small slotfilling dialogue tasks. Neural reinforcement learning models have been used both for chatbot systems, for example simulating dialogues between two dialogue systems, rewarding good conversational properties like coherence and ease of answering (Li et al., 2016a), and for task-oriented dialogue (Williams et al., 2017). ", "Bloom_type": "remember", "question": "In what decade did machine learning models begin to be applied to dialogue tasks?", "options": ["The 1990s", "The 1980s", "The 1970s", "The 1960s"], "complexity": 0}, {"id": 16, "context": "By around 2010 the GUS architecture finally began to be widely used commercially in dialogue systems on phones like Apple`s SIRI (Bellegarda, 2013) and other digital assistants. ", "Bloom_type": "remember", "question": "In what year did the GUS architecture start being widely used commercially for dialogue systems?", "options": ["2010", "2005", "2008", "2012"], "complexity": 0}, {"id": 17, "context": "Could we improve on word error rate as a metric? It would be nice, for example, to have something that didn`t give equal weight to every word, perhaps valuing content words like Tuesday more than function words like a or of. While researchers generally agree that this would be a good idea, it has proved difficult to agree on a metric that works in every application of ASR. For dialogue systems, however, where the desired semantic output is more clear, a metric called slot error rate or concept error rate has proved extremely useful; it is discussed in Chapter 15 on page 317. ", "Bloom_type": "remember", "question": "What does the metric called slot error rate or concept error rate help with in dialogue systems?", "options": ["Improving overall accuracy", "Reducing computational complexity", "Enhancing word error detection", "Increasing data processing speed"], "complexity": 0}, {"id": 18, "context": "The goal of text-to-speech (TTS) systems is to map from strings of letters to waveforms, a technology that`s important for a variety of applications from dialogue systems to games to education. ", "Bloom_type": "remember", "question": "In what application are TTS systems most commonly used?", "options": ["Educational tools", "Medical diagnosis", "Automotive entertainment", "Space exploration"], "complexity": 0}, {"id": 19, "context": "Although the idea of semantic roles dates back to Pan. ini, they were re-introduced into modern linguistics by Gruber (1965), Fillmore (1966) and Fillmore (1968). Fillmore had become interested in argument structure by studying Lucien Tesni`ere`s groundbreaking Elements de Syntaxe Structurale (Tesni`ere, 1959) in which the term dependency` was introduced and the foundations were laid for dependency grammar. Following Tesni`ere`s terminology, Fillmore first referred to argument roles as actants (Fillmore, 1966) but quickly switched to the term case, (see Fillmore (2003)) and proposed a universal list of semantic roles or cases (Agent, Patient, Instrument, etc.), that could be taken on by the arguments of predicates. Verbs would be listed in the lexicon with their case frame, the list of obligatory (or optional) case arguments. The idea that semantic roles could provide an intermediate level of semantic representation that could help map from syntactic parse structures to deeper, more fully-specified representations of meaning was quickly adopted in natural language processing, and systems for extracting case frames were created for machine translation (Wilks, 1973), question-answering (Hendrix et al., 1973), spoken-language processing (Nash-Webber, 1975), and dialogue systems (Bobrow et al., 1977). Generalpurpose semantic role labelers were developed. The earliest ones (Simmons, 1973) first parsed a sentence by means of an ATN (Augmented Transition Network) parser. Each verb then had a set of rules specifying how the parse should be mapped to semantic roles. These rules mainly made reference to grammatical functions (subject, object, complement of specific prepositions) but also checked constituent internal features such as the animacy of head nouns. Later systems assigned roles from prebuilt parse trees, again by using dictionaries with verb-specific case frames (Levin 1977, Marcus 1980). ", "Bloom_type": "remember", "question": "In what year did Fillmore introduce the concept of semantic roles?", "options": ["1965", "1966", "1967", "1968"], "complexity": 0}, {"id": 20, "context": "Detecting emotion has the potential to improve a number of language processing tasks. Emotion recognition could help dialogue systems like tutoring systems detect that a student was unhappy, bored, hesitant, confident, and so on. Automatically detecting emotions in reviews or customer responses (anger, dissatisfaction, trust) could help businesses recognize specific problem areas or ones that are going well. Emotion can play a role in medical NLP tasks like helping diagnose depression or suicidal intent. Detecting emotions expressed toward characters in novels might play a role in understanding how different social groups were viewed by society at different times. ", "Bloom_type": "remember", "question": "In what way can emotion detection benefit dialogue systems?", "options": ["It can enhance the accuracy of speech-to-text conversion.", "It can improve the efficiency of natural language generation.", "It can increase the speed of data entry for users.", "It can provide personalized recommendations based on user preferences."], "complexity": 0}, {"id": 21, "context": "Coreference is an important component of natural language processing. A dialogue system that has just told the user There is a 2pm flight on United and a 4pm one on Cathay Pacific must know which flight the user means by I`ll take the second one. A question answering system that uses Wikipedia to answer a question about Marie Curie must know who she was in the sentence She was born in Warsaw. And a machine translation system translating from a language like Spanish, in which pronouns can be dropped, must use coreference from the previous sentence to decide whether the Spanish sentence Me encanta el conocimiento, dice.` should be translated as I love knowledge, he says`, or I love knowledge, she says`. Indeed, this example comes from an actual news article in El Pas about a female professor and was mistranslated as he in machine translation because of inaccurate coreference resolution (Schiebinger, 2013). ", "Bloom_type": "remember", "question": "In what way does coreference help a dialogue system understand user intent?", "options": ["It enables the system to identify the specific meaning of pronouns based on their context.", "It helps the system remember past conversations.", "It allows the system to recognize synonyms.", "It assists the system in understanding the grammatical structure of sentences."], "complexity": 0}, {"id": 22, "context": "In this second part of the book we introduce fundamental NLP applications: machine translation, information retrieval, question answering, dialogue systems, and speech recognition. ", "Bloom_type": "comprehension", "question": "Which of the following is NOT an example of a fundamental NLP application mentioned in the context?", "options": ["Machine Translation", "Information Retrieval", "Dialogue Systems", "Speech Recognition"], "complexity": 1}, {"id": 23, "context": "This chapter introduces the fundamental algorithms of programs that use conversation to interact with users. We often distinguish between two kinds of architectures. Task-oriented dialogue systems converse with users to accomplish fixed tasks like controlling appliances or finding restaurants, relying on a data structure called the frame, which represents the knowledge a system needs to acquire from the user (like the time to set an alarm clock). Chatbots, by contrast, are designed to mimic the longer and more unstructured conversations or chats` characteristic of human-human interaction. Modern systems incorporate aspects of both; industrial chatbots like ChatGPT can carry on longer unstructured conversations; industrial digital assistants like Siri or Alexa are generally frame-based dialogue systems. ", "Bloom_type": "comprehension", "question": "What distinguishes task-oriented dialogue systems from chatbots?", "options": ["Chatbots typically engage in longer, less structured conversations mimicking human interactions.", "Task-oriented dialogue systems rely on a frame for acquiring user knowledge.", "Modern systems focus solely on incorporating elements of both types.", "None of the above"], "complexity": 1}, {"id": 24, "context": "The fact that chatbots and dialogue systems are designed for human-computer interaction has strong implications for their design and use. Many of these implications already became clear in one of the earliest chatbots, ELIZA (Weizenbaum, 1966). ELIZA was designed to simulate a Rogerian psychologist, based on a branch of clinical psychology whose methods involve drawing the patient out by reflecting patient`s statements back at them. Rogerian interactions are the rare type of conversation in which, as Weizenbaum points out, one can assume the pose of knowing almost nothing of the real world. If a patient says I went for a long boat ride and the psychiatrist says Tell me about boats, you don`t assume she didn`t know what ", "Bloom_type": "comprehension", "question": "What aspect of ELIZA's design reflects its ability to engage in conversations similar to those found in Rogerian therapy?", "options": ["ELIZA's use of reflective questioning techniques", "ELIZA's reliance on predefined scripts", "ELIZA's focus on complex problem-solving algorithms", "ELIZA's implementation of natural language processing"], "complexity": 1}, {"id": 25, "context": "Turn structure has important implications for spoken dialogue. A human has to know when to stop talking; the client interrupts (in A16 and C17), so a system that was performing this role must know to stop talking (and that the user might be making a correction). A system also has to know when to start talking. For example, most of the time in conversation, speakers start their turns almost immediately after the other speaker finishes, without a long pause, because people are can usually predict when the other person is about to finish talking. Spoken dialogue systems must also detect whether a user is done speaking, so they can process the utterance and respond. This taskcalled endpointing or endpoint detection can be quite challenging because of noise and because people often pause in the middle of turns. ", "Bloom_type": "comprehension", "question": "What is one of the key challenges faced by spoken dialogue systems regarding endpoint detection?", "options": ["Detecting pauses in speech patterns", "Recognizing different languages", "Understanding complex sentence structures", "Predicting future actions based on past behavior"], "complexity": 1}, {"id": 26, "context": "Full mixed initiative, while the norm for human-human conversations, can be difficult for dialogue systems. The most primitive dialogue systems tend to use system-initiative, where the system asks a question and the user can`t do anything until they answer it, or user-initiative like simple search engines, where the user specifies a query and the system passively responds. Even modern large language model-based dialogue systems, which come much closer to using full mixed initiative, often don`t have completely natural initiative switching. Getting this right is an important goal for modern systems. ", "Bloom_type": "comprehension", "question": "What type of dialogue systems are typically more common among humans compared to dialogue systems?", "options": ["User-initiative", "System-initiative", "Mixed initiative", "None of the above"], "complexity": 1}, {"id": 27, "context": "These subtle characteristics of human conversations (turns, speech acts, grounding, dialogue structure, initiative, and implicature) are among the reasons it is difficult to build dialogue systems that can carry on natural conversations with humans. Many of these challenges are active areas of dialogue systems research. ", "Bloom_type": "comprehension", "question": "What aspects of human conversations make it challenging for dialogue systems to mimic natural conversation?", "options": ["Subtle characteristics such as turns, speech acts, grounding, dialogue structure, initiative, and implicature", "Complex vocabulary and grammar rules", "Highly structured dialogues", "Limited use of technology"], "complexity": 1}, {"id": 28, "context": "The frame and its slots in a task-based dialogue system specify what the system needs to know to perform its task. A hotel reservation system needs dates and locations. An alarm clock system needs a time. The system`s goal is to fill the slots in the frame with the fillers the user intends, and then perform the relevant action for the user (answering a question, or booking a flight). ", "Bloom_type": "comprehension", "question": "What does a dialogue system need to do its job effectively?", "options": ["Fill in the slots in the frame with user inputs", "Understand natural language commands", "Store user preferences permanently", "Generate random responses"], "complexity": 1}, {"id": 29, "context": "We can make a very simple frame-based dialogue system by wrapping a small amount of code around this slot extractor. Mainly we just need to ask the user questions until all the slots are full, do a database query, then report back to the user, using hand-built templates for generating sentences. ", "Bloom_type": "comprehension", "question": "What is the primary function of a dialoguesystem? ", "options": ["To extract slots from user input", "To build handcrafted templates", "To perform database queries", "To generate responses based on user input"], "complexity": 1}, {"id": 30, "context": "It is a common practice for dialogue systems to use further labeled data for finetuning. One function of this fine-tuning step is to improve the quality of the dialogue, training the system to produce responses that are sensible and interesting. Another function might be to improve safety, keeping a dialogue system from suggesting harmful actions (like financial fraud, medical harm, inciting hatred, or abusing the user or other people). ", "Bloom_type": "comprehension", "question": "What two functions does the fine-tuning step serve for dialogue systems?", "options": ["Improving the quality of the dialogue and improving safety", "Training the system to respond sensibly and improving its accuracy", "Enhancing the efficiency of the dialogue and reducing latency", "Increasing the complexity of the dialogue and expanding vocabulary"], "complexity": 1}, {"id": 31, "context": "In the simplest method for improving quality and safety, speakers of the language are given an initial prompt and instructions to have high-quality, safe dialogues. They then interact with an initial dialogue system and their responses are used to finetune the model, usually as part of the instruct tuning step we introduced in Chapter 12. Thus a dialogue system learns to answer questions, follow other instructions, and also carry on high-quality, safe dialogues, in a single multi-task learning format. ", "Bloom_type": "comprehension", "question": "In the context of improving quality and safety through dialogue systems, how do they learn to perform various tasks?", "options": ["Through interaction with an initial dialogue system where responses guide the model\u2019s development.", "By fine-tuning the model based on user feedback alone.", "Using only predefined scripts for all possible interactions.", "Learning exclusively from pre-existing datasets."], "complexity": 1}, {"id": 32, "context": "3. Iteratively test the design on users: An iterative design cycle with embedded user testing is essential in system design (Nielsen 1992, Cole et al. 1997, Yankelovich et al. 1995, Landauer 1995). For example in a well-known incident, an early dialogue system required the user to press a key to interrupt the system (Stifelman et al., 1993). But user testing showed users barged in (interrupted, talking over the system), which led to a redesign of the system to recognize overlapped speech. It`s also important to incorporate value sensitive design, in which we carefully consider during the design process the benefits, harms and possible stakeholders of the resulting system (Friedman et al. 2017, Friedman and Hendry 2019). ", "Bloom_type": "comprehension", "question": "What aspect of dialogue systems requires careful consideration during the design process?", "options": ["Ethical implications", "User interaction patterns", "System performance metrics", "Technical specifications"], "complexity": 1}, {"id": 33, "context": "Another important ethical issue is privacy. Already in the first days of ELIZA, Weizenbaum pointed out the privacy implications of people`s revelations to the chatbot. The ubiquity of in-home dialogue systems means they may often overhear private information (Henderson et al., 2017). If a chatbot is human-like, users are also more likely to disclose private information, and less likely to worry about the harm of this disclosure (Ischen et al., 2019). In general, chatbots that are trained on transcripts of human-human or human-machine conversation must anonymize personally identifiable information. ", "Bloom_type": "comprehension", "question": "Explain how privacy issues arise from using dialogue systems?", "options": ["Privacy concerns emerge when dialogue systems overhear personal conversations.", "Users tend to share more sensitive information due to the presence of chatbots.", "The training data for chatbots can reveal user identities.", "Dialogue systems always ensure complete anonymity."], "complexity": 1}, {"id": 34, "context": "These ethical issues are an important area of investigation, including finding ways to mitigate problems of abuse and toxicity, like detecting and responding appropriately to toxic contexts (Wolf et al. 2017, Dinan et al. 2020, Xu et al. 2020). Value sensitive design, carefully considering possible harms in advance (Friedman et al. 2017, Friedman and Hendry 2019) is also important; (Dinan et al., 2021) give a number of suggestions for best practices in dialogue system design. For example getting informed consent from participants, whether they are used for training, or whether they are interacting with a deployed system is important. Because dialogue systems by definition involve human participants, researchers also work on these issues with the Institutional Review Boards (IRB) at their institutions, who help protect the safety of experimental subjects. ", "Bloom_type": "comprehension", "question": "What aspect of dialogue systems involves careful consideration of potential harms before implementation?", "options": ["Ethical issues", "Technical specifications", "User interface design", "Data privacy policies"], "complexity": 1}, {"id": 35, "context": "Chatbots and dialogue systems are crucial speech and language processing applications that are already widely used commercially. ", "Bloom_type": "comprehension", "question": "Explain how dialogue systems differ from chatbots in their commercial usage?", "options": ["Dialogue systems are designed for customer service, while chatbots are not.", "Chatbots are more advanced than dialogue systems.", "Chatbots can handle complex conversations better than dialogue systems.", "Dialogue systems are typically used for educational purposes, whereas chatbots are not."], "complexity": 1}, {"id": 36, "context": " Dialogue systems are a kind of human-computer interaction, and general HCI principles apply in their design, including the role of the user, simulations such as Wizard-of-Oz systems, and the importance of iterative design and testing on real users. ", "Bloom_type": "comprehension", "question": "What principle from general Human-Computer Interaction (HCI) applies most directly when designing dialogue systems?", "options": ["The importance of iterative design and testing on real users", "Simulations such as Wizard-of-Oz systems", "General HCI principles", "User roles"], "complexity": 1}, {"id": 37, "context": "Computational-implemented theories of dialogue blossomed in the 1970. That period saw the very influential GUS system (Bobrow et al., 1977), which in the late 1970s established the frame-based paradigm that became the dominant industrial paradigm for dialogue systems for over 30 years. ", "Bloom_type": "comprehension", "question": "What was one of the most significant developments in computational dialogue systems during the 1970s?", "options": ["The development of the Frame-Based Paradigm", "The emergence of natural language processing techniques", "The invention of the first chatbot", "The introduction of machine learning algorithms"], "complexity": 1}, {"id": 38, "context": "In the 1990s, machine learning models that had first been applied to natural language processing began to be applied to dialogue tasks like slot filling (Miller et al. 1994, Pieraccini et al. 1991). This period also saw lots of analytic work on the linguistic properties of dialogue acts and on machine-learning-based methods for their detection. (Sag and Liberman 1975, Hinkelman and Allen 1989, Nagata and Morimoto 1994, Goodwin 1996, Chu-Carroll 1998, Shriberg et al. 1998, Stolcke et al. 2000, Gravano et al. 2012. This work strongly informed the development of the dialogue-state model (Larsson and Traum, 2000). Dialogue state tracking quickly became an important problem for task-oriented dialogue, and there has been an influential annual evaluation of state-tracking algorithms (Williams et al., 2016). The turn of the century saw a line of work on applying reinforcement learning to dialogue, which first came out of AT&T and Bell Laboratories with work on MDP dialogue systems (Walker 2000, Levin et al. 2000, Singh et al. 2002) along with work on cue phrases, prosody, and rejection and confirmation. Reinforcement learning research turned quickly to the more sophisticated POMDP models (Roy et al. 2000, Lemon et al. 2006, Williams and Young 2007) applied to small slotfilling dialogue tasks. Neural reinforcement learning models have been used both for chatbot systems, for example simulating dialogues between two dialogue systems, rewarding good conversational properties like coherence and ease of answering (Li et al., 2016a), and for task-oriented dialogue (Williams et al., 2017). ", "Bloom_type": "comprehension", "question": "What was one significant area of focus during the early applications of machine learning to dialogue systems in the 1990s?", "options": ["Dialogue act analysis", "Natural language generation", "Machine learning techniques", "Reinforcement learning approaches"], "complexity": 1}, {"id": 39, "context": "By around 2010 the GUS architecture finally began to be widely used commercially in dialogue systems on phones like Apple`s SIRI (Bellegarda, 2013) and other digital assistants. ", "Bloom_type": "comprehension", "question": "What was one of the key developments in dialogue systems by around 2010?", "options": ["The widespread commercial use of GUS architecture", "The introduction of Siri-like digital assistants", "The development of more advanced natural language processing algorithms", "The integration of artificial intelligence into everyday devices"], "complexity": 1}, {"id": 40, "context": "Could we improve on word error rate as a metric? It would be nice, for example, to have something that didn`t give equal weight to every word, perhaps valuing content words like Tuesday more than function words like a or of. While researchers generally agree that this would be a good idea, it has proved difficult to agree on a metric that works in every application of ASR. For dialogue systems, however, where the desired semantic output is more clear, a metric called slot error rate or concept error rate has proved extremely useful; it is discussed in Chapter 15 on page 317. ", "Bloom_type": "comprehension", "question": "What type of error rate is particularly effective for dialogue systems?", "options": ["Slot error rate or concept error rate", "Word error rate", "Sentence error rate", "Phonetic error rate"], "complexity": 1}, {"id": 41, "context": "The goal of text-to-speech (TTS) systems is to map from strings of letters to waveforms, a technology that`s important for a variety of applications from dialogue systems to games to education. ", "Bloom_type": "comprehension", "question": "What are some potential applications of TTS systems beyond just dialogue systems?", "options": ["Medical records management", "Games and education", "Weather forecasting", "Automotive navigation systems"], "complexity": 1}, {"id": 42, "context": "Although the idea of semantic roles dates back to Pan. ini, they were re-introduced into modern linguistics by Gruber (1965), Fillmore (1966) and Fillmore (1968). Fillmore had become interested in argument structure by studying Lucien Tesni`ere`s groundbreaking Elements de Syntaxe Structurale (Tesni`ere, 1959) in which the term dependency` was introduced and the foundations were laid for dependency grammar. Following Tesni`ere`s terminology, Fillmore first referred to argument roles as actants (Fillmore, 1966) but quickly switched to the term case, (see Fillmore (2003)) and proposed a universal list of semantic roles or cases (Agent, Patient, Instrument, etc.), that could be taken on by the arguments of predicates. Verbs would be listed in the lexicon with their case frame, the list of obligatory (or optional) case arguments. The idea that semantic roles could provide an intermediate level of semantic representation that could help map from syntactic parse structures to deeper, more fully-specified representations of meaning was quickly adopted in natural language processing, and systems for extracting case frames were created for machine translation (Wilks, 1973), question-answering (Hendrix et al., 1973), spoken-language processing (Nash-Webber, 1975), and dialogue systems (Bobrow et al., 1977). Generalpurpose semantic role labelers were developed. The earliest ones (Simmons, 1973) first parsed a sentence by means of an ATN (Augmented Transition Network) parser. Each verb then had a set of rules specifying how the parse should be mapped to semantic roles. These rules mainly made reference to grammatical functions (subject, object, complement of specific prepositions) but also checked constituent internal features such as the animacy of head nouns. Later systems assigned roles from prebuilt parse trees, again by using dictionaries with verb-specific case frames (Levin 1977, Marcus 1980). ", "Bloom_type": "comprehension", "question": "What did Fillmore propose regarding semantic roles in relation to predicate arguments?", "options": ["Fillmore suggested a universal list of semantic roles including Agent, Patient, Instrument, etc.", "Fillmore initially called them actants before switching to the term case.", "Fillmore believed semantic roles could only be found in syntactic parse structures.", "Fillmore thought semantic roles were irrelevant to mapping from syntactic parse structures."], "complexity": 1}, {"id": 43, "context": "Detecting emotion has the potential to improve a number of language processing tasks. Emotion recognition could help dialogue systems like tutoring systems detect that a student was unhappy, bored, hesitant, confident, and so on. Automatically detecting emotions in reviews or customer responses (anger, dissatisfaction, trust) could help businesses recognize specific problem areas or ones that are going well. Emotion can play a role in medical NLP tasks like helping diagnose depression or suicidal intent. Detecting emotions expressed toward characters in novels might play a role in understanding how different social groups were viewed by society at different times. ", "Bloom_type": "comprehension", "question": "How does detecting emotions potentially benefit dialogue systems?", "options": ["Enhancing the ability of dialogue systems to understand user sentiments and behaviors", "Improving language processing tasks for all types of users equally", "Increasing the speed of dialogues between humans and machines", "Reducing the need for human intervention in dialogue processes"], "complexity": 1}, {"id": 44, "context": "Coreference is an important component of natural language processing. A dialogue system that has just told the user There is a 2pm flight on United and a 4pm one on Cathay Pacific must know which flight the user means by I`ll take the second one. A question answering system that uses Wikipedia to answer a question about Marie Curie must know who she was in the sentence She was born in Warsaw. And a machine translation system translating from a language like Spanish, in which pronouns can be dropped, must use coreference from the previous sentence to decide whether the Spanish sentence Me encanta el conocimiento, dice.` should be translated as I love knowledge, he says`, or I love knowledge, she says`. Indeed, this example comes from an actual news article in El Pas about a female professor and was mistranslated as he in machine translation because of inaccurate coreference resolution (Schiebinger, 2013). ", "Bloom_type": "comprehension", "question": "Explain how coreference works in different types of dialogue systems?", "options": ["Coreference enables dialogue systems to resolve ambiguities in speech.", "Coreference helps users understand the meaning of sentences better.", "Coreference allows machines to remember past conversations more effectively.", "Coreference improves the accuracy of translations between languages."], "complexity": 1}, {"id": 45, "context": "In this second part of the book we introduce fundamental NLP applications: machine translation, information retrieval, question answering, dialogue systems, and speech recognition. ", "Bloom_type": "application", "question": "Which application is not directly mentioned as being introduced in the second part?", "options": ["machine translation", "information retrieval", "question answering", "speech recognition"], "complexity": 2}, {"id": 46, "context": "This chapter introduces the fundamental algorithms of programs that use conversation to interact with users. We often distinguish between two kinds of architectures. Task-oriented dialogue systems converse with users to accomplish fixed tasks like controlling appliances or finding restaurants, relying on a data structure called the frame, which represents the knowledge a system needs to acquire from the user (like the time to set an alarm clock). Chatbots, by contrast, are designed to mimic the longer and more unstructured conversations or chats` characteristic of human-human interaction. Modern systems incorporate aspects of both; industrial chatbots like ChatGPT can carry on longer unstructured conversations; industrial digital assistants like Siri or Alexa are generally frame-based dialogue systems. ", "Bloom_type": "application", "question": "What distinguishes task-oriented dialogue systems from chatbots?", "options": ["Chatbots are designed to mimic human-human interactions.", "Task-oriented dialogue systems rely on a data structure called the frame.", "Modern systems incorporate aspects of both types.", "Industrial chatbots can carry on longer unstructured conversations."], "complexity": 2}, {"id": 47, "context": "The fact that chatbots and dialogue systems are designed for human-computer interaction has strong implications for their design and use. Many of these implications already became clear in one of the earliest chatbots, ELIZA (Weizenbaum, 1966). ELIZA was designed to simulate a Rogerian psychologist, based on a branch of clinical psychology whose methods involve drawing the patient out by reflecting patient`s statements back at them. Rogerian interactions are the rare type of conversation in which, as Weizenbaum points out, one can assume the pose of knowing almost nothing of the real world. If a patient says I went for a long boat ride and the psychiatrist says Tell me about boats, you don`t assume she didn`t know what ", "Bloom_type": "application", "question": "What is a key characteristic of Rogerian interactions?", "options": ["The therapist reflects the patient\u2019s statements back at them.", "The therapist assumes complete knowledge of the patient\u2019s experiences.", "The therapist avoids any direct questions.", "The therapist provides solutions immediately."], "complexity": 2}, {"id": 48, "context": "Turn structure has important implications for spoken dialogue. A human has to know when to stop talking; the client interrupts (in A16 and C17), so a system that was performing this role must know to stop talking (and that the user might be making a correction). A system also has to know when to start talking. For example, most of the time in conversation, speakers start their turns almost immediately after the other speaker finishes, without a long pause, because people are can usually predict when the other person is about to finish talking. Spoken dialogue systems must also detect whether a user is done speaking, so they can process the utterance and respond. This taskcalled endpointing or endpoint detection can be quite challenging because of noise and because people often pause in the middle of turns. ", "Bloom_type": "application", "question": "What is an essential skill for a spoken dialogue system?", "options": ["Detecting the end of a user's speech", "Understanding the language of the user", "Predicting the next word the user will say", "Recognizing different accents"], "complexity": 2}, {"id": 49, "context": "Full mixed initiative, while the norm for human-human conversations, can be difficult for dialogue systems. The most primitive dialogue systems tend to use system-initiative, where the system asks a question and the user can`t do anything until they answer it, or user-initiative like simple search engines, where the user specifies a query and the system passively responds. Even modern large language model-based dialogue systems, which come much closer to using full mixed initiative, often don`t have completely natural initiative switching. Getting this right is an important goal for modern systems. ", "Bloom_type": "application", "question": "What is an important goal for modern dialogue systems?", "options": ["To achieve complete natural initiative switching.", "To make the conversation more complex.", "To increase the efficiency of data processing.", "To improve the understanding of user queries."], "complexity": 2}, {"id": 50, "context": "These subtle characteristics of human conversations (turns, speech acts, grounding, dialogue structure, initiative, and implicature) are among the reasons it is difficult to build dialogue systems that can carry on natural conversations with humans. Many of these challenges are active areas of dialogue systems research. ", "Bloom_type": "application", "question": "What aspect of human conversation makes it challenging for dialogue systems to mimic?", "options": ["The subtlety of social cues", "The complexity of language structures", "The unpredictability of human emotions", "The variability of cultural backgrounds"], "complexity": 2}, {"id": 51, "context": "The frame and its slots in a task-based dialogue system specify what the system needs to know to perform its task. A hotel reservation system needs dates and locations. An alarm clock system needs a time. The system`s goal is to fill the slots in the frame with the fillers the user intends, and then perform the relevant action for the user (answering a question, or booking a flight). ", "Bloom_type": "application", "question": "What should be filled in the slot `time` of an alarm clock system?", "options": ["Time", "Dates", "Locations", "Actions"], "complexity": 2}, {"id": 52, "context": "We can make a very simple frame-based dialogue system by wrapping a small amount of code around this slot extractor. Mainly we just need to ask the user questions until all the slots are full, do a database query, then report back to the user, using hand-built templates for generating sentences. ", "Bloom_type": "application", "question": "What is the first step in building a simple frame-based dialogue system?", "options": ["Ask initial questions", "Design the conversation flow", "Create hand-built templates", "Extract slots from input"], "complexity": 2}, {"id": 53, "context": "It is a common practice for dialogue systems to use further labeled data for finetuning. One function of this fine-tuning step is to improve the quality of the dialogue, training the system to produce responses that are sensible and interesting. Another function might be to improve safety, keeping a dialogue system from suggesting harmful actions (like financial fraud, medical harm, inciting hatred, or abusing the user or other people). ", "Bloom_type": "application", "question": "What is another function of fine-tuning a dialogue system besides improving the quality of the dialogue?", "options": ["To ensure the dialogue remains safe and ethical.", "To make the dialogue more complex.", "To increase the speed of response generation.", "To enhance the system's ability to handle unexpected inputs."], "complexity": 2}, {"id": 54, "context": "In the simplest method for improving quality and safety, speakers of the language are given an initial prompt and instructions to have high-quality, safe dialogues. They then interact with an initial dialogue system and their responses are used to finetune the model, usually as part of the instruct tuning step we introduced in Chapter 12. Thus a dialogue system learns to answer questions, follow other instructions, and also carry on high-quality, safe dialogues, in a single multi-task learning format. ", "Bloom_type": "application", "question": "What is the first step in creating a dialogue system?", "options": ["Collecting user data for training purposes", "Designing the architecture of the dialogue system", "Training the dialogue system using feedback from users", "Selecting appropriate dialogue models"], "complexity": 2}, {"id": 55, "context": "3. Iteratively test the design on users: An iterative design cycle with embedded user testing is essential in system design (Nielsen 1992, Cole et al. 1997, Yankelovich et al. 1995, Landauer 1995). For example in a well-known incident, an early dialogue system required the user to press a key to interrupt the system (Stifelman et al., 1993). But user testing showed users barged in (interrupted, talking over the system), which led to a redesign of the system to recognize overlapped speech. It`s also important to incorporate value sensitive design, in which we carefully consider during the design process the benefits, harms and possible stakeholders of the resulting system (Friedman et al. 2017, Friedman and Hendry 2019). ", "Bloom_type": "application", "question": "What is the primary goal of incorporating value-sensitive design in the development of a dialogue system?", "options": ["To balance the potential benefits, harms, and stakeholder impacts", "To ensure the system is aesthetically pleasing", "To make sure the system can handle all types of input equally", "To focus solely on technical improvements"], "complexity": 2}, {"id": 56, "context": "Another important ethical issue is privacy. Already in the first days of ELIZA, Weizenbaum pointed out the privacy implications of people`s revelations to the chatbot. The ubiquity of in-home dialogue systems means they may often overhear private information (Henderson et al., 2017). If a chatbot is human-like, users are also more likely to disclose private information, and less likely to worry about the harm of this disclosure (Ischen et al., 2019). In general, chatbots that are trained on transcripts of human-human or human-machine conversation must anonymize personally identifiable information. ", "Bloom_type": "application", "question": "What should be done with personal data collected during conversations with a dialogue system?", "options": ["Anonymize it immediately after collection.", "Store it for future reference only.", "Use it to improve the chatbot's performance.", "Delete it once the conversation ends."], "complexity": 2}, {"id": 57, "context": "These ethical issues are an important area of investigation, including finding ways to mitigate problems of abuse and toxicity, like detecting and responding appropriately to toxic contexts (Wolf et al. 2017, Dinan et al. 2020, Xu et al. 2020). Value sensitive design, carefully considering possible harms in advance (Friedman et al. 2017, Friedman and Hendry 2019) is also important; (Dinan et al., 2021) give a number of suggestions for best practices in dialogue system design. For example getting informed consent from participants, whether they are used for training, or whether they are interacting with a deployed system is important. Because dialogue systems by definition involve human participants, researchers also work on these issues with the Institutional Review Boards (IRB) at their institutions, who help protect the safety of experimental subjects. ", "Bloom_type": "application", "question": "What is an important aspect of designing dialogue systems to ensure ethical considerations?", "options": ["Consider potential risks and develop strategies to address them proactively.", "Ensure all users receive equal treatment regardless of their background.", "Implement strict data privacy policies to prevent any misuse of user data.", "Focus solely on improving the accuracy of the dialogue system."], "complexity": 2}, {"id": 58, "context": "Chatbots and dialogue systems are crucial speech and language processing applications that are already widely used commercially. ", "Bloom_type": "application", "question": "What is the primary function of a dialogue system?", "options": ["To facilitate human-computer interaction through natural language", "To provide entertainment through jokes and puns", "To analyze user queries for search engine optimization", "To translate between different languages automatically"], "complexity": 2}, {"id": 59, "context": " Dialogue systems are a kind of human-computer interaction, and general HCI principles apply in their design, including the role of the user, simulations such as Wizard-of-Oz systems, and the importance of iterative design and testing on real users. ", "Bloom_type": "application", "question": "What is an important aspect of designing dialogue systems?", "options": ["Considering the role of the user throughout the development process", "Using only natural language processing techniques", "Focusing solely on machine learning algorithms", "Ignoring the need for iterative testing with real users"], "complexity": 2}, {"id": 60, "context": "Computational-implemented theories of dialogue blossomed in the 1970. That period saw the very influential GUS system (Bobrow et al., 1977), which in the late 1970s established the frame-based paradigm that became the dominant industrial paradigm for dialogue systems for over 30 years. ", "Bloom_type": "application", "question": "What was the primary paradigm used in dialogue systems during the late 1970s?", "options": ["Frame-based paradigm", "Rule-based approach", "Logic programming paradigm", "Natural language processing paradigm"], "complexity": 2}, {"id": 61, "context": "In the 1990s, machine learning models that had first been applied to natural language processing began to be applied to dialogue tasks like slot filling (Miller et al. 1994, Pieraccini et al. 1991). This period also saw lots of analytic work on the linguistic properties of dialogue acts and on machine-learning-based methods for their detection. (Sag and Liberman 1975, Hinkelman and Allen 1989, Nagata and Morimoto 1994, Goodwin 1996, Chu-Carroll 1998, Shriberg et al. 1998, Stolcke et al. 2000, Gravano et al. 2012. This work strongly informed the development of the dialogue-state model (Larsson and Traum, 2000). Dialogue state tracking quickly became an important problem for task-oriented dialogue, and there has been an influential annual evaluation of state-tracking algorithms (Williams et al., 2016). The turn of the century saw a line of work on applying reinforcement learning to dialogue, which first came out of AT&T and Bell Laboratories with work on MDP dialogue systems (Walker 2000, Levin et al. 2000, Singh et al. 2002) along with work on cue phrases, prosody, and rejection and confirmation. Reinforcement learning research turned quickly to the more sophisticated POMDP models (Roy et al. 2000, Lemon et al. 2006, Williams and Young 2007) applied to small slotfilling dialogue tasks. Neural reinforcement learning models have been used both for chatbot systems, for example simulating dialogues between two dialogue systems, rewarding good conversational properties like coherence and ease of answering (Li et al., 2016a), and for task-oriented dialogue (Williams et al., 2017). ", "Bloom_type": "application", "question": "What was a significant application of machine learning in dialogue systems during the early 2000s?", "options": ["Applied reinforcement learning to improve dialogue performance", "Developed neural networks for speech recognition", "Introduced new languages for dialogue translation", "Created advanced visual interfaces for user interaction"], "complexity": 2}, {"id": 62, "context": "By around 2010 the GUS architecture finally began to be widely used commercially in dialogue systems on phones like Apple`s SIRI (Bellegarda, 2013) and other digital assistants. ", "Bloom_type": "application", "question": "What was the primary application of the GUS architecture before 2010?", "options": ["It was primarily used for scientific research.", "It was mainly applied in educational software.", "It was extensively utilized in desktop applications.", "It was predominantly employed in gaming consoles."], "complexity": 2}, {"id": 63, "context": "Could we improve on word error rate as a metric? It would be nice, for example, to have something that didn`t give equal weight to every word, perhaps valuing content words like Tuesday more than function words like a or of. While researchers generally agree that this would be a good idea, it has proved difficult to agree on a metric that works in every application of ASR. For dialogue systems, however, where the desired semantic output is more clear, a metric called slot error rate or concept error rate has proved extremely useful; it is discussed in Chapter 15 on page 317. ", "Bloom_type": "application", "question": "What type of error rate could potentially provide better accuracy in dialogue systems?", "options": ["Slot error rate", "Word error rate", "Sentence error rate", "Phonetic error rate"], "complexity": 2}, {"id": 64, "context": "The goal of text-to-speech (TTS) systems is to map from strings of letters to waveforms, a technology that`s important for a variety of applications from dialogue systems to games to education. ", "Bloom_type": "application", "question": "What is the primary function of TTS systems?", "options": ["To create interactive dialogues", "To convert images to audio", "To translate languages", "To enhance visual content"], "complexity": 2}, {"id": 65, "context": "Although the idea of semantic roles dates back to Pan. ini, they were re-introduced into modern linguistics by Gruber (1965), Fillmore (1966) and Fillmore (1968). Fillmore had become interested in argument structure by studying Lucien Tesni`ere`s groundbreaking Elements de Syntaxe Structurale (Tesni`ere, 1959) in which the term dependency` was introduced and the foundations were laid for dependency grammar. Following Tesni`ere`s terminology, Fillmore first referred to argument roles as actants (Fillmore, 1966) but quickly switched to the term case, (see Fillmore (2003)) and proposed a universal list of semantic roles or cases (Agent, Patient, Instrument, etc.), that could be taken on by the arguments of predicates. Verbs would be listed in the lexicon with their case frame, the list of obligatory (or optional) case arguments. The idea that semantic roles could provide an intermediate level of semantic representation that could help map from syntactic parse structures to deeper, more fully-specified representations of meaning was quickly adopted in natural language processing, and systems for extracting case frames were created for machine translation (Wilks, 1973), question-answering (Hendrix et al., 1973), spoken-language processing (Nash-Webber, 1975), and dialogue systems (Bobrow et al., 1977). Generalpurpose semantic role labelers were developed. The earliest ones (Simmons, 1973) first parsed a sentence by means of an ATN (Augmented Transition Network) parser. Each verb then had a set of rules specifying how the parse should be mapped to semantic roles. These rules mainly made reference to grammatical functions (subject, object, complement of specific prepositions) but also checked constituent internal features such as the animacy of head nouns. Later systems assigned roles from prebuilt parse trees, again by using dictionaries with verb-specific case frames (Levin 1977, Marcus 1980). ", "Bloom_type": "application", "question": "What did Fillmore propose regarding semantic roles?", "options": ["He defined a universal list of semantic roles.", "He suggested creating a new language.", "He invented a new type of grammar.", "He introduced the concept of syntax."], "complexity": 2}, {"id": 66, "context": "Detecting emotion has the potential to improve a number of language processing tasks. Emotion recognition could help dialogue systems like tutoring systems detect that a student was unhappy, bored, hesitant, confident, and so on. Automatically detecting emotions in reviews or customer responses (anger, dissatisfaction, trust) could help businesses recognize specific problem areas or ones that are going well. Emotion can play a role in medical NLP tasks like helping diagnose depression or suicidal intent. Detecting emotions expressed toward characters in novels might play a role in understanding how different social groups were viewed by society at different times. ", "Bloom_type": "application", "question": "Which task could benefit most from automatically detecting emotions?", "options": ["Enhancing sentiment analysis in marketing campaigns", "Automatically translating languages", "Improving natural language generation for automated writing", "Optimizing chatbots for better user engagement"], "complexity": 2}, {"id": 67, "context": "Coreference is an important component of natural language processing. A dialogue system that has just told the user There is a 2pm flight on United and a 4pm one on Cathay Pacific must know which flight the user means by I`ll take the second one. A question answering system that uses Wikipedia to answer a question about Marie Curie must know who she was in the sentence She was born in Warsaw. And a machine translation system translating from a language like Spanish, in which pronouns can be dropped, must use coreference from the previous sentence to decide whether the Spanish sentence Me encanta el conocimiento, dice.` should be translated as I love knowledge, he says`, or I love knowledge, she says`. Indeed, this example comes from an actual news article in El Pas about a female professor and was mistranslated as he in machine translation because of inaccurate coreference resolution (Schiebinger, 2013). ", "Bloom_type": "application", "question": "What strategy does a dialogue system employ when it needs to understand which flight the user refers to?", "options": ["Use coreference resolution techniques to identify the intended meaning.", "Ignore all flights mentioned in the conversation.", "Ask the user for clarification each time.", "Translate the entire conversation word-for-word."], "complexity": 2}]}, "edit distance": {"max_id": 23, "Questions": [{"id": 0, "context": "In the first part of the book we introduce the fundamental suite of algorithmic tools that make up the modern neural language model that is the heart of end-to-end NLP systems. We begin with tokenization and preprocessing, as well as useful algorithms like computing edit distance, and then proceed to the tasks of classification, logistic regression, neural networks, proceeding through feedforward networks, recurrent networks, and then transformers. We`ll also see the role of embeddings as a model of word meaning. ", "Bloom_type": "remember", "question": "In what type of system are neural language models used?", "options": ["Natural Language Processing (NLP)", "Audio recognition", "Image processing", "Robotics"], "complexity": 0}, {"id": 1, "context": "Finally, we`ll need to compare words and other strings. We`ll introduce a metric called edit distance that measures how similar two strings are based on the number of edits (insertions, deletions, substitutions) it takes to change one string into the other. Edit distance is an algorithm with applications throughout language processing, from spelling correction to speech recognition to coreference resolution. ", "Bloom_type": "remember", "question": "In what field does the concept of edit distance find application?", "options": ["Artificial Intelligence", "Computer Science", "Mathematics", "Physics"], "complexity": 0}, {"id": 2, "context": "Edit distance gives us a way to quantify these intuitions about string similarity. More formally, the minimum edit distance between two strings is defined as the minimum number of editing operations (operations like insertion, deletion, substitution) needed to transform one string into another. ", "Bloom_type": "remember", "question": "What does the minimum edit distance measure?", "options": ["The minimum number of editing operations required to transform one string into another", "The maximum number of characters that can be inserted", "The total length of both strings combined", "The difference in character counts between the two strings"], "complexity": 0}, {"id": 3, "context": "How do we find the minimum edit distance? We can think of this as a search task, in which we are searching for the shortest patha sequence of editsfrom one string to another. ", "Bloom_type": "remember", "question": "In the context of finding the minimum edit distance between two strings, what does the term \"search task\" refer to?", "options": ["Calculating the total number of operations needed to transform one string into another", "The process of comparing characters in both strings", "Determining the longest common subsequence between the two strings", "Identifying all possible transformations from one string to another"], "complexity": 0}, {"id": 4, "context": "The intuition of a dynamic programming problem is that a large problem can be solved by properly combining the solutions to various subproblems. Consider the shortest path of transformed words that represents the minimum edit distance between the strings intention and execution shown in Fig. 2.16. ", "Bloom_type": "remember", "question": "In the context of solving problems using dynamic programming, what does the concept of breaking down a larger problem into smaller subproblems help achieve?", "options": ["Reducing computational complexity", "Simplifying complex algorithms", "Increasing memory usage", "Enhancing accuracy of results"], "complexity": 0}, {"id": 5, "context": "Alignment Knowing the minimum edit distance is useful for algorithms like finding potential spelling error corrections. But the edit distance algorithm is important in another way; with a small change, it can also provide the minimum cost alignment between two strings. Aligning two strings is useful throughout speech and language processing. In speech recognition, minimum edit distance alignment is used to compute the word error rate (Chapter 16). Alignment plays a role in machine translation, in which sentences in a parallel corpus (a corpus with a text in two languages) need to be matched to each other. ", "Bloom_type": "remember", "question": "In what application of string alignment does the concept of minimum edit distance play a crucial role?", "options": ["Speech recognition", "Machine learning", "Natural language processing", "Image recognition"], "complexity": 0}, {"id": 6, "context": " The minimum edit distance between two strings is the minimum number of operations it takes to edit one into the other. Minimum edit distance can be computed by dynamic programming, which also results in an alignment of the two strings. ", "Bloom_type": "remember", "question": "What does the minimum edit distance measure?", "options": ["The number of characters that need to be inserted, deleted, or substituted to transform one string into another", "The maximum similarity between two strings", "The total length of the two strings combined", "The time complexity of the algorithm used for computing the edit distance"], "complexity": 0}, {"id": 7, "context": "For more on Herdan`s law and Heaps` Law, see Herdan (1960, p. 28), Heaps (1978), Egghe (2007) and Baayen (2001); For more on edit distance, see Gusfield (1997). Our example measuring the edit distance from intention` to execution` was adapted from Kruskal (1983). There are various publicly available packages to compute edit distance, including Unix diff and the NIST sclite program (NIST, 2005). ", "Bloom_type": "remember", "question": "What is an important concept for understanding Herdan\u2019s law and Heaps\u2019 Law?", "options": ["Edit distance", "Herdan\u2019s law", "Heaps\u2019 Law", "Diffusion of innovations"], "complexity": 0}, {"id": 8, "context": "In the first part of the book we introduce the fundamental suite of algorithmic tools that make up the modern neural language model that is the heart of end-to-end NLP systems. We begin with tokenization and preprocessing, as well as useful algorithms like computing edit distance, and then proceed to the tasks of classification, logistic regression, neural networks, proceeding through feedforward networks, recurrent networks, and then transformers. We`ll also see the role of embeddings as a model of word meaning. ", "Bloom_type": "comprehension", "question": "What is one important algorithm introduced early in the book for processing natural language data?", "options": ["Edit Distance", "Logistic Regression", "Neural Networks", "Feedforward Networks"], "complexity": 1}, {"id": 9, "context": "Finally, we`ll need to compare words and other strings. We`ll introduce a metric called edit distance that measures how similar two strings are based on the number of edits (insertions, deletions, substitutions) it takes to change one string into the other. Edit distance is an algorithm with applications throughout language processing, from spelling correction to speech recognition to coreference resolution. ", "Bloom_type": "comprehension", "question": "What does the edit distance measure?", "options": ["The similarity between two strings by counting the number of insertions, deletions, and substitutions needed.", "The length of the longest common subsequence between two strings.", "The total number of characters in both strings combined.", "The frequency of each character in both strings."], "complexity": 1}, {"id": 10, "context": "Edit distance gives us a way to quantify these intuitions about string similarity. More formally, the minimum edit distance between two strings is defined as the minimum number of editing operations (operations like insertion, deletion, substitution) needed to transform one string into another. ", "Bloom_type": "comprehension", "question": "What does the edit distance measure?", "options": ["The maximum number of characters that can be inserted, deleted, or substituted to make two strings equal.", "The total length of the two strings combined.", "The average number of characters that need to be changed to convert one string into another.", "The exact sequence of each character change required to transform one string into another."], "complexity": 1}, {"id": 11, "context": "How do we find the minimum edit distance? We can think of this as a search task, in which we are searching for the shortest patha sequence of editsfrom one string to another. ", "Bloom_type": "comprehension", "question": "What does finding the minimum edit distance involve?", "options": ["Determining the most efficient sequence of operations from one string to another", "Identifying the longest common subsequence", "Calculating the total number of characters in two strings", "Finding the exact match between two strings"], "complexity": 1}, {"id": 12, "context": "The intuition of a dynamic programming problem is that a large problem can be solved by properly combining the solutions to various subproblems. Consider the shortest path of transformed words that represents the minimum edit distance between the strings intention and execution shown in Fig. 2.16. ", "Bloom_type": "comprehension", "question": "What does the concept of edit distance refer to in this context?", "options": ["The number of operations needed to transform one string into another", "The length of the longest common substring", "The total sum of characters in two strings", "The difference in frequency of each character in two strings"], "complexity": 1}, {"id": 13, "context": "Alignment Knowing the minimum edit distance is useful for algorithms like finding potential spelling error corrections. But the edit distance algorithm is important in another way; with a small change, it can also provide the minimum cost alignment between two strings. Aligning two strings is useful throughout speech and language processing. In speech recognition, minimum edit distance alignment is used to compute the word error rate (Chapter 16). Alignment plays a role in machine translation, in which sentences in a parallel corpus (a corpus with a text in two languages) need to be matched to each other. ", "Bloom_type": "comprehension", "question": "What is an example of how alignment using minimum edit distance contributes to speech and language processing?", "options": ["Both A and B are examples", "Aligning two strings to find potential spelling errors", "Computing the word error rate in speech recognition", "Matching sentences in a parallel corpus for machine translation"], "complexity": 1}, {"id": 14, "context": " The minimum edit distance between two strings is the minimum number of operations it takes to edit one into the other. Minimum edit distance can be computed by dynamic programming, which also results in an alignment of the two strings. ", "Bloom_type": "comprehension", "question": "What does the minimum edit distance measure?", "options": ["The difference between two strings", "The similarity between two strings", "The length of two strings", "The complexity of string manipulation"], "complexity": 1}, {"id": 15, "context": "For more on Herdan`s law and Heaps` Law, see Herdan (1960, p. 28), Heaps (1978), Egghe (2007) and Baayen (2001); For more on edit distance, see Gusfield (1997). Our example measuring the edit distance from intention` to execution` was adapted from Kruskal (1983). There are various publicly available packages to compute edit distance, including Unix diff and the NIST sclite program (NIST, 2005). ", "Bloom_type": "comprehension", "question": "What does the edit distance measure in the context of computing?", "options": ["The difference between two strings", "The similarity between two strings", "The length of two strings", "The number of operations required to transform one string into another"], "complexity": 1}, {"id": 16, "context": "In the first part of the book we introduce the fundamental suite of algorithmic tools that make up the modern neural language model that is the heart of end-to-end NLP systems. We begin with tokenization and preprocessing, as well as useful algorithms like computing edit distance, and then proceed to the tasks of classification, logistic regression, neural networks, proceeding through feedforward networks, recurrent networks, and then transformers. We`ll also see the role of embeddings as a model of word meaning. ", "Bloom_type": "application", "question": "What is the next step after calculating the edit distance in the sequence of operations described?", "options": ["Use embeddings to represent words in the vocabulary", "Implement a simple classifier using logistic regression", "Train a neural network on a dataset", "Develop a transformer-based model for natural language processing"], "complexity": 2}, {"id": 17, "context": "Finally, we`ll need to compare words and other strings. We`ll introduce a metric called edit distance that measures how similar two strings are based on the number of edits (insertions, deletions, substitutions) it takes to change one string into the other. Edit distance is an algorithm with applications throughout language processing, from spelling correction to speech recognition to coreference resolution. ", "Bloom_type": "application", "question": "What does the edit distance measure?", "options": ["The total number of characters that need to be inserted, deleted, or substituted to convert one string into another.", "The length of the longest common subsequence between two strings.", "The frequency of each character in both strings.", "The similarity ratio between two strings."], "complexity": 2}, {"id": 18, "context": "Edit distance gives us a way to quantify these intuitions about string similarity. More formally, the minimum edit distance between two strings is defined as the minimum number of editing operations (operations like insertion, deletion, substitution) needed to transform one string into another. ", "Bloom_type": "application", "question": "What does the edit distance measure?", "options": ["The minimum number of edits required to convert one string into another", "The maximum length of the strings", "The total number of characters in both strings combined", "The longest common subsequence between the two strings"], "complexity": 2}, {"id": 19, "context": "How do we find the minimum edit distance? We can think of this as a search task, in which we are searching for the shortest patha sequence of editsfrom one string to another. ", "Bloom_type": "application", "question": "What is the first step in finding the minimum edit distance between two strings?", "options": ["Identify all possible sequences of edits", "Calculate the total number of operations needed", "Find the longest common subsequence", "Map each character of both strings onto a coordinate system"], "complexity": 2}, {"id": 20, "context": "The intuition of a dynamic programming problem is that a large problem can be solved by properly combining the solutions to various subproblems. Consider the shortest path of transformed words that represents the minimum edit distance between the strings intention and execution shown in Fig. 2.16. ", "Bloom_type": "application", "question": "What is the first step in solving the edit distance problem?", "options": ["Initialize a matrix for storing intermediate results", "Calculate the length of each string separately", "Identify all possible transformations between the two strings", "Find the longest common substring between the two strings"], "complexity": 2}, {"id": 21, "context": "Alignment Knowing the minimum edit distance is useful for algorithms like finding potential spelling error corrections. But the edit distance algorithm is important in another way; with a small change, it can also provide the minimum cost alignment between two strings. Aligning two strings is useful throughout speech and language processing. In speech recognition, minimum edit distance alignment is used to compute the word error rate (Chapter 16). Alignment plays a role in machine translation, in which sentences in a parallel corpus (a corpus with a text in two languages) need to be matched to each other. ", "Bloom_type": "application", "question": "In speech recognition, what does the minimum edit distance alignment help calculate?", "options": ["The word error rate", "The maximum similarity score", "The total number of characters in both strings", "The average word length of the strings"], "complexity": 2}, {"id": 22, "context": " The minimum edit distance between two strings is the minimum number of operations it takes to edit one into the other. Minimum edit distance can be computed by dynamic programming, which also results in an alignment of the two strings. ", "Bloom_type": "application", "question": "What is the first step in computing the minimum edit distance between two strings?", "options": ["Identify the characters at each position in both strings.", "Choose the operation (insertion, deletion, substitution) for each pair of corresponding characters.", "Calculate the total number of possible edits needed.", "Align the two strings using dynamic programming."], "complexity": 2}, {"id": 23, "context": "For more on Herdan`s law and Heaps` Law, see Herdan (1960, p. 28), Heaps (1978), Egghe (2007) and Baayen (2001); For more on edit distance, see Gusfield (1997). Our example measuring the edit distance from intention` to execution` was adapted from Kruskal (1983). There are various publicly available packages to compute edit distance, including Unix diff and the NIST sclite program (NIST, 2005). ", "Bloom_type": "application", "question": "Which method is used for computing the edit distance between two strings?", "options": ["Unix diff", "Gusfield (1997)", "Kruskal (1983)", "NIST sclite"], "complexity": 2}]}, "semantic role": {"max_id": 41, "Questions": [{"id": 0, "context": "Up to now we have been describing text classification tasks with only two classes. But lots of classification tasks in language processing have more than two classes. For sentiment analysis we generally have 3 classes (positive, negative, neutral) and even more classes are common for tasks like part-of-speech tagging, word sense disambiguation, semantic role labeling, emotion detection, and so on. Luckily the naive Bayes algorithm is already a multi-class classification algorithm. ", "Bloom_type": "remember", "question": "In which type of classification task do we often use three or more classes?", "options": ["Sentiment analysis", "Part-of-speech tagging", "Word sense disambiguation", "Emotion detection"], "complexity": 0}, {"id": 1, "context": "Semantic Frames and Roles Closely related to semantic fields is the idea of a semantic frame. A semantic frame is a set of words that denote perspectives or participants in a particular type of event. A commercial transaction, for example, is a kind of event in which one entity trades money to another entity in return for some good or service, after which the good changes hands or perhaps the service is performed. This event can be encoded lexically by using verbs like buy (the event from the perspective of the buyer), sell (from the perspective of the seller), pay (focusing on the monetary aspect), or nouns like buyer. Frames have semantic roles (like buyer, seller, goods, money), and words in a sentence can take on these roles. Knowing that buy and sell have this relation makes it possible for a system to know that a sentence like Sam bought the book from Ling could be paraphrased as Ling sold the book to Sam, and that Sam has the role of the buyer in the frame and Ling the seller. Being able to recognize such paraphrases is important for question answering, and can help in shifting perspective for machine translation. ", "Bloom_type": "remember", "question": "What are semantic frames closely related to?", "options": ["Semantic roles", "Lexical items", "Phrasal structures", "Sentences"], "complexity": 0}, {"id": 2, "context": "They demonstrated near state-of-the-art performance on a number of standard shared tasks including part-of-speech tagging, chunking, named entity recognition and semantic role labeling without the use of hand-engineered features. ", "Bloom_type": "remember", "question": "What did they demonstrate with their model?", "options": ["Semantic role labeling", "State-of-the-art accuracy", "Efficient feature engineering", "Improved computational speed"], "complexity": 0}, {"id": 3, "context": "Approaches that married LSTMs with pretrained collections of word-embeddings based on word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) quickly came to dominate many common tasks: part-of-speech tagging (Ling et al., 2015), syntactic chunking (Sgaard and Goldberg, 2016), named entity recognition (Chiu and Nichols, 2016; Ma and Hovy, 2016), opinion mining (Irsoy and Cardie, 2014), semantic role labeling (Zhou and Xu, 2015a) and AMR parsing (Foland and Martin, 2016). As with the earlier surge of progress involving statistical machine learning, these advances were made possible by the availability of training data provided by CONLL, SemEval, and other shared tasks, as well as shared resources such as Ontonotes (Pradhan et al., 2007b), and PropBank (Palmer et al., 2005). ", "Bloom_type": "remember", "question": "Which approach did not significantly contribute to advancements in semantic role labeling?", "options": ["Part-of-speech tagging", "Syntactic chunking", "Named entity recognition", "AMR parsing"], "complexity": 0}, {"id": 4, "context": "underlying parsing algorithm. This flexibility has led to the development of a diverse set of transition systems that address different aspects of syntax and semantics including: assigning part of speech tags (Choi and Palmer, 2011a), allowing the generation of non-projective dependency structures (Nivre, 2009), assigning semantic roles (Choi and Palmer, 2011b), and parsing texts containing multiple languages (Bhat et al., 2017). ", "Bloom_type": "remember", "question": "What aspect of syntax and semantics is addressed by the underlying parsing algorithm?", "options": ["Semantic role assignment", "Part-of-speech tagging", "Dependency structure generation", "Text parsing across multiple languages"], "complexity": 0}, {"id": 5, "context": "In this chapter we introduce a level of representation that captures the commonality between these sentences: there was a purchase event, the participants were XYZ Corp and some stock, and XYZ Corp was the buyer. These shallow semantic representations , semantic roles, express the role that arguments of a predicate take in the event, codified in databases like PropBank and FrameNet. We`ll introduce semantic role labeling, the task of assigning roles to spans in sentences, and selectional restrictions, the preferences that predicates express about their arguments, such as the fact that the theme of eat is generally something edible. ", "Bloom_type": "remember", "question": "In what way do semantic roles help in understanding the structure of events described by sentences?", "options": ["Semantic roles offer a simplified view of events, focusing on the relationships between different parts of speech.", "Semantic roles provide a detailed account of all possible outcomes of an event.", "Semantic roles are used to identify the grammatical function of each word within a sentence.", "Semantic roles help in determining the exact time when an event occurs."], "complexity": 0}, {"id": 6, "context": "The main reason computational systems use semantic roles is to act as a shallow meaning representation that can let us make simple inferences that aren`t possible from the pure surface string of words, or even from the parse tree. To extend the earlier examples, if a document says that Company A acquired Company B, we`d like to know that this answers the query Was Company B acquired? despite the fact that the two sentences have very different surface syntax. Similarly, this shallow semantics might act as a useful intermediate language in machine translation. ", "Bloom_type": "remember", "question": "In what way do computational systems utilize semantic roles?", "options": ["To simplify the process of natural language processing", "To enhance the complexity of sentence structures", "To provide deeper understanding of word meanings", "To increase the length of sentences"], "complexity": 0}, {"id": 7, "context": "A PropBank semantic role labeling would allow us to infer the commonality in the event structures of the following three examples, that is, that in each case Big Fruit Co. is the AGENT and the price of bananas is the THEME, despite the differing surface forms. ", "Bloom_type": "remember", "question": "In the examples provided, what does Big Fruit Co. play a role as?", "options": ["AGENT", "THEME", "OBJECT", "PREDICATE"], "complexity": 0}, {"id": 8, "context": "Semantic role labeling (sometimes shortened as SRL) is the task of automatically finding the semantic roles of each argument of each predicate in a sentence. Current approaches to semantic role labeling are based on supervised machine learning, often using the FrameNet and PropBank resources to specify what counts as a predicate, define the set of roles used in the task, and provide training and test sets. ", "Bloom_type": "remember", "question": "In the process of semantic role labeling, which resource is primarily utilized for defining the set of roles?", "options": ["FrameNet", "PropBank", "WordNet", "Moby Dick"], "complexity": 0}, {"id": 9, "context": "For each of these predicates, the algorithm examines each node in the parse tree and uses supervised classification to decide the semantic role (if any) it plays for this predicate. Given a labeled training set such as PropBank or FrameNet, a feature vector is extracted for each node, using feature templates described in the next subsection. A 1-of-N classifier is then trained to predict a semantic role for each constituent given these features, where N is the number of potential semantic roles plus an extra NONE role for non-role constituents. Any standard classification algorithms can be used. Finally, for each test sentence to be labeled, the classifier is run on each relevant constituent. ", "Bloom_type": "remember", "question": "In the process of labeling sentences with semantic roles, what type of machine learning model is typically used?", "options": ["Support Vector Machine", "Random Forest", "K-Nearest Neighbors", "Naive Bayes"], "complexity": 0}, {"id": 10, "context": "Although the idea of semantic roles dates back to Pan. ini, they were re-introduced into modern linguistics by Gruber (1965), Fillmore (1966) and Fillmore (1968). Fillmore had become interested in argument structure by studying Lucien Tesni`ere`s groundbreaking Elements de Syntaxe Structurale (Tesni`ere, 1959) in which the term dependency` was introduced and the foundations were laid for dependency grammar. Following Tesni`ere`s terminology, Fillmore first referred to argument roles as actants (Fillmore, 1966) but quickly switched to the term case, (see Fillmore (2003)) and proposed a universal list of semantic roles or cases (Agent, Patient, Instrument, etc.), that could be taken on by the arguments of predicates. Verbs would be listed in the lexicon with their case frame, the list of obligatory (or optional) case arguments. The idea that semantic roles could provide an intermediate level of semantic representation that could help map from syntactic parse structures to deeper, more fully-specified representations of meaning was quickly adopted in natural language processing, and systems for extracting case frames were created for machine translation (Wilks, 1973), question-answering (Hendrix et al., 1973), spoken-language processing (Nash-Webber, 1975), and dialogue systems (Bobrow et al., 1977). Generalpurpose semantic role labelers were developed. The earliest ones (Simmons, 1973) first parsed a sentence by means of an ATN (Augmented Transition Network) parser. Each verb then had a set of rules specifying how the parse should be mapped to semantic roles. These rules mainly made reference to grammatical functions (subject, object, complement of specific prepositions) but also checked constituent internal features such as the animacy of head nouns. Later systems assigned roles from prebuilt parse trees, again by using dictionaries with verb-specific case frames (Levin 1977, Marcus 1980). ", "Bloom_type": "remember", "question": "According to the context, what did Fillmore propose?", "options": ["Universal list of semantic roles", "Semantic roles", "Dependency grammar", "Argument structure"], "complexity": 0}, {"id": 11, "context": "The combination of rich linguistic annotation and corpus-based approach instantiated in FrameNet and PropBank led to a revival of automatic approaches to semantic role labeling, first on FrameNet (Gildea and Jurafsky, 2000) and then on PropBank data (Gildea and Palmer, 2002, inter alia). The problem first addressed in the 1970s by handwritten rules was thus now generally recast as one of supervised machine learning enabled by large and consistent databases. Many popular features used for role labeling are defined in Gildea and Jurafsky (2002), Surdeanu et al. (2003), Xue and Palmer (2004), Pradhan et al. (2005), Che et al. (2009), and Zhao et al. (2009). The use of dependency rather than constituency parses was introduced in the CoNLL-2008 shared task (Surdeanu et al., 2008). For surveys see Palmer et al. (2010) and M`arquez et al. (2008). ", "Bloom_type": "remember", "question": "In what year did the problem of semantic role labeling first receive attention?", "options": ["1970", "1960", "1980", "1990"], "complexity": 0}, {"id": 12, "context": "The use of neural approaches to semantic role labeling was pioneered by Collobert et al. (2011), who applied a CRF on top of a convolutional net. Early work like Foland, Jr. and Martin (2015) focused on using dependency features. Later work eschewed syntactic features altogether; Zhou and Xu (2015b) introduced the use of a stacked (6-8 layer) biLSTM architecture, and (He et al., 2017) showed how to augment the biLSTM architecture with highway networks and also replace the CRF with A* decoding that make it possible to apply a wide variety of global constraints in SRL decoding. ", "Bloom_type": "remember", "question": "In what year did Collobert et al. pioneer the application of neural approaches to semantic role labeling?", "options": ["2013", "2014", "2015", "2016"], "complexity": 0}, {"id": 13, "context": "Selectional preference has been widely studied beyond the selectional association models of Resnik (1993) and Resnik (1996). Methods have included clustering (Rooth et al., 1999), discriminative learning (Bergsma et al., 2008a), and topic models (Seaghdha 2010, Ritter et al. 2010b), and constraints can be expressed at the level of words or classes (Agirre and Martinez, 2001). Selectional preferences have also been successfully integrated into semantic role labeling (Erk 2007, Zapirain et al. 2013, Do et al. 2017). ", "Bloom_type": "remember", "question": "Which method for studying selectional preferences involves expressing constraints at the level of words or classes?", "options": ["All of the above", "Clustering", "Discriminative learning", "Topic modeling"], "complexity": 0}, {"id": 14, "context": "Up to now we have been describing text classification tasks with only two classes. But lots of classification tasks in language processing have more than two classes. For sentiment analysis we generally have 3 classes (positive, negative, neutral) and even more classes are common for tasks like part-of-speech tagging, word sense disambiguation, semantic role labeling, emotion detection, and so on. Luckily the naive Bayes algorithm is already a multi-class classification algorithm. ", "Bloom_type": "comprehension", "question": "What does the phrase'semantic role' refer to in the context?", "options": ["The role of a sentence in conveying meaning.", "The function of a word within a sentence.", "The type of data being classified.", "The number of classes in a classification task."], "complexity": 1}, {"id": 15, "context": "Semantic Frames and Roles Closely related to semantic fields is the idea of a semantic frame. A semantic frame is a set of words that denote perspectives or participants in a particular type of event. A commercial transaction, for example, is a kind of event in which one entity trades money to another entity in return for some good or service, after which the good changes hands or perhaps the service is performed. This event can be encoded lexically by using verbs like buy (the event from the perspective of the buyer), sell (from the perspective of the seller), pay (focusing on the monetary aspect), or nouns like buyer. Frames have semantic roles (like buyer, seller, goods, money), and words in a sentence can take on these roles. Knowing that buy and sell have this relation makes it possible for a system to know that a sentence like Sam bought the book from Ling could be paraphrased as Ling sold the book to Sam, and that Sam has the role of the buyer in the frame and Ling the seller. Being able to recognize such paraphrases is important for question answering, and can help in shifting perspective for machine translation. ", "Bloom_type": "comprehension", "question": "What are semantic roles in the context of semantic frames?", "options": ["Semantic roles represent the objects involved in an event described by a sentence.", "Semantic roles refer to the relationships between different parts of speech within a sentence.", "Semantic roles describe the actions performed by entities in a sentence.", "Semantic roles indicate the grammatical function of each word in a sentence."], "complexity": 1}, {"id": 16, "context": "They demonstrated near state-of-the-art performance on a number of standard shared tasks including part-of-speech tagging, chunking, named entity recognition and semantic role labeling without the use of hand-engineered features. ", "Bloom_type": "comprehension", "question": "What type of task did they demonstrate superior performance in among those mentioned?", "options": ["Semantic Role Labeling", "Part-of-speech tagging", "Chunking", "Named Entity Recognition"], "complexity": 1}, {"id": 17, "context": "Approaches that married LSTMs with pretrained collections of word-embeddings based on word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) quickly came to dominate many common tasks: part-of-speech tagging (Ling et al., 2015), syntactic chunking (Sgaard and Goldberg, 2016), named entity recognition (Chiu and Nichols, 2016; Ma and Hovy, 2016), opinion mining (Irsoy and Cardie, 2014), semantic role labeling (Zhou and Xu, 2015a) and AMR parsing (Foland and Martin, 2016). As with the earlier surge of progress involving statistical machine learning, these advances were made possible by the availability of training data provided by CONLL, SemEval, and other shared tasks, as well as shared resources such as Ontonotes (Pradhan et al., 2007b), and PropBank (Palmer et al., 2005). ", "Bloom_type": "comprehension", "question": "Explain how approaches combining LSTM models with pre-trained word embeddings have contributed significantly to various natural language processing tasks.", "options": ["They improve understanding of sentence structure through better integration of contextualized representations.", "These methods enhance performance through improved accuracy in parts-of-speech tagging.", "This combination leads to enhanced ability for identifying entities within sentences.", "It allows for more effective analysis of opinions expressed in texts."], "complexity": 1}, {"id": 18, "context": "underlying parsing algorithm. This flexibility has led to the development of a diverse set of transition systems that address different aspects of syntax and semantics including: assigning part of speech tags (Choi and Palmer, 2011a), allowing the generation of non-projective dependency structures (Nivre, 2009), assigning semantic roles (Choi and Palmer, 2011b), and parsing texts containing multiple languages (Bhat et al., 2017). ", "Bloom_type": "comprehension", "question": "What aspect of syntax and semantics does the underlying parsing algorithm focus on?", "options": ["Assigning semantic roles", "Assigning part of speech tags", "Generating non-projective dependency structures", "Parsing texts containing multiple languages"], "complexity": 1}, {"id": 19, "context": "In this chapter we introduce a level of representation that captures the commonality between these sentences: there was a purchase event, the participants were XYZ Corp and some stock, and XYZ Corp was the buyer. These shallow semantic representations , semantic roles, express the role that arguments of a predicate take in the event, codified in databases like PropBank and FrameNet. We`ll introduce semantic role labeling, the task of assigning roles to spans in sentences, and selectional restrictions, the preferences that predicates express about their arguments, such as the fact that the theme of eat is generally something edible. ", "Bloom_type": "comprehension", "question": "What are shallow semantic representations introduced in the chapter?", "options": ["Semantic roles", "Predicate expressions", "Argument structures", "Event descriptions"], "complexity": 1}, {"id": 20, "context": "The main reason computational systems use semantic roles is to act as a shallow meaning representation that can let us make simple inferences that aren`t possible from the pure surface string of words, or even from the parse tree. To extend the earlier examples, if a document says that Company A acquired Company B, we`d like to know that this answers the query Was Company B acquired? despite the fact that the two sentences have very different surface syntax. Similarly, this shallow semantics might act as a useful intermediate language in machine translation. ", "Bloom_type": "comprehension", "question": "Explain why semantic roles are important for computational systems?", "options": ["To enhance the accuracy of natural language processing", "To provide a deep understanding of the sentence structure", "To simplify complex sentences into basic components", "To increase the complexity of translations between languages"], "complexity": 1}, {"id": 21, "context": "A PropBank semantic role labeling would allow us to infer the commonality in the event structures of the following three examples, that is, that in each case Big Fruit Co. is the AGENT and the price of bananas is the THEME, despite the differing surface forms. ", "Bloom_type": "comprehension", "question": "What is the primary function of semantic roles in PropBank?", "options": ["To classify the agent and theme in events", "To identify the subject of an event", "To determine the object of an event", "To analyze the causative relationship between actions"], "complexity": 1}, {"id": 22, "context": "Semantic role labeling (sometimes shortened as SRL) is the task of automatically finding the semantic roles of each argument of each predicate in a sentence. Current approaches to semantic role labeling are based on supervised machine learning, often using the FrameNet and PropBank resources to specify what counts as a predicate, define the set of roles used in the task, and provide training and test sets. ", "Bloom_type": "comprehension", "question": "What does semantic role labeling involve?", "options": ["Finding the semantic roles of arguments within predicates in a sentence.", "Automatically identifying the grammatical function of each word in a sentence.", "Determining the syntactic structure of sentences.", "Classifying verbs into different parts of speech."], "complexity": 1}, {"id": 23, "context": "For each of these predicates, the algorithm examines each node in the parse tree and uses supervised classification to decide the semantic role (if any) it plays for this predicate. Given a labeled training set such as PropBank or FrameNet, a feature vector is extracted for each node, using feature templates described in the next subsection. A 1-of-N classifier is then trained to predict a semantic role for each constituent given these features, where N is the number of potential semantic roles plus an extra NONE role for non-role constituents. Any standard classification algorithms can be used. Finally, for each test sentence to be labeled, the classifier is run on each relevant constituent. ", "Bloom_type": "comprehension", "question": "What does the algorithm do when examining nodes in a parse tree?", "options": ["It predicts the semantic role based on the features extracted from the node.", "It decides if there are any semantic roles played by the node.", "It classifies all constituents into one of two categories: role or non-role.", "It ignores the nodes and focuses only on the labels."], "complexity": 1}, {"id": 24, "context": "Although the idea of semantic roles dates back to Pan. ini, they were re-introduced into modern linguistics by Gruber (1965), Fillmore (1966) and Fillmore (1968). Fillmore had become interested in argument structure by studying Lucien Tesni`ere`s groundbreaking Elements de Syntaxe Structurale (Tesni`ere, 1959) in which the term dependency` was introduced and the foundations were laid for dependency grammar. Following Tesni`ere`s terminology, Fillmore first referred to argument roles as actants (Fillmore, 1966) but quickly switched to the term case, (see Fillmore (2003)) and proposed a universal list of semantic roles or cases (Agent, Patient, Instrument, etc.), that could be taken on by the arguments of predicates. Verbs would be listed in the lexicon with their case frame, the list of obligatory (or optional) case arguments. The idea that semantic roles could provide an intermediate level of semantic representation that could help map from syntactic parse structures to deeper, more fully-specified representations of meaning was quickly adopted in natural language processing, and systems for extracting case frames were created for machine translation (Wilks, 1973), question-answering (Hendrix et al., 1973), spoken-language processing (Nash-Webber, 1975), and dialogue systems (Bobrow et al., 1977). Generalpurpose semantic role labelers were developed. The earliest ones (Simmons, 1973) first parsed a sentence by means of an ATN (Augmented Transition Network) parser. Each verb then had a set of rules specifying how the parse should be mapped to semantic roles. These rules mainly made reference to grammatical functions (subject, object, complement of specific prepositions) but also checked constituent internal features such as the animacy of head nouns. Later systems assigned roles from prebuilt parse trees, again by using dictionaries with verb-specific case frames (Levin 1977, Marcus 1980). ", "Bloom_type": "comprehension", "question": "What did Fillmore initially refer to as argument roles before switching to the term case?", "options": ["Grammatical functions", "Head nouns", "Subject and object", "Verb and noun"], "complexity": 1}, {"id": 25, "context": "The combination of rich linguistic annotation and corpus-based approach instantiated in FrameNet and PropBank led to a revival of automatic approaches to semantic role labeling, first on FrameNet (Gildea and Jurafsky, 2000) and then on PropBank data (Gildea and Palmer, 2002, inter alia). The problem first addressed in the 1970s by handwritten rules was thus now generally recast as one of supervised machine learning enabled by large and consistent databases. Many popular features used for role labeling are defined in Gildea and Jurafsky (2002), Surdeanu et al. (2003), Xue and Palmer (2004), Pradhan et al. (2005), Che et al. (2009), and Zhao et al. (2009). The use of dependency rather than constituency parses was introduced in the CoNLL-2008 shared task (Surdeanu et al., 2008). For surveys see Palmer et al. (2010) and M`arquez et al. (2008). ", "Bloom_type": "comprehension", "question": "What methodological shift occurred in the study of semantic roles due to advancements in technology and database availability?", "options": ["From manual rule-based systems to automated machine learning techniques", "From lexical analysis to syntactic parsing", "From corpus-based studies to individual case analyses", "From qualitative research to quantitative methods"], "complexity": 1}, {"id": 26, "context": "The use of neural approaches to semantic role labeling was pioneered by Collobert et al. (2011), who applied a CRF on top of a convolutional net. Early work like Foland, Jr. and Martin (2015) focused on using dependency features. Later work eschewed syntactic features altogether; Zhou and Xu (2015b) introduced the use of a stacked (6-8 layer) biLSTM architecture, and (He et al., 2017) showed how to augment the biLSTM architecture with highway networks and also replace the CRF with A* decoding that make it possible to apply a wide variety of global constraints in SRL decoding. ", "Bloom_type": "comprehension", "question": "What approach did Collobert et al. take for semantic role labeling?", "options": ["CRF on top of a convolutional net", "Dependency features", "BiLSTM architecture", "Highway networks"], "complexity": 1}, {"id": 27, "context": "Selectional preference has been widely studied beyond the selectional association models of Resnik (1993) and Resnik (1996). Methods have included clustering (Rooth et al., 1999), discriminative learning (Bergsma et al., 2008a), and topic models (Seaghdha 2010, Ritter et al. 2010b), and constraints can be expressed at the level of words or classes (Agirre and Martinez, 2001). Selectional preferences have also been successfully integrated into semantic role labeling (Erk 2007, Zapirain et al. 2013, Do et al. 2017). ", "Bloom_type": "comprehension", "question": "How are selectional preferences applied in semantic role labeling?", "options": ["Selectional preferences integrate into the model by classifying objects.", "Selectional preferences are only used for word-level constraints.", "Semantic roles are directly assigned based on predefined rules.", "No integration occurs; they remain separate processes."], "complexity": 1}, {"id": 28, "context": "Up to now we have been describing text classification tasks with only two classes. But lots of classification tasks in language processing have more than two classes. For sentiment analysis we generally have 3 classes (positive, negative, neutral) and even more classes are common for tasks like part-of-speech tagging, word sense disambiguation, semantic role labeling, emotion detection, and so on. Luckily the naive Bayes algorithm is already a multi-class classification algorithm. ", "Bloom_type": "application", "question": "What is an example of a task that can benefit from using the Naive Bayes algorithm?", "options": ["Sentiment analysis", "Text summarization", "Machine translation", "Image recognition"], "complexity": 2}, {"id": 29, "context": "Semantic Frames and Roles Closely related to semantic fields is the idea of a semantic frame. A semantic frame is a set of words that denote perspectives or participants in a particular type of event. A commercial transaction, for example, is a kind of event in which one entity trades money to another entity in return for some good or service, after which the good changes hands or perhaps the service is performed. This event can be encoded lexically by using verbs like buy (the event from the perspective of the buyer), sell (from the perspective of the seller), pay (focusing on the monetary aspect), or nouns like buyer. Frames have semantic roles (like buyer, seller, goods, money), and words in a sentence can take on these roles. Knowing that buy and sell have this relation makes it possible for a system to know that a sentence like Sam bought the book from Ling could be paraphrased as Ling sold the book to Sam, and that Sam has the role of the buyer in the frame and Ling the seller. Being able to recognize such paraphrases is important for question answering, and can help in shifting perspective for machine translation. ", "Bloom_type": "application", "question": "What are semantic frames used for?", "options": ["To describe the relationships between entities in a sentence", "To categorize objects based on their properties", "To identify the subjects and predicates in a sentence", "To determine the meaning of individual words"], "complexity": 2}, {"id": 30, "context": "They demonstrated near state-of-the-art performance on a number of standard shared tasks including part-of-speech tagging, chunking, named entity recognition and semantic role labeling without the use of hand-engineered features. ", "Bloom_type": "application", "question": "What type of task did they perform without using hand-engineered features?", "options": ["Natural language processing (NLP)", "Machine learning classification", "Image recognition", "Audio analysis"], "complexity": 2}, {"id": 31, "context": "Approaches that married LSTMs with pretrained collections of word-embeddings based on word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) quickly came to dominate many common tasks: part-of-speech tagging (Ling et al., 2015), syntactic chunking (Sgaard and Goldberg, 2016), named entity recognition (Chiu and Nichols, 2016; Ma and Hovy, 2016), opinion mining (Irsoy and Cardie, 2014), semantic role labeling (Zhou and Xu, 2015a) and AMR parsing (Foland and Martin, 2016). As with the earlier surge of progress involving statistical machine learning, these advances were made possible by the availability of training data provided by CONLL, SemEval, and other shared tasks, as well as shared resources such as Ontonotes (Pradhan et al., 2007b), and PropBank (Palmer et al., 2005). ", "Bloom_type": "application", "question": "What is the primary method used for semantic role labeling?", "options": ["Combining LSTMs with Word2Vec and GloVe", "Using pre-trained embeddings only", "Training solely on individual datasets", "Utilizing shared resources exclusively"], "complexity": 2}, {"id": 32, "context": "underlying parsing algorithm. This flexibility has led to the development of a diverse set of transition systems that address different aspects of syntax and semantics including: assigning part of speech tags (Choi and Palmer, 2011a), allowing the generation of non-projective dependency structures (Nivre, 2009), assigning semantic roles (Choi and Palmer, 2011b), and parsing texts containing multiple languages (Bhat et al., 2017). ", "Bloom_type": "application", "question": "Which aspect of syntax is primarily addressed by the underlying parsing algorithm?", "options": ["Assigning semantic roles", "Assigning part of speech tags", "Allowing the generation of non-projective dependency structures", "Parsing texts containing multiple languages"], "complexity": 2}, {"id": 33, "context": "In this chapter we introduce a level of representation that captures the commonality between these sentences: there was a purchase event, the participants were XYZ Corp and some stock, and XYZ Corp was the buyer. These shallow semantic representations , semantic roles, express the role that arguments of a predicate take in the event, codified in databases like PropBank and FrameNet. We`ll introduce semantic role labeling, the task of assigning roles to spans in sentences, and selectional restrictions, the preferences that predicates express about their arguments, such as the fact that the theme of eat is generally something edible. ", "Bloom_type": "application", "question": "What does the term'semantic role' refer to in the context?", "options": ["The position of an argument within a sentence", "The type of food eaten by an entity", "The preference for certain types of objects over others", "The sequence of events in a story"], "complexity": 2}, {"id": 34, "context": "The main reason computational systems use semantic roles is to act as a shallow meaning representation that can let us make simple inferences that aren`t possible from the pure surface string of words, or even from the parse tree. To extend the earlier examples, if a document says that Company A acquired Company B, we`d like to know that this answers the query Was Company B acquired? despite the fact that the two sentences have very different surface syntax. Similarly, this shallow semantics might act as a useful intermediate language in machine translation. ", "Bloom_type": "application", "question": "What is the primary purpose of using semantic roles in computational systems?", "options": ["To simplify the interpretation of ambiguous phrases.", "To enhance the accuracy of natural language processing algorithms.", "To provide a deeper understanding of sentence structure.", "To facilitate complex logical reasoning tasks."], "complexity": 2}, {"id": 35, "context": "A PropBank semantic role labeling would allow us to infer the commonality in the event structures of the following three examples, that is, that in each case Big Fruit Co. is the AGENT and the price of bananas is the THEME, despite the differing surface forms. ", "Bloom_type": "application", "question": "What is the primary purpose of using PropBank for semantic role labeling?", "options": ["To categorize events based on their roles", "To identify the subject of the sentence", "To determine the object of the sentence", "To analyze the grammatical structure"], "complexity": 2}, {"id": 36, "context": "Semantic role labeling (sometimes shortened as SRL) is the task of automatically finding the semantic roles of each argument of each predicate in a sentence. Current approaches to semantic role labeling are based on supervised machine learning, often using the FrameNet and PropBank resources to specify what counts as a predicate, define the set of roles used in the task, and provide training and test sets. ", "Bloom_type": "application", "question": "What is the first step in developing an automated system for semantic role labeling?", "options": ["Collecting annotated data", "Training neural networks", "Designing algorithms", "Selecting appropriate resources"], "complexity": 2}, {"id": 37, "context": "For each of these predicates, the algorithm examines each node in the parse tree and uses supervised classification to decide the semantic role (if any) it plays for this predicate. Given a labeled training set such as PropBank or FrameNet, a feature vector is extracted for each node, using feature templates described in the next subsection. A 1-of-N classifier is then trained to predict a semantic role for each constituent given these features, where N is the number of potential semantic roles plus an extra NONE role for non-role constituents. Any standard classification algorithms can be used. Finally, for each test sentence to be labeled, the classifier is run on each relevant constituent. ", "Bloom_type": "application", "question": "Which step involves extracting features for each node in the parse tree?", "options": ["Examining each node in the parse tree", "Training the classifier with labeled data", "Running the classifier on each relevant constituent", "Deciding the semantic role for each predicate"], "complexity": 2}, {"id": 38, "context": "Although the idea of semantic roles dates back to Pan. ini, they were re-introduced into modern linguistics by Gruber (1965), Fillmore (1966) and Fillmore (1968). Fillmore had become interested in argument structure by studying Lucien Tesni`ere`s groundbreaking Elements de Syntaxe Structurale (Tesni`ere, 1959) in which the term dependency` was introduced and the foundations were laid for dependency grammar. Following Tesni`ere`s terminology, Fillmore first referred to argument roles as actants (Fillmore, 1966) but quickly switched to the term case, (see Fillmore (2003)) and proposed a universal list of semantic roles or cases (Agent, Patient, Instrument, etc.), that could be taken on by the arguments of predicates. Verbs would be listed in the lexicon with their case frame, the list of obligatory (or optional) case arguments. The idea that semantic roles could provide an intermediate level of semantic representation that could help map from syntactic parse structures to deeper, more fully-specified representations of meaning was quickly adopted in natural language processing, and systems for extracting case frames were created for machine translation (Wilks, 1973), question-answering (Hendrix et al., 1973), spoken-language processing (Nash-Webber, 1975), and dialogue systems (Bobrow et al., 1977). Generalpurpose semantic role labelers were developed. The earliest ones (Simmons, 1973) first parsed a sentence by means of an ATN (Augmented Transition Network) parser. Each verb then had a set of rules specifying how the parse should be mapped to semantic roles. These rules mainly made reference to grammatical functions (subject, object, complement of specific prepositions) but also checked constituent internal features such as the animacy of head nouns. Later systems assigned roles from prebuilt parse trees, again by using dictionaries with verb-specific case frames (Levin 1977, Marcus 1980). ", "Bloom_type": "application", "question": "What did Fillmore propose regarding semantic roles?", "options": ["He defined a universal list of semantic roles.", "He suggested a new word.", "He coined the term 'dependency'.", "He introduced the concept of argument structure."], "complexity": 2}, {"id": 39, "context": "The combination of rich linguistic annotation and corpus-based approach instantiated in FrameNet and PropBank led to a revival of automatic approaches to semantic role labeling, first on FrameNet (Gildea and Jurafsky, 2000) and then on PropBank data (Gildea and Palmer, 2002, inter alia). The problem first addressed in the 1970s by handwritten rules was thus now generally recast as one of supervised machine learning enabled by large and consistent databases. Many popular features used for role labeling are defined in Gildea and Jurafsky (2002), Surdeanu et al. (2003), Xue and Palmer (2004), Pradhan et al. (2005), Che et al. (2009), and Zhao et al. (2009). The use of dependency rather than constituency parses was introduced in the CoNLL-2008 shared task (Surdeanu et al., 2008). For surveys see Palmer et al. (2010) and M`arquez et al. (2008). ", "Bloom_type": "application", "question": "What is the main difference between using constituency and dependency parses for semantic role labeling?", "options": ["Dependency parses can handle larger datasets more efficiently.", "Dependency parses require more computational resources.", "Constituency parses provide better accuracy for all types of roles.", "Constituency parses are easier to implement but less accurate."], "complexity": 2}, {"id": 40, "context": "The use of neural approaches to semantic role labeling was pioneered by Collobert et al. (2011), who applied a CRF on top of a convolutional net. Early work like Foland, Jr. and Martin (2015) focused on using dependency features. Later work eschewed syntactic features altogether; Zhou and Xu (2015b) introduced the use of a stacked (6-8 layer) biLSTM architecture, and (He et al., 2017) showed how to augment the biLSTM architecture with highway networks and also replace the CRF with A* decoding that make it possible to apply a wide variety of global constraints in SRL decoding. ", "Bloom_type": "application", "question": "What is an early approach used for semantic role labeling?", "options": ["Dependency features", "CRF on top of a convolutional net", "Highway networks", "BiLSTM architecture"], "complexity": 2}, {"id": 41, "context": "Selectional preference has been widely studied beyond the selectional association models of Resnik (1993) and Resnik (1996). Methods have included clustering (Rooth et al., 1999), discriminative learning (Bergsma et al., 2008a), and topic models (Seaghdha 2010, Ritter et al. 2010b), and constraints can be expressed at the level of words or classes (Agirre and Martinez, 2001). Selectional preferences have also been successfully integrated into semantic role labeling (Erk 2007, Zapirain et al. 2013, Do et al. 2017). ", "Bloom_type": "application", "question": "Which method is used for integrating selectional preferences with semantic role labeling?", "options": ["Clustering", "Discriminative Learning", "Topic Models", "Word Constraints"], "complexity": 2}]}, "text classification": {"max_id": 56, "Questions": [{"id": 0, "context": "The simplest way to do smoothing is to add one to all the n-gram counts, before we normalize them into probabilities. All the counts that used to be zero will now have a count of 1, the counts of 1 will be 2, and so on. This algorithm is called Laplace smoothing. Laplace smoothing does not perform well enough to be used in modern n-gram models, but it usefully introduces many of the concepts that we see in other smoothing algorithms, gives a useful baseline, and is also a practical smoothing algorithm for other tasks like text classification (Chapter 4). ", "Bloom_type": "remember", "question": "What is an example of a simple smoothing technique mentioned in the context?", "options": ["Adding one to all n-gram counts", "Using higher-order n-grams", "Removing zero counts from n-grams", "Applying Bayesian inference"], "complexity": 0}, {"id": 1, "context": "Another thing we might want to know about a text is the language it`s written in. Texts on social media, for example, can be in any number of languages and we`ll need to apply different processing. The task of language id is thus the first step in most language processing pipelines. Related text classification tasks like authorship attribution determining a text`s author are also relevant to the digital humanities, social sciences, and forensic linguistics. ", "Bloom_type": "remember", "question": "In what field do texts on social media often contain various languages?", "options": ["Natural Language Processing", "Digital Humanities", "Forensic Linguistics", "Social Sciences"], "complexity": 0}, {"id": 2, "context": "Finally, one of the oldest tasks in text classification is assigning a library subject category or topic label to a text. Deciding whether a research paper concerns epidemiology or instead, perhaps, embryology, is an important component of information retrieval. Various sets of subject categories exist, such as the MeSH (Medical Subject Headings) thesaurus. In fact, as we will see, subject category classification is the task for which the naive Bayes algorithm was invented in 1961 (Maron, 1961). Classification is essential for tasks below the level of the document as well. We`ve already seen period disambiguation (deciding if a period is the end of a sentence or part of a word), and word tokenization (deciding if a character should be a word boundary). Even language modeling can be viewed as classification: each word can be thought of as a class, and so predicting the next word is classifying the context-so-far into a class for each next word. A part-of-speech tagger (Chapter 17) classifies each occurrence of a word in a sentence as, e.g., a noun or a verb. ", "Bloom_type": "remember", "question": "In what year did the naive Bayes algorithm get its invention?", "options": ["1961", "1950", "1940", "1930"], "complexity": 0}, {"id": 3, "context": "Formally, the task of supervised classification is to take an input x and a fixed set of output classes Y = Y . For text classification, we`ll sometimes talk about c (for class) instead of y as our output variable, and d (for document) instead of x as our input variable. In the supervised situation we have a training set of N documents that have each been hand(d1, c1), ...., (dN, cN) . Our goal is to learn a classifier that is labeled with a class: } { C, where C is capable of mapping from a new document d to its correct class c some set of useful document classes. A probabilistic classifier additionally will tell us the probability of the observation being in the class. This full distribution over the classes can be useful information for downstream decisions; avoiding making discrete decisions early on can be useful when combining systems. ", "Bloom_type": "remember", "question": "In text classification, what does the output variable represent?", "options": ["The probability of a class", "The number of classes", "The type of document", "The content of the document"], "complexity": 0}, {"id": 4, "context": "Finally, some systems choose to completely ignore another class of words: stop words, very frequent words like the and a. This can be done by sorting the vocabulary by frequency in the training set, and defining the top 10100 vocabulary entries as stop words, or alternatively by using one of the many predefined stop word lists available online. Then each instance of these stop words is simply removed from both training and test documents as if it had never occurred. In most text classification applications, however, using a stop word list doesn`t improve performance, and so it is more common to make use of the entire vocabulary and not use a stop word list. ", "Bloom_type": "remember", "question": "Why are stop words typically ignored in text classification?", "options": ["To enhance the accuracy of the classification results", "To increase the complexity of the model", "To reduce computational resources needed for processing", "To simplify the process of feature extraction"], "complexity": 0}, {"id": 5, "context": "While standard naive Bayes text classification can work well for sentiment analysis, some small changes are generally employed that improve performance. ", "Bloom_type": "remember", "question": "In what type of analysis do small modifications often enhance the effectiveness of standard naive Bayes text classification?", "options": ["Sentiment analysis", "Topic modeling", "Information retrieval", "Machine learning algorithms"], "complexity": 0}, {"id": 6, "context": "Consider the task of spam detection, deciding if a particular piece of email is an example of spam (unsolicited bulk email)one of the first applications of naive Bayes to text classification (Sahami et al., 1998). ", "Bloom_type": "remember", "question": "In what application did naive Bayes first appear for text classification?", "options": ["Spam detection", "Sentiment analysis", "Topic modeling", "Named entity recognition"], "complexity": 0}, {"id": 7, "context": "Up to now we have been describing text classification tasks with only two classes. But lots of classification tasks in language processing have more than two classes. For sentiment analysis we generally have 3 classes (positive, negative, neutral) and even more classes are common for tasks like part-of-speech tagging, word sense disambiguation, semantic role labeling, emotion detection, and so on. Luckily the naive Bayes algorithm is already a multi-class classification algorithm. ", "Bloom_type": "remember", "question": "In what type of task does the naive Bayes algorithm excel when dealing with more than two classes?", "options": ["Sentiment analysis", "Part-of-speech tagging", "Word sense disambiguation", "Emotion detection"], "complexity": 0}, {"id": 8, "context": "Multinomial naive Bayes text classification was proposed by Maron (1961) at the RAND Corporation for the task of assigning subject categories to journal abstracts. His model introduced most of the features of the modern form presented here, approximating the classification task with one-of categorization, and implementing add- smoothing and information-based feature selection. ", "Bloom_type": "remember", "question": "In what year did Maron propose his method for text classification?", "options": ["1960", "1950", "1970", "1980"], "complexity": 0}, {"id": 9, "context": "The conditional independence assumptions of naive Bayes and the idea of Bayesian analysis of text seems to have arisen multiple times. The same year as Maron`s paper, Minsky (1961) proposed a naive Bayes classifier for vision and other artificial intelligence problems, and Bayesian techniques were also applied to the text classification task of authorship attribution by Mosteller and Wallace (1963). It had long been known that Alexander Hamilton, John Jay, and James Madison wrote the anonymously-published Federalist papers in 17871788 to persuade New York to ratify the United States Constitution. Yet although some of the 85 essays were clearly attributable to one author or another, the authorship of 12 were in dispute between Hamilton and Madison. Mosteller and Wallace (1963) trained a Bayesian probabilistic model of the writing of Hamilton and another model on the writings of Madison, then computed the maximum-likelihood author for each of the disputed essays. Naive Bayes was first applied to spam detection in Heckerman et al. (1998). Metsis et al. (2006), Pang et al. (2002), and Wang and Manning (2012) show that using boolean attributes with multinomial naive Bayes works better than full counts. Binary multinomial naive Bayes is sometimes confused with another variant of naive Bayes that also uses a binary representation of whether a term occurs in a document: Multivariate Bernoulli naive Bayes. The Bernoulli variant instead estimates P(w c) as the fraction of documents that contain a term, and includes a | probability for whether a term is not in a document. McCallum and Nigam (1998) and Wang and Manning (2012) show that the multivariate Bernoulli variant of naive Bayes doesn`t work as well as the multinomial algorithm for sentiment or other text tasks. ", "Bloom_type": "remember", "question": "In what way did Bayesian analysis of text classification differ from previous methods?", "options": ["It combined both categorical and numerical data.", "It used only categorical data.", "It relied solely on visual cues.", "It focused exclusively on numerical data."], "complexity": 0}, {"id": 10, "context": "There are a variety of sources covering the many kinds of text classification tasks. For sentiment analysis see Pang and Lee (2008), and Liu and Zhang (2012). Stamatatos (2009) surveys authorship attribute algorithms. On language identification see Jauhiainen et al. (2019); Jaech et al. (2016) is an important early neural system. The task of newswire indexing was often used as a test case for text classification algorithms, based on the Reuters-21578 collection of newswire articles. ", "Bloom_type": "remember", "question": "Which source provides an overview of surveying authorship attribute algorithms?", "options": ["Stamatatos (2009)", "Pang and Lee (2008)", "Liu and Zhang (2012)", "Jaech et al. (2016)"], "complexity": 0}, {"id": 11, "context": "Furthermore, in addition to its use as a classifier, logistic regression in NLP and many other fields is widely used as an analytic tool for testing hypotheses about the effect of various explanatory variables (features). In text classification, perhaps we want to know if logically negative words (no, not, never) are more likely to be associated with negative sentiment, or if negative reviews of movies are more likely to discuss the cinematography. However, in doing so it`s necessary to control for potential confounds: other factors that might influence sentiment (the movie genre, the year it was made, perhaps the length of the review in words). Or we might be studying the relationship between NLP-extracted linguistic features and non-linguistic outcomes (hospital readmissions, political outcomes, or product sales), but need to control for confounds (the age of the patient, the county of voting, the brand of the product). In such cases, logistic regression allows us to test whether some feature is associated with some outcome above and beyond the effect of other features. ", "Bloom_type": "remember", "question": "In what field is logistic regression primarily used besides being a classifier?", "options": ["NLP", "Chemistry", "Physics", "Economics"], "complexity": 0}, {"id": 12, "context": " Logistic regression can be used with two classes (e.g., positive and negative sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.). ", "Bloom_type": "remember", "question": "In which type of classification problem is multinomial logistic regression particularly useful?", "options": ["Multiclass classification", "Binary classification", "Unsupervised learning", "Regression analysis"], "complexity": 0}, {"id": 13, "context": "At the same time in the early 1990s logistic regression was developed and applied to NLP at IBM Research under the name maximum entropy modeling or maxent (Berger et al., 1996), seemingly independent of the statistical literature. Under that name it was applied to language modeling (Rosenfeld, 1996), part-of-speech tagging (Ratnaparkhi, 1996), parsing (Ratnaparkhi, 1997), coreference resolution (Kehler, 1997b), and text classification (Nigam et al., 1999). ", "Bloom_type": "remember", "question": "In what year did logistic regression become widely used for text classification?", "options": ["1996", "1985", "1992", "1994"], "complexity": 0}, {"id": 14, "context": "This chapter introduces a deep learning architecture that offers an alternative way of representing time: recurrent neural networks (RNNs), and their variants like LSTMs. RNNs have a mechanism that deals directly with the sequential nature of language, allowing them to handle the temporal nature of language without the use of arbitrary fixed-sized windows. The recurrent network offers a new way to represent the prior context, in its recurrent connections, allowing the model`s decision to depend on information from hundreds of words in the past. We`ll see how to apply the model to the task of language modeling, to text classification tasks like sentiment analysis, and to sequence modeling tasks like part-of-speech tagging (a task we`ll return to in detail in Chapter 17). ", "Bloom_type": "remember", "question": "In what type of machine learning applications is the concept of recurrent neural networks particularly useful?", "options": ["Sequence modeling", "Image recognition", "Sentiment analysis", "Object detection"], "complexity": 0}, {"id": 15, "context": "Another use of RNNs is to classify entire sequences rather than the tokens within them. This is the set of tasks commonly called text classification, like sentiment analysis or spam detection, in which we classify a text into two or three classes (like positive or negative), as well as classification tasks with a large number of categories, like document-level topic classification, or message routing for customer service applications. ", "Bloom_type": "remember", "question": "In what type of task are RNNs primarily used for classifying entire sequences instead of individual tokens?", "options": ["Sentiment analysis", "Image recognition", "Speech synthesis", "Music composition"], "complexity": 0}, {"id": 16, "context": "Note that in this approach we don`t need intermediate outputs for the words in the sequence preceding the last element. Therefore, there are no loss terms associated with those elements. Instead, the loss function used to train the weights in the network is based entirely on the final text classification task. The output from the softmax output from the feedforward classifier together with a cross-entropy loss drives the training. The error signal from the classification is backpropagated all the way through the weights in the feedforward classifier through, to its input, and then through to the three sets of weights in the RNN as described earlier in Section 8.1.2. The training regimen that uses the loss from a downstream application to adjust the weights all the way through the network is referred to as end-to-end training. ", "Bloom_type": "remember", "question": "In text classification, why is it important not to use intermediate outputs for the words before the last element?", "options": ["To focus solely on the final classification outcome", "To reduce computational complexity", "To improve model accuracy", "To decrease the amount of data needed"], "complexity": 0}, {"id": 17, "context": "The task of sequence classification is to classify an entire sequence of text with a single label. This set of tasks is commonly called text classification, like sentiment analysis or spam detection (Chapter 4) in which we classify a text into two or three classes (like positive or negative), as well as classification tasks with a large number of categories, like document-level topic classification. ", "Bloom_type": "remember", "question": "In what type of classification problem does the task involve classifying an entire sequence of text?", "options": ["Sentiment analysis", "Image recognition", "Text summarization", "Machine translation"], "complexity": 0}, {"id": 18, "context": " Affect can be detected, just like sentiment, by using standard supervised text classification techniques, using all the words or bigrams in a text as features. Additional features can be drawn from counts of words in lexicons. ", "Bloom_type": "remember", "question": "Which method is used for detecting affect in texts?", "options": ["Text classification", "Natural language processing", "Sentiment analysis", "Machine learning"], "complexity": 0}, {"id": 19, "context": "The simplest way to do smoothing is to add one to all the n-gram counts, before we normalize them into probabilities. All the counts that used to be zero will now have a count of 1, the counts of 1 will be 2, and so on. This algorithm is called Laplace smoothing. Laplace smoothing does not perform well enough to be used in modern n-gram models, but it usefully introduces many of the concepts that we see in other smoothing algorithms, gives a useful baseline, and is also a practical smoothing algorithm for other tasks like text classification (Chapter 4). ", "Bloom_type": "comprehension", "question": "What is an example of a task where Laplace smoothing can be applied besides n-gram modeling?", "options": ["Text classification", "Image recognition", "Speech synthesis", "Music composition"], "complexity": 1}, {"id": 20, "context": "Another thing we might want to know about a text is the language it`s written in. Texts on social media, for example, can be in any number of languages and we`ll need to apply different processing. The task of language id is thus the first step in most language processing pipelines. Related text classification tasks like authorship attribution determining a text`s author are also relevant to the digital humanities, social sciences, and forensic linguistics. ", "Bloom_type": "comprehension", "question": "What is one of the primary applications of text classification?", "options": ["Classifying the type of document (e.g., news article, email)", "Identifying the genre of a book", "Determining the sentiment of a tweet", "Predicting future stock prices based on historical data"], "complexity": 1}, {"id": 21, "context": "Finally, one of the oldest tasks in text classification is assigning a library subject category or topic label to a text. Deciding whether a research paper concerns epidemiology or instead, perhaps, embryology, is an important component of information retrieval. Various sets of subject categories exist, such as the MeSH (Medical Subject Headings) thesaurus. In fact, as we will see, subject category classification is the task for which the naive Bayes algorithm was invented in 1961 (Maron, 1961). Classification is essential for tasks below the level of the document as well. We`ve already seen period disambiguation (deciding if a period is the end of a sentence or part of a word), and word tokenization (deciding if a character should be a word boundary). Even language modeling can be viewed as classification: each word can be thought of as a class, and so predicting the next word is classifying the context-so-far into a class for each next word. A part-of-speech tagger (Chapter 17) classifies each occurrence of a word in a sentence as, e.g., a noun or a verb. ", "Bloom_type": "comprehension", "question": "What is the primary purpose of text classification in the context of information retrieval?", "options": ["Assigning a library subject category or topic label to a text", "Deciding the genre of literature", "Classifying sentences based on their grammatical structure", "Determining the meaning of ambiguous phrases"], "complexity": 1}, {"id": 22, "context": "Formally, the task of supervised classification is to take an input x and a fixed set of output classes Y = Y . For text classification, we`ll sometimes talk about c (for class) instead of y as our output variable, and d (for document) instead of x as our input variable. In the supervised situation we have a training set of N documents that have each been hand(d1, c1), ...., (dN, cN) . Our goal is to learn a classifier that is labeled with a class: } { C, where C is capable of mapping from a new document d to its correct class c some set of useful document classes. A probabilistic classifier additionally will tell us the probability of the observation being in the class. This full distribution over the classes can be useful information for downstream decisions; avoiding making discrete decisions early on can be useful when combining systems. ", "Bloom_type": "comprehension", "question": "In the context of text classification, how does the use of probabilities differ from using just a single class label?", "options": ["Both options B and C are true.", "Probabilities provide more accurate predictions than class labels.", "Class labels are sufficient for all practical purposes.", "Probabilities allow for continuous decision-making processes."], "complexity": 1}, {"id": 23, "context": "Finally, some systems choose to completely ignore another class of words: stop words, very frequent words like the and a. This can be done by sorting the vocabulary by frequency in the training set, and defining the top 10100 vocabulary entries as stop words, or alternatively by using one of the many predefined stop word lists available online. Then each instance of these stop words is simply removed from both training and test documents as if it had never occurred. In most text classification applications, however, using a stop word list doesn`t improve performance, and so it is more common to make use of the entire vocabulary and not use a stop word list. ", "Bloom_type": "comprehension", "question": "Explain why using a stop word list might not always improve performance in text classification applications?", "options": ["Because stop words are usually rare and therefore have little impact on the model\u2019s accuracy.", "Because stop words are too specific and do not represent the overall meaning of the document.", "Because stop words are irrelevant to the task at hand and thus should be ignored entirely.", "Because stop words are frequently used in natural language processing and thus overusing them could lead to confusion."], "complexity": 1}, {"id": 24, "context": "While standard naive Bayes text classification can work well for sentiment analysis, some small changes are generally employed that improve performance. ", "Bloom_type": "comprehension", "question": "What does the response indicate about the use of standard naive Bayes text classification?", "options": ["It works best when modified for sentiment analysis.", "It cannot handle sentiment analysis.", "It performs poorly for sentiment analysis.", "It requires no modifications for sentiment analysis."], "complexity": 1}, {"id": 25, "context": "Consider the task of spam detection, deciding if a particular piece of email is an example of spam (unsolicited bulk email)one of the first applications of naive Bayes to text classification (Sahami et al., 1998). ", "Bloom_type": "comprehension", "question": "What was one of the early applications of Naive Bayes for text classification?", "options": ["Spam detection", "Sentiment analysis", "Topic modeling", "Named entity recognition"], "complexity": 1}, {"id": 26, "context": "Up to now we have been describing text classification tasks with only two classes. But lots of classification tasks in language processing have more than two classes. For sentiment analysis we generally have 3 classes (positive, negative, neutral) and even more classes are common for tasks like part-of-speech tagging, word sense disambiguation, semantic role labeling, emotion detection, and so on. Luckily the naive Bayes algorithm is already a multi-class classification algorithm. ", "Bloom_type": "comprehension", "question": "What is the primary difference between binary and multi-class text classification?", "options": ["Multi-class text classification can handle more complex scenarios where texts belong to different classes, unlike binary classification.", "Binary text classification involves only positive and negative sentiments, while multi-class involves all possible sentiments.", "Multi-class text classification uses a single class for all texts, whereas binary classification divides texts into two categories.", "Binary text classification requires more computational resources due to its simplicity compared to multi-class."], "complexity": 1}, {"id": 27, "context": "Multinomial naive Bayes text classification was proposed by Maron (1961) at the RAND Corporation for the task of assigning subject categories to journal abstracts. His model introduced most of the features of the modern form presented here, approximating the classification task with one-of categorization, and implementing add- smoothing and information-based feature selection. ", "Bloom_type": "comprehension", "question": "What did Maron propose for the task of assigning subject categories to journal abstracts using multinomial naive Bayes text classification?", "options": ["Both A and B", "Multinomial Naive Bayes algorithm", "The use of add-smoothing techniques", "Information-based feature selection methods"], "complexity": 1}, {"id": 28, "context": "The conditional independence assumptions of naive Bayes and the idea of Bayesian analysis of text seems to have arisen multiple times. The same year as Maron`s paper, Minsky (1961) proposed a naive Bayes classifier for vision and other artificial intelligence problems, and Bayesian techniques were also applied to the text classification task of authorship attribution by Mosteller and Wallace (1963). It had long been known that Alexander Hamilton, John Jay, and James Madison wrote the anonymously-published Federalist papers in 17871788 to persuade New York to ratify the United States Constitution. Yet although some of the 85 essays were clearly attributable to one author or another, the authorship of 12 were in dispute between Hamilton and Madison. Mosteller and Wallace (1963) trained a Bayesian probabilistic model of the writing of Hamilton and another model on the writings of Madison, then computed the maximum-likelihood author for each of the disputed essays. Naive Bayes was first applied to spam detection in Heckerman et al. (1998). Metsis et al. (2006), Pang et al. (2002), and Wang and Manning (2012) show that using boolean attributes with multinomial naive Bayes works better than full counts. Binary multinomial naive Bayes is sometimes confused with another variant of naive Bayes that also uses a binary representation of whether a term occurs in a document: Multivariate Bernoulli naive Bayes. The Bernoulli variant instead estimates P(w c) as the fraction of documents that contain a term, and includes a | probability for whether a term is not in a document. McCallum and Nigam (1998) and Wang and Manning (2012) show that the multivariate Bernoulli variant of naive Bayes doesn`t work as well as the multinomial algorithm for sentiment or other text tasks. ", "Bloom_type": "comprehension", "question": "What are the key differences between naive Bayes classifiers and Bayesian models specifically applied to text classification?", "options": ["Bayesian models assume conditional independence among features while naive Bayes does not.", "Naive Bayes assumes conditional independence among features while Bayesian models do not.", "Both naive Bayes and Bayesian models assume conditional independence among features.", "Neither naive Bayes nor Bayesian models assume conditional independence among features."], "complexity": 1}, {"id": 29, "context": "There are a variety of sources covering the many kinds of text classification tasks. For sentiment analysis see Pang and Lee (2008), and Liu and Zhang (2012). Stamatatos (2009) surveys authorship attribute algorithms. On language identification see Jauhiainen et al. (2019); Jaech et al. (2016) is an important early neural system. The task of newswire indexing was often used as a test case for text classification algorithms, based on the Reuters-21578 collection of newswire articles. ", "Bloom_type": "comprehension", "question": "What type of algorithm is most commonly used for sentiment analysis?", "options": ["Pang and Lee (2008)", "Stomatatos (2009)", "Jaech et al. (2016)", "Jauhiainen et al. (2019)"], "complexity": 1}, {"id": 30, "context": "Furthermore, in addition to its use as a classifier, logistic regression in NLP and many other fields is widely used as an analytic tool for testing hypotheses about the effect of various explanatory variables (features). In text classification, perhaps we want to know if logically negative words (no, not, never) are more likely to be associated with negative sentiment, or if negative reviews of movies are more likely to discuss the cinematography. However, in doing so it`s necessary to control for potential confounds: other factors that might influence sentiment (the movie genre, the year it was made, perhaps the length of the review in words). Or we might be studying the relationship between NLP-extracted linguistic features and non-linguistic outcomes (hospital readmissions, political outcomes, or product sales), but need to control for confounds (the age of the patient, the county of voting, the brand of the product). In such cases, logistic regression allows us to test whether some feature is associated with some outcome above and beyond the effect of other features. ", "Bloom_type": "comprehension", "question": "In the context of text classification, how does logistic regression help analyze relationships between features and outcomes?", "options": ["It aids in understanding the correlation between different features and their effects on sentiment.", "It helps identify which features have no impact on the outcome.", "It assists in determining the effectiveness of controlling for confounding variables.", "It enables researchers to predict future trends based solely on historical data."], "complexity": 1}, {"id": 31, "context": " Logistic regression can be used with two classes (e.g., positive and negative sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.). ", "Bloom_type": "comprehension", "question": "Explain how multinomial logistic regression differs from binary logistic regression in the context of text classification?", "options": ["Multinomial logistic regression uses more than two classes while binary logistic regression only uses one class.", "Binary logistic regression can handle more complex data structures than multinomial logistic regression.", "Multinomial logistic regression requires less computational power than binary logistic regression.", "Both A and B are correct."], "complexity": 1}, {"id": 32, "context": "At the same time in the early 1990s logistic regression was developed and applied to NLP at IBM Research under the name maximum entropy modeling or maxent (Berger et al., 1996), seemingly independent of the statistical literature. Under that name it was applied to language modeling (Rosenfeld, 1996), part-of-speech tagging (Ratnaparkhi, 1996), parsing (Ratnaparkhi, 1997), coreference resolution (Kehler, 1997b), and text classification (Nigam et al., 1999). ", "Bloom_type": "comprehension", "question": "Explain how text classification was initially applied using the name'maximum entropy modeling'?", "options": ["Logistic regression was independently developed for text classification purposes.", "Text classification was first introduced by statisticians.", "Maximum entropy modeling was specifically designed for text classification.", "The name'maximum entropy modeling' was coined by linguists."], "complexity": 1}, {"id": 33, "context": "This chapter introduces a deep learning architecture that offers an alternative way of representing time: recurrent neural networks (RNNs), and their variants like LSTMs. RNNs have a mechanism that deals directly with the sequential nature of language, allowing them to handle the temporal nature of language without the use of arbitrary fixed-sized windows. The recurrent network offers a new way to represent the prior context, in its recurrent connections, allowing the model`s decision to depend on information from hundreds of words in the past. We`ll see how to apply the model to the task of language modeling, to text classification tasks like sentiment analysis, and to sequence modeling tasks like part-of-speech tagging (a task we`ll return to in detail in Chapter 17). ", "Bloom_type": "comprehension", "question": "How does the recurrent neural network differ from other methods for handling sequential data?", "options": ["It can capture long-term dependencies within sequences.", "It uses fixed-size windows to analyze sequences.", "It relies solely on static features extracted from input data.", "It requires explicit labeling of each word in the sequence."], "complexity": 1}, {"id": 34, "context": "Another use of RNNs is to classify entire sequences rather than the tokens within them. This is the set of tasks commonly called text classification, like sentiment analysis or spam detection, in which we classify a text into two or three classes (like positive or negative), as well as classification tasks with a large number of categories, like document-level topic classification, or message routing for customer service applications. ", "Bloom_type": "comprehension", "question": "What does the term 'text classification' refer to in relation to RNNs?", "options": ["Determining the category of a document or message", "Classifying individual tokens within a sequence", "Analyzing the overall sentiment of a text", "Predicting future trends based on historical data"], "complexity": 1}, {"id": 35, "context": "Note that in this approach we don`t need intermediate outputs for the words in the sequence preceding the last element. Therefore, there are no loss terms associated with those elements. Instead, the loss function used to train the weights in the network is based entirely on the final text classification task. The output from the softmax output from the feedforward classifier together with a cross-entropy loss drives the training. The error signal from the classification is backpropagated all the way through the weights in the feedforward classifier through, to its input, and then through to the three sets of weights in the RNN as described earlier in Section 8.1.2. The training regimen that uses the loss from a downstream application to adjust the weights all the way through the network is referred to as end-to-end training. ", "Bloom_type": "comprehension", "question": "What type of training does the method described use?", "options": ["End-to-end training", "Backward propagation training", "Forward propagation training", "No training used"], "complexity": 1}, {"id": 36, "context": "The task of sequence classification is to classify an entire sequence of text with a single label. This set of tasks is commonly called text classification, like sentiment analysis or spam detection (Chapter 4) in which we classify a text into two or three classes (like positive or negative), as well as classification tasks with a large number of categories, like document-level topic classification. ", "Bloom_type": "comprehension", "question": "What is the primary focus when classifying sequences of text?", "options": ["Determining the overall category for the entire sequence of text", "Classifying individual words within each sentence", "Analyzing the grammatical structure of sentences", "Identifying specific keywords within each word"], "complexity": 1}, {"id": 37, "context": " Affect can be detected, just like sentiment, by using standard supervised text classification techniques, using all the words or bigrams in a text as features. Additional features can be drawn from counts of words in lexicons. ", "Bloom_type": "comprehension", "question": "What technique can be used for detecting affect in texts?", "options": ["Text Classification", "Natural Language Processing (NLP)", "Sentiment Analysis", "Machine Learning Algorithms"], "complexity": 1}, {"id": 38, "context": "The simplest way to do smoothing is to add one to all the n-gram counts, before we normalize them into probabilities. All the counts that used to be zero will now have a count of 1, the counts of 1 will be 2, and so on. This algorithm is called Laplace smoothing. Laplace smoothing does not perform well enough to be used in modern n-gram models, but it usefully introduces many of the concepts that we see in other smoothing algorithms, gives a useful baseline, and is also a practical smoothing algorithm for other tasks like text classification (Chapter 4). ", "Bloom_type": "application", "question": "What is an example of a simple smoothing technique mentioned in the context?", "options": ["Addition of one to all n-gram counts", "Subtraction of one from all n-gram counts", "Multiplication of all n-gram counts by two", "Division of all n-gram counts by three"], "complexity": 2}, {"id": 39, "context": "Another thing we might want to know about a text is the language it`s written in. Texts on social media, for example, can be in any number of languages and we`ll need to apply different processing. The task of language id is thus the first step in most language processing pipelines. Related text classification tasks like authorship attribution determining a text`s author are also relevant to the digital humanities, social sciences, and forensic linguistics. ", "Bloom_type": "application", "question": "What is the initial step in applying different processing techniques when dealing with texts?", "options": ["Identify the language of the text", "Classify the author of the text", "Analyze the content of the text", "Extract keywords from the text"], "complexity": 2}, {"id": 40, "context": "Finally, one of the oldest tasks in text classification is assigning a library subject category or topic label to a text. Deciding whether a research paper concerns epidemiology or instead, perhaps, embryology, is an important component of information retrieval. Various sets of subject categories exist, such as the MeSH (Medical Subject Headings) thesaurus. In fact, as we will see, subject category classification is the task for which the naive Bayes algorithm was invented in 1961 (Maron, 1961). Classification is essential for tasks below the level of the document as well. We`ve already seen period disambiguation (deciding if a period is the end of a sentence or part of a word), and word tokenization (deciding if a character should be a word boundary). Even language modeling can be viewed as classification: each word can be thought of as a class, and so predicting the next word is classifying the context-so-far into a class for each next word. A part-of-speech tagger (Chapter 17) classifies each occurrence of a word in a sentence as, e.g., a noun or a verb. ", "Bloom_type": "application", "question": "What is the primary goal of text classification?", "options": ["To categorize documents into predefined topics", "To classify texts based on their content", "To identify the author of a text", "To translate text into another language"], "complexity": 2}, {"id": 41, "context": "Formally, the task of supervised classification is to take an input x and a fixed set of output classes Y = Y . For text classification, we`ll sometimes talk about c (for class) instead of y as our output variable, and d (for document) instead of x as our input variable. In the supervised situation we have a training set of N documents that have each been hand(d1, c1), ...., (dN, cN) . Our goal is to learn a classifier that is labeled with a class: } { C, where C is capable of mapping from a new document d to its correct class c some set of useful document classes. A probabilistic classifier additionally will tell us the probability of the observation being in the class. This full distribution over the classes can be useful information for downstream decisions; avoiding making discrete decisions early on can be useful when combining systems. ", "Bloom_type": "application", "question": "In the context of text classification, what does the notation (d, c) represent?", "options": ["The document and the label it belongs to", "The label and the document it refers to", "The label and the document it contains", "The document and the label it provides"], "complexity": 2}, {"id": 42, "context": "Finally, some systems choose to completely ignore another class of words: stop words, very frequent words like the and a. This can be done by sorting the vocabulary by frequency in the training set, and defining the top 10100 vocabulary entries as stop words, or alternatively by using one of the many predefined stop word lists available online. Then each instance of these stop words is simply removed from both training and test documents as if it had never occurred. In most text classification applications, however, using a stop word list doesn`t improve performance, and so it is more common to make use of the entire vocabulary and not use a stop word list. ", "Bloom_type": "application", "question": "What method could be used to remove stop words from the dataset?", "options": ["Use a predefined stop word list for removal.", "Filter out stop words based on their frequency in the training set.", "Remove all words with high frequency from the training set.", "Ignore stop words entirely."], "complexity": 2}, {"id": 43, "context": "While standard naive Bayes text classification can work well for sentiment analysis, some small changes are generally employed that improve performance. ", "Bloom_type": "application", "question": "What is an example of improving performance in text classification?", "options": ["Both A) and B)", "Using a more complex model instead of naive Bayes", "Increasing the size of the training dataset", "Applying feature selection techniques"], "complexity": 2}, {"id": 44, "context": "Consider the task of spam detection, deciding if a particular piece of email is an example of spam (unsolicited bulk email)one of the first applications of naive Bayes to text classification (Sahami et al., 1998). ", "Bloom_type": "application", "question": "What method should be used initially when classifying emails as either spam or not spam?", "options": ["Naive Bayes Classification", "Decision Tree Analysis", "Support Vector Machines", "K-Nearest Neighbors"], "complexity": 2}, {"id": 45, "context": "Up to now we have been describing text classification tasks with only two classes. But lots of classification tasks in language processing have more than two classes. For sentiment analysis we generally have 3 classes (positive, negative, neutral) and even more classes are common for tasks like part-of-speech tagging, word sense disambiguation, semantic role labeling, emotion detection, and so on. Luckily the naive Bayes algorithm is already a multi-class classification algorithm. ", "Bloom_type": "application", "question": "What is an example of a multi-class classification task?", "options": ["Sentiment analysis", "Part-of-speech tagging", "Word sense disambiguation", "Semantic role labeling"], "complexity": 2}, {"id": 46, "context": "Multinomial naive Bayes text classification was proposed by Maron (1961) at the RAND Corporation for the task of assigning subject categories to journal abstracts. His model introduced most of the features of the modern form presented here, approximating the classification task with one-of categorization, and implementing add- smoothing and information-based feature selection. ", "Bloom_type": "application", "question": "What method did Maron propose for text classification?", "options": ["Naive Bayes Classification", "Support Vector Machines", "K-means Clustering", "Decision Trees"], "complexity": 2}, {"id": 47, "context": "The conditional independence assumptions of naive Bayes and the idea of Bayesian analysis of text seems to have arisen multiple times. The same year as Maron`s paper, Minsky (1961) proposed a naive Bayes classifier for vision and other artificial intelligence problems, and Bayesian techniques were also applied to the text classification task of authorship attribution by Mosteller and Wallace (1963). It had long been known that Alexander Hamilton, John Jay, and James Madison wrote the anonymously-published Federalist papers in 17871788 to persuade New York to ratify the United States Constitution. Yet although some of the 85 essays were clearly attributable to one author or another, the authorship of 12 were in dispute between Hamilton and Madison. Mosteller and Wallace (1963) trained a Bayesian probabilistic model of the writing of Hamilton and another model on the writings of Madison, then computed the maximum-likelihood author for each of the disputed essays. Naive Bayes was first applied to spam detection in Heckerman et al. (1998). Metsis et al. (2006), Pang et al. (2002), and Wang and Manning (2012) show that using boolean attributes with multinomial naive Bayes works better than full counts. Binary multinomial naive Bayes is sometimes confused with another variant of naive Bayes that also uses a binary representation of whether a term occurs in a document: Multivariate Bernoulli naive Bayes. The Bernoulli variant instead estimates P(w c) as the fraction of documents that contain a term, and includes a | probability for whether a term is not in a document. McCallum and Nigam (1998) and Wang and Manning (2012) show that the multivariate Bernoulli variant of naive Bayes doesn`t work as well as the multinomial algorithm for sentiment or other text tasks. ", "Bloom_type": "application", "question": "What method did Mosteller and Wallace use to determine the authorship of disputed essays?", "options": ["Naive Bayes", "Multinomial Naive Bayes", "Bernoulli Naive Bayes", "Boolean Attributes"], "complexity": 2}, {"id": 48, "context": "There are a variety of sources covering the many kinds of text classification tasks. For sentiment analysis see Pang and Lee (2008), and Liu and Zhang (2012). Stamatatos (2009) surveys authorship attribute algorithms. On language identification see Jauhiainen et al. (2019); Jaech et al. (2016) is an important early neural system. The task of newswire indexing was often used as a test case for text classification algorithms, based on the Reuters-21578 collection of newswire articles. ", "Bloom_type": "application", "question": "Which source provides insights into the development of newswire indexing techniques?", "options": ["Jauhiainen et al. (2019)", "Pang and Lee (2008)", "Jaech et al. (2016)", "Liu and Zhang (2012)"], "complexity": 2}, {"id": 49, "context": "Furthermore, in addition to its use as a classifier, logistic regression in NLP and many other fields is widely used as an analytic tool for testing hypotheses about the effect of various explanatory variables (features). In text classification, perhaps we want to know if logically negative words (no, not, never) are more likely to be associated with negative sentiment, or if negative reviews of movies are more likely to discuss the cinematography. However, in doing so it`s necessary to control for potential confounds: other factors that might influence sentiment (the movie genre, the year it was made, perhaps the length of the review in words). Or we might be studying the relationship between NLP-extracted linguistic features and non-linguistic outcomes (hospital readmissions, political outcomes, or product sales), but need to control for confounds (the age of the patient, the county of voting, the brand of the product). In such cases, logistic regression allows us to test whether some feature is associated with some outcome above and beyond the effect of other features. ", "Bloom_type": "application", "question": "In text classification, how can logistic regression help in controlling for potential confounds?", "options": ["Logistic regression helps in controlling for confounds by adjusting for other features.", "Logistic regression cannot control for confounds.", "Logistic regression automatically controls for all confounds.", "Logistic regression does not provide any way to control for confounds."], "complexity": 2}, {"id": 50, "context": " Logistic regression can be used with two classes (e.g., positive and negative sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.). ", "Bloom_type": "application", "question": "In the context of text classification, how is multinomial logistic regression utilized?", "options": ["It classifies texts into three or more categories.", "It categorizes texts into only two classes.", "It labels each word individually within a sentence.", "It predicts the probability distribution over all possible classes."], "complexity": 2}, {"id": 51, "context": "At the same time in the early 1990s logistic regression was developed and applied to NLP at IBM Research under the name maximum entropy modeling or maxent (Berger et al., 1996), seemingly independent of the statistical literature. Under that name it was applied to language modeling (Rosenfeld, 1996), part-of-speech tagging (Ratnaparkhi, 1996), parsing (Ratnaparkhi, 1997), coreference resolution (Kehler, 1997b), and text classification (Nigam et al., 1999). ", "Bloom_type": "application", "question": "What method was used for text classification before the development of logistic regression?", "options": ["Maximum Entropy Modeling", "Logistic Regression", "Naive Bayes Classifier", "Support Vector Machines"], "complexity": 2}, {"id": 52, "context": "This chapter introduces a deep learning architecture that offers an alternative way of representing time: recurrent neural networks (RNNs), and their variants like LSTMs. RNNs have a mechanism that deals directly with the sequential nature of language, allowing them to handle the temporal nature of language without the use of arbitrary fixed-sized windows. The recurrent network offers a new way to represent the prior context, in its recurrent connections, allowing the model`s decision to depend on information from hundreds of words in the past. We`ll see how to apply the model to the task of language modeling, to text classification tasks like sentiment analysis, and to sequence modeling tasks like part-of-speech tagging (a task we`ll return to in detail in Chapter 17). ", "Bloom_type": "application", "question": "What is the primary advantage of using recurrent neural networks over traditional methods for handling sequential data?", "options": ["They can capture long-term dependencies in sequences.", "They can only work with static input.", "They require large amounts of labeled data.", "They are less accurate than traditional methods."], "complexity": 2}, {"id": 53, "context": "Another use of RNNs is to classify entire sequences rather than the tokens within them. This is the set of tasks commonly called text classification, like sentiment analysis or spam detection, in which we classify a text into two or three classes (like positive or negative), as well as classification tasks with a large number of categories, like document-level topic classification, or message routing for customer service applications. ", "Bloom_type": "application", "question": "What type of task involves classifying an entire sequence instead of individual tokens?", "options": ["Document-level topic classification", "Sentiment analysis", "Spam detection", "Message routing"], "complexity": 2}, {"id": 54, "context": "Note that in this approach we don`t need intermediate outputs for the words in the sequence preceding the last element. Therefore, there are no loss terms associated with those elements. Instead, the loss function used to train the weights in the network is based entirely on the final text classification task. The output from the softmax output from the feedforward classifier together with a cross-entropy loss drives the training. The error signal from the classification is backpropagated all the way through the weights in the feedforward classifier through, to its input, and then through to the three sets of weights in the RNN as described earlier in Section 8.1.2. The training regimen that uses the loss from a downstream application to adjust the weights all the way through the network is referred to as end-to-end training. ", "Bloom_type": "application", "question": "What is the primary method used to train the weights in the neural network?", "options": ["Cross-entropy loss", "Backpropagation through time (BPTT)", "Gradient descent", "Batch normalization"], "complexity": 2}, {"id": 55, "context": "The task of sequence classification is to classify an entire sequence of text with a single label. This set of tasks is commonly called text classification, like sentiment analysis or spam detection (Chapter 4) in which we classify a text into two or three classes (like positive or negative), as well as classification tasks with a large number of categories, like document-level topic classification. ", "Bloom_type": "application", "question": "What is the common name for the task of classifying sequences of text?", "options": ["Text categorization", "Sequence labeling", "Sentence classification", "Paragraph classification"], "complexity": 2}, {"id": 56, "context": " Affect can be detected, just like sentiment, by using standard supervised text classification techniques, using all the words or bigrams in a text as features. Additional features can be drawn from counts of words in lexicons. ", "Bloom_type": "application", "question": "Which technique is used for detecting affect in texts?", "options": ["Supervised Text Classification", "Natural Language Processing (NLP)", "Machine Learning Algorithms", "Sentiment Analysis"], "complexity": 2}]}, "hidden layer": {"max_id": 53, "Questions": [{"id": 0, "context": "By the next decade, Bengio et al. (2003) and Bengio et al. (2006) showed that neural language models could also be used to develop embeddings as part of the task of word prediction. Collobert and Weston (2007), Collobert and Weston (2008), and Collobert et al. (2011) then demonstrated that embeddings could be used to represent word meanings for a number of NLP tasks. Turian et al. (2010) compared the value of different kinds of embeddings for different NLP tasks. Mikolov et al. (2011) showed that recurrent neural nets could be used as language models. The idea of simplifying the hidden layer of these neural net language models to create the skipgram (and also CBOW) algorithms was proposed by Mikolov et al. (2013a). The negative sampling training algorithm was proposed in Mikolov et al. (2013b). There are numerous surveys of static embeddings and their parameterizations (Bullinaria and Levy 2007, Bullinaria and Levy 2012, Lapesa and Evert 2014, Kiela and Clark 2014, Levy et al. 2015). ", "Bloom_type": "remember", "question": "In which year did Bengio et al. publish their work on using neural language models for developing embeddings?", "options": ["2003", "2006", "2007", "2009"], "complexity": 0}, {"id": 1, "context": "Instead, a modern neural network is a network of small computing units, each of which takes a vector of input values and produces a single output value. In this chapter we introduce the neural net applied to classification. The architecture we introduce is called a feedforward network because the computation proceeds iteratively from one layer of units to the next. The use of modern neural nets is often called deep learning, because modern networks are often deep (have many layers). Neural networks share much of the same mathematics as logistic regression. But neural networks are a more powerful classifier than logistic regression, and indeed a minimal neural network (technically one with a single hidden layer`) can be shown to learn any function. ", "Bloom_type": "remember", "question": "In the context of neural networks, what does having a single hidden layer mean?", "options": ["It implies the network can perform any task.", "It means the network has only one layer.", "It indicates the network uses fewer parameters.", "It suggests the network is less complex."], "complexity": 0}, {"id": 2, "context": "It`s also instructive to look at the intermediate results, the outputs of the two hidden nodes h1 and h2. We showed in the previous paragraph that the h vector for the inputs x = [0, 0] was [0, 0]. Fig. 7.7b shows the values of the h layer for all 4 inputs. Notice that hidden representations of the two input points x = [0, 1] and x = [1, 0] (the two cases with XOR output = 1) are merged to the single point h = [1, 0]. The merger makes it easy to linearly separate the positive and negative cases of XOR. In other words, we can view the hidden layer of the network as forming a representation of the input. ", "Bloom_type": "remember", "question": "In the context provided, what does the merging of hidden representations for certain input pairs indicate?", "options": ["It demonstrates how the hidden layer helps in separating different classes.", "It indicates that these input pairs do not exist.", "It suggests that the XOR operation is not possible.", "It implies that the network cannot learn complex patterns."], "complexity": 0}, {"id": 3, "context": "In this example we just stipulated the weights in Fig. 7.6. But for real examples the weights for neural networks are learned automatically using the error backpropagation algorithm to be introduced in Section 7.5. That means the hidden layers will learn to form useful representations. This intuition, that neural networks can automatically learn useful representations of the input, is one of their key advantages, and one that we will return to again and again in later chapters. ", "Bloom_type": "remember", "question": "In neural networks, what do hidden layers help with?", "options": ["They improve accuracy by learning features from raw inputs", "They increase computational speed", "They reduce the need for training data", "They decrease the complexity of the model"], "complexity": 0}, {"id": 4, "context": "That means we can think of a neural network classifier with one hidden layer as building a vector h which is a hidden layer representation of the input, and then running standard multinomial logistic regression on the features that the network develops in h. By contrast, in Chapter 5 the features were mainly designed by hand via feature templates. So a neural network is like multinomial logistic regression, but (a) with many layers, since a deep neural network is like layer after layer of logistic regression classifiers; (b) with those intermediate layers having many possible activation functions (tanh, ReLU, sigmoid) instead of just sigmoid (although we`ll continue to use  for convenience to mean any activation function); (c) rather than forming the features by feature templates, the prior layers of the network induce the feature representations themselves. ", "Bloom_type": "remember", "question": "In what way does a neural network differ from traditional multinomial logistic regression?", "options": ["It requires more manual feature design.", "It uses fewer layers.", "It has only one type of activation function.", "It forms features using handcrafted templates."], "complexity": 0}, {"id": 5, "context": "Fig. 7.10 shows a sketch of this architecture. As we mentioned earlier, adding this hidden layer to our logistic regression classifier allows the network to represent the non-linear interactions between features. This alone might give us a better sentiment classifier. ", "Bloom_type": "remember", "question": "In the architecture shown in Fig. 7.10, what is added to the logistic regression classifier to improve its ability to handle non-linear interactions?", "options": ["Hidden layers", "Linear layers", "Convolutional layers", "Pooling layers"], "complexity": 0}, {"id": 6, "context": "Fig. 8.1 illustrates the structure of an RNN. As with ordinary feedforward networks, an input vector representing the current input, xt , is multiplied by a weight matrix and then passed through a non-linear activation function to compute the values for a layer of hidden units. This hidden layer is then used to calculate a corresponding output, yt . In a departure from our earlier window-based approach, sequences are processed by presenting one item at a time to the network. We`ll use subscripts to represent time, thus xt will mean the input vector x at time t. The key difference from a feedforward network lies in the recurrent link shown in the figure with the dashed line. This link augments the input to the computation at the hidden layer with the value of the hidden layer from the preceding point in time. ", "Bloom_type": "remember", "question": "In an RNN, what does the hidden layer do?", "options": ["It receives the input vector and passes it through a non-linear activation function.", "It processes the entire sequence of inputs simultaneously.", "It calculates the next input based on the previous outputs.", "It stores the weights between different layers."], "complexity": 0}, {"id": 7, "context": "The hidden layer from the previous time step provides a form of memory, or context, that encodes earlier processing and informs the decisions to be made at later points in time. Critically, this approach does not impose a fixed-length limit on this prior context; the context embodied in the previous hidden layer can include information extending back to the beginning of the sequence. ", "Bloom_type": "remember", "question": "In what way does the hidden layer provide context for decision-making?", "options": ["It retains information about past events indefinitely.", "It stores specific data points directly.", "It uses patterns learned during training to predict future outcomes.", "It filters out irrelevant information before making decisions."], "complexity": 0}, {"id": 8, "context": "Adding this temporal dimension makes RNNs appear to be more complex than non-recurrent architectures. But in reality, they`re not all that different. Given an input vector and the values for the hidden layer from the previous time step, we`re still performing the standard feedforward calculation introduced in Chapter 7. To see this, consider Fig. 8.2 which clarifies the nature of the recurrence and how it factors into the computation at the hidden layer. The most significant change lies in the new set of weights, U, that connect the hidden layer from the previous time step to the current hidden layer. These weights determine how the network makes use of past context in calculating the output for the current input. As with the other weights in the network, these connections are trained via backpropagation. ", "Bloom_type": "remember", "question": "In a recurrent neural network (RNN), what is the primary difference between adding a temporal dimension and using a non-recurrent architecture?", "options": ["The addition of a temporal dimension allows the network to learn sequential patterns better.", "Non-recurrent architectures do not require any form of training.", "Recurrence only affects the output layer, not the hidden layers.", "Hidden layers become less important when added to an RNN."], "complexity": 0}, {"id": 9, "context": "Tailoring the backpropagation algorithm to this situation leads to a two-pass algorithm for training the weights in RNNs. In the first pass, we perform forward inference, computing ht , yt , accumulating the loss at each step in time, saving the value of the hidden layer at each step for use at the next time step. In the second phase, we process the sequence in reverse, computing the required gradients as we go, computing and saving the error term for use in the hidden layer for each step backward in time. This general approach is commonly referred to as backpropagation through time (Werbos 1974, Rumelhart et al. 1986, Werbos 1990). ", "Bloom_type": "remember", "question": "In the backpropagation through time method, what does the first pass involve?", "options": ["Accumulating the loss at each time step", "Forward inference with no hidden layers", "Computing the hidden layer values only", "Processing the sequence in reverse order"], "complexity": 0}, {"id": 10, "context": "To apply RNNs in this setting, we pass the text to be classified through the RNN a word at a time generating a new hidden layer representation at each time step. We can then take the hidden layer for the last token of the text, hn, to constitute a compressed representation of the entire sequence. We can pass this representation hn to a feedforward network that chooses a class via a softmax over the possible classes. Fig. 8.8 illustrates this approach. ", "Bloom_type": "remember", "question": "In the process of applying Recurrent Neural Networks (RNNs) to classify text, what is generated at each time step?", "options": ["Hidden state", "Output vector", "Feature map", "Context window"], "complexity": 0}, {"id": 11, "context": "Technically an autoregressive model is a model that predicts a value at time t based on a linear function of the previous values at times t 2, and so on. Although language models are not linear (since they have many layers of non-linearities), we loosely refer to this generation technique as autoregressive generation since the word generated at each time step is conditioned on the word selected by the network from the previous step. Fig. 8.9 illustrates this approach. In this figure, the details of the RNN`s hidden layers and recurrent connections are hidden within the blue block. This simple architecture underlies state-of-the-art approaches to applications such as machine translation, summarization, and question answering. The key to these approaches is to prime the generation component with an appropriate context. That is, instead of simply using <s> to get things started we can provide a richer task-appropriate context; for translation the context is the sentence in the source language; for summarization it`s the long text we want to summarize. ", "Bloom_type": "remember", "question": "In the context of autoregressive generation, what does the hidden layer represent?", "options": ["The sequence of words being predicted", "The initial input to the model", "The output of the neural network", "The context-free grammar rules"], "complexity": 0}, {"id": 12, "context": "One reason for the inability of RNNs to carry forward critical information is that the hidden layers, and, by extension, the weights that determine the values in the hidden layer, are being asked to perform two tasks simultaneously: provide information useful for the current decision, and updating and carrying forward information required for future decisions. ", "Bloom_type": "remember", "question": "In recurrent neural networks (RNNs), why do hidden layers struggle with carrying forward critical information?", "options": ["Hidden layers are designed to forget past information.", "Hidden layers cannot learn new patterns.", "Hidden layers require too much computational power.", "Hidden layers are not connected to the output layer."], "complexity": 0}, {"id": 13, "context": "The most commonly used such extension to RNNs is the long short-term memory (LSTM) network (Hochreiter and Schmidhuber, 1997). LSTMs divide the context management problem into two subproblems: removing information no longer needed from the context, and adding information likely to be needed for later decision making. The key to solving both problems is to learn how to manage this context rather than hard-coding a strategy into the architecture. LSTMs accomplish this by first adding an explicit context layer to the architecture (in addition to the usual recurrent hidden layer), and through the use of specialized neural units that make use of gates to control the flow of information into and out of the units that comprise the network layers. These gates are implemented through the use of additional weights that operate sequentially on the input, and previous hidden layer, and previous context layers. ", "Bloom_type": "remember", "question": "In LSTM networks, what does the added explicit context layer do?", "options": ["It controls the flow of information between different layers", "It stores all past inputs", "It adds new features to the data", "It removes irrelevant information"], "complexity": 0}, {"id": 14, "context": "Fig. 8.13 illustrates the complete computation for a single LSTM unit. Given the appropriate weights for the various gates, an LSTM accepts as input the context layer, and hidden layer from the previous time step, along with the current input vector. It then generates updated context and hidden vectors as output. ", "Bloom_type": "remember", "question": "In the context of LSTM units, what does the hidden layer represent?", "options": ["The memory cells storing past states", "The initial state before any inputs are fed into the network", "The external data feeding into the LSTM", "The final result after processing all inputs"], "complexity": 0}, {"id": 15, "context": "Fig. 8.17 shows an English source text (the green witch arrived), a sentence separator token (<s>, and a Spanish target text (llego la bruja verde). To translate a source text, we run it through the network performing forward inference to generate hidden states until we get to the end of the source. Then we begin autoregressive generation, asking for a word in the context of the hidden layer from the end of the source input as well as the end-of-sentence marker. Subsequent words are conditioned on the previous hidden state and the embedding for the last word generated. ", "Bloom_type": "remember", "question": "In translation using neural networks, what is the purpose of generating hidden states before translating?", "options": ["To predict the next word in the target language", "To identify the most frequent words", "To improve the accuracy of the translation model", "To reduce computational complexity"], "complexity": 0}, {"id": 16, "context": " In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t ", "Bloom_type": "remember", "question": "In RNNs, what is the role of the hidden layer?", "options": ["It processes all elements simultaneously.", "It stores the entire sequence for future use.", "It predicts the next element in the sequence.", "It calculates the final output only."], "complexity": 0}, {"id": 17, "context": "Feedforward layer The feedforward layer is a fully-connected 2-layer network, i.e., one hidden layer, two weight matrices, as introduced in Chapter 7. The weights are the same for each token position i , but are different from layer to layer. It is common to make the dimensionality dff of the hidden layer of the feedforward network be larger than the model dimensionality d. (For example in the original transformer model, d = 512 and dff = 2048.) ", "Bloom_type": "remember", "question": "In a feedforward layer with a hidden layer, what does the term \"hidden layer\" refer to?", "options": ["The intermediate layers between the input and output layers", "The first layer before the input layer", "The second layer after the input layer", "The output layer after processing"], "complexity": 0}, {"id": 18, "context": "By the next decade, Bengio et al. (2003) and Bengio et al. (2006) showed that neural language models could also be used to develop embeddings as part of the task of word prediction. Collobert and Weston (2007), Collobert and Weston (2008), and Collobert et al. (2011) then demonstrated that embeddings could be used to represent word meanings for a number of NLP tasks. Turian et al. (2010) compared the value of different kinds of embeddings for different NLP tasks. Mikolov et al. (2011) showed that recurrent neural nets could be used as language models. The idea of simplifying the hidden layer of these neural net language models to create the skipgram (and also CBOW) algorithms was proposed by Mikolov et al. (2013a). The negative sampling training algorithm was proposed in Mikolov et al. (2013b). There are numerous surveys of static embeddings and their parameterizations (Bullinaria and Levy 2007, Bullinaria and Levy 2012, Lapesa and Evert 2014, Kiela and Clark 2014, Levy et al. 2015). ", "Bloom_type": "comprehension", "question": "What were some key developments in using neural network language models for embedding purposes before the year 2013?", "options": ["All of the above", "Collobert and Weston (2007), Collobert and Weston (2008), and Collobert et al. (2011)", "Mikolov et al. (2011) and the negative sampling training algorithm", "Turian et al. (2010) and the comparison of different types of embeddings"], "complexity": 1}, {"id": 19, "context": "Instead, a modern neural network is a network of small computing units, each of which takes a vector of input values and produces a single output value. In this chapter we introduce the neural net applied to classification. The architecture we introduce is called a feedforward network because the computation proceeds iteratively from one layer of units to the next. The use of modern neural nets is often called deep learning, because modern networks are often deep (have many layers). Neural networks share much of the same mathematics as logistic regression. But neural networks are a more powerful classifier than logistic regression, and indeed a minimal neural network (technically one with a single hidden layer`) can be shown to learn any function. ", "Bloom_type": "comprehension", "question": "What does the introduction of a single hidden layer in a neural network imply about its capability compared to logistic regression?", "options": ["The single hidden layer restricts the complexity of learned functions.", "The single hidden layer allows for less complex functions.", "The single hidden layer enables deeper understanding of data patterns.", "The single hidden layer has no impact on the complexity of learned functions."], "complexity": 1}, {"id": 20, "context": "It`s also instructive to look at the intermediate results, the outputs of the two hidden nodes h1 and h2. We showed in the previous paragraph that the h vector for the inputs x = [0, 0] was [0, 0]. Fig. 7.7b shows the values of the h layer for all 4 inputs. Notice that hidden representations of the two input points x = [0, 1] and x = [1, 0] (the two cases with XOR output = 1) are merged to the single point h = [1, 0]. The merger makes it easy to linearly separate the positive and negative cases of XOR. In other words, we can view the hidden layer of the network as forming a representation of the input. ", "Bloom_type": "comprehension", "question": "What does the merging of hidden representations for specific input points indicate about the hidden layer's role in the neural network?", "options": ["It enhances the ability to handle complex patterns through non-linear combinations.", "It simplifies the computation by reducing the number of weights.", "It allows for direct visualization of the input data.", "It increases the complexity of the model unnecessarily."], "complexity": 1}, {"id": 21, "context": "In this example we just stipulated the weights in Fig. 7.6. But for real examples the weights for neural networks are learned automatically using the error backpropagation algorithm to be introduced in Section 7.5. That means the hidden layers will learn to form useful representations. This intuition, that neural networks can automatically learn useful representations of the input, is one of their key advantages, and one that we will return to again and again in later chapters. ", "Bloom_type": "comprehension", "question": "What does the learning of weights in neural networks imply about their ability to perform tasks?", "options": ["They can automatically learn useful representations.", "They cannot learn any representation.", "They can only learn simple patterns.", "Their performance depends solely on the initial weights."], "complexity": 1}, {"id": 22, "context": "That means we can think of a neural network classifier with one hidden layer as building a vector h which is a hidden layer representation of the input, and then running standard multinomial logistic regression on the features that the network develops in h. By contrast, in Chapter 5 the features were mainly designed by hand via feature templates. So a neural network is like multinomial logistic regression, but (a) with many layers, since a deep neural network is like layer after layer of logistic regression classifiers; (b) with those intermediate layers having many possible activation functions (tanh, ReLU, sigmoid) instead of just sigmoid (although we`ll continue to use  for convenience to mean any activation function); (c) rather than forming the features by feature templates, the prior layers of the network induce the feature representations themselves. ", "Bloom_type": "comprehension", "question": "How does a neural network differ from traditional logistic regression when using multiple hidden layers?", "options": ["It has more complex layers and various activation functions.", "It uses fewer layers and simpler activation functions.", "It forms features manually through feature templates.", "It simplifies the learning process by reducing the number of parameters."], "complexity": 1}, {"id": 23, "context": "Fig. 7.10 shows a sketch of this architecture. As we mentioned earlier, adding this hidden layer to our logistic regression classifier allows the network to represent the non-linear interactions between features. This alone might give us a better sentiment classifier. ", "Bloom_type": "comprehension", "question": "What does the addition of a hidden layer in a neural network allow it to do?", "options": ["It enhances the ability to capture complex patterns.", "It increases the speed of computation.", "It simplifies the training process.", "It reduces the need for data preprocessing."], "complexity": 1}, {"id": 24, "context": "Fig. 8.1 illustrates the structure of an RNN. As with ordinary feedforward networks, an input vector representing the current input, xt , is multiplied by a weight matrix and then passed through a non-linear activation function to compute the values for a layer of hidden units. This hidden layer is then used to calculate a corresponding output, yt . In a departure from our earlier window-based approach, sequences are processed by presenting one item at a time to the network. We`ll use subscripts to represent time, thus xt will mean the input vector x at time t. The key difference from a feedforward network lies in the recurrent link shown in the figure with the dashed line. This link augments the input to the computation at the hidden layer with the value of the hidden layer from the preceding point in time. ", "Bloom_type": "comprehension", "question": "What distinguishes the hidden layer in an RNN compared to a feedforward neural network?", "options": ["The hidden layer incorporates feedback connections from previous time steps.", "The hidden layer uses a different type of activation function.", "The hidden layer processes inputs sequentially rather than simultaneously.", "The hidden layer does not involve any non-linear transformations."], "complexity": 1}, {"id": 25, "context": "The hidden layer from the previous time step provides a form of memory, or context, that encodes earlier processing and informs the decisions to be made at later points in time. Critically, this approach does not impose a fixed-length limit on this prior context; the context embodied in the previous hidden layer can include information extending back to the beginning of the sequence. ", "Bloom_type": "comprehension", "question": "How does the hidden layer contribute to decision-making in neural networks?", "options": ["It combines current and past information to inform future actions.", "It stores all past data for future reference.", "It retains only the most recent input data for immediate use.", "It uses the entire history of inputs to make predictions."], "complexity": 1}, {"id": 26, "context": "Adding this temporal dimension makes RNNs appear to be more complex than non-recurrent architectures. But in reality, they`re not all that different. Given an input vector and the values for the hidden layer from the previous time step, we`re still performing the standard feedforward calculation introduced in Chapter 7. To see this, consider Fig. 8.2 which clarifies the nature of the recurrence and how it factors into the computation at the hidden layer. The most significant change lies in the new set of weights, U, that connect the hidden layer from the previous time step to the current hidden layer. These weights determine how the network makes use of past context in calculating the output for the current input. As with the other weights in the network, these connections are trained via backpropagation. ", "Bloom_type": "comprehension", "question": "How does adding a temporal dimension affect Recurrent Neural Networks (RNNs) compared to non-recurrent architectures?", "options": ["It doesn't make any difference; RNNs remain just as simple as non-recurrent architectures.", "It makes RNNs simpler by reducing complexity.", "Adding a temporal dimension increases the computational load significantly.", "The addition of a temporal dimension allows RNNs to learn sequential patterns better."], "complexity": 1}, {"id": 27, "context": "Tailoring the backpropagation algorithm to this situation leads to a two-pass algorithm for training the weights in RNNs. In the first pass, we perform forward inference, computing ht , yt , accumulating the loss at each step in time, saving the value of the hidden layer at each step for use at the next time step. In the second phase, we process the sequence in reverse, computing the required gradients as we go, computing and saving the error term for use in the hidden layer for each step backward in time. This general approach is commonly referred to as backpropagation through time (Werbos 1974, Rumelhart et al. 1986, Werbos 1990). ", "Bloom_type": "comprehension", "question": "What does the backpropagation through time (BPTT) method involve in the training of RNNs?", "options": ["Forward inference followed by backward inference", "Forward inference only", "Backward inference only", "No specific order mentioned"], "complexity": 1}, {"id": 28, "context": "To apply RNNs in this setting, we pass the text to be classified through the RNN a word at a time generating a new hidden layer representation at each time step. We can then take the hidden layer for the last token of the text, hn, to constitute a compressed representation of the entire sequence. We can pass this representation hn to a feedforward network that chooses a class via a softmax over the possible classes. Fig. 8.8 illustrates this approach. ", "Bloom_type": "comprehension", "question": "What does the hidden layer represent in the context of applying RNNs?", "options": ["The compressed representation of the entire sequence", "The current state of the input data", "The next word in the sequence", "The final output of the RNN"], "complexity": 1}, {"id": 29, "context": "Technically an autoregressive model is a model that predicts a value at time t based on a linear function of the previous values at times t 2, and so on. Although language models are not linear (since they have many layers of non-linearities), we loosely refer to this generation technique as autoregressive generation since the word generated at each time step is conditioned on the word selected by the network from the previous step. Fig. 8.9 illustrates this approach. In this figure, the details of the RNN`s hidden layers and recurrent connections are hidden within the blue block. This simple architecture underlies state-of-the-art approaches to applications such as machine translation, summarization, and question answering. The key to these approaches is to prime the generation component with an appropriate context. That is, instead of simply using <s> to get things started we can provide a richer task-appropriate context; for translation the context is the sentence in the source language; for summarization it`s the long text we want to summarize. ", "Bloom_type": "comprehension", "question": "What does the hidden layer in an autoregressive model represent?", "options": ["The internal structure of the model itself", "The sequence of words being generated", "The output of the neural network", "The input data fed into the model"], "complexity": 1}, {"id": 30, "context": "One reason for the inability of RNNs to carry forward critical information is that the hidden layers, and, by extension, the weights that determine the values in the hidden layer, are being asked to perform two tasks simultaneously: provide information useful for the current decision, and updating and carrying forward information required for future decisions. ", "Bloom_type": "comprehension", "question": "What dual task does the hidden layer in an RNN have to perform?", "options": ["Both A) Updating and carrying forward information required for future decisions and B) Providing information useful for the current decision.", "Updating and carrying forward information required for past decisions only.", "Providing information useful for the current decision only.", "Neither A) nor B)"], "complexity": 1}, {"id": 31, "context": "The most commonly used such extension to RNNs is the long short-term memory (LSTM) network (Hochreiter and Schmidhuber, 1997). LSTMs divide the context management problem into two subproblems: removing information no longer needed from the context, and adding information likely to be needed for later decision making. The key to solving both problems is to learn how to manage this context rather than hard-coding a strategy into the architecture. LSTMs accomplish this by first adding an explicit context layer to the architecture (in addition to the usual recurrent hidden layer), and through the use of specialized neural units that make use of gates to control the flow of information into and out of the units that comprise the network layers. These gates are implemented through the use of additional weights that operate sequentially on the input, and previous hidden layer, and previous context layers. ", "Bloom_type": "comprehension", "question": "What does the addition of a hidden layer do in LSTM networks?", "options": ["It separates the context management problem into smaller parts.", "It adds more neurons to the network.", "It allows for more complex computations within each time step.", "It simplifies the learning process by reducing the number of parameters."], "complexity": 1}, {"id": 32, "context": "Fig. 8.13 illustrates the complete computation for a single LSTM unit. Given the appropriate weights for the various gates, an LSTM accepts as input the context layer, and hidden layer from the previous time step, along with the current input vector. It then generates updated context and hidden vectors as output. ", "Bloom_type": "comprehension", "question": "What are two layers mentioned in the context when describing how an LSTM works?", "options": ["Context layer and hidden layer", "Input gate and forget gate", "Output gate and update gate", "Memory cell and state vector"], "complexity": 1}, {"id": 33, "context": "Fig. 8.17 shows an English source text (the green witch arrived), a sentence separator token (<s>, and a Spanish target text (llego la bruja verde). To translate a source text, we run it through the network performing forward inference to generate hidden states until we get to the end of the source. Then we begin autoregressive generation, asking for a word in the context of the hidden layer from the end of the source input as well as the end-of-sentence marker. Subsequent words are conditioned on the previous hidden state and the embedding for the last word generated. ", "Bloom_type": "comprehension", "question": "In the translation process using the network, how does the hidden layer contribute to the output?", "options": ["It generates new words based on the current hidden state.", "It separates sentences within the source text.", "It predicts the next word in the target language.", "It combines the source and target texts into one cohesive piece."], "complexity": 1}, {"id": 34, "context": " In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t ", "Bloom_type": "comprehension", "question": "In RNNs, how does the output of each neural unit depend?", "options": ["On the current input at time t and the hidden layer from time t", "Only on the current input at time t", "Only on the previous hidden layer", "Not dependent on any layers"], "complexity": 1}, {"id": 35, "context": "Feedforward layer The feedforward layer is a fully-connected 2-layer network, i.e., one hidden layer, two weight matrices, as introduced in Chapter 7. The weights are the same for each token position i , but are different from layer to layer. It is common to make the dimensionality dff of the hidden layer of the feedforward network be larger than the model dimensionality d. (For example in the original transformer model, d = 512 and dff = 2048.) ", "Bloom_type": "comprehension", "question": "What distinguishes a feedforward layer from other types of layers in neural networks?", "options": ["It has only one hidden layer.", "It uses a single weight matrix per token position.", "Its dimensions are always smaller than those of the input layer.", "It requires more parameters compared to recurrent layers."], "complexity": 1}, {"id": 36, "context": "By the next decade, Bengio et al. (2003) and Bengio et al. (2006) showed that neural language models could also be used to develop embeddings as part of the task of word prediction. Collobert and Weston (2007), Collobert and Weston (2008), and Collobert et al. (2011) then demonstrated that embeddings could be used to represent word meanings for a number of NLP tasks. Turian et al. (2010) compared the value of different kinds of embeddings for different NLP tasks. Mikolov et al. (2011) showed that recurrent neural nets could be used as language models. The idea of simplifying the hidden layer of these neural net language models to create the skipgram (and also CBOW) algorithms was proposed by Mikolov et al. (2013a). The negative sampling training algorithm was proposed in Mikolov et al. (2013b). There are numerous surveys of static embeddings and their parameterizations (Bullinaria and Levy 2007, Bullinaria and Levy 2012, Lapesa and Evert 2014, Kiela and Clark 2014, Levy et al. 2015). ", "Bloom_type": "application", "question": "What is the first step in developing an embedding using neural language models?", "options": ["Creating a hidden layer structure", "Selecting a dataset", "Training the model on a large corpus", "Choosing between static and dynamic embeddings"], "complexity": 2}, {"id": 37, "context": "Instead, a modern neural network is a network of small computing units, each of which takes a vector of input values and produces a single output value. In this chapter we introduce the neural net applied to classification. The architecture we introduce is called a feedforward network because the computation proceeds iteratively from one layer of units to the next. The use of modern neural nets is often called deep learning, because modern networks are often deep (have many layers). Neural networks share much of the same mathematics as logistic regression. But neural networks are a more powerful classifier than logistic regression, and indeed a minimal neural network (technically one with a single hidden layer`) can be shown to learn any function. ", "Bloom_type": "application", "question": "What does it mean when a neural network is said to have a single hidden layer?", "options": ["The network consists of a single layer of interconnected nodes.", "The network only contains one type of neuron.", "The network processes data through just one set of neurons before moving on to another.", "The network uses only one method for processing inputs."], "complexity": 2}, {"id": 38, "context": "It`s also instructive to look at the intermediate results, the outputs of the two hidden nodes h1 and h2. We showed in the previous paragraph that the h vector for the inputs x = [0, 0] was [0, 0]. Fig. 7.7b shows the values of the h layer for all 4 inputs. Notice that hidden representations of the two input points x = [0, 1] and x = [1, 0] (the two cases with XOR output = 1) are merged to the single point h = [1, 0]. The merger makes it easy to linearly separate the positive and negative cases of XOR. In other words, we can view the hidden layer of the network as forming a representation of the input. ", "Bloom_type": "application", "question": "What is the purpose of merging the hidden representations of the two input points?", "options": ["To simplify the decision boundary for separating XOR", "To increase the complexity of the model", "To reduce computational cost", "To enhance data privacy"], "complexity": 2}, {"id": 39, "context": "In this example we just stipulated the weights in Fig. 7.6. But for real examples the weights for neural networks are learned automatically using the error backpropagation algorithm to be introduced in Section 7.5. That means the hidden layers will learn to form useful representations. This intuition, that neural networks can automatically learn useful representations of the input, is one of their key advantages, and one that we will return to again and again in later chapters. ", "Bloom_type": "application", "question": "What does it mean when we say that neural networks can automatically learn useful representations?", "options": ["The network learns patterns directly from the data without human intervention.", "The network memorizes all the data perfectly.", "The network only uses predefined rules to solve problems.", "The network requires explicit instructions on how to represent inputs."], "complexity": 2}, {"id": 40, "context": "That means we can think of a neural network classifier with one hidden layer as building a vector h which is a hidden layer representation of the input, and then running standard multinomial logistic regression on the features that the network develops in h. By contrast, in Chapter 5 the features were mainly designed by hand via feature templates. So a neural network is like multinomial logistic regression, but (a) with many layers, since a deep neural network is like layer after layer of logistic regression classifiers; (b) with those intermediate layers having many possible activation functions (tanh, ReLU, sigmoid) instead of just sigmoid (although we`ll continue to use  for convenience to mean any activation function); (c) rather than forming the features by feature templates, the prior layers of the network induce the feature representations themselves. ", "Bloom_type": "application", "question": "What does the hidden layer represent in a neural network classifier?", "options": ["The intermediate feature representations", "The output layer", "The weights of the connections between neurons", "The final classification decision"], "complexity": 2}, {"id": 41, "context": "Fig. 7.10 shows a sketch of this architecture. As we mentioned earlier, adding this hidden layer to our logistic regression classifier allows the network to represent the non-linear interactions between features. This alone might give us a better sentiment classifier. ", "Bloom_type": "application", "question": "What is added to the logistic regression classifier to improve its performance?", "options": ["Hidden layers", "Linear layers", "Convolutional layers", "Pooling layers"], "complexity": 2}, {"id": 42, "context": "Fig. 8.1 illustrates the structure of an RNN. As with ordinary feedforward networks, an input vector representing the current input, xt , is multiplied by a weight matrix and then passed through a non-linear activation function to compute the values for a layer of hidden units. This hidden layer is then used to calculate a corresponding output, yt . In a departure from our earlier window-based approach, sequences are processed by presenting one item at a time to the network. We`ll use subscripts to represent time, thus xt will mean the input vector x at time t. The key difference from a feedforward network lies in the recurrent link shown in the figure with the dashed line. This link augments the input to the computation at the hidden layer with the value of the hidden layer from the preceding point in time. ", "Bloom_type": "application", "question": "What does the recurrent link in the RNN diagram signify?", "options": ["It denotes the sequence processing method.", "It represents the next input vector.", "It indicates the previous hidden state.", "It shows the connection between two different layers."], "complexity": 2}, {"id": 43, "context": "The hidden layer from the previous time step provides a form of memory, or context, that encodes earlier processing and informs the decisions to be made at later points in time. Critically, this approach does not impose a fixed-length limit on this prior context; the context embodied in the previous hidden layer can include information extending back to the beginning of the sequence. ", "Bloom_type": "application", "question": "What is the role of the hidden layer in the neural network?", "options": ["It retains information about past states to influence current decision-making.", "It stores all past inputs for future predictions.", "It processes input data directly without any encoding.", "It only remembers recent inputs but discards older ones."], "complexity": 2}, {"id": 44, "context": "Adding this temporal dimension makes RNNs appear to be more complex than non-recurrent architectures. But in reality, they`re not all that different. Given an input vector and the values for the hidden layer from the previous time step, we`re still performing the standard feedforward calculation introduced in Chapter 7. To see this, consider Fig. 8.2 which clarifies the nature of the recurrence and how it factors into the computation at the hidden layer. The most significant change lies in the new set of weights, U, that connect the hidden layer from the previous time step to the current hidden layer. These weights determine how the network makes use of past context in calculating the output for the current input. As with the other weights in the network, these connections are trained via backpropagation. ", "Bloom_type": "application", "question": "What is the primary difference between recurrent neural networks (RNNs) and non-recurrent architectures when considering their complexity?", "options": ["The addition of a temporal dimension does not affect the complexity of RNNs.", "RNNs require additional layers compared to non-recurrent architectures.", "Non-recurrent architectures perform calculations faster than RNNs.", "RNNs can only handle static inputs whereas non-recurrent architectures can handle dynamic ones."], "complexity": 2}, {"id": 45, "context": "Tailoring the backpropagation algorithm to this situation leads to a two-pass algorithm for training the weights in RNNs. In the first pass, we perform forward inference, computing ht , yt , accumulating the loss at each step in time, saving the value of the hidden layer at each step for use at the next time step. In the second phase, we process the sequence in reverse, computing the required gradients as we go, computing and saving the error term for use in the hidden layer for each step backward in time. This general approach is commonly referred to as backpropagation through time (Werbos 1974, Rumelhart et al. 1986, Werbos 1990). ", "Bloom_type": "application", "question": "What does the second phase of the two-pass algorithm involve?", "options": ["Processing the sequence in reverse while calculating gradients", "Performing forward inference on the entire sequence", "Updating the weights using accumulated losses", "Saving the hidden layer values for future reference"], "complexity": 2}, {"id": 46, "context": "To apply RNNs in this setting, we pass the text to be classified through the RNN a word at a time generating a new hidden layer representation at each time step. We can then take the hidden layer for the last token of the text, hn, to constitute a compressed representation of the entire sequence. We can pass this representation hn to a feedforward network that chooses a class via a softmax over the possible classes. Fig. 8.8 illustrates this approach. ", "Bloom_type": "application", "question": "What is the first step in applying RNNs to classify text?", "options": ["Generate a hidden layer representation for every word in the text.", "Pass the entire text to the RNN as a single input.", "Use the final hidden layer representation of the last word as the output.", "Feed the entire sequence through a feedforward network."], "complexity": 2}, {"id": 47, "context": "Technically an autoregressive model is a model that predicts a value at time t based on a linear function of the previous values at times t 2, and so on. Although language models are not linear (since they have many layers of non-linearities), we loosely refer to this generation technique as autoregressive generation since the word generated at each time step is conditioned on the word selected by the network from the previous step. Fig. 8.9 illustrates this approach. In this figure, the details of the RNN`s hidden layers and recurrent connections are hidden within the blue block. This simple architecture underlies state-of-the-art approaches to applications such as machine translation, summarization, and question answering. The key to these approaches is to prime the generation component with an appropriate context. That is, instead of simply using <s> to get things started we can provide a richer task-appropriate context; for translation the context is the sentence in the source language; for summarization it`s the long text we want to summarize. ", "Bloom_type": "application", "question": "In an autoregressive model, what does the hidden layer represent?", "options": ["The internal structure of the model that processes sequences of words.", "The output of the neural network after processing all input data.", "The sequence of words generated by the model up to a certain point.", "The weights used in the linear functions applied to past inputs."], "complexity": 2}, {"id": 48, "context": "One reason for the inability of RNNs to carry forward critical information is that the hidden layers, and, by extension, the weights that determine the values in the hidden layer, are being asked to perform two tasks simultaneously: provide information useful for the current decision, and updating and carrying forward information required for future decisions. ", "Bloom_type": "application", "question": "What strategy can help mitigate the issue of hidden layers in RNNs?", "options": ["Implement skip connections", "Increase the number of hidden layers", "Reduce the size of the hidden layers", "Use simpler activation functions"], "complexity": 2}, {"id": 49, "context": "The most commonly used such extension to RNNs is the long short-term memory (LSTM) network (Hochreiter and Schmidhuber, 1997). LSTMs divide the context management problem into two subproblems: removing information no longer needed from the context, and adding information likely to be needed for later decision making. The key to solving both problems is to learn how to manage this context rather than hard-coding a strategy into the architecture. LSTMs accomplish this by first adding an explicit context layer to the architecture (in addition to the usual recurrent hidden layer), and through the use of specialized neural units that make use of gates to control the flow of information into and out of the units that comprise the network layers. These gates are implemented through the use of additional weights that operate sequentially on the input, and previous hidden layer, and previous context layers. ", "Bloom_type": "application", "question": "Which part of the LSTM network specifically handles the gating mechanism to control the flow of information?", "options": ["Both the recurrent hidden layer and the context layer", "The recurrent hidden layer only", "The context layer only", "None of the above"], "complexity": 2}, {"id": 50, "context": "Fig. 8.13 illustrates the complete computation for a single LSTM unit. Given the appropriate weights for the various gates, an LSTM accepts as input the context layer, and hidden layer from the previous time step, along with the current input vector. It then generates updated context and hidden vectors as output. ", "Bloom_type": "application", "question": "In the LSTM model, what is the role of the hidden layer?", "options": ["It updates the state based on the input and previous states.", "It stores the entire sequence of inputs.", "It processes the input data through activation functions.", "It holds the learned representations of the input."], "complexity": 2}, {"id": 51, "context": "Fig. 8.17 shows an English source text (the green witch arrived), a sentence separator token (<s>, and a Spanish target text (llego la bruja verde). To translate a source text, we run it through the network performing forward inference to generate hidden states until we get to the end of the source. Then we begin autoregressive generation, asking for a word in the context of the hidden layer from the end of the source input as well as the end-of-sentence marker. Subsequent words are conditioned on the previous hidden state and the embedding for the last word generated. ", "Bloom_type": "application", "question": "What is the first step in translating a source text using this method?", "options": ["Run the source text through the network performing forward inference", "Begin autoregressive generation with the end-of-sentence marker", "Generate a word based on the last hidden state and the last word generated", "Identify the hidden layers in the network"], "complexity": 2}, {"id": 52, "context": " In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t ", "Bloom_type": "application", "question": "What is the role of the hidden layer in a Simple RNN?", "options": ["It combines the current input with previous outputs.", "It processes all elements simultaneously.", "It stores the entire sequence for future reference.", "It predicts the next element in the sequence."], "complexity": 2}, {"id": 53, "context": "Feedforward layer The feedforward layer is a fully-connected 2-layer network, i.e., one hidden layer, two weight matrices, as introduced in Chapter 7. The weights are the same for each token position i , but are different from layer to layer. It is common to make the dimensionality dff of the hidden layer of the feedforward network be larger than the model dimensionality d. (For example in the original transformer model, d = 512 and dff = 2048.) ", "Bloom_type": "application", "question": "What does it mean when we say the dimensions of the hidden layer in a feedforward network are larger than the model dimensionality?", "options": ["The hidden layer has more parameters than the input layer.", "The hidden layer contains more neurons than the input layer.", "The hidden layer uses a different activation function compared to the output layer.", "The hidden layer processes data through multiple layers before reaching the final output."], "complexity": 2}]}, "antecedent": {"max_id": 11, "Questions": [{"id": 0, "context": "Once the classifier is trained, it is applied to each test sentence in a clustering step. For each mention i in a document, the classifier considers each of the prior i 1 mentions. In closest-first clustering (Soon et al., 2001), the classifier is run right to left (from mention i 1 down to mention 1) and the first antecedent with probability > .5 is linked to i. If no antecedent has probably > 0.5, no antecedent is selected for i. In best-first clustering, the classifier is run on all i 1 antecedents and the most probable preceding mention is chosen as the antecedent for i. The transitive closure of the pairwise relation is taken as the cluster. ", "Bloom_type": "remember", "question": "In which type of clustering does the classifier consider each of the prior i-1 mentions before deciding on an antecedent?", "options": ["Closest-first clustering", "Best-first clustering", "Sequential clustering", "Random clustering"], "complexity": 0}, {"id": 1, "context": "The mention ranking model directly compares candidate antecedents to each other, choosing the highest-scoring antecedent for each anaphor. ", "Bloom_type": "remember", "question": "In the mention ranking model, what is compared between candidates?", "options": ["Antecedents are compared with each other.", "Candidates are ranked based on their similarity.", "Candidates are ranked based on their relevance.", "Candidates are ranked based on their frequency of occurrence."], "complexity": 0}, {"id": 2, "context": "In early formulations, for mention i, the classifier decides which of the prior mentions is the antecedent (Denis and Baldridge, 2008). But suppose i is 1 in fact not anaphoric, and none of the antecedents should be chosen? Such a model would need to run a separate anaphoricity classifier on i. Instead, it turns out to be better to jointly learn anaphoricity detection and coreference together with a single loss (Rahman and Ng, 2009). ", "Bloom_type": "remember", "question": "In the context provided, what does the classifier decide when deciding which of the prior mentions is the antecedent?", "options": ["The classifier decides which of the prior mentions is the antecedent based on their relevance.", "The classifier decides which of the prior mentions is the antecedent by comparing them directly.", "The classifier decides which of the prior mentions is the antecedent using a predefined rule.", "The classifier decides which of the prior mentions is the antecedent randomly."], "complexity": 0}, {"id": 3, "context": "The move from mention-pair to mention-ranking approaches was pioneered by Yang et al. (2003) and Iida et al. (2003) who proposed pairwise ranking methods, then extended by Denis and Baldridge (2008) who proposed to do ranking via a softmax over all prior mentions. The idea of doing mention detection, anaphoricity, and coreference jointly in a single end-to-end model grew out of the early proposal of Ng (2005b) to use a dummy antecedent for mention-ranking, allowing non-referential` to be a choice for coreference classifiers, Denis and Baldridge`s 2007 joint system combining anaphoricity classifier probabilities with coreference probabilities, the Denis and Baldridge (2008) ranking model, and the Rahman and Ng (2009) proposal to train the two models jointly with a single objective. ", "Bloom_type": "remember", "question": "In what year did Denis and Baldridge propose their joint system combining anaphoricity classifier probabilities with coreference probabilities?", "options": ["2005", "2003", "2004", "2006"], "complexity": 0}, {"id": 4, "context": "Once the classifier is trained, it is applied to each test sentence in a clustering step. For each mention i in a document, the classifier considers each of the prior i 1 mentions. In closest-first clustering (Soon et al., 2001), the classifier is run right to left (from mention i 1 down to mention 1) and the first antecedent with probability > .5 is linked to i. If no antecedent has probably > 0.5, no antecedent is selected for i. In best-first clustering, the classifier is run on all i 1 antecedents and the most probable preceding mention is chosen as the antecedent for i. The transitive closure of the pairwise relation is taken as the cluster. ", "Bloom_type": "comprehension", "question": "In which type of clustering does the classifier consider each of the prior i-1 mentions when determining an antecedent for mention i?", "options": ["Closest-first clustering", "Best-first clustering", "Both types use this method", "Neither type uses this method"], "complexity": 1}, {"id": 5, "context": "The mention ranking model directly compares candidate antecedents to each other, choosing the highest-scoring antecedent for each anaphor. ", "Bloom_type": "comprehension", "question": "What does the mention ranking model compare when it ranks candidate antecedents?", "options": ["The similarity between antecedents", "The length of sentences containing antecedents", "The frequency of occurrence of antecedents", "The grammatical structure of antecedents"], "complexity": 1}, {"id": 6, "context": "In early formulations, for mention i, the classifier decides which of the prior mentions is the antecedent (Denis and Baldridge, 2008). But suppose i is 1 in fact not anaphoric, and none of the antecedents should be chosen? Such a model would need to run a separate anaphoricity classifier on i. Instead, it turns out to be better to jointly learn anaphoricity detection and coreference together with a single loss (Rahman and Ng, 2009). ", "Bloom_type": "comprehension", "question": "In the context provided, explain how models handle cases where a mention i is not anaphoric?", "options": ["Models rely on a joint learning approach combining anaphoricity detection and coreference resolution.", "Models use a separate anaphoricity classifier exclusively.", "Models decide based solely on syntactic features.", "Models ignore non-anaphoric mentions entirely."], "complexity": 1}, {"id": 7, "context": "The move from mention-pair to mention-ranking approaches was pioneered by Yang et al. (2003) and Iida et al. (2003) who proposed pairwise ranking methods, then extended by Denis and Baldridge (2008) who proposed to do ranking via a softmax over all prior mentions. The idea of doing mention detection, anaphoricity, and coreference jointly in a single end-to-end model grew out of the early proposal of Ng (2005b) to use a dummy antecedent for mention-ranking, allowing non-referential` to be a choice for coreference classifiers, Denis and Baldridge`s 2007 joint system combining anaphoricity classifier probabilities with coreference probabilities, the Denis and Baldridge (2008) ranking model, and the Rahman and Ng (2009) proposal to train the two models jointly with a single objective. ", "Bloom_type": "comprehension", "question": "What did Denis and Baldridge propose to extend their previous work?", "options": ["all of the above", "mention detection", "anaphoricity", "coreference"], "complexity": 1}, {"id": 8, "context": "Once the classifier is trained, it is applied to each test sentence in a clustering step. For each mention i in a document, the classifier considers each of the prior i 1 mentions. In closest-first clustering (Soon et al., 2001), the classifier is run right to left (from mention i 1 down to mention 1) and the first antecedent with probability > .5 is linked to i. If no antecedent has probably > 0.5, no antecedent is selected for i. In best-first clustering, the classifier is run on all i 1 antecedents and the most probable preceding mention is chosen as the antecedent for i. The transitive closure of the pairwise relation is taken as the cluster. ", "Bloom_type": "application", "question": "In which type of clustering does the classifier consider each of the prior i-1 mentions when determining an antecedent for mention i?", "options": ["Closest-first clustering", "Best-first clustering", "Sequential clustering", "Random clustering"], "complexity": 2}, {"id": 9, "context": "The mention ranking model directly compares candidate antecedents to each other, choosing the highest-scoring antecedent for each anaphor. ", "Bloom_type": "application", "question": "What is the first step in the response of the mention ranking model?", "options": ["Directly comparing candidate antecedents to each other", "Comparing candidate antecedents", "Choosing the highest-scoring antecedent", "Anaphora resolution"], "complexity": 2}, {"id": 10, "context": "In early formulations, for mention i, the classifier decides which of the prior mentions is the antecedent (Denis and Baldridge, 2008). But suppose i is 1 in fact not anaphoric, and none of the antecedents should be chosen? Such a model would need to run a separate anaphoricity classifier on i. Instead, it turns out to be better to jointly learn anaphoricity detection and coreference together with a single loss (Rahman and Ng, 2009). ", "Bloom_type": "application", "question": "In this scenario, how would you plan to handle cases where mention i is not anaphoric?", "options": ["Jointly learn anaphoricity detection and coreference together with a single loss", "Run a separate anaphoricity classifier on i", "Decide which of the prior mentions is the antecedent based solely on method1", "Use only method2 to determine the antecedent"], "complexity": 2}, {"id": 11, "context": "The move from mention-pair to mention-ranking approaches was pioneered by Yang et al. (2003) and Iida et al. (2003) who proposed pairwise ranking methods, then extended by Denis and Baldridge (2008) who proposed to do ranking via a softmax over all prior mentions. The idea of doing mention detection, anaphoricity, and coreference jointly in a single end-to-end model grew out of the early proposal of Ng (2005b) to use a dummy antecedent for mention-ranking, allowing non-referential` to be a choice for coreference classifiers, Denis and Baldridge`s 2007 joint system combining anaphoricity classifier probabilities with coreference probabilities, the Denis and Baldridge (2008) ranking model, and the Rahman and Ng (2009) proposal to train the two models jointly with a single objective. ", "Bloom_type": "application", "question": "What is the initial step in developing a method for detecting antecedents?", "options": ["Proposing a new method based on existing techniques", "Designing a neural network architecture", "Selecting appropriate training data sets", "Implementing a coreference resolution algorithm"], "complexity": 2}]}, "speech recognition": {"max_id": 95, "Questions": [{"id": 0, "context": "Finally, we`ll need to compare words and other strings. We`ll introduce a metric called edit distance that measures how similar two strings are based on the number of edits (insertions, deletions, substitutions) it takes to change one string into the other. Edit distance is an algorithm with applications throughout language processing, from spelling correction to speech recognition to coreference resolution. ", "Bloom_type": "remember", "question": "In what field does the concept of edit distance play a significant role?", "options": ["Artificial Intelligence", "Computer Science", "Mathematics", "Physics"], "complexity": 0}, {"id": 1, "context": "We still have decisions to make! For example, should we consider a capitalized string (like They) and one that is uncapitalized (like they) to be the same word type? The answer is that it depends on the task! They and they might be lumped together as the same type in some tasks, like speech recognition, where we care more about the sequence of words and less about the formatting, while for other tasks, such as deciding whether a particular word is a name of a person or location (namedentity tagging), capitalization is a useful feature and is retained. Sometimes we keep around two versions of a particular NLP model, one with capitalization and one without capitalization. ", "Bloom_type": "remember", "question": "In speech recognition, how are capitalized strings typically treated compared to uncapitalized ones?", "options": ["The treatment of capitalized vs. uncapitalized strings varies depending on the specific task.", "Capitalized strings are always considered different from uncapitalized ones.", "Capitalized strings are treated equally to uncapitalized ones in all contexts.", "Speech recognition models often treat capitalized strings differently than uncapitalized ones."], "complexity": 0}, {"id": 2, "context": "Alignment Knowing the minimum edit distance is useful for algorithms like finding potential spelling error corrections. But the edit distance algorithm is important in another way; with a small change, it can also provide the minimum cost alignment between two strings. Aligning two strings is useful throughout speech and language processing. In speech recognition, minimum edit distance alignment is used to compute the word error rate (Chapter 16). Alignment plays a role in machine translation, in which sentences in a parallel corpus (a corpus with a text in two languages) need to be matched to each other. ", "Bloom_type": "remember", "question": "In what application of speech recognition does the concept of minimum edit distance play a crucial role?", "options": ["Automatic speech recognition", "Machine learning", "Speech synthesis", "Text-to-speech conversion"], "complexity": 0}, {"id": 3, "context": "The best way to evaluate the performance of a language model is to embed it in an application and measure how much the application improves. Such end-to-end evaluation is called extrinsic evaluation. Extrinsic evaluation is the only way to know if a particular improvement in the language model (or any component) is really going to help the task at hand. Thus for evaluating n-gram language models that are a component of some task like speech recognition or machine translation, we can compare the performance of two candidate language models by running the speech recognizer or machine translator twice, once with each language model, and seeing which gives the more accurate transcription. ", "Bloom_type": "remember", "question": "In speech recognition tasks, what method is used to determine the effectiveness of a new language model?", "options": ["Comparing the accuracy of transcriptions between different models", "Analyzing the internal structure of the models", "Measuring the speed of processing time", "Evaluating the user satisfaction level"], "complexity": 0}, {"id": 4, "context": "The resurgence of n-gram language models came from Fred Jelinek and colleagues at the IBM Thomas J. Watson Research Center, who were influenced by Shannon, and James Baker at CMU, who was influenced by the prior, classified work of Leonard Baum and colleagues on these topics at labs like the US Institute for Defense Analyses (IDA) after they were declassified. Independently these two labs successfully used n-grams in their speech recognition systems at the same time (Baker 1975b, Jelinek et al. 1975, Baker 1975a, Bahl et al. 1983, Jelinek 1990). The terms language model and perplexity were first used for this technology by the IBM group. Jelinek and his colleagues used the term language model in a pretty modern way, to mean the entire set of linguistic influences on word sequence probabilities, including grammar, semantics, discourse, and even speaker characteristics, rather than just the particular n-gram model itself. ", "Bloom_type": "remember", "question": "In what year did James Baker collaborate with Fred Jelinek on developing speech recognition technologies?", "options": ["1975", "1960", "1983", "1990"], "complexity": 0}, {"id": 5, "context": "By the 1990s larger neural networks began to be applied to many practical language processing tasks as well, like handwriting recognition (LeCun et al. 1989) and speech recognition (Morgan and Bourlard 1990). By the early 2000s, improvements in computer hardware and advances in optimization and training techniques made it possible to train even larger and deeper networks, leading to the modern term deep learning (Hinton et al. 2006, Bengio et al. 2007). We cover more related history in Chapter 8 and Chapter 16. ", "Bloom_type": "remember", "question": "In what decade did advancements in speech recognition technology become significant due to improvements in hardware and training techniques?", "options": ["The early 2000s", "The late 1980s", "The mid-1990s", "The late 1990s"], "complexity": 0}, {"id": 6, "context": "For applications that involve much longer input sequences, such as speech recognition, character-level processing, or streaming continuous inputs, unrolling an entire input sequence may not be feasible. In these cases, we can unroll the input into manageable fixed-length segments and treat each segment as a distinct training item. ", "Bloom_type": "remember", "question": "In speech recognition systems, what technique is used when dealing with long input sequences?", "options": ["Unrolling", "Chunking", "Segmentation", "Compression"], "complexity": 0}, {"id": 7, "context": "While theoretically interesting, the difficulty with training RNNs and managing context over long sequences impeded progress on practical applications. This situation changed with the introduction of LSTMs in Hochreiter and Schmidhuber (1997) and Gers et al. (2000). Impressive performance gains were demonstrated on tasks at the boundary of signal processing and language processing including phoneme recognition (Graves and Schmidhuber, 2005), handwriting recognition (Graves et al., 2007) and most significantly speech recognition (Graves et al., 2013). Interest in applying neural networks to practical NLP problems surged with the work of Collobert and Weston (2008) and Collobert et al. (2011). These efforts made use of learned word embeddings, convolutional networks, and end-to-end training. ", "Bloom_type": "remember", "question": "Which breakthrough technology enabled significant advancements in speech recognition?", "options": ["LSTM", "RNN", "GRU", "CNN"], "complexity": 0}, {"id": 8, "context": "N-gram language models were very widely used over the next 30 years and more, across a wide variety of NLP tasks like speech recognition and machine translations, often as one of multiple components of the model. The contexts for these n-gram models grew longer, with 5-gram models used quite commonly by very efficient LM toolkits (Stolcke, 2002; Heafield, 2011). ", "Bloom_type": "remember", "question": "Which type of language model was used most frequently alongside N-gram models for various natural language processing tasks?", "options": ["Statistical models", "Rule-based systems", "Neural networks", "Symbolic logic"], "complexity": 0}, {"id": 9, "context": "In this second part of the book we introduce fundamental NLP applications: machine translation, information retrieval, question answering, dialogue systems, and speech recognition. ", "Bloom_type": "remember", "question": "In what area of natural language processing are we introduced to fundamental applications such as machine translation, information retrieval, question answering, dialogue systems, and speech recognition?", "options": ["Speech recognition", "Machine learning", "Text analysis", "Image processing"], "complexity": 0}, {"id": 10, "context": "Minimum Bayes risk decoding can also be used for other NLP tasks; indeed it was widely applied to speech recognition (Stolcke et al., 1997; Goel and Byrne, 2000) before being applied to machine translation (Kumar and Byrne, 2004), and has been shown to work well across many other generation tasks as well (e.g., summarization, dialogue, and image captioning (Suzgun et al., 2023a)). ", "Bloom_type": "remember", "question": "In what application did minimum Bayes risk decoding precedently show effectiveness?", "options": ["Natural language processing", "Machine learning", "Speech synthesis", "Image classification"], "complexity": 0}, {"id": 11, "context": "Once automatic metrics like BLEU were developed (Papineni et al., 2002), the discriminative log linear formulation (Och and Ney, 2004), drawing from the IBM MaxEnt work (Berger et al., 1996), was used to directly optimize evaluation metrics like BLEU in a method known as Minimum Error Rate Training, or MERT (Och, 2003), also drawing from speech recognition models (Chou et al., 1993). Toolkits like GIZA (Och and Ney, 2003) and Moses (Koehn et al. 2006, Zens and Ney 2007) were widely used. ", "Bloom_type": "remember", "question": "What is the name of the toolkits widely used for speech recognition tasks?", "options": ["Both A and B", "GIZA", "Moses", "Neither A nor B"], "complexity": 0}, {"id": 12, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "remember", "question": "What is a key component of automatic speech recognition technology?", "options": ["Text-to-speech conversion", "Speech synthesis", "Voice modulation", "Noise reduction"], "complexity": 0}, {"id": 13, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "remember", "question": "What is a key benefit of using speech recognition technology?", "options": ["It enhances the accuracy of written communication.", "It can replace human workers.", "It allows for real-time translation between languages.", "It provides immediate feedback on grammar."], "complexity": 0}, {"id": 14, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "remember", "question": "What is the process of converting spoken language into written text?", "options": ["Automatic speech recognition", "Speech synthesis", "Text-to-speech conversion", "Voiceprint identification"], "complexity": 0}, {"id": 15, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "remember", "question": "What is a key benefit of automatic speech recognition technology?", "options": ["It enhances the user experience by converting spoken language into written text.", "It can improve accuracy in typing.", "It allows for real-time translation between languages.", "It reduces the need for human translators."], "complexity": 0}, {"id": 16, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "remember", "question": "What is a key benefit of using speech recognition technology?", "options": ["It can transcribe spoken language into written text automatically.", "It allows for real-time translation.", "It requires no human intervention during operation.", "It enhances the user interface with visual elements."], "complexity": 0}, {"id": 17, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "remember", "question": "What is a key component of automatic speech recognition technology?", "options": ["Text-to-speech conversion", "Voice modulation", "Speech synthesis", "Audio compression"], "complexity": 0}, {"id": 18, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "remember", "question": "What is a key benefit of using speech recognition technology?", "options": ["It enhances the accuracy of written communication.", "It can replace human workers.", "It allows for real-time translation between languages.", "It provides immediate feedback on grammar errors."], "complexity": 0}, {"id": 19, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "remember", "question": "What is a key feature of automatic speech recognition technology?", "options": ["It recognizes and transcribes human speech accurately.", "It converts written text into spoken language.", "It allows users to type on a computer using voice commands.", "It translates foreign languages into English."], "complexity": 0}, {"id": 20, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "remember", "question": "What is a key benefit of using speech recognition technology?", "options": ["It enhances communication by converting spoken language into written text.", "It can improve accuracy in typing.", "It allows for real-time translation between languages.", "It reduces the need for human translators."], "complexity": 0}, {"id": 21, "context": "Earlier work sometimes used McNemar`s test for significance, but McNemar`s is only applicable when the errors made by the system are independent, which is not true in continuous speech recognition, where errors made on a word are extremely dependent on errors made on neighboring words. ", "Bloom_type": "remember", "question": "In speech recognition, why is McNemar\u2019s test not suitable?", "options": ["Because it does not account for word dependencies.", "Because it requires continuous speech data.", "Because it assumes independence of errors.", "Because it cannot handle large datasets."], "complexity": 0}, {"id": 22, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "remember", "question": "What is a key benefit of using speech recognition technology?", "options": ["All of the above", "It can improve accuracy for people with disabilities.", "It allows users to type faster than they speak.", "It reduces the need for physical keyboards."], "complexity": 0}, {"id": 23, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "remember", "question": "What is the process of converting spoken language into written text?", "options": ["Text-to-speech conversion", "Speech synthesis", "Speech encoding", "Automatic speech recognition"], "complexity": 0}, {"id": 24, "context": "While we have focused on speech recognition and TTS in this chapter, there are a wide variety of speech-related tasks. ", "Bloom_type": "remember", "question": "In what other speech-related tasks besides speech recognition is there a wide variety?", "options": ["Speech translation", "Speech synthesis", "Voice cloning", "Speech encryption"], "complexity": 0}, {"id": 25, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "remember", "question": "What is a key benefit of using speech recognition technology?", "options": ["It reduces the need for manual data entry tasks.", "It can improve accuracy in typing.", "It allows for real-time translation between languages.", "It enhances the user interface design."], "complexity": 0}, {"id": 26, "context": " Two common paradigms for speech recognition are the encoder-decoder with attention model, and models based on the CTC loss function. Attentionbased models have higher accuracies, but models based on CTC more easily adapt to streaming: outputting graphemes online instead of waiting until the acoustic input is complete. ", "Bloom_type": "remember", "question": "Which type of speech recognition model has higher accuracy?", "options": ["Encoder-Decoder with Attention Model", "CTC-based models", "Both types equally accurately", "None of the above"], "complexity": 0}, {"id": 27, "context": "ASR A number of speech recognition systems were developed by the late 1940s and early 1950s. An early Bell Labs system could recognize any of the 10 digits from a single speaker (Davis et al., 1952). This system had 10 speaker-dependent stored patterns, one for each digit, each of which roughly represented the first two vowel formants in the digit. They achieved 97%99% accuracy by choosing the pattern that had the highest relative correlation coefficient with the input. Fry (1959) and Denes (1959) built a phoneme recognizer at University College, London, that recognized four vowels and nine consonants based on a similar pattern-recognition principle. Fry and Denes`s system was the first to use phoneme transition probabilities to constrain the recognizer. ", "Bloom_type": "remember", "question": "In what decade did the development of speech recognition systems begin?", "options": ["1940s", "1800s", "1930s", "1960s"], "complexity": 0}, {"id": 28, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "remember", "question": "What is the process of converting spoken language into written text?", "options": ["Automatic speech recognition", "Speech synthesis", "Text-to-speech conversion", "Voiceprint identification"], "complexity": 0}, {"id": 29, "context": "Meanwhile early work had proposed the CTC loss function by 2006 (Graves et al., 2006), and by 2012 the RNN-Transducer was defined and applied to phone recognition (Graves 2012, Graves et al. 2013), and then to end-to-end speech recognition rescoring (Graves and Jaitly, 2014), and then recognition (Maas et al., 2015), (Our dewith advances such as specialized beam search (Hannun et al., 2014). scription of CTC in the chapter draws on Hannun (2017), which we encourage the interested reader to follow). ", "Bloom_type": "remember", "question": "In what year did researchers first propose the concept of using Recurrent Neural Networks with Transducers for phoneme recognition?", "options": ["2012", "2006", "2014", "2015"], "complexity": 0}, {"id": 30, "context": "18.4 Discuss how to augment a parser to deal with input that may be incorrect, for example, containing spelling errors or mistakes arising from automatic speech recognition. ", "Bloom_type": "remember", "question": "What is one way to improve a parser when dealing with input that might contain spelling errors or mistakes due to automatic speech recognition?", "options": ["Implement error correction techniques", "Increase the size of the training data set", "Use more sophisticated machine learning algorithms", "Reduce the complexity of the grammar rules"], "complexity": 0}, {"id": 31, "context": "Due to the difficulty of porting systems from one domain to another, attention shifted to machine learning approaches. Early supervised learning approaches to IE (Cardie 1993, Cardie 1994, Riloff 1993, Soderland et al. 1995, Huffman 1996) focused on automating the knowledge acquisition process, mainly for finite-state rule-based systems. Their success, and the earlier success of HMM-based speech recognition, led to the use of sequence labeling (HMMs: Bikel et al. 1997; MEMMs McCallum et al. 2000; CRFs: Lafferty et al. 2001), and a wide exploration of features (Zhou et al., 2005). Neural approaches followed from the pioneering results of Collobert et al. (2011), who applied a CRF on top of a convolutional net. ", "Bloom_type": "remember", "question": "Which approach was not directly influenced by early successes in speech recognition technology?", "options": ["Supervised Learning", "Sequence Labeling", "Neural Approaches", "Feature Exploration"], "complexity": 0}, {"id": 32, "context": "Finally, we`ll need to compare words and other strings. We`ll introduce a metric called edit distance that measures how similar two strings are based on the number of edits (insertions, deletions, substitutions) it takes to change one string into the other. Edit distance is an algorithm with applications throughout language processing, from spelling correction to speech recognition to coreference resolution. ", "Bloom_type": "comprehension", "question": "What is an example of an application for the edit distance algorithm?", "options": ["Speech recognition", "Spelling correction", "Coreference resolution", "Both A and B"], "complexity": 1}, {"id": 33, "context": "We still have decisions to make! For example, should we consider a capitalized string (like They) and one that is uncapitalized (like they) to be the same word type? The answer is that it depends on the task! They and they might be lumped together as the same type in some tasks, like speech recognition, where we care more about the sequence of words and less about the formatting, while for other tasks, such as deciding whether a particular word is a name of a person or location (namedentity tagging), capitalization is a useful feature and is retained. Sometimes we keep around two versions of a particular NLP model, one with capitalization and one without capitalization. ", "Bloom_type": "comprehension", "question": "In the context of speech recognition, how are capitalized strings typically treated compared to uncapitalized ones?", "options": ["Speech recognition considers capitalized strings as the same type as uncapitalized strings.", "Capitalized strings are always considered different from uncapitalized strings.", "Speech recognition treats capitalized strings and uncapitalized strings equally.", "Speech recognition does not differentiate between capitalized and uncapitalized strings."], "complexity": 1}, {"id": 34, "context": "Alignment Knowing the minimum edit distance is useful for algorithms like finding potential spelling error corrections. But the edit distance algorithm is important in another way; with a small change, it can also provide the minimum cost alignment between two strings. Aligning two strings is useful throughout speech and language processing. In speech recognition, minimum edit distance alignment is used to compute the word error rate (Chapter 16). Alignment plays a role in machine translation, in which sentences in a parallel corpus (a corpus with a text in two languages) need to be matched to each other. ", "Bloom_type": "comprehension", "question": "Explain how alignment plays a role in speech recognition and machine translation?", "options": ["Alignment enhances the efficiency of machine translation by ensuring precise sentence matching.", "Alignment helps reduce the computational complexity of speech recognition.", "Alignment improves the accuracy of word error rates by minimizing changes in string lengths.", "Alignment simplifies the process of spell correction in speech recognition."], "complexity": 1}, {"id": 35, "context": "The best way to evaluate the performance of a language model is to embed it in an application and measure how much the application improves. Such end-to-end evaluation is called extrinsic evaluation. Extrinsic evaluation is the only way to know if a particular improvement in the language model (or any component) is really going to help the task at hand. Thus for evaluating n-gram language models that are a component of some task like speech recognition or machine translation, we can compare the performance of two candidate language models by running the speech recognizer or machine translator twice, once with each language model, and seeing which gives the more accurate transcription. ", "Bloom_type": "comprehension", "question": "Explain the concept of extrinsic evaluation in the context of speech recognition?", "options": ["It evaluates the performance of a language model through its integration into applications.", "It measures the accuracy of a language model.", "It assesses the effectiveness of components within a system.", "It compares the results of different language models."], "complexity": 1}, {"id": 36, "context": "The resurgence of n-gram language models came from Fred Jelinek and colleagues at the IBM Thomas J. Watson Research Center, who were influenced by Shannon, and James Baker at CMU, who was influenced by the prior, classified work of Leonard Baum and colleagues on these topics at labs like the US Institute for Defense Analyses (IDA) after they were declassified. Independently these two labs successfully used n-grams in their speech recognition systems at the same time (Baker 1975b, Jelinek et al. 1975, Baker 1975a, Bahl et al. 1983, Jelinek 1990). The terms language model and perplexity were first used for this technology by the IBM group. Jelinek and his colleagues used the term language model in a pretty modern way, to mean the entire set of linguistic influences on word sequence probabilities, including grammar, semantics, discourse, and even speaker characteristics, rather than just the particular n-gram model itself. ", "Bloom_type": "comprehension", "question": "What did Fred Jelinek and his colleagues use n-grams for in their speech recognition systems?", "options": ["To classify speech signals into categories", "To predict future speech sounds", "To improve pronunciation accuracy", "To enhance audio quality"], "complexity": 1}, {"id": 37, "context": "By the 1990s larger neural networks began to be applied to many practical language processing tasks as well, like handwriting recognition (LeCun et al. 1989) and speech recognition (Morgan and Bourlard 1990). By the early 2000s, improvements in computer hardware and advances in optimization and training techniques made it possible to train even larger and deeper networks, leading to the modern term deep learning (Hinton et al. 2006, Bengio et al. 2007). We cover more related history in Chapter 8 and Chapter 16. ", "Bloom_type": "comprehension", "question": "What was one significant advancement in speech recognition technology by the early 2000s?", "options": ["The implementation of faster processors and better training methods for improving accuracy", "The development of large-scale neural networks for image recognition", "The introduction of smaller neural networks for natural language processing", "The use of traditional machine learning algorithms for voice synthesis"], "complexity": 1}, {"id": 38, "context": "For applications that involve much longer input sequences, such as speech recognition, character-level processing, or streaming continuous inputs, unrolling an entire input sequence may not be feasible. In these cases, we can unroll the input into manageable fixed-length segments and treat each segment as a distinct training item. ", "Bloom_type": "comprehension", "question": "What technique do we use when dealing with long input sequences for speech recognition? ", "options": ["Breaking down the input into smaller segments", "Unrolling the entire input sequence", "Using machine learning algorithms only", "Combining all techniques mentioned"], "complexity": 1}, {"id": 39, "context": "While theoretically interesting, the difficulty with training RNNs and managing context over long sequences impeded progress on practical applications. This situation changed with the introduction of LSTMs in Hochreiter and Schmidhuber (1997) and Gers et al. (2000). Impressive performance gains were demonstrated on tasks at the boundary of signal processing and language processing including phoneme recognition (Graves and Schmidhuber, 2005), handwriting recognition (Graves et al., 2007) and most significantly speech recognition (Graves et al., 2013). Interest in applying neural networks to practical NLP problems surged with the work of Collobert and Weston (2008) and Collobert et al. (2011). These efforts made use of learned word embeddings, convolutional networks, and end-to-end training. ", "Bloom_type": "comprehension", "question": "What was one significant breakthrough in speech recognition technology mentioned in the passage?", "options": ["The successful implementation of speech recognition systems", "The development of LSTM models by Hochreiter and Schmidhuber (1997)", "The integration of deep learning techniques into natural language processing", "The improvement in phoneme recognition using RNNs"], "complexity": 1}, {"id": 40, "context": "N-gram language models were very widely used over the next 30 years and more, across a wide variety of NLP tasks like speech recognition and machine translations, often as one of multiple components of the model. The contexts for these n-gram models grew longer, with 5-gram models used quite commonly by very efficient LM toolkits (Stolcke, 2002; Heafield, 2011). ", "Bloom_type": "comprehension", "question": "What was the primary use of n-gram language models before their widespread adoption?", "options": ["Speech recognition", "Machine translation", "Text generation", "Sentiment analysis"], "complexity": 1}, {"id": 41, "context": "In this second part of the book we introduce fundamental NLP applications: machine translation, information retrieval, question answering, dialogue systems, and speech recognition. ", "Bloom_type": "comprehension", "question": "Which of the following is NOT an example of a fundamental NLP application mentioned in the context?", "options": ["Information Retrieval", "Machine Translation", "Speech Recognition", "Dialogue Systems"], "complexity": 1}, {"id": 42, "context": "Minimum Bayes risk decoding can also be used for other NLP tasks; indeed it was widely applied to speech recognition (Stolcke et al., 1997; Goel and Byrne, 2000) before being applied to machine translation (Kumar and Byrne, 2004), and has been shown to work well across many other generation tasks as well (e.g., summarization, dialogue, and image captioning (Suzgun et al., 2023a)). ", "Bloom_type": "comprehension", "question": "What task did minimum Bayes risk decoding apply to before being applied to machine translation?", "options": ["Speech recognition", "Machine translation", "Image captioning", "Dialogue"], "complexity": 1}, {"id": 43, "context": "Once automatic metrics like BLEU were developed (Papineni et al., 2002), the discriminative log linear formulation (Och and Ney, 2004), drawing from the IBM MaxEnt work (Berger et al., 1996), was used to directly optimize evaluation metrics like BLEU in a method known as Minimum Error Rate Training, or MERT (Och, 2003), also drawing from speech recognition models (Chou et al., 1993). Toolkits like GIZA (Och and Ney, 2003) and Moses (Koehn et al. 2006, Zens and Ney 2007) were widely used. ", "Bloom_type": "comprehension", "question": "What technique was used to directly optimize evaluation metrics like BLEU in speech recognition?", "options": ["Minimum Error Rate Training (MERT)", "GIZA", "Moses", "IBM MaxEnt"], "complexity": 1}, {"id": 44, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "comprehension", "question": "What does automatic speech recognition primarily focus on?", "options": ["Transcription of spoken language into written text", "Text-to-speech conversion", "Speech synthesis", "Voice identification and authentication"], "complexity": 1}, {"id": 45, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "comprehension", "question": "What does automatic speech recognition typically convert into text?", "options": ["Audio signals", "Visual images", "Physical objects", "Chemical compounds"], "complexity": 1}, {"id": 46, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "comprehension", "question": "What does automatic speech recognition primarily focus on?", "options": ["Converting spoken language into written text", "Translating written texts into spoken language", "Recognizing specific words within sentences", "Generating new speech based on input text"], "complexity": 1}, {"id": 47, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "comprehension", "question": "What does automatic speech recognition primarily focus on?", "options": ["Transcription of spoken language into written text", "Text-to-speech conversion", "Speech synthesis", "Both A) and B)"], "complexity": 1}, {"id": 48, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "comprehension", "question": "What does automatic speech recognition typically convert spoken language into?", "options": ["Text", "Audio", "Video", "Speech"], "complexity": 1}, {"id": 49, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "comprehension", "question": "What does automatic speech recognition involve?", "options": ["Converting spoken language into digital data", "Processing audio signals into written text", "Translating languages between different scripts", "Encoding images into sound waves"], "complexity": 1}, {"id": 50, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "comprehension", "question": "What does automatic speech recognition primarily focus on?", "options": ["Processing audio signals for understanding human speech", "Translating written texts into spoken language", "Converting spoken language into written text", "Generating new speech based on pre-recorded phrases"], "complexity": 1}, {"id": 51, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "comprehension", "question": "What does automatic speech recognition primarily focus on?", "options": ["Converting spoken words into typed text", "Translating written texts into spoken language", "Recognizing specific words from recorded audio", "Generating new audio based on input text"], "complexity": 1}, {"id": 52, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "comprehension", "question": "What does automatic speech recognition primarily focus on?", "options": ["Transcription of spoken language into written text", "Text-to-speech conversion", "Speech synthesis", "Both A) and B)"], "complexity": 1}, {"id": 53, "context": "Earlier work sometimes used McNemar`s test for significance, but McNemar`s is only applicable when the errors made by the system are independent, which is not true in continuous speech recognition, where errors made on a word are extremely dependent on errors made on neighboring words. ", "Bloom_type": "comprehension", "question": "Explain why McNemar\u2019s test may not be suitable for continuous speech recognition?", "options": ["Due to the dependency of errors on neighboring words.", "Because it requires data from two different subjects.", "It cannot handle continuous speech accurately.", "The test is too complex for practical use."], "complexity": 1}, {"id": 54, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "comprehension", "question": "What does automatic speech recognition involve?", "options": ["Converting spoken language into digital data", "Processing audio signals into written text", "Translating languages from one to another", "Recording voice messages for future reference"], "complexity": 1}, {"id": 55, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "comprehension", "question": "What does automatic speech recognition typically convert spoken language into?", "options": ["Written text", "Audio recordings", "Visual images", "Digital signals"], "complexity": 1}, {"id": 56, "context": "While we have focused on speech recognition and TTS in this chapter, there are a wide variety of speech-related tasks. ", "Bloom_type": "comprehension", "question": "What aspect of speech-related tasks does the chapter NOT focus on?", "options": ["Speech synthesis (TTS)", "Speech recognition", "Voice commands", "Speech understanding"], "complexity": 1}, {"id": 57, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "comprehension", "question": "What does automatic speech recognition primarily focus on?", "options": ["Transcription of spoken language into written text", "Text-to-speech conversion", "Speech synthesis", "Both A) and B)"], "complexity": 1}, {"id": 58, "context": " Two common paradigms for speech recognition are the encoder-decoder with attention model, and models based on the CTC loss function. Attentionbased models have higher accuracies, but models based on CTC more easily adapt to streaming: outputting graphemes online instead of waiting until the acoustic input is complete. ", "Bloom_type": "comprehension", "question": "Which type of speech recognition model typically adapts better to real-time processing?", "options": ["CTC Loss Function Models", "Encoder-Decoder with Attention Model", "Both equally well-adapted", "Neither adapts well to streaming"], "complexity": 1}, {"id": 59, "context": "ASR A number of speech recognition systems were developed by the late 1940s and early 1950s. An early Bell Labs system could recognize any of the 10 digits from a single speaker (Davis et al., 1952). This system had 10 speaker-dependent stored patterns, one for each digit, each of which roughly represented the first two vowel formants in the digit. They achieved 97%99% accuracy by choosing the pattern that had the highest relative correlation coefficient with the input. Fry (1959) and Denes (1959) built a phoneme recognizer at University College, London, that recognized four vowels and nine consonants based on a similar pattern-recognition principle. Fry and Denes`s system was the first to use phoneme transition probabilities to constrain the recognizer. ", "Bloom_type": "comprehension", "question": "What was the primary method used in early speech recognition systems to distinguish between different sounds?", "options": ["Phoneme transition probabilities", "Speaker-dependent stored patterns", "Vowel formant frequencies", "Consonant duration analysis"], "complexity": 1}, {"id": 60, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "comprehension", "question": "What does automatic speech recognition primarily focus on?", "options": ["Converting spoken words into typed text", "Translating written texts into spoken language", "Recognizing individual sounds within a sentence", "Analyzing emotional content in speech"], "complexity": 1}, {"id": 61, "context": "Meanwhile early work had proposed the CTC loss function by 2006 (Graves et al., 2006), and by 2012 the RNN-Transducer was defined and applied to phone recognition (Graves 2012, Graves et al. 2013), and then to end-to-end speech recognition rescoring (Graves and Jaitly, 2014), and then recognition (Maas et al., 2015), (Our dewith advances such as specialized beam search (Hannun et al., 2014). scription of CTC in the chapter draws on Hannun (2017), which we encourage the interested reader to follow). ", "Bloom_type": "comprehension", "question": "What advancements have been made in speech recognition since its inception?", "options": ["All of the above", "The development of specialized beam search algorithms.", "Improvements in the implementation of CTC loss functions.", "Enhancements in the definition and application of RNN-Transducer models."], "complexity": 1}, {"id": 62, "context": "18.4 Discuss how to augment a parser to deal with input that may be incorrect, for example, containing spelling errors or mistakes arising from automatic speech recognition. ", "Bloom_type": "comprehension", "question": "What does the response suggest about dealing with incorrect inputs in a parser?", "options": ["The parser can handle both correct and incorrect inputs.", "Speech recognition improves parser accuracy.", "Incorrect inputs are ignored by the parser.", "Spelling errors cannot be corrected."], "complexity": 1}, {"id": 63, "context": "Due to the difficulty of porting systems from one domain to another, attention shifted to machine learning approaches. Early supervised learning approaches to IE (Cardie 1993, Cardie 1994, Riloff 1993, Soderland et al. 1995, Huffman 1996) focused on automating the knowledge acquisition process, mainly for finite-state rule-based systems. Their success, and the earlier success of HMM-based speech recognition, led to the use of sequence labeling (HMMs: Bikel et al. 1997; MEMMs McCallum et al. 2000; CRFs: Lafferty et al. 2001), and a wide exploration of features (Zhou et al., 2005). Neural approaches followed from the pioneering results of Collobert et al. (2011), who applied a CRF on top of a convolutional net. ", "Bloom_type": "comprehension", "question": "What was the primary focus of early supervised learning approaches to IE before the advent of neural networks?", "options": ["Automating the knowledge acquisition process", "Improving feature extraction techniques", "Enhancing computational efficiency", "Increasing accuracy through reinforcement learning"], "complexity": 1}, {"id": 64, "context": "Finally, we`ll need to compare words and other strings. We`ll introduce a metric called edit distance that measures how similar two strings are based on the number of edits (insertions, deletions, substitutions) it takes to change one string into the other. Edit distance is an algorithm with applications throughout language processing, from spelling correction to speech recognition to coreference resolution. ", "Bloom_type": "application", "question": "What is the primary application of the edit distance algorithm?", "options": ["All of the above", "Spelling correction", "Speech recognition", "Coreference resolution"], "complexity": 2}, {"id": 65, "context": "We still have decisions to make! For example, should we consider a capitalized string (like They) and one that is uncapitalized (like they) to be the same word type? The answer is that it depends on the task! They and they might be lumped together as the same type in some tasks, like speech recognition, where we care more about the sequence of words and less about the formatting, while for other tasks, such as deciding whether a particular word is a name of a person or location (namedentity tagging), capitalization is a useful feature and is retained. Sometimes we keep around two versions of a particular NLP model, one with capitalization and one without capitalization. ", "Bloom_type": "application", "question": "In the field of speech recognition, how are capitalized strings typically treated compared to their uncapitalized counterparts?", "options": ["Their treatment varies depending on the specific application.", "They are always considered different types.", "They are often grouped together as the same type.", "Their capitalization is irrelevant."], "complexity": 2}, {"id": 66, "context": "Alignment Knowing the minimum edit distance is useful for algorithms like finding potential spelling error corrections. But the edit distance algorithm is important in another way; with a small change, it can also provide the minimum cost alignment between two strings. Aligning two strings is useful throughout speech and language processing. In speech recognition, minimum edit distance alignment is used to compute the word error rate (Chapter 16). Alignment plays a role in machine translation, in which sentences in a parallel corpus (a corpus with a text in two languages) need to be matched to each other. ", "Bloom_type": "application", "question": "In speech recognition, what does the minimum edit distance alignment help calculate?", "options": ["The word error rate", "The number of words spoken", "The speed at which speech is produced", "The accuracy of pronunciation"], "complexity": 2}, {"id": 67, "context": "The best way to evaluate the performance of a language model is to embed it in an application and measure how much the application improves. Such end-to-end evaluation is called extrinsic evaluation. Extrinsic evaluation is the only way to know if a particular improvement in the language model (or any component) is really going to help the task at hand. Thus for evaluating n-gram language models that are a component of some task like speech recognition or machine translation, we can compare the performance of two candidate language models by running the speech recognizer or machine translator twice, once with each language model, and seeing which gives the more accurate transcription. ", "Bloom_type": "application", "question": "What is the primary method used to determine the effectiveness of a language model in tasks such as speech recognition?", "options": ["Comparing the accuracy of transcriptions using different language models.", "Analyzing the internal structure of the language model.", "Testing the model on unrelated datasets.", "Implementing the model in a real-world application."], "complexity": 2}, {"id": 68, "context": "The resurgence of n-gram language models came from Fred Jelinek and colleagues at the IBM Thomas J. Watson Research Center, who were influenced by Shannon, and James Baker at CMU, who was influenced by the prior, classified work of Leonard Baum and colleagues on these topics at labs like the US Institute for Defense Analyses (IDA) after they were declassified. Independently these two labs successfully used n-grams in their speech recognition systems at the same time (Baker 1975b, Jelinek et al. 1975, Baker 1975a, Bahl et al. 1983, Jelinek 1990). The terms language model and perplexity were first used for this technology by the IBM group. Jelinek and his colleagues used the term language model in a pretty modern way, to mean the entire set of linguistic influences on word sequence probabilities, including grammar, semantics, discourse, and even speaker characteristics, rather than just the particular n-gram model itself. ", "Bloom_type": "application", "question": "What did Jelinek and his colleagues originally call the system they developed using n-grams?", "options": ["Language Model", "Speech Recognition System", "Perplexity", "Word Sequence Probability"], "complexity": 2}, {"id": 69, "context": "By the 1990s larger neural networks began to be applied to many practical language processing tasks as well, like handwriting recognition (LeCun et al. 1989) and speech recognition (Morgan and Bourlard 1990). By the early 2000s, improvements in computer hardware and advances in optimization and training techniques made it possible to train even larger and deeper networks, leading to the modern term deep learning (Hinton et al. 2006, Bengio et al. 2007). We cover more related history in Chapter 8 and Chapter 16. ", "Bloom_type": "application", "question": "What was the first application of large neural networks for language processing?", "options": ["Handwriting recognition", "Speech recognition", "Machine translation", "Natural language understanding"], "complexity": 2}, {"id": 70, "context": "For applications that involve much longer input sequences, such as speech recognition, character-level processing, or streaming continuous inputs, unrolling an entire input sequence may not be feasible. In these cases, we can unroll the input into manageable fixed-length segments and treat each segment as a distinct training item. ", "Bloom_type": "application", "question": "What is a common approach for handling long input sequences in speech recognition?", "options": ["Break down the input into smaller segments", "Unroll the entire input sequence at once", "Process the entire input sequence continuously", "Use machine learning algorithms directly on the entire sequence"], "complexity": 2}, {"id": 71, "context": "While theoretically interesting, the difficulty with training RNNs and managing context over long sequences impeded progress on practical applications. This situation changed with the introduction of LSTMs in Hochreiter and Schmidhuber (1997) and Gers et al. (2000). Impressive performance gains were demonstrated on tasks at the boundary of signal processing and language processing including phoneme recognition (Graves and Schmidhuber, 2005), handwriting recognition (Graves et al., 2007) and most significantly speech recognition (Graves et al., 2013). Interest in applying neural networks to practical NLP problems surged with the work of Collobert and Weston (2008) and Collobert et al. (2011). These efforts made use of learned word embeddings, convolutional networks, and end-to-end training. ", "Bloom_type": "application", "question": "What was a significant factor contributing to the improvement in speech recognition using neural networks?", "options": ["The development of Long Short-Term Memory (LSTM) networks", "Theoretical interest in machine learning models", "The introduction of Convolutional Neural Networks (CNN)", "The application of pre-trained word embeddings"], "complexity": 2}, {"id": 72, "context": "N-gram language models were very widely used over the next 30 years and more, across a wide variety of NLP tasks like speech recognition and machine translations, often as one of multiple components of the model. The contexts for these n-gram models grew longer, with 5-gram models used quite commonly by very efficient LM toolkits (Stolcke, 2002; Heafield, 2011). ", "Bloom_type": "application", "question": "What was the primary application of N-gram language models before their widespread usage?", "options": ["Speech recognition", "Machine learning algorithms", "Natural Language Processing (NLP)", "Image processing"], "complexity": 2}, {"id": 73, "context": "In this second part of the book we introduce fundamental NLP applications: machine translation, information retrieval, question answering, dialogue systems, and speech recognition. ", "Bloom_type": "application", "question": "Which application is not directly mentioned as being introduced in the book?", "options": ["dialogue systems", "machine translation", "information retrieval", "question answering"], "complexity": 2}, {"id": 74, "context": "Minimum Bayes risk decoding can also be used for other NLP tasks; indeed it was widely applied to speech recognition (Stolcke et al., 1997; Goel and Byrne, 2000) before being applied to machine translation (Kumar and Byrne, 2004), and has been shown to work well across many other generation tasks as well (e.g., summarization, dialogue, and image captioning (Suzgun et al., 2023a)). ", "Bloom_type": "application", "question": "What is an example of how minimum Bayes risk decoding was initially applied?", "options": ["For enhancing machine translation", "To improve the accuracy of image captioning", "For generating summaries", "In developing dialogue systems"], "complexity": 2}, {"id": 75, "context": "Once automatic metrics like BLEU were developed (Papineni et al., 2002), the discriminative log linear formulation (Och and Ney, 2004), drawing from the IBM MaxEnt work (Berger et al., 1996), was used to directly optimize evaluation metrics like BLEU in a method known as Minimum Error Rate Training, or MERT (Och, 2003), also drawing from speech recognition models (Chou et al., 1993). Toolkits like GIZA (Och and Ney, 2003) and Moses (Koehn et al. 2006, Zens and Ney 2007) were widely used. ", "Bloom_type": "application", "question": "What is the primary tool for developing speech recognition systems?", "options": ["Moses", "BLEU", "GIZA", "Minimum Error Rate Training"], "complexity": 2}, {"id": 76, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "application", "question": "What is the first step in converting speech to text?", "options": ["Digitize voice signals using an acoustic model", "Transcribe audio into written text", "Convert spoken words into digital format", "Analyze the phonetic structure of the speech"], "complexity": 2}, {"id": 77, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "application", "question": "What is the first step in converting spoken language into written text?", "options": ["Transcribe spoken language into text", "Convert spoken language into audio", "Digitize spoken language", "Record spoken language"], "complexity": 2}, {"id": 78, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "application", "question": "What is the first step in converting speech to text?", "options": ["Convert the audio signal to digital format", "Transcribe the audio file", "Extract phonemes from the audio stream", "Generate text output"], "complexity": 2}, {"id": 79, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "application", "question": "What is the first step in converting speech to text?", "options": ["Convert the audio signal to digital format", "Transcribe the audio file", "Extract phonemes from the audio stream", "Generate an output text string"], "complexity": 2}, {"id": 80, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "application", "question": "What is the first step in converting speech to text?", "options": ["Convert the audio signal to digital format", "Transcribe the audio file", "Extract the phonemes from the audio", "Identify the speaker"], "complexity": 2}, {"id": 81, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "application", "question": "What is the first step in converting speech to text?", "options": ["Convert the audio signal to digital format", "Transcribe the audio file", "Extract the phonemes from the audio", "Analyze the acoustic features of the speech"], "complexity": 2}, {"id": 82, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "application", "question": "What is the first step in converting speech to text?", "options": ["Record the speaker\u2019s voice", "Transcribe audio into text using speech-to-text software", "Convert the recorded audio back into speech", "Analyze the spoken language for meaning"], "complexity": 2}, {"id": 83, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "application", "question": "What is the first step in converting spoken language into written text?", "options": ["Digitize spoken language using speech-to-text software", "Transcribe speech into audio files", "Convert audio files to digital format", "Extract text data from recorded audio"], "complexity": 2}, {"id": 84, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "application", "question": "What is the first step in converting speech to text?", "options": ["Record the speech with a microphone", "Transcribe audio into text using speech-to-text software", "Convert the recorded audio file to text format", "Listen to the speech and type it manually"], "complexity": 2}, {"id": 85, "context": "Earlier work sometimes used McNemar`s test for significance, but McNemar`s is only applicable when the errors made by the system are independent, which is not true in continuous speech recognition, where errors made on a word are extremely dependent on errors made on neighboring words. ", "Bloom_type": "application", "question": "What alternative method can be used for significance testing in continuous speech recognition?", "options": ["Implement an algorithm that accounts for dependencies between errors", "Use McNemar`s test directly", "Ignore the dependency of errors and continue with McNemar`s test", "Switch to another statistical test entirely"], "complexity": 2}, {"id": 86, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "application", "question": "What is the first step in converting speech to text?", "options": ["Convert the audio signal to digital format", "Transcribe the audio file", "Extract the spoken words from the audio", "Generate a transcript of the speech"], "complexity": 2}, {"id": 87, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "application", "question": "What is the first step in converting speech to text?", "options": ["Convert audio signals to digital format", "Transcribe audio into written text", "Extract spoken words using algorithms", "Digitize voice recordings"], "complexity": 2}, {"id": 88, "context": "While we have focused on speech recognition and TTS in this chapter, there are a wide variety of speech-related tasks. ", "Bloom_type": "application", "question": "What is an example of a speech-related task besides speech recognition and TTS?", "options": ["Voice cloning", "Speech synthesis", "Music composition", "Language translation"], "complexity": 2}, {"id": 89, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "application", "question": "What is the first step in converting spoken language into written text?", "options": ["Transcribe audio signals into digital format", "Convert speech patterns into visual representations", "Generate text based on pre-recorded scripts", "Identify keywords and phrases in the speech"], "complexity": 2}, {"id": 90, "context": " Two common paradigms for speech recognition are the encoder-decoder with attention model, and models based on the CTC loss function. Attentionbased models have higher accuracies, but models based on CTC more easily adapt to streaming: outputting graphemes online instead of waiting until the acoustic input is complete. ", "Bloom_type": "application", "question": "Which paradigm allows for real-time processing of speech inputs?", "options": ["Both Paradigms Allow Real-Time Processing", "Encoder-Decoder with Attention Model", "CTC Loss Function Models", "Neither Paradigm Allows Real-Time Processing"], "complexity": 2}, {"id": 91, "context": "ASR A number of speech recognition systems were developed by the late 1940s and early 1950s. An early Bell Labs system could recognize any of the 10 digits from a single speaker (Davis et al., 1952). This system had 10 speaker-dependent stored patterns, one for each digit, each of which roughly represented the first two vowel formants in the digit. They achieved 97%99% accuracy by choosing the pattern that had the highest relative correlation coefficient with the input. Fry (1959) and Denes (1959) built a phoneme recognizer at University College, London, that recognized four vowels and nine consonants based on a similar pattern-recognition principle. Fry and Denes`s system was the first to use phoneme transition probabilities to constrain the recognizer. ", "Bloom_type": "application", "question": "Which method did Fry and Denes use to improve the accuracy of their phoneme recognizer?", "options": ["Phoneme transition probabilities", "Speaker-independent training", "Pattern matching", "Vowel formant analysis"], "complexity": 2}, {"id": 92, "context": " AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ", "Bloom_type": "application", "question": "What is the first step in converting speech to text?", "options": ["Convert the audio signal to digital format", "Transcribe the audio file", "Extract phonemes from the audio", "Generate text output"], "complexity": 2}, {"id": 93, "context": "Meanwhile early work had proposed the CTC loss function by 2006 (Graves et al., 2006), and by 2012 the RNN-Transducer was defined and applied to phone recognition (Graves 2012, Graves et al. 2013), and then to end-to-end speech recognition rescoring (Graves and Jaitly, 2014), and then recognition (Maas et al., 2015), (Our dewith advances such as specialized beam search (Hannun et al., 2014). scription of CTC in the chapter draws on Hannun (2017), which we encourage the interested reader to follow). ", "Bloom_type": "application", "question": "In what year did the RNN-Transducer framework first become widely used for speech recognition tasks?", "options": ["2012", "2006", "2014", "2015"], "complexity": 2}, {"id": 94, "context": "18.4 Discuss how to augment a parser to deal with input that may be incorrect, for example, containing spelling errors or mistakes arising from automatic speech recognition. ", "Bloom_type": "application", "question": "What is an effective strategy to improve the accuracy of a parser when dealing with input that might contain spelling errors or mistakes due to speech recognition?", "options": ["Use advanced machine learning techniques to detect and correct common errors automatically.", "Ignore all erroneous inputs completely.", "Increase the complexity of the parser to handle more variations.", "Implement a feedback loop that continuously updates the model based on user corrections."], "complexity": 2}, {"id": 95, "context": "Due to the difficulty of porting systems from one domain to another, attention shifted to machine learning approaches. Early supervised learning approaches to IE (Cardie 1993, Cardie 1994, Riloff 1993, Soderland et al. 1995, Huffman 1996) focused on automating the knowledge acquisition process, mainly for finite-state rule-based systems. Their success, and the earlier success of HMM-based speech recognition, led to the use of sequence labeling (HMMs: Bikel et al. 1997; MEMMs McCallum et al. 2000; CRFs: Lafferty et al. 2001), and a wide exploration of features (Zhou et al., 2005). Neural approaches followed from the pioneering results of Collobert et al. (2011), who applied a CRF on top of a convolutional net. ", "Bloom_type": "application", "question": "What was the primary focus of early supervised learning approaches in IE?", "options": ["Automating the knowledge acquisition process", "Improving computational efficiency", "Enhancing feature extraction techniques", "Developing new algorithms for pattern recognition"], "complexity": 2}]}, "temporal": {"max_id": 44, "Questions": [{"id": 0, "context": "Language is an inherently temporal phenomenon. Spoken language is a sequence of acoustic events over time, and we comprehend and produce both spoken and written language as a sequential input stream. The temporal nature of language is reflected in the metaphors we use; we talk of the flow of conversations, news feeds, and twitter streams, all of which emphasize that language is a sequence that unfolds in time. ", "Bloom_type": "remember", "question": "In what way does language reflect its inherent temporality?", "options": ["Language reflects its inherent temporality through metaphors such as the flow of conversations, news feeds, and Twitter streams.", "Language reflects its inherent temporality by emphasizing the static nature of language.", "Language reflects its inherent temporality by using abstract concepts instead of concrete ones.", "Language reflects its inherent temporality only when it is written down."], "complexity": 0}, {"id": 1, "context": "This chapter introduces a deep learning architecture that offers an alternative way of representing time: recurrent neural networks (RNNs), and their variants like LSTMs. RNNs have a mechanism that deals directly with the sequential nature of language, allowing them to handle the temporal nature of language without the use of arbitrary fixed-sized windows. The recurrent network offers a new way to represent the prior context, in its recurrent connections, allowing the model`s decision to depend on information from hundreds of words in the past. We`ll see how to apply the model to the task of language modeling, to text classification tasks like sentiment analysis, and to sequence modeling tasks like part-of-speech tagging (a task we`ll return to in detail in Chapter 17). ", "Bloom_type": "remember", "question": "What does the term \"temporal\" refer to in this context?", "options": ["The order of events in time", "The duration of events", "The flow of time over space", "The size of data sets"], "complexity": 0}, {"id": 2, "context": "In this section we briefly introduce an alternative to encoder-decoder: an algorithm and loss function called CTC, short for Connectionist Temporal Classification (Graves et al., 2006), that deals with these problems in a very different way. The intuition of CTC is to output a single character for every frame of the input, so that ", "Bloom_type": "remember", "question": "What does the connectionist temporal classification method aim to achieve?", "options": ["It aims to classify characters based on their position in time.", "It focuses on improving the accuracy of word-level predictions.", "It seeks to reduce computational complexity by using fewer parameters.", "It attempts to handle sequence data more effectively than traditional methods."], "complexity": 0}, {"id": 3, "context": "Next, we discuss event extraction, the task of finding events in which these entities participate, like, in our sample text, the fare increases by United and American and the reporting events said and cite. Events are also situated in time, occurring at a particular date or time, and events can be related temporally, happening before or after or simultaneously with each other. We`ll need to recognize temporal expressions like Friday, Thursday or two days from now and times such as 3:30 P.M., and normalize them onto specific calendar dates or times. We`ll need to link Friday to the time of United`s announcement, Thursday to the previous day`s fare increase, and we`ll need to produce a timeline in which United`s announcement follows the fare increase and American`s announcement follows both of those events. ", "Bloom_type": "remember", "question": "In the discussion about event extraction, what aspect is being emphasized?", "options": ["The recognition of temporal expressions", "The identification of entities", "The normalization of times", "The linking of events"], "complexity": 0}, {"id": 4, "context": "Various versions of the event extraction task exist, depending on the goal. For example in the TempEval shared tasks (Verhagen et al. 2009) the goal is to extract events and aspects like their aspectual and temporal properties. Events are to be classified as actions, states, reporting events (say, report, tell, explain), perception events, and so on. The aspect, tense, and modality of each event also needs to be extracted. Thus for example the various said events in the sample text would be annotated as (class=REPORTING, tense=PAST, aspect=PERFECTIVE). ", "Bloom_type": "remember", "question": "In the TempEval shared tasks, what specific property of an event does not need to be extracted?", "options": ["Aspect", "Tense", "Modality", "Temporal Property"], "complexity": 0}, {"id": 5, "context": "Various kinds of temporal representation systems can be used to talk about temporal ordering relationship. One of the most commonly used in computational modeling is the interval algebra of Allen (1984). Allen models all events and time expressions as intervals there is no representation for points (although intervals can be very short). In order to deal with intervals without points, he identifies 13 primitive relations that can hold between these temporal intervals. Fig. 20.11 shows these 13 Allen relations. ", "Bloom_type": "remember", "question": "In Allen\u2019s model of temporal representation, what are the 13 primitive relations identified?", "options": ["Allen relations", "Temporal relations", "Spatial relations", "Logical relations"], "complexity": 0}, {"id": 6, "context": "The TimeBank corpus consists of American English text annotated with temporal information (Pustejovsky et al., 2003). The annotations use TimeML (Saur et al., 2006), a markup language for time based on Allen`s interval algebra discussed above (Allen, 1984). There are three types of TimeML objects: an EVENT represent events and states, a TIME represents time expressions like dates, and a LINK represents various relationships between events and times (event-event, event-time, and timetime). The links include temporal links (TLINK) for the 13 Allen relations, aspectual links (ALINK) for aspectual relationships between events and subevents, and SLINKS which mark factuality. ", "Bloom_type": "remember", "question": "In the TimeBank corpus, what type of annotation uses Allen\u2019s interval algebra?", "options": ["Temporal", "Aspectual", "Event-based", "Time-based"], "complexity": 0}, {"id": 7, "context": "1. Extracting temporal expressions 2. Normalizing these expressions, by converting them to a standard format. 3. Linking events to times and extracting time graphs and timelines ", "Bloom_type": "remember", "question": "What is the main goal of normalizing temporal expressions?", "options": ["To convert them into a standard format", "To extract time graphs and timelines", "To link events to times", "None of the above"], "complexity": 0}, {"id": 8, "context": "Temporal expressions are phrases that refer to absolute points in time, relative times, durations, and sets of these. Absolute temporal expressions are those that can be mapped directly to calendar dates, times of day, or both. Relative temporal expressions map to particular times through some other reference point (as in a week from last Tuesday). Finally, durations denote spans of time at varying levels of granularity (seconds, minutes, days, weeks, centuries, etc.). Figure 20.15 lists some sample temporal expressions in each of these categories. ", "Bloom_type": "remember", "question": "Which type of temporal expression refers to durations?", "options": ["Sample temporal expressions", "Absolute temporal expressions", "Relative temporal expressions", "Sets of these"], "complexity": 0}, {"id": 9, "context": "Temporal normalization is the task of mapping a temporal expression to a point in time or to a duration. Points in time correspond to calendar dates, to times of day, or both. Durations primarily consist of lengths of time. Normalized times ", "Bloom_type": "remember", "question": "In temporal normalization, what does a normalized time refer to?", "options": ["Both points in time and durations", "Points in time corresponding to calendar dates", "Times of day only", "Only durations"], "complexity": 0}, {"id": 10, "context": "The first temporal expression in the text proper refers to a particular week of the year. In the ISO standard, weeks are numbered from 01 to 53, with the first week of the year being the one that has the first Thursday of the year. These weeks are represented with the template YYYY-Wnn. The ISO week for our document date is week 27; thus the value for last week is represented as 2007-W26. ", "Bloom_type": "remember", "question": "In which way does the ISO standard define weeks?", "options": ["They begin with the first Thursday of each year.", "They refer to specific days within a month.", "They start counting from the first day of January.", "They use a numerical system based on years."], "complexity": 0}, {"id": 11, "context": "to as the document`s temporal anchor. The values of temporal expressions such as today, yesterday, or tomorrow can all be computed with respect to this temporal anchor. The semantic procedure for today simply assigns the anchor, and the attachments for tomorrow and yesterday add a day and subtract a day from the anchor, respectively. Of course, given the cyclic nature of our representations for months, weeks, days, and times of day, our temporal arithmetic procedures must use modulo arithmetic appropriate to the time unit being used. ", "Bloom_type": "remember", "question": "In what way does the document utilize the concept of temporal anchors?", "options": ["To provide a reference point for calculating future dates.", "To establish a chronological sequence within the document.", "To ensure consistency across different time units.", "To facilitate calculations involving past events."], "complexity": 0}, {"id": 12, "context": "Relative temporal expressions are handled with temporal arithmetic similar to that used for today and yesterday. The document date indicates that our example article is ISO week 27, so the expression last week normalizes to the current week minus 1. To resolve ambiguous next and last expressions we consider the distance from the anchoring date to the nearest unit. Next Friday can refer either to the immediately next Friday or to the Friday following that, but the closer the document date is to a Friday, the more likely it is that the phrase will skip the nearest one. Such ambiguities are handled by encoding language and domain-specific heuristics into the temporal attachments. ", "Bloom_type": "remember", "question": "In what way do relative temporal expressions get normalized when dealing with dates?", "options": ["They are adjusted based on the closest available date.", "They are converted to absolute dates.", "They are simplified using standard abbreviations.", "They are translated into different languages."], "complexity": 0}, {"id": 13, "context": "The goal of temporal analysis, is to link times to events and then fit all these events into a complete timeline. This ambitious task is the subject of considerable current research but solving it with a high level of accuracy is beyond the capabilities of current systems. A somewhat simpler, but still useful, task is to impose a partial ordering on the events and temporal expressions mentioned in a text. Such an ordering can provide many of the same benefits as a true timeline. An example of such a partial ordering is the determination that the fare increase by American Airlines came after the fare increase by United in our sample text. Determining such an ordering can be viewed as a binary relation detection and classification task. ", "Bloom_type": "remember", "question": "In temporal analysis, what does linking times to events primarily aim to achieve?", "options": ["To create a detailed chronological account", "To identify patterns in historical data", "To predict future events accurately", "To simplify complex timelines"], "complexity": 0}, {"id": 14, "context": "Progress in this area continues to be stimulated by formal evaluations with shared benchmark datasets, including the Automatic Content Extraction (ACE) evaluations of 2000-2007 on named entity recognition, relation extraction, and temporal expressions1, the KBP (Knowledge Base Population) evaluations (Ji et al. 2010, Surdeanu 2013) of relation extraction tasks like slot filling (extracting attributes (slots`) like age, birthplace, and spouse for a given entity) and a series of SemEval workshops (Hendrickx et al., 2009). ", "Bloom_type": "remember", "question": "What is an example of a dataset used for evaluating temporal expressions?", "options": ["All of the above", "The ACE evaluations from 2000-2007", "The KBP evaluations (Ji et al. 2010)", "SemEval workshops"], "complexity": 0}, {"id": 15, "context": "Language is an inherently temporal phenomenon. Spoken language is a sequence of acoustic events over time, and we comprehend and produce both spoken and written language as a sequential input stream. The temporal nature of language is reflected in the metaphors we use; we talk of the flow of conversations, news feeds, and twitter streams, all of which emphasize that language is a sequence that unfolds in time. ", "Bloom_type": "comprehension", "question": "Explain how the concept of temporality influences our understanding and use of language?", "options": ["The temporal aspect of language affects how we perceive and interact with spoken and written communication.", "Language is timeless and does not change over time.", "Temporal concepts are irrelevant when discussing language.", "Language only exists in static forms and has no temporal dimension."], "complexity": 1}, {"id": 16, "context": "This chapter introduces a deep learning architecture that offers an alternative way of representing time: recurrent neural networks (RNNs), and their variants like LSTMs. RNNs have a mechanism that deals directly with the sequential nature of language, allowing them to handle the temporal nature of language without the use of arbitrary fixed-sized windows. The recurrent network offers a new way to represent the prior context, in its recurrent connections, allowing the model`s decision to depend on information from hundreds of words in the past. We`ll see how to apply the model to the task of language modeling, to text classification tasks like sentiment analysis, and to sequence modeling tasks like part-of-speech tagging (a task we`ll return to in detail in Chapter 17). ", "Bloom_type": "comprehension", "question": "Explain how Recurrent Neural Networks (RNNs) differ from other models when dealing with sequences of data?", "options": ["RNNs excel at handling sequences by maintaining internal state through recurrent connections, enabling long-term dependency resolution.", "RNNs are less efficient than traditional models due to their complex structure.", "RNNs can only analyze static patterns within sequences, lacking the ability to capture temporal dependencies.", "RNNs require pre-defined window sizes for processing each input, limiting their flexibility."], "complexity": 1}, {"id": 17, "context": "In this section we briefly introduce an alternative to encoder-decoder: an algorithm and loss function called CTC, short for Connectionist Temporal Classification (Graves et al., 2006), that deals with these problems in a very different way. The intuition of CTC is to output a single character for every frame of the input, so that ", "Bloom_type": "comprehension", "question": "What does the connectionist temporal classification aim to do differently from traditional methods?", "options": ["It outputs characters for each time step instead of frames.", "It focuses solely on sequence alignment rather than temporal relationships.", "It uses a decoder to predict sequences directly.", "It applies a fixed-length encoding scheme."], "complexity": 1}, {"id": 18, "context": "Next, we discuss event extraction, the task of finding events in which these entities participate, like, in our sample text, the fare increases by United and American and the reporting events said and cite. Events are also situated in time, occurring at a particular date or time, and events can be related temporally, happening before or after or simultaneously with each other. We`ll need to recognize temporal expressions like Friday, Thursday or two days from now and times such as 3:30 P.M., and normalize them onto specific calendar dates or times. We`ll need to link Friday to the time of United`s announcement, Thursday to the previous day`s fare increase, and we`ll need to produce a timeline in which United`s announcement follows the fare increase and American`s announcement follows both of those events. ", "Bloom_type": "comprehension", "question": "Explain how temporal expressions are identified and normalized in event extraction?", "options": ["Temporal expressions are automatically detected using natural language processing techniques.", "Temporal expressions are recognized through machine learning algorithms.", "Temporal expressions are manually annotated for accuracy.", "Temporal expressions are identified based on predefined rules."], "complexity": 1}, {"id": 19, "context": "Various versions of the event extraction task exist, depending on the goal. For example in the TempEval shared tasks (Verhagen et al. 2009) the goal is to extract events and aspects like their aspectual and temporal properties. Events are to be classified as actions, states, reporting events (say, report, tell, explain), perception events, and so on. The aspect, tense, and modality of each event also needs to be extracted. Thus for example the various said events in the sample text would be annotated as (class=REPORTING, tense=PAST, aspect=PERFECTIVE). ", "Bloom_type": "comprehension", "question": "What is the primary focus when extracting events and aspects from a text?", "options": ["Determining the aspectual and temporal properties of events", "Classifying events based on their temporal properties", "Extracting all types of events including actions, states, and reporting events", "Identifying the tense and modality of each event"], "complexity": 1}, {"id": 20, "context": "Various kinds of temporal representation systems can be used to talk about temporal ordering relationship. One of the most commonly used in computational modeling is the interval algebra of Allen (1984). Allen models all events and time expressions as intervals there is no representation for points (although intervals can be very short). In order to deal with intervals without points, he identifies 13 primitive relations that can hold between these temporal intervals. Fig. 20.11 shows these 13 Allen relations. ", "Bloom_type": "comprehension", "question": "What are the primary types of temporal representations mentioned in the context?", "options": ["Allen\u2019s interval algebra and point-based representations", "Allen\u2019s interval algebra and timeline diagrams", "Timeline diagrams and Allen\u2019s point-based representations", "Point-based representations and Allen\u2019s interval algebra"], "complexity": 1}, {"id": 21, "context": "The TimeBank corpus consists of American English text annotated with temporal information (Pustejovsky et al., 2003). The annotations use TimeML (Saur et al., 2006), a markup language for time based on Allen`s interval algebra discussed above (Allen, 1984). There are three types of TimeML objects: an EVENT represent events and states, a TIME represents time expressions like dates, and a LINK represents various relationships between events and times (event-event, event-time, and timetime). The links include temporal links (TLINK) for the 13 Allen relations, aspectual links (ALINK) for aspectual relationships between events and subevents, and SLINKS which mark factuality. ", "Bloom_type": "comprehension", "question": "What type of annotation uses Temporal Information in the TimeBank Corpus?", "options": ["Time Expressions TIME", "Temporal Links TLINK", "Aspectual Relationships ALINK", "Factuality SLINKS"], "complexity": 1}, {"id": 22, "context": "1. Extracting temporal expressions 2. Normalizing these expressions, by converting them to a standard format. 3. Linking events to times and extracting time graphs and timelines ", "Bloom_type": "comprehension", "question": "What are the three primary steps involved in processing temporal expressions?", "options": ["Extracting temporal expressions, normalizing these expressions, linking events to times", "Normalizing these expressions, linking events to times, creating time graphs and timelines", "Linking events to times, extracting temporal expressions, creating time graphs and timelines", "Creating time graphs and timelines, extracting temporal expressions, normalizing these expressions"], "complexity": 1}, {"id": 23, "context": "Temporal expressions are phrases that refer to absolute points in time, relative times, durations, and sets of these. Absolute temporal expressions are those that can be mapped directly to calendar dates, times of day, or both. Relative temporal expressions map to particular times through some other reference point (as in a week from last Tuesday). Finally, durations denote spans of time at varying levels of granularity (seconds, minutes, days, weeks, centuries, etc.). Figure 20.15 lists some sample temporal expressions in each of these categories. ", "Bloom_type": "comprehension", "question": "What type of temporal expression refers to an exact moment in time?", "options": ["Absolute temporal expressions", "Relative temporal expressions", "Duration expressions", "Set expressions"], "complexity": 1}, {"id": 24, "context": "Temporal normalization is the task of mapping a temporal expression to a point in time or to a duration. Points in time correspond to calendar dates, to times of day, or both. Durations primarily consist of lengths of time. Normalized times ", "Bloom_type": "comprehension", "question": "What distinguishes points in time from durations in temporal normalization?", "options": ["Points in time are universally represented by calendar dates, regardless of their nature (day, month, year), while durations can vary based on specific contexts.", "Points in time are always represented by calendar dates, while durations can include times of day.", "Points in time are typically expressed using calendar dates, whereas durations may involve times of day.", "Duration cannot be normalized."], "complexity": 1}, {"id": 25, "context": "The first temporal expression in the text proper refers to a particular week of the year. In the ISO standard, weeks are numbered from 01 to 53, with the first week of the year being the one that has the first Thursday of the year. These weeks are represented with the template YYYY-Wnn. The ISO week for our document date is week 27; thus the value for last week is represented as 2007-W26. ", "Bloom_type": "comprehension", "question": "What does the first temporal expression refer to in the text?", "options": ["The current week of the year", "The previous week of the year", "The next week of the year", "The entire year"], "complexity": 1}, {"id": 26, "context": "to as the document`s temporal anchor. The values of temporal expressions such as today, yesterday, or tomorrow can all be computed with respect to this temporal anchor. The semantic procedure for today simply assigns the anchor, and the attachments for tomorrow and yesterday add a day and subtract a day from the anchor, respectively. Of course, given the cyclic nature of our representations for months, weeks, days, and times of day, our temporal arithmetic procedures must use modulo arithmetic appropriate to the time unit being used. ", "Bloom_type": "comprehension", "question": "What does the term 'temporal' refer to in relation to documents?", "options": ["The chronological order of events within a document", "The location where a document was stored", "The date when a document was created", "The size of a document file"], "complexity": 1}, {"id": 27, "context": "Relative temporal expressions are handled with temporal arithmetic similar to that used for today and yesterday. The document date indicates that our example article is ISO week 27, so the expression last week normalizes to the current week minus 1. To resolve ambiguous next and last expressions we consider the distance from the anchoring date to the nearest unit. Next Friday can refer either to the immediately next Friday or to the Friday following that, but the closer the document date is to a Friday, the more likely it is that the phrase will skip the nearest one. Such ambiguities are handled by encoding language and domain-specific heuristics into the temporal attachments. ", "Bloom_type": "comprehension", "question": "What method does the document use to handle ambiguous next and last expressions?", "options": ["Encoding language and domain-specific heuristics into the temporal attachments.", "Using temporal arithmetic similar to today and yesterday.", "Considering the distance from the anchoring date to the nearest unit.", "Both A) and C)"], "complexity": 1}, {"id": 28, "context": "The goal of temporal analysis, is to link times to events and then fit all these events into a complete timeline. This ambitious task is the subject of considerable current research but solving it with a high level of accuracy is beyond the capabilities of current systems. A somewhat simpler, but still useful, task is to impose a partial ordering on the events and temporal expressions mentioned in a text. Such an ordering can provide many of the same benefits as a true timeline. An example of such a partial ordering is the determination that the fare increase by American Airlines came after the fare increase by United in our sample text. Determining such an ordering can be viewed as a binary relation detection and classification task. ", "Bloom_type": "comprehension", "question": "What is one simple task related to temporal analysis described in the context?", "options": ["Determining the chronological order of events within a document", "Creating a detailed timeline for historical events", "Analyzing the complexity of temporal expressions", "Predicting future events based on past data"], "complexity": 1}, {"id": 29, "context": "Progress in this area continues to be stimulated by formal evaluations with shared benchmark datasets, including the Automatic Content Extraction (ACE) evaluations of 2000-2007 on named entity recognition, relation extraction, and temporal expressions1, the KBP (Knowledge Base Population) evaluations (Ji et al. 2010, Surdeanu 2013) of relation extraction tasks like slot filling (extracting attributes (slots`) like age, birthplace, and spouse for a given entity) and a series of SemEval workshops (Hendrickx et al., 2009). ", "Bloom_type": "comprehension", "question": "What type of evaluation has been most influential in advancing progress in the field of automatic content extraction?", "options": ["Temporal expression evaluations", "Named entity recognition evaluations", "Relation extraction evaluations", "Slot filling evaluations"], "complexity": 1}, {"id": 30, "context": "Language is an inherently temporal phenomenon. Spoken language is a sequence of acoustic events over time, and we comprehend and produce both spoken and written language as a sequential input stream. The temporal nature of language is reflected in the metaphors we use; we talk of the flow of conversations, news feeds, and twitter streams, all of which emphasize that language is a sequence that unfolds in time. ", "Bloom_type": "application", "question": "In what way does the temporal aspect of language influence our understanding of communication?", "options": ["The flow of conversation can be metaphorically described using concepts like'stream'.", "Language only exists in written form.", "We perceive language as a static collection of words.", "Temporal aspects are irrelevant when interpreting language."], "complexity": 2}, {"id": 31, "context": "This chapter introduces a deep learning architecture that offers an alternative way of representing time: recurrent neural networks (RNNs), and their variants like LSTMs. RNNs have a mechanism that deals directly with the sequential nature of language, allowing them to handle the temporal nature of language without the use of arbitrary fixed-sized windows. The recurrent network offers a new way to represent the prior context, in its recurrent connections, allowing the model`s decision to depend on information from hundreds of words in the past. We`ll see how to apply the model to the task of language modeling, to text classification tasks like sentiment analysis, and to sequence modeling tasks like part-of-speech tagging (a task we`ll return to in detail in Chapter 17). ", "Bloom_type": "application", "question": "What is the primary advantage of using RNNs over traditional methods for handling sequential data?", "options": ["They are more flexible and can adapt to different contexts.", "They can only work with static input.", "They require large amounts of memory.", "They cannot capture long-term dependencies."], "complexity": 2}, {"id": 32, "context": "In this section we briefly introduce an alternative to encoder-decoder: an algorithm and loss function called CTC, short for Connectionist Temporal Classification (Graves et al., 2006), that deals with these problems in a very different way. The intuition of CTC is to output a single character for every frame of the input, so that ", "Bloom_type": "application", "question": "What does the CTC algorithm aim to achieve?", "options": ["It aims to classify characters based on their temporal sequence.", "It aims to predict the next word in a sentence.", "It aims to reduce the computational complexity of the model.", "It aims to improve the accuracy of the decoder."], "complexity": 2}, {"id": 33, "context": "Next, we discuss event extraction, the task of finding events in which these entities participate, like, in our sample text, the fare increases by United and American and the reporting events said and cite. Events are also situated in time, occurring at a particular date or time, and events can be related temporally, happening before or after or simultaneously with each other. We`ll need to recognize temporal expressions like Friday, Thursday or two days from now and times such as 3:30 P.M., and normalize them onto specific calendar dates or times. We`ll need to link Friday to the time of United`s announcement, Thursday to the previous day`s fare increase, and we`ll need to produce a timeline in which United`s announcement follows the fare increase and American`s announcement follows both of those events. ", "Bloom_type": "application", "question": "What is the first step in recognizing temporal expressions?", "options": ["Normalize the temporal expressions onto specific calendar dates or times", "Identify the entities involved in the event", "Find the specific calendar dates or times for the events", "Link the events based on their occurrence"], "complexity": 2}, {"id": 34, "context": "Various versions of the event extraction task exist, depending on the goal. For example in the TempEval shared tasks (Verhagen et al. 2009) the goal is to extract events and aspects like their aspectual and temporal properties. Events are to be classified as actions, states, reporting events (say, report, tell, explain), perception events, and so on. The aspect, tense, and modality of each event also needs to be extracted. Thus for example the various said events in the sample text would be annotated as (class=REPORTING, tense=PAST, aspect=PERFECTIVE). ", "Bloom_type": "application", "question": "In which type of event extraction task does the focus lie on extracting both temporal and aspectual properties?", "options": ["Temporal event extraction", "Aspect-based event extraction", "Aspectual event extraction", "Tense-based event extraction"], "complexity": 2}, {"id": 35, "context": "Various kinds of temporal representation systems can be used to talk about temporal ordering relationship. One of the most commonly used in computational modeling is the interval algebra of Allen (1984). Allen models all events and time expressions as intervals there is no representation for points (although intervals can be very short). In order to deal with intervals without points, he identifies 13 primitive relations that can hold between these temporal intervals. Fig. 20.11 shows these 13 Allen relations. ", "Bloom_type": "application", "question": "Which relation among the 13 primitive relations identified by Allen does not involve any point?", "options": ["overlap", "contain", "disjoint", "strictly contain"], "complexity": 2}, {"id": 36, "context": "The TimeBank corpus consists of American English text annotated with temporal information (Pustejovsky et al., 2003). The annotations use TimeML (Saur et al., 2006), a markup language for time based on Allen`s interval algebra discussed above (Allen, 1984). There are three types of TimeML objects: an EVENT represent events and states, a TIME represents time expressions like dates, and a LINK represents various relationships between events and times (event-event, event-time, and timetime). The links include temporal links (TLINK) for the 13 Allen relations, aspectual links (ALINK) for aspectual relationships between events and subevents, and SLINKS which mark factuality. ", "Bloom_type": "application", "question": "What is the primary purpose of using TimeML in annotating American English text?", "options": ["To identify and annotate temporal aspects within texts", "To categorize different types of sentences", "To classify linguistic features of spoken language", "To translate written text into spoken language"], "complexity": 2}, {"id": 37, "context": "1. Extracting temporal expressions 2. Normalizing these expressions, by converting them to a standard format. 3. Linking events to times and extracting time graphs and timelines ", "Bloom_type": "application", "question": "What is the first step in processing temporal data?", "options": ["Extracting temporal expressions", "Normalizing these expressions", "Linking events to times", "Extraction of time graphs and timelines"], "complexity": 2}, {"id": 38, "context": "Temporal expressions are phrases that refer to absolute points in time, relative times, durations, and sets of these. Absolute temporal expressions are those that can be mapped directly to calendar dates, times of day, or both. Relative temporal expressions map to particular times through some other reference point (as in a week from last Tuesday). Finally, durations denote spans of time at varying levels of granularity (seconds, minutes, days, weeks, centuries, etc.). Figure 20.15 lists some sample temporal expressions in each of these categories. ", "Bloom_type": "application", "question": "Which type of temporal expression refers to an exact moment in time?", "options": ["Absolute temporal expressions", "Relative temporal expressions", "Duration expressions", "Set expressions"], "complexity": 2}, {"id": 39, "context": "Temporal normalization is the task of mapping a temporal expression to a point in time or to a duration. Points in time correspond to calendar dates, to times of day, or both. Durations primarily consist of lengths of time. Normalized times ", "Bloom_type": "application", "question": "In temporal normalization, how are points in time typically represented?", "options": ["As absolute calendar dates", "As relative durations", "Both as absolute calendar dates and relative durations", "Neither as absolute calendar dates nor as relative durations"], "complexity": 2}, {"id": 40, "context": "The first temporal expression in the text proper refers to a particular week of the year. In the ISO standard, weeks are numbered from 01 to 53, with the first week of the year being the one that has the first Thursday of the year. These weeks are represented with the template YYYY-Wnn. The ISO week for our document date is week 27; thus the value for last week is represented as 2007-W26. ", "Bloom_type": "application", "question": "Which week number represents the current week after the response?", "options": ["Week 28", "Week 29", "Week 27", "Week 26"], "complexity": 2}, {"id": 41, "context": "to as the document`s temporal anchor. The values of temporal expressions such as today, yesterday, or tomorrow can all be computed with respect to this temporal anchor. The semantic procedure for today simply assigns the anchor, and the attachments for tomorrow and yesterday add a day and subtract a day from the anchor, respectively. Of course, given the cyclic nature of our representations for months, weeks, days, and times of day, our temporal arithmetic procedures must use modulo arithmetic appropriate to the time unit being used. ", "Bloom_type": "application", "question": "What is the first step in computing the value of a temporal expression?", "options": ["Assign the temporal anchor directly to the expression", "Add a day to the temporal anchor", "Subtract a day from the temporal anchor", "Compute the modulo arithmetic based on the time unit"], "complexity": 2}, {"id": 42, "context": "Relative temporal expressions are handled with temporal arithmetic similar to that used for today and yesterday. The document date indicates that our example article is ISO week 27, so the expression last week normalizes to the current week minus 1. To resolve ambiguous next and last expressions we consider the distance from the anchoring date to the nearest unit. Next Friday can refer either to the immediately next Friday or to the Friday following that, but the closer the document date is to a Friday, the more likely it is that the phrase will skip the nearest one. Such ambiguities are handled by encoding language and domain-specific heuristics into the temporal attachments. ", "Bloom_type": "application", "question": "What technique is used to handle ambiguous temporal expressions?", "options": ["Language and domain-specific heuristics", "Temporal arithmetic", "ISO week normalization", "Encoding of anchors"], "complexity": 2}, {"id": 43, "context": "The goal of temporal analysis, is to link times to events and then fit all these events into a complete timeline. This ambitious task is the subject of considerable current research but solving it with a high level of accuracy is beyond the capabilities of current systems. A somewhat simpler, but still useful, task is to impose a partial ordering on the events and temporal expressions mentioned in a text. Such an ordering can provide many of the same benefits as a true timeline. An example of such a partial ordering is the determination that the fare increase by American Airlines came after the fare increase by United in our sample text. Determining such an ordering can be viewed as a binary relation detection and classification task. ", "Bloom_type": "application", "question": "What is the first step in imposing a partial ordering on events and temporal expressions?", "options": ["Detect and classify binary relations between events.", "Identify the most common time periods mentioned in the text.", "Find the longest sequence of consecutive events in the text.", "Classify each event based on its type (e.g., airline fare increase)."], "complexity": 2}, {"id": 44, "context": "Progress in this area continues to be stimulated by formal evaluations with shared benchmark datasets, including the Automatic Content Extraction (ACE) evaluations of 2000-2007 on named entity recognition, relation extraction, and temporal expressions1, the KBP (Knowledge Base Population) evaluations (Ji et al. 2010, Surdeanu 2013) of relation extraction tasks like slot filling (extracting attributes (slots`) like age, birthplace, and spouse for a given entity) and a series of SemEval workshops (Hendrickx et al., 2009). ", "Bloom_type": "application", "question": "What is an example of a dataset used for evaluating temporal expression recognition?", "options": ["All of the above", "The ACE evaluations of 2000-2007 on named entity recognition.", "The KBP (Knowledge Base Population) evaluations (Ji et al. 2010, Surdeanu 2013) of relation extraction tasks.", "SemEval workshops (Hendrickx et al., 2009)"], "complexity": 2}]}, "large language": {"max_id": 44, "Questions": [{"id": 0, "context": "Word prediction is also central to NLP for another reason: large language models are built just by training them to predict words!! As we`ll see in chapters 7-9, large language models learn an enormous amount about language solely from being trained to predict upcoming words from neighboring words. ", "Bloom_type": "remember", "question": "Why is word prediction important in the development of large language models?", "options": ["To improve the accuracy of natural language understanding", "To enhance the speed of data processing", "To increase the complexity of algorithms", "To reduce the need for human input"], "complexity": 0}, {"id": 1, "context": "Unfortunately, running big NLP systems end-to-end is often very expensive. Instead, it`s helpful to have a metric that can be used to quickly evaluate potential improvements in a language model. An intrinsic evaluation metric is one that measures the quality of a model independent of any application. In the next section we`ll introduce perplexity, which is the standard intrinsic metric for measuring language model performance, both for simple n-gram language models and for the more sophisticated neural large language models of Chapter 9. ", "Bloom_type": "remember", "question": "What is an intrinsic evaluation metric for measuring language model performance?", "options": ["Perplexity", "Accuracy", "Precision", "Recall"], "complexity": 0}, {"id": 2, "context": "In this chapter we formalize this idea of pretraininglearning knowledge about language and the world from vast amounts of textand call the resulting pretrained language models large language models. Large language models exhibit remark", "Bloom_type": "remember", "question": "What do large language models learn from vast amounts of text?", "options": ["They learn about the world and language.", "They learn mathematical formulas.", "They learn how to play games.", "They learn to speak languages fluently."], "complexity": 0}, {"id": 3, "context": "We`ll then talk about the process of text generation. The application of LLMs to generate text has vastly broadened the scope of NLP,. Text generation, codegeneration, and image-generation together constitute the important new area of generative AI. We`ll introduce specific algorithms for generating text from a language model, like greedy decoding and sampling. And we`ll see that almost any NLP task can be modeled as word prediction in a large language model, if we think about it in the right way. We`ll work through an example of using large language models to solve one classic NLP task of summarization (generating a short text that summarizes some larger document). ", "Bloom_type": "remember", "question": "In what way does the use of large language models facilitate the development of generative AI?", "options": ["By expanding the range of tasks that can be automated", "By enabling more efficient data storage", "By increasing computational power needed", "By reducing the need for human input"], "complexity": 0}, {"id": 4, "context": "Consider the simple task of text completion, illustrated in Fig. 10.1. Here a language model is given a text prefix and is asked to generate a possible completion. Note that as the generation process proceeds, the model has direct access to the priming context as well as to all of its own subsequently generated outputs (at least as much as fits in the large context window). This ability to incorporate the entirety of the earlier context and generated outputs at each time step is the key to the power of large language models built from transformers. ", "Bloom_type": "remember", "question": "In the context described, what does the model use directly to generate completions?", "options": ["The entire previous context", "Only the current input text", "Previous generated outputs only", "None of the above"], "complexity": 0}, {"id": 5, "context": "So why should we care about predicting upcoming words or tokens? The insight of large language modeling is that many practical NLP tasks can be cast as word prediction, and that a powerful-enough language model can solve them with a high degree of accuracy. For example, we can cast sentiment analysis as language modeling by giving a language model a context like: ", "Bloom_type": "remember", "question": "In what way does casting practical NLP tasks as word prediction benefit large language models?", "options": ["It allows for more accurate predictions of future events.", "It enables the creation of highly efficient algorithms.", "It simplifies complex data structures into basic units.", "It enhances the ability to understand natural human speech."], "complexity": 0}, {"id": 6, "context": "Conditional generation can even be used to accomplish tasks that must generate longer responses. Consider the task of text summarization, which is to take a long text, such as a full-length article, and produce an effective shorter summary of it. We can cast summarization as language modeling by giving a large language model a text, and follow the text by a token like tl;dr; this token is short for something like ", "Bloom_type": "remember", "question": "In conditional generation, how can we use a large language model to summarize a long text?", "options": ["By following the original text with a token indicating the end of the summary", "By providing the model with a specific keyword", "By using a predefined template for summaries", "By randomly generating new content"], "complexity": 0}, {"id": 7, "context": "The performance of large language models has shown to be mainly determined by 3 factors: model size (the number of parameters not counting embeddings), dataset size (the amount of training data), and the amount of compute used for training. That is, we can improve a model by adding parameters (adding more layers or having wider contexts or both), by training on more data, or by training for more iterations. The relationships between these factors and performance are known as scaling laws. Roughly speaking, the performance of a large language model (the loss) scales as a power-law with each of these three properties of model training. ", "Bloom_type": "remember", "question": "In the context of large language models, what determines their performance?", "options": ["Both A and B", "The number of parameters", "The amount of compute used for training", "None of the above"], "complexity": 0}, {"id": 8, "context": "Fine-tuning can be very difficult with very large language models, because there are enormous numbers of parameters to train; each pass of batch gradient descent has to backpropagate through many many huge layers. This makes finetuning huge language models extremely expensive in processing power, in memory, and in time. For this reason, there are alternative methods that allow a model to be finetuned without changing all the parameters. Such methods are called parameter-efficient fine tuning or sometimes PEFT, because we efficiently select a subset of parameters to update when finetuning. For example we freeze some of the parameters (don`t change them), and only update some particular subset of parameters. ", "Bloom_type": "remember", "question": "What is a method used for finetuning large language models that allows selecting specific subsets of parameters instead of updating all of them?", "options": ["PEFT (Parameter-Efficient Fine Tuning)", "Batch Gradient Descent", "Backpropagation", "Fine-tuning algorithms"], "complexity": 0}, {"id": 9, "context": " Many NLP taskssuch as question answering, summarization, sentiment, and machine translationcan be cast as tasks of word prediction and hence addressed with Large language models. ", "Bloom_type": "remember", "question": "In what type of model are many natural language processing (NLP) tasks primarily addressed?", "options": ["Large Language Models", "Convolutional Neural Networks", "Recurrent Neural Networks", "Transformer Models"], "complexity": 0}, {"id": 10, "context": "BERT and other early transformer-based language models were trained on about 3.3 billion words (a combination of English Wikipedia and a corpus of book texts called BooksCorpus (Zhu et al., 2015) that is no longer used for intellectual property reasons). Modern masked language models are now trained on much larger datasets of web text, filtered a bit, and augmented by higher-quality data like Wikipedia, the same as those we discussed for the causal large language models of Chapter 9. Multilingual models similarly use webtext and multilingual Wikipedia. For example the XLM-R model was trained on about 300 billion tokens in 100 languages, taken from the web via Common Crawl (https://commoncrawl.org/). ", "Bloom_type": "remember", "question": "What type of dataset did modern masked language models primarily train on compared to earlier transformers?", "options": ["Web text", "English Wikipedia", "BooksCorpus", "Wikipedia"], "complexity": 0}, {"id": 11, "context": "Question answering systems are designed to fill human information needs. Since a lot of information is present in text form (on the web or in other data like our email, or books), question answering is closely related to the task behind search engines. Indeed, the distinction is becoming ever more fuzzy, as modern search engines are integrated with large language models trained to do question answering. Question answering systems often focus on a useful subset of information needs: factoid questions, questions of fact or reasoning that can be answered with simple facts expressed in short or medium-length texts, like the following: ", "Bloom_type": "remember", "question": "In what way does the integration of large language models affect the distinction between question answering and search engines?", "options": ["It blurs the lines between these two tasks.", "It makes the distinction less important.", "It eliminates the need for search engines.", "It creates new types of questions."], "complexity": 0}, {"id": 12, "context": " The reader stage is implemented by retrieval-augmented generation, in which a large language model is prompted with the query and a set of documents and then conditionally generates a novel answer. ", "Bloom_type": "remember", "question": "In what way does the reader stage utilize a large language model?", "options": ["It prompts the model with specific questions.", "It uses the model to analyze existing data.", "It trains the model on new datasets.", "It generates answers based on previous knowledge."], "complexity": 0}, {"id": 13, "context": "Early work on large language models showed that they stored sufficient knowledge in the pretraining process to answer questions (Petroni et al., 2019; Raffel et al., 2020; Radford et al., 2019; Roberts et al., 2020), at first not competitively with special-purpose question answerers, but then surpassing them. Retrieval-augmented generation algorithms were first introduced as a way to improve language modeling (Khandelwal et al., 2019), but were quickly applied to question answering (Izacard et al., 2022; Ram et al., 2023; Shi et al., 2023). ", "Bloom_type": "remember", "question": "What did early work on large language models show about their ability to answer questions?", "options": ["Their performance was initially inferior to those of question answerers.", "They could only answer simple questions.", "They surpassed specialized question answerers.", "They required extensive training data for effective question answering."], "complexity": 0}, {"id": 14, "context": "Full mixed initiative, while the norm for human-human conversations, can be difficult for dialogue systems. The most primitive dialogue systems tend to use system-initiative, where the system asks a question and the user can`t do anything until they answer it, or user-initiative like simple search engines, where the user specifies a query and the system passively responds. Even modern large language model-based dialogue systems, which come much closer to using full mixed initiative, often don`t have completely natural initiative switching. Getting this right is an important goal for modern systems. ", "Bloom_type": "remember", "question": "In the context of dialogue systems, what does the term \"large language\" refer to?", "options": ["Models with vast amounts of data for training", "Systems that are larger than humans", "Dialogue models that understand complex languages", "Software programs designed to handle large volumes of language"], "complexity": 0}, {"id": 15, "context": "Word prediction is also central to NLP for another reason: large language models are built just by training them to predict words!! As we`ll see in chapters 7-9, large language models learn an enormous amount about language solely from being trained to predict upcoming words from neighboring words. ", "Bloom_type": "comprehension", "question": "Explain why word prediction is crucial for building large language models?", "options": ["To improve the accuracy of predictions based on previous words", "To enhance the understanding of sentence structure", "To increase the speed of data processing", "To reduce computational complexity"], "complexity": 1}, {"id": 16, "context": "Unfortunately, running big NLP systems end-to-end is often very expensive. Instead, it`s helpful to have a metric that can be used to quickly evaluate potential improvements in a language model. An intrinsic evaluation metric is one that measures the quality of a model independent of any application. In the next section we`ll introduce perplexity, which is the standard intrinsic metric for measuring language model performance, both for simple n-gram language models and for the more sophisticated neural large language models of Chapter 9. ", "Bloom_type": "comprehension", "question": "What does the intrinsic evaluation metric measure in language models?", "options": ["The accuracy of predictions made by the language model", "The complexity of the language model", "The speed at which the language model processes data", "The efficiency of training the language model"], "complexity": 1}, {"id": 17, "context": "In this chapter we formalize this idea of pretraininglearning knowledge about language and the world from vast amounts of textand call the resulting pretrained language models large language models. Large language models exhibit remark", "Bloom_type": "comprehension", "question": "Explain how large language models are formed by learning from extensive textual data.", "options": ["The model learns by analyzing patterns within vast quantities of text, which it then applies to understanding and generating human-like language.", "Large language models are created through direct interaction with humans.", "They are built using specialized algorithms designed for natural language processing.", "These models are trained exclusively on images rather than text."], "complexity": 1}, {"id": 18, "context": "We`ll then talk about the process of text generation. The application of LLMs to generate text has vastly broadened the scope of NLP,. Text generation, codegeneration, and image-generation together constitute the important new area of generative AI. We`ll introduce specific algorithms for generating text from a language model, like greedy decoding and sampling. And we`ll see that almost any NLP task can be modeled as word prediction in a large language model, if we think about it in the right way. We`ll work through an example of using large language models to solve one classic NLP task of summarization (generating a short text that summarizes some larger document). ", "Bloom_type": "comprehension", "question": "Explain how large language models are applied in various areas of natural language processing (NLP)?", "options": ["Text generation, code generation, and image generation are all applications of large language models.", "Large language models are only used for text generation.", "The primary use of large language models is for image generation.", "Large language models have no direct applications in NLP."], "complexity": 1}, {"id": 19, "context": "Consider the simple task of text completion, illustrated in Fig. 10.1. Here a language model is given a text prefix and is asked to generate a possible completion. Note that as the generation process proceeds, the model has direct access to the priming context as well as to all of its own subsequently generated outputs (at least as much as fits in the large context window). This ability to incorporate the entirety of the earlier context and generated outputs at each time step is the key to the power of large language models built from transformers. ", "Bloom_type": "comprehension", "question": "What is the primary feature of large language models mentioned in the context?", "options": ["The use of transformers for context integration", "The complexity of text completion tasks", "The importance of context windows", "The necessity of generating completions"], "complexity": 1}, {"id": 20, "context": "So why should we care about predicting upcoming words or tokens? The insight of large language modeling is that many practical NLP tasks can be cast as word prediction, and that a powerful-enough language model can solve them with a high degree of accuracy. For example, we can cast sentiment analysis as language modeling by giving a language model a context like: ", "Bloom_type": "comprehension", "question": "How does large language models assist in solving practical natural language processing tasks?", "options": ["By predicting the next word or token in a sequence", "By directly analyzing human emotions in texts", "By creating new languages for communication", "Both B) and C)"], "complexity": 1}, {"id": 21, "context": "Conditional generation can even be used to accomplish tasks that must generate longer responses. Consider the task of text summarization, which is to take a long text, such as a full-length article, and produce an effective shorter summary of it. We can cast summarization as language modeling by giving a large language model a text, and follow the text by a token like tl;dr; this token is short for something like ", "Bloom_type": "comprehension", "question": "What does conditional generation allow us to do in relation to generating longer responses?", "options": ["It helps us condense texts into shorter versions efficiently.", "It allows us to create more detailed summaries.", "It enables us to write longer articles directly.", "It prevents the use of any summarization techniques."], "complexity": 1}, {"id": 22, "context": "The performance of large language models has shown to be mainly determined by 3 factors: model size (the number of parameters not counting embeddings), dataset size (the amount of training data), and the amount of compute used for training. That is, we can improve a model by adding parameters (adding more layers or having wider contexts or both), by training on more data, or by training for more iterations. The relationships between these factors and performance are known as scaling laws. Roughly speaking, the performance of a large language model (the loss) scales as a power-law with each of these three properties of model training. ", "Bloom_type": "comprehension", "question": "Explain how the performance of large language models is influenced by their model size, dataset size, and compute usage?", "options": ["Improving all three factors\u2014model size, dataset size, and compute usage\u2014significantly boosts performance.", "Adding more layers increases the model size, leading to better performance.", "Increasing the dataset size improves performance because it provides more examples for training.", "Training for longer iterations enhances performance due to deeper learning."], "complexity": 1}, {"id": 23, "context": "Fine-tuning can be very difficult with very large language models, because there are enormous numbers of parameters to train; each pass of batch gradient descent has to backpropagate through many many huge layers. This makes finetuning huge language models extremely expensive in processing power, in memory, and in time. For this reason, there are alternative methods that allow a model to be finetuned without changing all the parameters. Such methods are called parameter-efficient fine tuning or sometimes PEFT, because we efficiently select a subset of parameters to update when finetuning. For example we freeze some of the parameters (don`t change them), and only update some particular subset of parameters. ", "Bloom_type": "comprehension", "question": "What is an alternative method for finetuning large language models that allows selecting specific subsets of parameters during training?", "options": ["PEFT (Parameter-Efficient Fine-Tuning)", "Batch Gradient Descent", "Adam Optimizer", "SGD (Stochastic Gradient Descent)"], "complexity": 1}, {"id": 24, "context": " Many NLP taskssuch as question answering, summarization, sentiment, and machine translationcan be cast as tasks of word prediction and hence addressed with Large language models. ", "Bloom_type": "comprehension", "question": "Explain how large language models can address various NLP tasks.", "options": ["By predicting individual words in sentences.", "Through complex neural networks designed for specific tasks.", "Using pre-trained models for general language understanding.", "By focusing solely on syntax analysis."], "complexity": 1}, {"id": 25, "context": "BERT and other early transformer-based language models were trained on about 3.3 billion words (a combination of English Wikipedia and a corpus of book texts called BooksCorpus (Zhu et al., 2015) that is no longer used for intellectual property reasons). Modern masked language models are now trained on much larger datasets of web text, filtered a bit, and augmented by higher-quality data like Wikipedia, the same as those we discussed for the causal large language models of Chapter 9. Multilingual models similarly use webtext and multilingual Wikipedia. For example the XLM-R model was trained on about 300 billion tokens in 100 languages, taken from the web via Common Crawl (https://commoncrawl.org/). ", "Bloom_type": "comprehension", "question": "What distinguishes modern masked language models from earlier ones?", "options": ["They use more diverse training data including web text.", "They focus solely on academic literature.", "Their training dataset size remains constant.", "They do not incorporate any external sources."], "complexity": 1}, {"id": 26, "context": "Question answering systems are designed to fill human information needs. Since a lot of information is present in text form (on the web or in other data like our email, or books), question answering is closely related to the task behind search engines. Indeed, the distinction is becoming ever more fuzzy, as modern search engines are integrated with large language models trained to do question answering. Question answering systems often focus on a useful subset of information needs: factoid questions, questions of fact or reasoning that can be answered with simple facts expressed in short or medium-length texts, like the following: ", "Bloom_type": "comprehension", "question": "What aspect of question answering systems makes them closely related to search engine functionality?", "options": ["The integration with large language models", "The ability to understand natural language queries", "Their capacity for storing vast amounts of data", "The use of machine learning algorithms"], "complexity": 1}, {"id": 27, "context": " The reader stage is implemented by retrieval-augmented generation, in which a large language model is prompted with the query and a set of documents and then conditionally generates a novel answer. ", "Bloom_type": "comprehension", "question": "What does the reader stage involve when using a large language model?", "options": ["Prompting the model with a query and selecting relevant documents for conditional generation", "Generating a response based solely on predefined templates", "Using only pre-existing knowledge without any prompts", "Repeating previous responses to maintain consistency"], "complexity": 1}, {"id": 28, "context": "Early work on large language models showed that they stored sufficient knowledge in the pretraining process to answer questions (Petroni et al., 2019; Raffel et al., 2020; Radford et al., 2019; Roberts et al., 2020), at first not competitively with special-purpose question answerers, but then surpassing them. Retrieval-augmented generation algorithms were first introduced as a way to improve language modeling (Khandelwal et al., 2019), but were quickly applied to question answering (Izacard et al., 2022; Ram et al., 2023; Shi et al., 2023). ", "Bloom_type": "comprehension", "question": "How did early work on large language models demonstrate their ability to answer questions?", "options": ["They struggled initially but eventually outperformed.", "They could only store limited knowledge.", "They surpassed specialized question answerers.", "Their performance was inconsistent."], "complexity": 1}, {"id": 29, "context": "Full mixed initiative, while the norm for human-human conversations, can be difficult for dialogue systems. The most primitive dialogue systems tend to use system-initiative, where the system asks a question and the user can`t do anything until they answer it, or user-initiative like simple search engines, where the user specifies a query and the system passively responds. Even modern large language model-based dialogue systems, which come much closer to using full mixed initiative, often don`t have completely natural initiative switching. Getting this right is an important goal for modern systems. ", "Bloom_type": "comprehension", "question": "What type of dialogue systems are typically more common among humans compared to dialogue systems?", "options": ["System-initiative", "User-initiative", "Both System-initiative and User-initiative equally", "None of the above"], "complexity": 1}, {"id": 30, "context": "Word prediction is also central to NLP for another reason: large language models are built just by training them to predict words!! As we`ll see in chapters 7-9, large language models learn an enormous amount about language solely from being trained to predict upcoming words from neighboring words. ", "Bloom_type": "application", "question": "What does it mean when a large language model learns about language through predicting upcoming words?", "options": ["The model uses its predictions to improve future predictions.", "The model memorizes every word it encounters.", "The model focuses on understanding sentence structure.", "The model only predicts words based on random data."], "complexity": 2}, {"id": 31, "context": "Unfortunately, running big NLP systems end-to-end is often very expensive. Instead, it`s helpful to have a metric that can be used to quickly evaluate potential improvements in a language model. An intrinsic evaluation metric is one that measures the quality of a model independent of any application. In the next section we`ll introduce perplexity, which is the standard intrinsic metric for measuring language model performance, both for simple n-gram language models and for the more sophisticated neural large language models of Chapter 9. ", "Bloom_type": "application", "question": "What does perplexity measure in language models?", "options": ["The likelihood of generating random sequences compared to expected ones", "The speed at which the model processes input", "The accuracy of predictions made by the model on test data", "The number of errors made by the model during training"], "complexity": 2}, {"id": 32, "context": "In this chapter we formalize this idea of pretraininglearning knowledge about language and the world from vast amounts of textand call the resulting pretrained language models large language models. Large language models exhibit remark", "Bloom_type": "application", "question": "What is the main characteristic of large language models?", "options": ["They learn general knowledge about language and the world.", "They are trained on small datasets.", "They can only understand simple sentences.", "They require minimal computational resources."], "complexity": 2}, {"id": 33, "context": "We`ll then talk about the process of text generation. The application of LLMs to generate text has vastly broadened the scope of NLP,. Text generation, codegeneration, and image-generation together constitute the important new area of generative AI. We`ll introduce specific algorithms for generating text from a language model, like greedy decoding and sampling. And we`ll see that almost any NLP task can be modeled as word prediction in a large language model, if we think about it in the right way. We`ll work through an example of using large language models to solve one classic NLP task of summarization (generating a short text that summarizes some larger document). ", "Bloom_type": "application", "question": "What is the first step in applying Large Language Models (LLMs) to generate text?", "options": ["Choose a pre-trained LLM", "Identify the target document", "Define the desired output length", "Initialize the input sequence"], "complexity": 2}, {"id": 34, "context": "Consider the simple task of text completion, illustrated in Fig. 10.1. Here a language model is given a text prefix and is asked to generate a possible completion. Note that as the generation process proceeds, the model has direct access to the priming context as well as to all of its own subsequently generated outputs (at least as much as fits in the large context window). This ability to incorporate the entirety of the earlier context and generated outputs at each time step is the key to the power of large language models built from transformers. ", "Bloom_type": "application", "question": "What is the primary advantage of using large language models based on transformers?", "options": ["They can utilize the entire history of their interactions.", "They can only handle short texts.", "They cannot incorporate any previous context.", "They are limited to generating new sentences."], "complexity": 2}, {"id": 35, "context": "So why should we care about predicting upcoming words or tokens? The insight of large language modeling is that many practical NLP tasks can be cast as word prediction, and that a powerful-enough language model can solve them with a high degree of accuracy. For example, we can cast sentiment analysis as language modeling by giving a language model a context like: ", "Bloom_type": "application", "question": "What does it mean when we say that large language models can predict future words?", "options": ["The model predicts the next word based on the last few words.", "The model predicts the next word based on past words.", "The model predicts the entire sentence before writing it down.", "The model predicts the previous word based on the current word."], "complexity": 2}, {"id": 36, "context": "Conditional generation can even be used to accomplish tasks that must generate longer responses. Consider the task of text summarization, which is to take a long text, such as a full-length article, and produce an effective shorter summary of it. We can cast summarization as language modeling by giving a large language model a text, and follow the text by a token like tl;dr; this token is short for something like ", "Bloom_type": "application", "question": "What is the purpose of using conditional generation in text summarization?", "options": ["To generate a concise summary based on specific conditions", "To create a more detailed summary", "To reduce the length of the original text", "To increase the complexity of the summary"], "complexity": 2}, {"id": 37, "context": "The performance of large language models has shown to be mainly determined by 3 factors: model size (the number of parameters not counting embeddings), dataset size (the amount of training data), and the amount of compute used for training. That is, we can improve a model by adding parameters (adding more layers or having wider contexts or both), by training on more data, or by training for more iterations. The relationships between these factors and performance are known as scaling laws. Roughly speaking, the performance of a large language model (the loss) scales as a power-law with each of these three properties of model training. ", "Bloom_type": "application", "question": "Which factor among model size, dataset size, and compute usage primarily affects the performance of a large language model?", "options": ["All of the above", "Model size", "Dataset size", "Compute usage"], "complexity": 2}, {"id": 38, "context": "Fine-tuning can be very difficult with very large language models, because there are enormous numbers of parameters to train; each pass of batch gradient descent has to backpropagate through many many huge layers. This makes finetuning huge language models extremely expensive in processing power, in memory, and in time. For this reason, there are alternative methods that allow a model to be finetuned without changing all the parameters. Such methods are called parameter-efficient fine tuning or sometimes PEFT, because we efficiently select a subset of parameters to update when finetuning. For example we freeze some of the parameters (don`t change them), and only update some particular subset of parameters. ", "Bloom_type": "application", "question": "What is an alternative method for finetuning large language models?", "options": ["Implement parameter-efficient fine-tuning techniques", "Increase the number of training batches per epoch", "Use smaller learning rates", "Reduce the size of the dataset"], "complexity": 2}, {"id": 39, "context": " Many NLP taskssuch as question answering, summarization, sentiment, and machine translationcan be cast as tasks of word prediction and hence addressed with Large language models. ", "Bloom_type": "application", "question": "What is the primary focus of large language models?", "options": ["Analyzing text data", "Processing images", "Predicting future events", "Generating creative content"], "complexity": 2}, {"id": 40, "context": "BERT and other early transformer-based language models were trained on about 3.3 billion words (a combination of English Wikipedia and a corpus of book texts called BooksCorpus (Zhu et al., 2015) that is no longer used for intellectual property reasons). Modern masked language models are now trained on much larger datasets of web text, filtered a bit, and augmented by higher-quality data like Wikipedia, the same as those we discussed for the causal large language models of Chapter 9. Multilingual models similarly use webtext and multilingual Wikipedia. For example the XLM-R model was trained on about 300 billion tokens in 100 languages, taken from the web via Common Crawl (https://commoncrawl.org/). ", "Bloom_type": "application", "question": "What is the typical size of the dataset used to train modern transformers?", "options": ["More than 100 billion words", "Less than 1 million words", "Between 1 million and 100 million words", "The exact number is not specified"], "complexity": 2}, {"id": 41, "context": "Question answering systems are designed to fill human information needs. Since a lot of information is present in text form (on the web or in other data like our email, or books), question answering is closely related to the task behind search engines. Indeed, the distinction is becoming ever more fuzzy, as modern search engines are integrated with large language models trained to do question answering. Question answering systems often focus on a useful subset of information needs: factoid questions, questions of fact or reasoning that can be answered with simple facts expressed in short or medium-length texts, like the following: ", "Bloom_type": "application", "question": "What type of questions do question answering systems primarily address?", "options": ["Questions asking for detailed factual information", "Questions requiring complex logical reasoning", "Questions about personal experiences", "Questions seeking creative solutions"], "complexity": 2}, {"id": 42, "context": " The reader stage is implemented by retrieval-augmented generation, in which a large language model is prompted with the query and a set of documents and then conditionally generates a novel answer. ", "Bloom_type": "application", "question": "What does the reader stage involve when using a large language model?", "options": ["The model retrieves relevant documents and generates an answer based on them.", "The model only uses pre-existing knowledge to generate answers.", "The model prompts users directly for their queries.", "The model combines both document retrieval and conditional generation."], "complexity": 2}, {"id": 43, "context": "Early work on large language models showed that they stored sufficient knowledge in the pretraining process to answer questions (Petroni et al., 2019; Raffel et al., 2020; Radford et al., 2019; Roberts et al., 2020), at first not competitively with special-purpose question answerers, but then surpassing them. Retrieval-augmented generation algorithms were first introduced as a way to improve language modeling (Khandelwal et al., 2019), but were quickly applied to question answering (Izacard et al., 2022; Ram et al., 2023; Shi et al., 2023). ", "Bloom_type": "application", "question": "What was the initial purpose of retrieval-augmented generation algorithms?", "options": ["To enhance language modeling capabilities", "To replace traditional question-answering systems entirely", "To reduce computational costs for training models", "To increase the speed of model processing"], "complexity": 2}, {"id": 44, "context": "Full mixed initiative, while the norm for human-human conversations, can be difficult for dialogue systems. The most primitive dialogue systems tend to use system-initiative, where the system asks a question and the user can`t do anything until they answer it, or user-initiative like simple search engines, where the user specifies a query and the system passively responds. Even modern large language model-based dialogue systems, which come much closer to using full mixed initiative, often don`t have completely natural initiative switching. Getting this right is an important goal for modern systems. ", "Bloom_type": "application", "question": "What is the main challenge in achieving full mixed initiative in dialogue systems?", "options": ["The inability to switch between system and user initiatives smoothly.", "The difficulty of understanding complex queries.", "The lack of natural language processing capabilities.", "The need for more computational resources."], "complexity": 2}]}, "neural network": {"max_id": 127, "Questions": [{"id": 0, "context": "In the first part of the book we introduce the fundamental suite of algorithmic tools that make up the modern neural language model that is the heart of end-to-end NLP systems. We begin with tokenization and preprocessing, as well as useful algorithms like computing edit distance, and then proceed to the tasks of classification, logistic regression, neural networks, proceeding through feedforward networks, recurrent networks, and then transformers. We`ll also see the role of embeddings as a model of word meaning. ", "Bloom_type": "remember", "question": "In what order do they discuss the different types of neural networks in the book?", "options": ["preprocessing, tokenization, classification, logistic regression, neural networks, feedforward networks, recurrent networks, and then transformers", "tokenization, preprocessing, classification, logistic regression, neural networks, feedforward networks, recurrent networks, and then transformers", "classification, logistic regression, neural networks, feedforward networks, recurrent networks, and then transformers, preprocessing, tokenization", "logistic regression, neural networks, feedforward networks, recurrent networks, and then transformers, preprocessing, tokenization"], "complexity": 0}, {"id": 1, "context": "Large language models are based on neural networks rather than n-grams, enabling them to solve the two major problems with n-grams: (1) the number of parameters increases exponentially as the n-gram order increases, and (2) n-grams have no way to generalize from training examples to test set examples unless they use identical words. Neural language models instead project words into a continuous space in which words with similar contexts have similar representations. We`ll introduce transformer-based large language models in Chapter 9, along the way introducing feedforward language models (Bengio et al. 2006, Schwenk 2007) in Chapter 7 and recurrent language models (Mikolov, 2012) in Chapter 8. ", "Bloom_type": "remember", "question": "What enables large language models to overcome the limitations of n-grams?", "options": ["The use of neural networks", "The increase in computational power", "The introduction of recurrent language models", "The development of deep learning algorithms"], "complexity": 0}, {"id": 2, "context": "In this chapter we introduce an algorithm that is admirably suited for discovering the link between features or clues and some particular outcome: logistic regression. Indeed, logistic regression is one of the most important analytic tools in the social and natural sciences. In natural language processing, logistic regression is the baseline supervised machine learning algorithm for classification, and also has a very close relationship with neural networks. As we will see in Chapter 7, a neural network can be viewed as a series of logistic regression classifiers stacked on top of each other. Thus the classification and machine learning techniques introduced here will play an important role throughout the book. ", "Bloom_type": "remember", "question": "In what way does logistic regression relate to neural networks?", "options": ["Logistic regression is a foundational concept that forms the basis of neural networks.", "Logistic regression is used only for numerical data.", "Logistic regression is a type of neural network.", "Logistic regression cannot be applied to real-world problems."], "complexity": 0}, {"id": 3, "context": "Having input data with comparable range is useful when comparing values across features. Data scaling is especially important in large neural networks, since it helps speed up gradient descent. ", "Bloom_type": "remember", "question": "In what type of neural networks does data scaling become particularly crucial due to its impact on the efficiency of the learning process?", "options": ["Feedforward Neural Networks", "Convolutional Neural Networks", "Recurrent Neural Networks", "Autoencoder Neural Networks"], "complexity": 0}, {"id": 4, "context": "For logistic regression, this loss function is conveniently convex. A convex function has at most one minimum; there are no local minima to get stuck in, so gradient descent starting from any point is guaranteed to find the minimum. (By contrast, the loss for multi-layer neural networks is non-convex, and gradient descent may get stuck in local minima for neural network training and never find the global optimum.) ", "Bloom_type": "remember", "question": "Which of the following statements about neural networks is true?", "options": ["Gradient descent can only find local minima in neural networks.", "Neural networks always converge to the global optimum.", "Convex functions do not have a minimum.", "Multi-layer neural networks cannot be trained using gradient descent."], "complexity": 0}, {"id": 5, "context": "The revolutionary intuition here is that we can just use running text as implicitly supervised training data for such a classifier; a word c that occurs near the target word apricot acts as gold correct answer` to the question Is word c likely to show up near apricot? This method, often called self-supervision, avoids the need for any sort of hand-labeled supervision signal. This idea was first proposed in the task of neural language modeling, when Bengio et al. (2003) and Collobert et al. (2011) showed that a neural language model (a neural network that learned to predict the next word from prior words) could just use the next word in running text as its supervision signal, and could be used to learn an embedding representation for each word as part of doing this prediction task. ", "Bloom_type": "remember", "question": "In what type of machine learning tasks did Bengio et al. propose using neural networks for self-supervised learning?", "options": ["Natural Language Processing", "Image recognition", "Speech Recognition", "Computer Vision"], "complexity": 0}, {"id": 6, "context": "Instead, a modern neural network is a network of small computing units, each of which takes a vector of input values and produces a single output value. In this chapter we introduce the neural net applied to classification. The architecture we introduce is called a feedforward network because the computation proceeds iteratively from one layer of units to the next. The use of modern neural nets is often called deep learning, because modern networks are often deep (have many layers). Neural networks share much of the same mathematics as logistic regression. But neural networks are a more powerful classifier than logistic regression, and indeed a minimal neural network (technically one with a single hidden layer`) can be shown to learn any function. ", "Bloom_type": "remember", "question": "What does the term \"feedforward network\" refer to?", "options": ["A network where data flows through all layers before reaching the output", "A network where data flows back and forth between layers", "A network where data flows only once through the first layer", "A network where data flows through all layers but not necessarily forward"], "complexity": 0}, {"id": 7, "context": "Neural net classifiers are different from logistic regression in another way. With logistic regression, we applied the regression classifier to many different tasks by developing many rich kinds of feature templates based on domain knowledge. When working with neural networks, it is more common to avoid most uses of rich handderived features, instead building neural networks that take raw words as inputs and learn to induce features as part of the process of learning to classify. We saw examples of this kind of representation learning for embeddings in Chapter 6. Nets that are very deep are particularly good at representation learning. For that reason deep neural nets are the right tool for tasks that offer sufficient data to learn features automatically. ", "Bloom_type": "remember", "question": "In what way do neural networks differ from logistic regression when using them for classification?", "options": ["Neural networks use fewer types of features.", "Logistic regression requires less training data than neural networks.", "Neural networks cannot handle complex tasks.", "Logistic regression can only classify categorical data."], "complexity": 0}, {"id": 8, "context": "In this chapter we`ll introduce feedforward networks as classifiers, and also apply them to the simple task of language modeling: assigning probabilities to word sequences and predicting upcoming words. In subsequent chapters we`ll introduce many other aspects of neural models, such as recurrent neural networks (Chapter 8), the Transformer (Chapter 9), and masked language modeling (Chapter 11). ", "Bloom_type": "remember", "question": "In what way do feedforward networks differ from recurrent neural networks?", "options": ["Feedforward networks use feedback connections whereas recurrent neural networks do not.", "Feedforward networks are used for sequence prediction only.", "Recurrent neural networks can process sequential data but not feedforward networks.", "Recurrent neural networks require more computational resources than feedforward networks."], "complexity": 0}, {"id": 9, "context": "The building block of a neural network is a single computational unit. A unit takes a set of real valued numbers as input, performs some computation on them, and produces an output. ", "Bloom_type": "remember", "question": "In a neural network, what does each computational unit do?", "options": ["It applies mathematical operations", "It processes textual data", "It calculates the sum of all inputs", "It generates random outputs"], "complexity": 0}, {"id": 10, "context": "Early in the history of neural networks it was realized that the power of neural networks, as with the real neurons that inspired them, comes from combining these units into larger networks. ", "Bloom_type": "remember", "question": "What is a key characteristic of neural networks?", "options": ["They combine smaller units into larger networks.", "They are based on biological neurons.", "They can only process numerical data.", "They require large amounts of computational resources."], "complexity": 0}, {"id": 11, "context": "In this example we just stipulated the weights in Fig. 7.6. But for real examples the weights for neural networks are learned automatically using the error backpropagation algorithm to be introduced in Section 7.5. That means the hidden layers will learn to form useful representations. This intuition, that neural networks can automatically learn useful representations of the input, is one of their key advantages, and one that we will return to again and again in later chapters. ", "Bloom_type": "remember", "question": "What does the learning process involve when training a neural network?", "options": ["The weights are adjusted based on the error between predicted outputs and actual outcomes.", "The weights are manually set by the user.", "The hidden layers do not participate in forming useful representations.", "The neural network cannot learn any useful representations."], "complexity": 0}, {"id": 12, "context": "That means we can think of a neural network classifier with one hidden layer as building a vector h which is a hidden layer representation of the input, and then running standard multinomial logistic regression on the features that the network develops in h. By contrast, in Chapter 5 the features were mainly designed by hand via feature templates. So a neural network is like multinomial logistic regression, but (a) with many layers, since a deep neural network is like layer after layer of logistic regression classifiers; (b) with those intermediate layers having many possible activation functions (tanh, ReLU, sigmoid) instead of just sigmoid (although we`ll continue to use  for convenience to mean any activation function); (c) rather than forming the features by feature templates, the prior layers of the network induce the feature representations themselves. ", "Bloom_type": "remember", "question": "In what way does a neural network differ from traditional logistic regression?", "options": ["It requires more manual feature design.", "It uses fewer layers.", "It has simpler activation functions.", "It relies solely on predefined templates."], "complexity": 0}, {"id": 13, "context": "Third, gradient descent requires knowing the gradient of the loss function, the vector that contains the partial derivative of the loss function with respect to each of the parameters. In logistic regression, for each observation we could directly compute the derivative of the loss function with respect to an individual w or b. But for neural networks, with millions of parameters in many layers, it`s much harder to see how to compute the partial derivative of some weight in layer 1 when the loss is attached to some much later layer. How do we partial out the loss over all those intermediate layers? The answer is the algorithm called error backpropagation or backward differentiation. ", "Bloom_type": "remember", "question": "In neural networks, what technique is used to calculate the partial derivatives of weights across multiple layers?", "options": ["Backward differentiation", "Gradient descent", "Forward propagation", "Error minimization"], "complexity": 0}, {"id": 14, "context": "The cross-entropy loss that is used in neural networks is the same one we saw for logistic regression. If the neural network is being used as a binary classifier, with the sigmoid at the final layer, the loss function is the same logistic regression loss we saw in Eq. 5.23: ", "Bloom_type": "remember", "question": "In what type of classification problem does the cross-entropy loss used in neural networks match the logistic regression loss described by Eq. 5.23?", "options": ["Binary classification", "Multiclass classification", "Regression analysis", "Clustering"], "complexity": 0}, {"id": 15, "context": "For logistic regression we can initialize gradient descent with all the weights and biases having the value 0. In neural networks, by contrast, we need to initialize the weights with small random numbers. It`s also helpful to normalize the input values to have 0 mean and unit variance. ", "Bloom_type": "remember", "question": "In neural networks, why is it important to initialize the weights with small random numbers?", "options": ["To prevent overfitting of the model", "To ensure the activation function works correctly", "To make the training process faster", "To increase the accuracy of predictions"], "complexity": 0}, {"id": 16, "context": "Tuning of hyperparameters is also important. The parameters of a neural network are the weights W and biases b; those are learned by gradient descent. The hyperparameters are things that are chosen by the algorithm designer; optimal values are tuned on a devset rather than by gradient descent learning on the training set. Hyperparameters include the learning rate , the mini-batch size, the model architecture (the number of layers, the number of hidden nodes per layer, the choice of activation functions), how to regularize, and so on. Gradient descent itself also has many architectural variants such as Adam (Kingma and Ba, 2015). ", "Bloom_type": "remember", "question": "In machine learning, what are hyperparameters?", "options": ["Architecture choices made before training begins", "Parameters that are automatically adjusted during training", "Weights and biases that are learned from data", "Learning rates that determine the speed of convergence"], "complexity": 0}, {"id": 17, "context": " Neural networks are built out of neural units, originally inspired by biological ", "Bloom_type": "remember", "question": "What is an example of a neural unit?", "options": ["Neurons", "Synapses", "Glial cells", "Myelin sheath"], "complexity": 0}, {"id": 18, "context": " The power of neural networks comes from the ability of early layers to learn ", "Bloom_type": "remember", "question": "How do early layers of a neural network contribute to its learning capabilities?", "options": ["By recognizing patterns through unsupervised learning", "By processing raw data directly", "By using complex mathematical equations for classification", "By applying reinforcement learning techniques"], "complexity": 0}, {"id": 19, "context": " Neural networks are trained by optimization algorithms like gradient de", "Bloom_type": "remember", "question": "How do neural networks learn from data?", "options": ["By optimizing weights using gradient descent", "Through supervised learning only", "Using random initialization of parameters", "With no feedback mechanism"], "complexity": 0}, {"id": 20, "context": " Neural language models use a neural network as a probabilistic classifier, to ", "Bloom_type": "remember", "question": "Neural networks are used in which type of model?", "options": ["Classification", "Probability", "Regression", "Prediction"], "complexity": 0}, {"id": 21, "context": "The origins of neural networks lie in the 1940s McCulloch-Pitts neuron (McCulloch and Pitts, 1943), a simplified model of the biological neuron as a kind of computing element that could be described in terms of propositional logic. By the late 1950s and early 1960s, a number of labs (including Frank Rosenblatt at Cornell and Bernard Widrow at Stanford) developed research into neural networks; this phase saw the development of the perceptron (Rosenblatt, 1958), and the transformation of the threshold into a bias, a notation we still use (Widrow and Hoff, 1960). ", "Bloom_type": "remember", "question": "In what decade did researchers begin developing neural networks after the initial work on the McCulloch-Pitts neuron?", "options": ["The 1950s", "The 1920s", "The 1930s", "The 1960s"], "complexity": 0}, {"id": 22, "context": "The field of neural networks declined after it was shown that a single perceptron unit was unable to model functions as simple as XOR (Minsky and Papert, 1969). While some small amount of work continued during the next two decades, a major revival for the field didn`t come until the 1980s, when practical tools for building deeper networks like error backpropagation became widespread (Rumelhart et al., 1986). During the 1980s a wide variety of neural network and related architectures were developed, particularly for applications in psychology and cognitive science (Rumelhart and McClelland 1986b, McClelland and Elman 1986, Rumelhart and McClelland 1986a, Elman 1990), for which the term connectionist or parallel distributed processing was often used (Feldman and Ballard 1982, Smolensky 1988). Many of the principles and techniques developed in this period are foundational to modern work, including the ideas of distributed representations (Hinton, 1986), recurrent networks (Elman, 1990), and the use of tensors for compositionality (Smolensky, 1990). ", "Bloom_type": "remember", "question": "In what decade did the field of neural networks experience a significant resurgence?", "options": ["1980s", "1950s", "1970s", "1990s"], "complexity": 0}, {"id": 23, "context": "By the 1990s larger neural networks began to be applied to many practical language processing tasks as well, like handwriting recognition (LeCun et al. 1989) and speech recognition (Morgan and Bourlard 1990). By the early 2000s, improvements in computer hardware and advances in optimization and training techniques made it possible to train even larger and deeper networks, leading to the modern term deep learning (Hinton et al. 2006, Bengio et al. 2007). We cover more related history in Chapter 8 and Chapter 16. ", "Bloom_type": "remember", "question": "In what decade did advancements in computer hardware and optimization techniques enable the development of large and complex neural networks?", "options": ["The early 2000s", "The late 1980s", "The mid-1990s", "The late 1990s"], "complexity": 0}, {"id": 24, "context": "There are a number of excellent books on the subject. Goldberg (2017) has superb coverage of neural networks for natural language processing. For neural networks in general see Goodfellow et al. (2016) and Nielsen (2015). ", "Bloom_type": "remember", "question": "Which book provides an overview of neural networks suitable for beginners?", "options": ["Nielsen (2015)", "Goldberg (2017)", "Goodfellow et al. (2016)", "None of the above"], "complexity": 0}, {"id": 25, "context": "This chapter introduces a deep learning architecture that offers an alternative way of representing time: recurrent neural networks (RNNs), and their variants like LSTMs. RNNs have a mechanism that deals directly with the sequential nature of language, allowing them to handle the temporal nature of language without the use of arbitrary fixed-sized windows. The recurrent network offers a new way to represent the prior context, in its recurrent connections, allowing the model`s decision to depend on information from hundreds of words in the past. We`ll see how to apply the model to the task of language modeling, to text classification tasks like sentiment analysis, and to sequence modeling tasks like part-of-speech tagging (a task we`ll return to in detail in Chapter 17). ", "Bloom_type": "remember", "question": "What is a key feature of Recurrent Neural Networks (RNNs) compared to other models?", "options": ["They focus solely on the current input without considering previous inputs.", "They can only process static data.", "They require large amounts of labeled training data.", "They are less accurate than traditional models."], "complexity": 0}, {"id": 26, "context": "A recurrent neural network (RNN) is any network that contains a cycle within its network connections, meaning that the value of some unit is directly, or indirectly, dependent on its own earlier outputs as an input. While powerful, such networks are difficult to reason about and to train. However, within the general class of recurrent networks there are constrained architectures that have proven to be extremely effective when applied to language. In this section, we consider a class of recurrent networks referred to as Elman Networks (Elman, 1990) or simple recurrent networks. These networks are useful in their own right and serve as the basis for more complex approaches like the Long Short-Term Memory (LSTM) networks discussed ", "Bloom_type": "remember", "question": "In what type of neural network does each unit depend on its own previous output through cycles?", "options": ["Recurrent Neural Network", "Convolutional Neural Network", "Feedforward Neural Network", "Deep Belief Network"], "complexity": 0}, {"id": 27, "context": "This chapter has introduced the concepts of recurrent neural networks and how they can be applied to language problems. Here`s a summary of the main points that we ", "Bloom_type": "remember", "question": "What is the primary focus of this chapter regarding neural networks?", "options": ["Analyzing their use in natural language processing", "Exploring different types of neural networks", "Discussing their applications in image recognition", "Investigating their role in machine learning algorithms"], "complexity": 0}, {"id": 28, "context": " In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t ", "Bloom_type": "remember", "question": "In what type of neural networks do elements in sequences not all get processed simultaneously but rather sequentially?", "options": ["Recurrent Neural Network", "Convolutional Neural Network", "Feedforward Neural Network", "Deep Belief Network"], "complexity": 0}, {"id": 29, "context": "While theoretically interesting, the difficulty with training RNNs and managing context over long sequences impeded progress on practical applications. This situation changed with the introduction of LSTMs in Hochreiter and Schmidhuber (1997) and Gers et al. (2000). Impressive performance gains were demonstrated on tasks at the boundary of signal processing and language processing including phoneme recognition (Graves and Schmidhuber, 2005), handwriting recognition (Graves et al., 2007) and most significantly speech recognition (Graves et al., 2013). Interest in applying neural networks to practical NLP problems surged with the work of Collobert and Weston (2008) and Collobert et al. (2011). These efforts made use of learned word embeddings, convolutional networks, and end-to-end training. ", "Bloom_type": "remember", "question": "Which breakthrough led to a significant improvement in the practical application of neural networks for natural language processing?", "options": ["The development of deep learning architectures", "The invention of backpropagation algorithm", "The introduction of recurrent neural networks", "The creation of convolutional neural networks"], "complexity": 0}, {"id": 30, "context": "The transformer is a neural network with a specific structure that includes a mechanism called self-attention or multi-head attention.1 Attention can be thought of as a way to build contextual representations of a token`s meaning by attending to and integrating information from surrounding tokens, helping the model learn how tokens relate to each other over large spans. ", "Bloom_type": "remember", "question": "What does the transformer use to integrate information from surrounding tokens?", "options": ["Self-attention", "Convolutional layers", "Pooling layers", "Max pooling"], "complexity": 0}, {"id": 31, "context": "Layer Norm At two stages in the transformer block we normalize the vector (Ba et al., 2016). This process, called layer norm (short for layer normalization), is one of many forms of normalization that can be used to improve training performance in deep neural networks by keeping the values of a hidden layer in a range that facilitates gradient-based training. ", "Bloom_type": "remember", "question": "In which part of the transformer block does layer norm occur?", "options": ["Both Encoder and Decoder", "Encoder", "Decoder", "Neither Encoder nor Decoder"], "complexity": 0}, {"id": 32, "context": "In a very influential series of papers developing the idea of neural language models, (Bengio et al. 2000; Bengio et al. 2003; Bengio et al. 2006), Yoshua Bengio and colleagues drew on the central ideas of both these lines of self-supervised language modeling work, (the discriminatively trained word predictor, and the pretrained embeddings). Like the maxent models of Rosenfeld, Bengio`s model used the next word in running text as its supervision signal. Like the LSA models, Bengio`s model learned an embedding, but unlike the LSA models did it as part of the process of language modeling. The Bengio et al. (2003) model was a neural language model: a neural network that learned to predict the next word from prior words, and did so via learning embeddings as part of the prediction process. ", "Bloom_type": "remember", "question": "In which paper did Yoshua Bengio introduce the concept of neural language models?", "options": ["Rosenfeld's maxent models", "LSA models", "The maxent models of Rosenfeld", "The LSA models"], "complexity": 0}, {"id": 33, "context": "For sequence classification we represent the entire input to be classified by a single vector. We can represent a sequence in various ways. One way is to take the sum or the mean of the last output vector from each token in the sequence. For BERT, we instead add a new unique token to the vocabulary called [CLS], and prepended it to the start of all input sequences, both during pretraining and encoding. The output vector in the final layer of the model for the [CLS] input represents the entire input sequence and serves as the input to a classifier head, a logistic regression or neural network classifier that makes the relevant decision. ", "Bloom_type": "remember", "question": "In sequence classification using a neural network, what does the addition of a special token `[CLS]` do?", "options": ["It allows the model to focus on the beginning of the sequence.", "It adds more tokens to the input sequence.", "It increases the complexity of the model.", "It reduces the size of the input vectors."], "complexity": 0}, {"id": 34, "context": "improvement searches typically use a combination of a fixed number of iterations in combination with a failure to improve after some period to time as stopping criteria. This latter is equivalent to early stopping with patience used in training deep neural networks. ", "Bloom_type": "remember", "question": "In what type of machine learning applications are early stopping techniques commonly applied?", "options": ["Neural Networks", "Support Vector Machines", "Decision Trees", "Random Forests"], "complexity": 0}, {"id": 35, "context": "Neural networks had been applied at various times to various aspects of machine translation; for example Schwenk et al. (2006) showed how to use neural language models to replace n-gram language models in a Spanish-English system based on IBM Model 4. The modern neural encoder-decoder approach was pioneered by Kalchbrenner and Blunsom (2013), who used a CNN encoder and an RNN decoder, and was first applied to MT by Bahdanau et al. (2015). The transformer encoderdecoder was proposed by Vaswani et al. (2017) (see the History section of Chapter 9). ", "Bloom_type": "remember", "question": "In what year did the transformer encoder-decoder architecture first appear in the field of machine translation?", "options": ["2017", "2006", "2013", "2015"], "complexity": 0}, {"id": 36, "context": "By around 1990 neural alternatives to the HMM/GMM architecture for ASR arose, based on a number of earlier experiments with neural networks for phoneme recognition and other speech tasks. Architectures included the time-delay neural network (TDNN)the first use of convolutional networks for speech (Waibel et al. 1989, Lang et al. 1990), RNNs (Robinson and Fallside, 1991), and the hybrid HMM/MLP architecture in which a feedforward neural network is trained as a phonetic classifier whose outputs are used as probability estimates for an HMM-based architecture (Morgan and Bourlard 1990, Bourlard and Morgan 1994, Morgan and Bourlard 1995). ", "Bloom_type": "remember", "question": "In what year did the first use of convolutional networks for speech occur?", "options": ["1989", "1975", "1982", "1990"], "complexity": 0}, {"id": 37, "context": "Over the next two decades a combination of Moore`s law and the rise of GPUs allowed deep neural networks with many layers. Performance was getting close to traditional systems on smaller tasks like TIMIT phone recognition by 2009 (Mohamed et al., 2009), and by 2012, the performance of hybrid systems had surpassed traditional HMM/GMM systems (Jaitly et al. 2012, Dahl et al. 2012, inter alia). Originally it seemed that unsupervised pretraining of the networks using a technique like deep belief networks was important, but by 2013, it was clear that for hybrid HMM/GMM feedforward networks, all that mattered was to use a lot of data and enough layers, although a few other components did improve performance: using log mel features instead of MFCCs, using dropout, and using rectified linear units (Deng et al. 2013, Maas et al. 2013, Dahl et al. 2013). ", "Bloom_type": "remember", "question": "In what year did the performance of hybrid systems surpass traditional HMM/GMM systems?", "options": ["2012", "2009", "2013", "2015"], "complexity": 0}, {"id": 38, "context": "Instead linguistic structure plays a number of new roles. One important role is for interpretability: to provide a useful interpretive lens on neural networks. Knowing that a particular layer or neuron may be computing something related to a particular kind of structure can help us break open the black box` and understand what the components of our language models are doing. ", "Bloom_type": "remember", "question": "In neural networks, why does knowing which layers or neurons compute specific structures help with understanding?", "options": ["It provides insights into how different parts of the model interact with each other.", "It allows direct access to the underlying hardware.", "It simplifies the training process by reducing computational complexity.", "It enables faster processing speeds."], "complexity": 0}, {"id": 39, "context": "The earliest disambiguation algorithms for parsing were based on probabilistic context-free grammars, first worked out by Booth (1969) and Salomaa (1969); see Appendix C for more history. Neural methods were first applied to parsing at around the same time as statistical parsing methods were developed (Henderson, 1994). In the earliest work neural networks were used to estimate some of the probabilities for statistical constituency parsers (Henderson, 2003, 2004; Emami and Jelinek, 2005) . The next decades saw a wide variety of neural parsing algorithms, including recursive neural architectures (Socher et al., 2011, 2013), encoder-decoder models (Vinyals et al., 2015; Choe and Charniak, 2016), and the idea of focusing on spans (Cross and Huang, 2016). For more on the span-based self-attention approach we describe in this chapter see Stern et al. (2017), Gaddy et al. (2018), Kitaev and Klein (2018), and Kitaev et al. (2019). See Chapter 20 for the parallel history of neural dependency parsing. ", "Bloom_type": "remember", "question": "In which decade did the development of neural parsing algorithms begin?", "options": ["1990s", "1960s", "1970s", "1980s"], "complexity": 0}, {"id": 40, "context": "State-of-the-art graph-based multilingual parsers are based on neural networks. Instead of extracting hand-designed features to represent each edge between words wi and w j, these parsers run the sentence through an encoder, and then pass the encoded representation of the two words wi and w j through a network that estimates a score for the edge i ", "Bloom_type": "remember", "question": "What is a key difference between traditional parser methods and state-of-the-art graph-based multilingual parsers?", "options": ["Traditional parsers use hand-crafted features instead of neural networks.", "Traditional parsers rely solely on syntactic rules, while neural networks handle semantic aspects.", "Neural networks require more computational resources than traditional parsers.", "Traditional parsers can only process one language at a time, whereas neural networks support multilingual processing."], "complexity": 0}, {"id": 41, "context": "The modern approach is instead to perform mention detection, anaphoricity, and coreference jointly in a single end-to-end model (Ng 2005b, Denis and Baldridge 2007, Rahman and Ng 2009). For example mention detection in the Lee et al. (2017b),2018 system is based on a single end-to-end neural network that computes a score for each mention being referential, a score for two mentions being coreference, and combines them to make a decision, training all these scores with a single end-to-end loss. We`ll describe this method in detail in Section 23.6. 7 ", "Bloom_type": "remember", "question": "In what way does the modern approach differ from traditional methods of detecting mentions, anaphors, and coreferences?", "options": ["It employs a single end-to-end neural network for all tasks.", "It uses multiple separate models for each task.", "It relies solely on rule-based systems.", "It requires manual annotation for data preparation."], "complexity": 0}, {"id": 42, "context": "In the first part of the book we introduce the fundamental suite of algorithmic tools that make up the modern neural language model that is the heart of end-to-end NLP systems. We begin with tokenization and preprocessing, as well as useful algorithms like computing edit distance, and then proceed to the tasks of classification, logistic regression, neural networks, proceeding through feedforward networks, recurrent networks, and then transformers. We`ll also see the role of embeddings as a model of word meaning. ", "Bloom_type": "comprehension", "question": "What are some key concepts covered before moving into neural networks?", "options": ["Tokenization and preprocessing, edit distance computation, and logistic regression", "Classification, feedforward networks, and transformer models", "Logistic regression, feedforward networks, and transformers", "Preprocessing, edit distance computation, and logistic regression"], "complexity": 1}, {"id": 43, "context": "Large language models are based on neural networks rather than n-grams, enabling them to solve the two major problems with n-grams: (1) the number of parameters increases exponentially as the n-gram order increases, and (2) n-grams have no way to generalize from training examples to test set examples unless they use identical words. Neural language models instead project words into a continuous space in which words with similar contexts have similar representations. We`ll introduce transformer-based large language models in Chapter 9, along the way introducing feedforward language models (Bengio et al. 2006, Schwenk 2007) in Chapter 7 and recurrent language models (Mikolov, 2012) in Chapter 8. ", "Bloom_type": "comprehension", "question": "Explain how neural networks differ from traditional n-gram models in their approach to handling word similarity and generalization?", "options": ["Neural networks project words into a continuous space where words with similar contexts have different representations.", "Traditional n-gram models use continuous spaces for representing words.", "Both neural networks and n-gram models rely solely on identical words for generalization.", "Feedforward language models do not handle word similarity effectively."], "complexity": 1}, {"id": 44, "context": "In this chapter we introduce an algorithm that is admirably suited for discovering the link between features or clues and some particular outcome: logistic regression. Indeed, logistic regression is one of the most important analytic tools in the social and natural sciences. In natural language processing, logistic regression is the baseline supervised machine learning algorithm for classification, and also has a very close relationship with neural networks. As we will see in Chapter 7, a neural network can be viewed as a series of logistic regression classifiers stacked on top of each other. Thus the classification and machine learning techniques introduced here will play an important role throughout the book. ", "Bloom_type": "comprehension", "question": "Explain how neural networks are related to logistic regression?", "options": ["Neural networks use logistic regression as their primary method of classification.", "Logistic regression is a type of neural network.", "Logistic regression is a subset of neural networks.", "Neural networks are unrelated to logistic regression."], "complexity": 1}, {"id": 45, "context": "Having input data with comparable range is useful when comparing values across features. Data scaling is especially important in large neural networks, since it helps speed up gradient descent. ", "Bloom_type": "comprehension", "question": "Explain why data scaling is particularly crucial for large neural networks?", "options": ["To ensure all features have similar ranges which aids in faster convergence during training.", "Data scaling reduces the computational complexity by simplifying calculations.", "Scaling ensures that all features are integers, making computations easier.", "It prevents overfitting by normalizing feature scales."], "complexity": 1}, {"id": 46, "context": "For logistic regression, this loss function is conveniently convex. A convex function has at most one minimum; there are no local minima to get stuck in, so gradient descent starting from any point is guaranteed to find the minimum. (By contrast, the loss for multi-layer neural networks is non-convex, and gradient descent may get stuck in local minima for neural network training and never find the global optimum.) ", "Bloom_type": "comprehension", "question": "Explain why gradient descent might fail to converge when applied to the loss function of a multi-layer neural network?", "options": ["Multi-layer neural networks have more complex structures.", "The loss function is always convex.", "Gradient descent can only handle linear functions.", "Neural networks do not require optimization."], "complexity": 1}, {"id": 47, "context": "The revolutionary intuition here is that we can just use running text as implicitly supervised training data for such a classifier; a word c that occurs near the target word apricot acts as gold correct answer` to the question Is word c likely to show up near apricot? This method, often called self-supervision, avoids the need for any sort of hand-labeled supervision signal. This idea was first proposed in the task of neural language modeling, when Bengio et al. (2003) and Collobert et al. (2011) showed that a neural language model (a neural network that learned to predict the next word from prior words) could just use the next word in running text as its supervision signal, and could be used to learn an embedding representation for each word as part of doing this prediction task. ", "Bloom_type": "comprehension", "question": "How does the concept of self-supervised learning apply to neural networks?", "options": ["By predicting the next word in a sentence based on previous words", "By using labeled images as input data", "By generating synthetic data to train the network", "By directly classifying unseen test data"], "complexity": 1}, {"id": 48, "context": "Instead, a modern neural network is a network of small computing units, each of which takes a vector of input values and produces a single output value. In this chapter we introduce the neural net applied to classification. The architecture we introduce is called a feedforward network because the computation proceeds iteratively from one layer of units to the next. The use of modern neural nets is often called deep learning, because modern networks are often deep (have many layers). Neural networks share much of the same mathematics as logistic regression. But neural networks are a more powerful classifier than logistic regression, and indeed a minimal neural network (technically one with a single hidden layer`) can be shown to learn any function. ", "Bloom_type": "comprehension", "question": "What distinguishes modern neural networks from traditional ones?", "options": ["Modern neural networks are based on different mathematical principles.", "Modern neural networks have fewer layers.", "Modern neural networks use simpler algorithms.", "Modern neural networks are less efficient."], "complexity": 1}, {"id": 49, "context": "Neural net classifiers are different from logistic regression in another way. With logistic regression, we applied the regression classifier to many different tasks by developing many rich kinds of feature templates based on domain knowledge. When working with neural networks, it is more common to avoid most uses of rich handderived features, instead building neural networks that take raw words as inputs and learn to induce features as part of the process of learning to classify. We saw examples of this kind of representation learning for embeddings in Chapter 6. Nets that are very deep are particularly good at representation learning. For that reason deep neural nets are the right tool for tasks that offer sufficient data to learn features automatically. ", "Bloom_type": "comprehension", "question": "How do neural networks differ from logistic regression when applying to classification tasks?", "options": ["Neural networks avoid using rich hand-derived features, focusing on learning features directly from input data.", "Logistic regression requires less data than neural networks.", "Neural networks use pre-defined feature templates, while logistic regression does not.", "Logistic regression can handle complex relationships between variables better than neural networks."], "complexity": 1}, {"id": 50, "context": "In this chapter we`ll introduce feedforward networks as classifiers, and also apply them to the simple task of language modeling: assigning probabilities to word sequences and predicting upcoming words. In subsequent chapters we`ll introduce many other aspects of neural models, such as recurrent neural networks (Chapter 8), the Transformer (Chapter 9), and masked language modeling (Chapter 11). ", "Bloom_type": "comprehension", "question": "What are some additional topics covered after introducing feedforward networks in this chapter?", "options": ["All of the above", "Recurrent Neural Networks (Chapter 8)", "The Transformer (Chapter 9)", "Masked Language Modeling (Chapter 11)"], "complexity": 1}, {"id": 51, "context": "The building block of a neural network is a single computational unit. A unit takes a set of real valued numbers as input, performs some computation on them, and produces an output. ", "Bloom_type": "comprehension", "question": "What does each component of a neural network do?", "options": ["Each component receives inputs from other components and outputs are processed by further units.", "Each component processes data through complex algorithms.", "Each component calculates probabilities for different outcomes based on input values.", "Each component combines all previous computations into a final decision."], "complexity": 1}, {"id": 52, "context": "Early in the history of neural networks it was realized that the power of neural networks, as with the real neurons that inspired them, comes from combining these units into larger networks. ", "Bloom_type": "comprehension", "question": "Explain how neural networks combine smaller units into larger networks for their power?", "options": ["Neurons are grouped together based on similar functions.", "Neurons are connected randomly.", "Each neuron processes data independently before being combined.", "The output of each neuron is averaged across all inputs."], "complexity": 1}, {"id": 53, "context": "In this example we just stipulated the weights in Fig. 7.6. But for real examples the weights for neural networks are learned automatically using the error backpropagation algorithm to be introduced in Section 7.5. That means the hidden layers will learn to form useful representations. This intuition, that neural networks can automatically learn useful representations of the input, is one of their key advantages, and one that we will return to again and again in later chapters. ", "Bloom_type": "comprehension", "question": "What does the learning process in neural networks involve?", "options": ["Learning weights through an algorithm", "Stipulating initial weights manually", "Forming representations by hand", "Repeating the same task repeatedly"], "complexity": 1}, {"id": 54, "context": "That means we can think of a neural network classifier with one hidden layer as building a vector h which is a hidden layer representation of the input, and then running standard multinomial logistic regression on the features that the network develops in h. By contrast, in Chapter 5 the features were mainly designed by hand via feature templates. So a neural network is like multinomial logistic regression, but (a) with many layers, since a deep neural network is like layer after layer of logistic regression classifiers; (b) with those intermediate layers having many possible activation functions (tanh, ReLU, sigmoid) instead of just sigmoid (although we`ll continue to use  for convenience to mean any activation function); (c) rather than forming the features by feature templates, the prior layers of the network induce the feature representations themselves. ", "Bloom_type": "comprehension", "question": "How does a neural network differ from traditional multinomial logistic regression in terms of its approach to feature development?", "options": ["Neural networks do not rely on feature templates and instead learn features through their own layers.", "Neural networks develop features manually using feature templates.", "Traditional multinomial logistic regression uses many layers of logistic regression classifiers.", "Both approaches are identical in how they handle feature development."], "complexity": 1}, {"id": 55, "context": "Third, gradient descent requires knowing the gradient of the loss function, the vector that contains the partial derivative of the loss function with respect to each of the parameters. In logistic regression, for each observation we could directly compute the derivative of the loss function with respect to an individual w or b. But for neural networks, with millions of parameters in many layers, it`s much harder to see how to compute the partial derivative of some weight in layer 1 when the loss is attached to some much later layer. How do we partial out the loss over all those intermediate layers? The answer is the algorithm called error backpropagation or backward differentiation. ", "Bloom_type": "comprehension", "question": "How does error backpropagation help in computing the partial derivatives of weights in deep neural networks?", "options": ["It computes the gradients by summing up the errors from all previous layers.", "It simplifies the computation by reducing the number of parameters.", "It allows direct calculation of gradients using only the output layer.", "It uses a technique called forward propagation to calculate the gradients."], "complexity": 1}, {"id": 56, "context": "The cross-entropy loss that is used in neural networks is the same one we saw for logistic regression. If the neural network is being used as a binary classifier, with the sigmoid at the final layer, the loss function is the same logistic regression loss we saw in Eq. 5.23: ", "Bloom_type": "comprehension", "question": "What is the response when using a neural network as a binary classifier?", "options": ["The same cross-entropy loss as in logistic regression", "A different type of loss function", "No loss function is used", "The output layer uses a softmax instead of a sigmoid"], "complexity": 1}, {"id": 57, "context": "For logistic regression we can initialize gradient descent with all the weights and biases having the value 0. In neural networks, by contrast, we need to initialize the weights with small random numbers. It`s also helpful to normalize the input values to have 0 mean and unit variance. ", "Bloom_type": "comprehension", "question": "How do we typically initialize the weights in neural networks compared to logistic regression?", "options": ["Logistic regression initializes weights with all zeros, while neural networks use small random numbers.", "Neural networks initialize weights with all zeros, whereas logistic regression uses small random numbers.", "Both logistic regression and neural networks initialize weights with all zeros.", "There is no difference; both methods initialize weights with all zeros."], "complexity": 1}, {"id": 58, "context": "Tuning of hyperparameters is also important. The parameters of a neural network are the weights W and biases b; those are learned by gradient descent. The hyperparameters are things that are chosen by the algorithm designer; optimal values are tuned on a devset rather than by gradient descent learning on the training set. Hyperparameters include the learning rate , the mini-batch size, the model architecture (the number of layers, the number of hidden nodes per layer, the choice of activation functions), how to regularize, and so on. Gradient descent itself also has many architectural variants such as Adam (Kingma and Ba, 2015). ", "Bloom_type": "comprehension", "question": "What aspects of tuning hyperparameters are typically done manually by the algorithm designer?", "options": ["Hyperparameters including learning rate, mini-batch size, model architecture, regularization techniques, and more", "The learning rate, mini-batch size, and regularization methods only", "Gradient descent algorithms and their variants exclusively", "Tuning hyperparameters is irrelevant for neural networks"], "complexity": 1}, {"id": 59, "context": "Finally, most modern neural networks are built using computation graph formalisms that make it easy and natural to do gradient computation and parallelization on vector-based GPUs (Graphic Processing Units). PyTorch (Paszke et al., 2017) and TensorFlow (Abadi et al., 2015) are two of the most popular. The interested reader should consult a neural network textbook for further details; some suggestions are at the end of the chapter. ", "Bloom_type": "comprehension", "question": "What makes modern neural networks easier to compute and parallelize?", "options": ["Both A and B", "The use of computation graphs", "Vector-based GPUs", "Neither A nor B"], "complexity": 1}, {"id": 60, "context": " Neural networks are built out of neural units, originally inspired by biological ", "Bloom_type": "comprehension", "question": "What do you think inspired the creation of neural networks?", "options": ["The study of human brain functions", "The need for faster computers", "The desire for more complex algorithms", "The search for artificial intelligence"], "complexity": 1}, {"id": 61, "context": " The power of neural networks comes from the ability of early layers to learn ", "Bloom_type": "comprehension", "question": "What aspect of neural networks allows for their powerful learning capabilities?", "options": ["The depth of hidden layers", "The complexity of architecture", "The speed of computation", "The amount of training data"], "complexity": 1}, {"id": 62, "context": " Neural networks are trained by optimization algorithms like gradient de", "Bloom_type": "comprehension", "question": "What is one way neural networks are typically trained?", "options": ["Using gradient descent algorithms", "Manual input adjustment", "Analyzing historical data", "Both B) and C)"], "complexity": 1}, {"id": 63, "context": " Neural language models use a neural network as a probabilistic classifier, to ", "Bloom_type": "comprehension", "question": "What does using a neural network for a probabilistic classifier entail?", "options": ["It enhances accuracy by learning patterns from large datasets.", "It involves training the model through backpropagation.", "It requires manual input for every classification task.", "It simplifies complex data into binary classifications."], "complexity": 1}, {"id": 64, "context": "The origins of neural networks lie in the 1940s McCulloch-Pitts neuron (McCulloch and Pitts, 1943), a simplified model of the biological neuron as a kind of computing element that could be described in terms of propositional logic. By the late 1950s and early 1960s, a number of labs (including Frank Rosenblatt at Cornell and Bernard Widrow at Stanford) developed research into neural networks; this phase saw the development of the perceptron (Rosenblatt, 1958), and the transformation of the threshold into a bias, a notation we still use (Widrow and Hoff, 1960). ", "Bloom_type": "comprehension", "question": "What was one of the key developments in the history of neural networks during the late 1950s and early 1960s?", "options": ["The development of the perceptron by Frank Rosenblatt", "The introduction of artificial neurons", "The creation of the first deep learning models", "The invention of backpropagation algorithm"], "complexity": 1}, {"id": 65, "context": "The field of neural networks declined after it was shown that a single perceptron unit was unable to model functions as simple as XOR (Minsky and Papert, 1969). While some small amount of work continued during the next two decades, a major revival for the field didn`t come until the 1980s, when practical tools for building deeper networks like error backpropagation became widespread (Rumelhart et al., 1986). During the 1980s a wide variety of neural network and related architectures were developed, particularly for applications in psychology and cognitive science (Rumelhart and McClelland 1986b, McClelland and Elman 1986, Rumelhart and McClelland 1986a, Elman 1990), for which the term connectionist or parallel distributed processing was often used (Feldman and Ballard 1982, Smolensky 1988). Many of the principles and techniques developed in this period are foundational to modern work, including the ideas of distributed representations (Hinton, 1986), recurrent networks (Elman, 1990), and the use of tensors for compositionality (Smolensky, 1990). ", "Bloom_type": "comprehension", "question": "What significant event marked a major revival in the field of neural networks?", "options": ["The publication of Minsky and Papert's book", "The development of deep learning algorithms", "The introduction of artificial neurons", "The invention of the perceptron unit"], "complexity": 1}, {"id": 66, "context": "By the 1990s larger neural networks began to be applied to many practical language processing tasks as well, like handwriting recognition (LeCun et al. 1989) and speech recognition (Morgan and Bourlard 1990). By the early 2000s, improvements in computer hardware and advances in optimization and training techniques made it possible to train even larger and deeper networks, leading to the modern term deep learning (Hinton et al. 2006, Bengio et al. 2007). We cover more related history in Chapter 8 and Chapter 16. ", "Bloom_type": "comprehension", "question": "What was one significant development in the field of neural networks around the late 1990s and early 2000s?", "options": ["The emergence of large-scale neural networks capable of handling complex tasks", "The use of smaller neural networks for less complex tasks", "The introduction of backpropagation algorithm", "The decline in computational power needed for training"], "complexity": 1}, {"id": 67, "context": "There are a number of excellent books on the subject. Goldberg (2017) has superb coverage of neural networks for natural language processing. For neural networks in general see Goodfellow et al. (2016) and Nielsen (2015). ", "Bloom_type": "comprehension", "question": "Which book provides comprehensive coverage of neural networks for general applications?", "options": ["Both B) and C)", "Goldberg (2017)", "Goodfellow et al. (2016)", "Nielsen (2015)"], "complexity": 1}, {"id": 68, "context": "This chapter introduces a deep learning architecture that offers an alternative way of representing time: recurrent neural networks (RNNs), and their variants like LSTMs. RNNs have a mechanism that deals directly with the sequential nature of language, allowing them to handle the temporal nature of language without the use of arbitrary fixed-sized windows. The recurrent network offers a new way to represent the prior context, in its recurrent connections, allowing the model`s decision to depend on information from hundreds of words in the past. We`ll see how to apply the model to the task of language modeling, to text classification tasks like sentiment analysis, and to sequence modeling tasks like part-of-speech tagging (a task we`ll return to in detail in Chapter 17). ", "Bloom_type": "comprehension", "question": "How do Recurrent Neural Networks (RNNs) differ from traditional feedforward neural networks in handling sequential data?", "options": ["Traditional feedforward networks require fixed-size input windows, unlike RNNs which adaptively update weights based on the current input.", "RNNs are less efficient at processing sequences compared to feedforward networks.", "Feedforward networks can only process static inputs, whereas RNNs can handle sequences.", "Both types of networks perform equally well on all types of sequential data."], "complexity": 1}, {"id": 69, "context": "A recurrent neural network (RNN) is any network that contains a cycle within its network connections, meaning that the value of some unit is directly, or indirectly, dependent on its own earlier outputs as an input. While powerful, such networks are difficult to reason about and to train. However, within the general class of recurrent networks there are constrained architectures that have proven to be extremely effective when applied to language. In this section, we consider a class of recurrent networks referred to as Elman Networks (Elman, 1990) or simple recurrent networks. These networks are useful in their own right and serve as the basis for more complex approaches like the Long Short-Term Memory (LSTM) networks discussed ", "Bloom_type": "comprehension", "question": "What distinguishes Elman Networks from other types of recurrent neural networks?", "options": ["Their architecture allows for easier reasoning and training compared to other types.", "They do not contain cycles within their network connections.", "They use fewer layers than LSTM networks.", "They cannot handle sequential data effectively."], "complexity": 1}, {"id": 70, "context": "This chapter has introduced the concepts of recurrent neural networks and how they can be applied to language problems. Here`s a summary of the main points that we ", "Bloom_type": "comprehension", "question": "What are some key aspects covered in this chapter regarding recurrent neural networks?", "options": ["The principles behind recurrent neural networks", "The history of neural networks", "The applications of neural networks in image recognition", "The development of deep learning models"], "complexity": 1}, {"id": 71, "context": " In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t ", "Bloom_type": "comprehension", "question": "How do RNNs handle sequence data?", "options": ["RNNs update their state using previous outputs.", "RNNs process all elements simultaneously.", "RNNs use only the current input for processing.", "RNNs ignore past inputs."], "complexity": 1}, {"id": 72, "context": "While theoretically interesting, the difficulty with training RNNs and managing context over long sequences impeded progress on practical applications. This situation changed with the introduction of LSTMs in Hochreiter and Schmidhuber (1997) and Gers et al. (2000). Impressive performance gains were demonstrated on tasks at the boundary of signal processing and language processing including phoneme recognition (Graves and Schmidhuber, 2005), handwriting recognition (Graves et al., 2007) and most significantly speech recognition (Graves et al., 2013). Interest in applying neural networks to practical NLP problems surged with the work of Collobert and Weston (2008) and Collobert et al. (2011). These efforts made use of learned word embeddings, convolutional networks, and end-to-end training. ", "Bloom_type": "comprehension", "question": "What was one significant improvement in training RNNs mentioned in the context?", "options": ["Improving gradient flow", "Reducing computational complexity", "Increasing memory usage", "Decreasing data dependency"], "complexity": 1}, {"id": 73, "context": "The transformer is a neural network with a specific structure that includes a mechanism called self-attention or multi-head attention.1 Attention can be thought of as a way to build contextual representations of a token`s meaning by attending to and integrating information from surrounding tokens, helping the model learn how tokens relate to each other over large spans. ", "Bloom_type": "comprehension", "question": "What does the self-attention mechanism in transformers do?", "options": ["It helps the model understand the importance of individual tokens.", "It allows the model to focus on only one token at a time.", "It integrates information from all previous tokens into the current token.", "It creates a fixed representation for every token."], "complexity": 1}, {"id": 74, "context": "Layer Norm At two stages in the transformer block we normalize the vector (Ba et al., 2016). This process, called layer norm (short for layer normalization), is one of many forms of normalization that can be used to improve training performance in deep neural networks by keeping the values of a hidden layer in a range that facilitates gradient-based training. ", "Bloom_type": "comprehension", "question": "What does layer norm do in the context of neural networks?", "options": ["It prevents vanishing gradients.", "It normalizes the weights of the neurons.", "It increases the complexity of the model.", "It enhances the learning rate during training."], "complexity": 1}, {"id": 75, "context": "In a very influential series of papers developing the idea of neural language models, (Bengio et al. 2000; Bengio et al. 2003; Bengio et al. 2006), Yoshua Bengio and colleagues drew on the central ideas of both these lines of self-supervised language modeling work, (the discriminatively trained word predictor, and the pretrained embeddings). Like the maxent models of Rosenfeld, Bengio`s model used the next word in running text as its supervision signal. Like the LSA models, Bengio`s model learned an embedding, but unlike the LSA models did it as part of the process of language modeling. The Bengio et al. (2003) model was a neural language model: a neural network that learned to predict the next word from prior words, and did so via learning embeddings as part of the prediction process. ", "Bloom_type": "comprehension", "question": "What distinguishes the Bengio et al. (2003) model from previous neural language models?", "options": ["It incorporated embeddings into the language modeling process.", "It utilized pretrained embeddings.", "It employed discriminative training for word predictions.", "It focused solely on predicting the next word."], "complexity": 1}, {"id": 76, "context": "For sequence classification we represent the entire input to be classified by a single vector. We can represent a sequence in various ways. One way is to take the sum or the mean of the last output vector from each token in the sequence. For BERT, we instead add a new unique token to the vocabulary called [CLS], and prepended it to the start of all input sequences, both during pretraining and encoding. The output vector in the final layer of the model for the [CLS] input represents the entire input sequence and serves as the input to a classifier head, a logistic regression or neural network classifier that makes the relevant decision. ", "Bloom_type": "comprehension", "question": "What does the output vector from the [CLS] token in BERT serve as?", "options": ["The representation of the entire input sequence", "The average of all tokens in the sequence", "The total sum of all tokens in the sequence", "The individual outputs of each token"], "complexity": 1}, {"id": 77, "context": "improvement searches typically use a combination of a fixed number of iterations in combination with a failure to improve after some period to time as stopping criteria. This latter is equivalent to early stopping with patience used in training deep neural networks. ", "Bloom_type": "comprehension", "question": "What does the response describe about stopping criteria for improvement searches?", "options": ["It explains how early stopping with patience works in training deep neural networks.", "It compares early stopping with patience in training shallow neural networks.", "It contrasts fixed iteration numbers with failure to improve periods in search methods.", "It discusses the differences between shallow and deep neural networks."], "complexity": 1}, {"id": 78, "context": "Neural networks had been applied at various times to various aspects of machine translation; for example Schwenk et al. (2006) showed how to use neural language models to replace n-gram language models in a Spanish-English system based on IBM Model 4. The modern neural encoder-decoder approach was pioneered by Kalchbrenner and Blunsom (2013), who used a CNN encoder and an RNN decoder, and was first applied to MT by Bahdanau et al. (2015). The transformer encoderdecoder was proposed by Vaswani et al. (2017) (see the History section of Chapter 9). ", "Bloom_type": "comprehension", "question": "What type of neural network was initially applied to machine translation by Bahdanau et al. (2015)?", "options": ["Transformer EncoderDecoder", "Modern Neural EncoderDecoder", "CNN Encoder", "RNN Decoder"], "complexity": 1}, {"id": 79, "context": "By around 1990 neural alternatives to the HMM/GMM architecture for ASR arose, based on a number of earlier experiments with neural networks for phoneme recognition and other speech tasks. Architectures included the time-delay neural network (TDNN)the first use of convolutional networks for speech (Waibel et al. 1989, Lang et al. 1990), RNNs (Robinson and Fallside, 1991), and the hybrid HMM/MLP architecture in which a feedforward neural network is trained as a phonetic classifier whose outputs are used as probability estimates for an HMM-based architecture (Morgan and Bourlard 1990, Bourlard and Morgan 1994, Morgan and Bourlard 1995). ", "Bloom_type": "comprehension", "question": "What was one of the earliest uses of neural networks in speech processing before the development of HMM/GMM architectures?", "options": ["Time-delay neural networks", "HMM/GMM architectures", "Convolutional networks", "Hybrid HMM/MLP architecture"], "complexity": 1}, {"id": 80, "context": "Over the next two decades a combination of Moore`s law and the rise of GPUs allowed deep neural networks with many layers. Performance was getting close to traditional systems on smaller tasks like TIMIT phone recognition by 2009 (Mohamed et al., 2009), and by 2012, the performance of hybrid systems had surpassed traditional HMM/GMM systems (Jaitly et al. 2012, Dahl et al. 2012, inter alia). Originally it seemed that unsupervised pretraining of the networks using a technique like deep belief networks was important, but by 2013, it was clear that for hybrid HMM/GMM feedforward networks, all that mattered was to use a lot of data and enough layers, although a few other components did improve performance: using log mel features instead of MFCCs, using dropout, and using rectified linear units (Deng et al. 2013, Maas et al. 2013, Dahl et al. 2013). ", "Bloom_type": "comprehension", "question": "What significant improvements were made to hybrid HMM/GMM feedforward networks after 2012?", "options": ["Improving feature extraction techniques", "Using more layers and increasing computational power", "Enhancing training algorithms", "Reducing the number of parameters in the model"], "complexity": 1}, {"id": 81, "context": "Instead linguistic structure plays a number of new roles. One important role is for interpretability: to provide a useful interpretive lens on neural networks. Knowing that a particular layer or neuron may be computing something related to a particular kind of structure can help us break open the black box` and understand what the components of our language models are doing. ", "Bloom_type": "comprehension", "question": "What does it mean when we say knowing which layer or neuron computes something about a specific type of structure helps us understand how our language models work?", "options": ["It allows us to see through the model\u2019s complexity.", "It makes the model more difficult to use.", "It increases the model\u2019s computational power.", "It decreases the accuracy of predictions."], "complexity": 1}, {"id": 82, "context": "The earliest disambiguation algorithms for parsing were based on probabilistic context-free grammars, first worked out by Booth (1969) and Salomaa (1969); see Appendix C for more history. Neural methods were first applied to parsing at around the same time as statistical parsing methods were developed (Henderson, 1994). In the earliest work neural networks were used to estimate some of the probabilities for statistical constituency parsers (Henderson, 2003, 2004; Emami and Jelinek, 2005) . The next decades saw a wide variety of neural parsing algorithms, including recursive neural architectures (Socher et al., 2011, 2013), encoder-decoder models (Vinyals et al., 2015; Choe and Charniak, 2016), and the idea of focusing on spans (Cross and Huang, 2016). For more on the span-based self-attention approach we describe in this chapter see Stern et al. (2017), Gaddy et al. (2018), Kitaev and Klein (2018), and Kitaev et al. (2019). See Chapter 20 for the parallel history of neural dependency parsing. ", "Bloom_type": "comprehension", "question": "What was the primary application of neural networks in early parsing algorithms?", "options": ["Statistical constituency parsing", "Recursive neural architectures", "Encoder-decoder models", "Span-based self-attention approaches"], "complexity": 1}, {"id": 83, "context": "State-of-the-art graph-based multilingual parsers are based on neural networks. Instead of extracting hand-designed features to represent each edge between words wi and w j, these parsers run the sentence through an encoder, and then pass the encoded representation of the two words wi and w j through a network that estimates a score for the edge i ", "Bloom_type": "comprehension", "question": "What is the primary difference between traditional feature extraction methods and modern approaches using neural networks in parsing sentences?", "options": ["Traditional methods use pre-defined rules, while neural networks learn from data.", "Neural networks require more computational resources than traditional methods.", "Traditional methods focus on syntax analysis, whereas neural networks handle semantics.", "Neither A nor B is true."], "complexity": 1}, {"id": 84, "context": "The modern approach is instead to perform mention detection, anaphoricity, and coreference jointly in a single end-to-end model (Ng 2005b, Denis and Baldridge 2007, Rahman and Ng 2009). For example mention detection in the Lee et al. (2017b),2018 system is based on a single end-to-end neural network that computes a score for each mention being referential, a score for two mentions being coreference, and combines them to make a decision, training all these scores with a single end-to-end loss. We`ll describe this method in detail in Section 23.6. 7 ", "Bloom_type": "comprehension", "question": "What does the modern approach involve when it comes to performing mention detection, anaphoricity, and coreference?", "options": ["Combining these tasks into one end-to-end model", "Performing these tasks separately using different models", "Using traditional methods exclusively", "Splitting the task into smaller subtasks"], "complexity": 1}, {"id": 85, "context": "In the first part of the book we introduce the fundamental suite of algorithmic tools that make up the modern neural language model that is the heart of end-to-end NLP systems. We begin with tokenization and preprocessing, as well as useful algorithms like computing edit distance, and then proceed to the tasks of classification, logistic regression, neural networks, proceeding through feedforward networks, recurrent networks, and then transformers. We`ll also see the role of embeddings as a model of word meaning. ", "Bloom_type": "application", "question": "What is the next step after introducing the fundamental suite of algorithmic tools for building a neural network?", "options": ["Introduce the concept of convolutional layers", "Discuss the history of neural networks", "Explain how to implement an artificial neuron", "Describe the importance of backpropagation"], "complexity": 2}, {"id": 86, "context": "Large language models are based on neural networks rather than n-grams, enabling them to solve the two major problems with n-grams: (1) the number of parameters increases exponentially as the n-gram order increases, and (2) n-grams have no way to generalize from training examples to test set examples unless they use identical words. Neural language models instead project words into a continuous space in which words with similar contexts have similar representations. We`ll introduce transformer-based large language models in Chapter 9, along the way introducing feedforward language models (Bengio et al. 2006, Schwenk 2007) in Chapter 7 and recurrent language models (Mikolov, 2012) in Chapter 8. ", "Bloom_type": "application", "question": "What is the main difference between traditional n-gram models and neural language models?", "options": ["Neural language models project words into a continuous space.", "Traditional n-gram models require more computational resources.", "Neural language models can handle larger datasets better.", "Neural language models cannot generalize well."], "complexity": 2}, {"id": 87, "context": "In this chapter we introduce an algorithm that is admirably suited for discovering the link between features or clues and some particular outcome: logistic regression. Indeed, logistic regression is one of the most important analytic tools in the social and natural sciences. In natural language processing, logistic regression is the baseline supervised machine learning algorithm for classification, and also has a very close relationship with neural networks. As we will see in Chapter 7, a neural network can be viewed as a series of logistic regression classifiers stacked on top of each other. Thus the classification and machine learning techniques introduced here will play an important role throughout the book. ", "Bloom_type": "application", "question": "What is the fundamental difference between logistic regression and neural networks?", "options": ["Logistic regression uses linear functions, while neural networks use non-linear functions.", "Logistic regression requires more computational resources than neural networks.", "Logistic regression is less accurate than neural networks.", "Logistic regression is used only for binary classification problems, whereas neural networks are versatile."], "complexity": 2}, {"id": 88, "context": "Having input data with comparable range is useful when comparing values across features. Data scaling is especially important in large neural networks, since it helps speed up gradient descent. ", "Bloom_type": "application", "question": "What technique should be applied to ensure comparability of feature values before feeding them into a large neural network?", "options": ["Data normalization", "Feature selection", "Dimensionality reduction", "Model training"], "complexity": 2}, {"id": 89, "context": "For logistic regression, this loss function is conveniently convex. A convex function has at most one minimum; there are no local minima to get stuck in, so gradient descent starting from any point is guaranteed to find the minimum. (By contrast, the loss for multi-layer neural networks is non-convex, and gradient descent may get stuck in local minima for neural network training and never find the global optimum.) ", "Bloom_type": "application", "question": "Which of the following statements about the loss functions is true regarding their nature?", "options": ["Multi-layer neural networks use a non-convex loss function.", "Logistic regression uses a non-convex loss function.", "Gradient descent always finds the global minimum for both logistic regression and neural networks.", "The loss for logistic regression can only be found using gradient descent."], "complexity": 2}, {"id": 90, "context": "The revolutionary intuition here is that we can just use running text as implicitly supervised training data for such a classifier; a word c that occurs near the target word apricot acts as gold correct answer` to the question Is word c likely to show up near apricot? This method, often called self-supervision, avoids the need for any sort of hand-labeled supervision signal. This idea was first proposed in the task of neural language modeling, when Bengio et al. (2003) and Collobert et al. (2011) showed that a neural language model (a neural network that learned to predict the next word from prior words) could just use the next word in running text as its supervision signal, and could be used to learn an embedding representation for each word as part of doing this prediction task. ", "Bloom_type": "application", "question": "What technique does Bengio et al. propose using running text as implicit supervision for?", "options": ["Neural language modeling", "Self-attention mechanism", "Word embeddings", "Sequence-to-sequence models"], "complexity": 2}, {"id": 91, "context": "Instead, a modern neural network is a network of small computing units, each of which takes a vector of input values and produces a single output value. In this chapter we introduce the neural net applied to classification. The architecture we introduce is called a feedforward network because the computation proceeds iteratively from one layer of units to the next. The use of modern neural nets is often called deep learning, because modern networks are often deep (have many layers). Neural networks share much of the same mathematics as logistic regression. But neural networks are a more powerful classifier than logistic regression, and indeed a minimal neural network (technically one with a single hidden layer`) can be shown to learn any function. ", "Bloom_type": "application", "question": "What does it mean when a neural network is described as having a single hidden layer?", "options": ["It implies the network can perform any task.", "It means the network only has one type of unit.", "It indicates the network uses fewer parameters.", "It suggests the network is less complex."], "complexity": 2}, {"id": 92, "context": "Neural net classifiers are different from logistic regression in another way. With logistic regression, we applied the regression classifier to many different tasks by developing many rich kinds of feature templates based on domain knowledge. When working with neural networks, it is more common to avoid most uses of rich handderived features, instead building neural networks that take raw words as inputs and learn to induce features as part of the process of learning to classify. We saw examples of this kind of representation learning for embeddings in Chapter 6. Nets that are very deep are particularly good at representation learning. For that reason deep neural nets are the right tool for tasks that offer sufficient data to learn features automatically. ", "Bloom_type": "application", "question": "What is a key difference between using rich hand-derived features and learning features directly through neural networks?", "options": ["Rich hand-derived features can handle complex relationships better than neural networks.", "Rich hand-derived features require less computational power.", "Learning features directly through neural networks requires more computational power.", "Neural networks cannot handle large datasets effectively."], "complexity": 2}, {"id": 93, "context": "In this chapter we`ll introduce feedforward networks as classifiers, and also apply them to the simple task of language modeling: assigning probabilities to word sequences and predicting upcoming words. In subsequent chapters we`ll introduce many other aspects of neural models, such as recurrent neural networks (Chapter 8), the Transformer (Chapter 9), and masked language modeling (Chapter 11). ", "Bloom_type": "application", "question": "What is the next step after applying feedforward networks for language modeling?", "options": ["Explore advanced techniques like Transformers", "Develop a new method for improving accuracy", "Introduce more complex architectures like RNNs", "Implement masked language modeling"], "complexity": 2}, {"id": 94, "context": "The building block of a neural network is a single computational unit. A unit takes a set of real valued numbers as input, performs some computation on them, and produces an output. ", "Bloom_type": "application", "question": "What does each component in a neural network represent?", "options": ["All of the above", "The entire network architecture", "A collection of interconnected nodes", "A single computational unit"], "complexity": 2}, {"id": 95, "context": "Early in the history of neural networks it was realized that the power of neural networks, as with the real neurons that inspired them, comes from combining these units into larger networks. ", "Bloom_type": "application", "question": "What is an essential step in developing a deep learning model?", "options": ["Combining smaller neural networks into a deeper architecture", "Selecting the right dataset only", "Training the neural network alone", "Implementing backpropagation algorithm"], "complexity": 2}, {"id": 96, "context": "In this example we just stipulated the weights in Fig. 7.6. But for real examples the weights for neural networks are learned automatically using the error backpropagation algorithm to be introduced in Section 7.5. That means the hidden layers will learn to form useful representations. This intuition, that neural networks can automatically learn useful representations of the input, is one of their key advantages, and one that we will return to again and again in later chapters. ", "Bloom_type": "application", "question": "What does the learning process involve when training a neural network?", "options": ["Adjusting the weights through an iterative process", "Stipulating the initial weights manually", "Determining the number of neurons in each layer", "Calculating the output of each neuron based on inputs"], "complexity": 2}, {"id": 97, "context": "That means we can think of a neural network classifier with one hidden layer as building a vector h which is a hidden layer representation of the input, and then running standard multinomial logistic regression on the features that the network develops in h. By contrast, in Chapter 5 the features were mainly designed by hand via feature templates. So a neural network is like multinomial logistic regression, but (a) with many layers, since a deep neural network is like layer after layer of logistic regression classifiers; (b) with those intermediate layers having many possible activation functions (tanh, ReLU, sigmoid) instead of just sigmoid (although we`ll continue to use  for convenience to mean any activation function); (c) rather than forming the features by feature templates, the prior layers of the network induce the feature representations themselves. ", "Bloom_type": "application", "question": "What does the development of features in a neural network differ from when using feature templates?", "options": ["It allows for more flexibility in feature representation.", "It uses fewer layers.", "It requires more manual design.", "It simplifies the model complexity."], "complexity": 2}, {"id": 98, "context": "Third, gradient descent requires knowing the gradient of the loss function, the vector that contains the partial derivative of the loss function with respect to each of the parameters. In logistic regression, for each observation we could directly compute the derivative of the loss function with respect to an individual w or b. But for neural networks, with millions of parameters in many layers, it`s much harder to see how to compute the partial derivative of some weight in layer 1 when the loss is attached to some much later layer. How do we partial out the loss over all those intermediate layers? The answer is the algorithm called error backpropagation or backward differentiation. ", "Bloom_type": "application", "question": "What is the method used to calculate the gradients of the loss function in deep learning models?", "options": ["Both forward and backward propagation are required", "Forward propagation only", "Backward propagation only", "No need for any method"], "complexity": 2}, {"id": 99, "context": "The cross-entropy loss that is used in neural networks is the same one we saw for logistic regression. If the neural network is being used as a binary classifier, with the sigmoid at the final layer, the loss function is the same logistic regression loss we saw in Eq. 5.23: ", "Bloom_type": "application", "question": "What is the response when applying the cross-entropy loss to a binary classification problem using a neural network?", "options": ["The difference between the predicted probability and the actual class label.", "The sum of all probabilities across all classes.", "The product of all probabilities across all classes.", "The ratio of the number of correctly classified instances to the total number of instances."], "complexity": 2}, {"id": 100, "context": "For logistic regression we can initialize gradient descent with all the weights and biases having the value 0. In neural networks, by contrast, we need to initialize the weights with small random numbers. It`s also helpful to normalize the input values to have 0 mean and unit variance. ", "Bloom_type": "application", "question": "What is an important step when initializing weights for a neural network?", "options": ["Normalize the input values to have a mean of 0 and standard deviation of 1.", "Initialize all weights and biases to zero.", "Use large random numbers for initialization.", "None of the above."], "complexity": 2}, {"id": 101, "context": "Tuning of hyperparameters is also important. The parameters of a neural network are the weights W and biases b; those are learned by gradient descent. The hyperparameters are things that are chosen by the algorithm designer; optimal values are tuned on a devset rather than by gradient descent learning on the training set. Hyperparameters include the learning rate , the mini-batch size, the model architecture (the number of layers, the number of hidden nodes per layer, the choice of activation functions), how to regularize, and so on. Gradient descent itself also has many architectural variants such as Adam (Kingma and Ba, 2015). ", "Bloom_type": "application", "question": "What does tuning hyperparameters involve?", "options": ["It involves optimizing the hyperparameters using techniques other than gradient descent.", "It involves adjusting the weights and biases manually.", "It involves changing the learning rate only.", "It involves modifying the model architecture directly."], "complexity": 2}, {"id": 102, "context": "Finally, most modern neural networks are built using computation graph formalisms that make it easy and natural to do gradient computation and parallelization on vector-based GPUs (Graphic Processing Units). PyTorch (Paszke et al., 2017) and TensorFlow (Abadi et al., 2015) are two of the most popular. The interested reader should consult a neural network textbook for further details; some suggestions are at the end of the chapter. ", "Bloom_type": "application", "question": "What is an essential step when building a neural network?", "options": ["Learn about the basics of neural networks", "Choose a random method for training", "Select a GPU for faster computations", "Implement the computation graph manually"], "complexity": 2}, {"id": 103, "context": " Neural networks are built out of neural units, originally inspired by biological ", "Bloom_type": "application", "question": "What is the first step in building a neural network?", "options": ["Designing the architecture of the neural network", "Selecting the type of activation function for each neuron", "Training the network with labeled data", "Implementing the weights and biases"], "complexity": 2}, {"id": 104, "context": " The power of neural networks comes from the ability of early layers to learn ", "Bloom_type": "application", "question": "What is the primary reason for the effectiveness of early layers in neural networks?", "options": ["All of the above", "They can detect patterns more efficiently than later layers.", "Later layers are less prone to overfitting compared to earlier ones.", "Early layers require fewer parameters, making them easier to train."], "complexity": 2}, {"id": 105, "context": " Neural networks are trained by optimization algorithms like gradient de", "Bloom_type": "application", "question": "What is the first step in training a neural network?", "options": ["Initialize weights randomly", "Compute gradients of loss function with respect to weights", "Select an optimizer algorithm", "Define the architecture of the neural network"], "complexity": 2}, {"id": 106, "context": " Neural language models use a neural network as a probabilistic classifier, to ", "Bloom_type": "application", "question": "What is the primary purpose of using a neural network in language models?", "options": ["To classify sentences based on their meaning", "To predict future events in a sentence", "To analyze the sentiment of a sentence", "To translate between different languages"], "complexity": 2}, {"id": 107, "context": "The origins of neural networks lie in the 1940s McCulloch-Pitts neuron (McCulloch and Pitts, 1943), a simplified model of the biological neuron as a kind of computing element that could be described in terms of propositional logic. By the late 1950s and early 1960s, a number of labs (including Frank Rosenblatt at Cornell and Bernard Widrow at Stanford) developed research into neural networks; this phase saw the development of the perceptron (Rosenblatt, 1958), and the transformation of the threshold into a bias, a notation we still use (Widrow and Hoff, 1960). ", "Bloom_type": "application", "question": "In what year did the first perceptron, which was a significant milestone in the development of neural networks, come into existence?", "options": ["1958", "1943", "1960", "1970"], "complexity": 2}, {"id": 108, "context": "The field of neural networks declined after it was shown that a single perceptron unit was unable to model functions as simple as XOR (Minsky and Papert, 1969). While some small amount of work continued during the next two decades, a major revival for the field didn`t come until the 1980s, when practical tools for building deeper networks like error backpropagation became widespread (Rumelhart et al., 1986). During the 1980s a wide variety of neural network and related architectures were developed, particularly for applications in psychology and cognitive science (Rumelhart and McClelland 1986b, McClelland and Elman 1986, Rumelhart and McClelland 1986a, Elman 1990), for which the term connectionist or parallel distributed processing was often used (Feldman and Ballard 1982, Smolensky 1988). Many of the principles and techniques developed in this period are foundational to modern work, including the ideas of distributed representations (Hinton, 1986), recurrent networks (Elman, 1990), and the use of tensors for compositionality (Smolensky, 1990). ", "Bloom_type": "application", "question": "What is the primary reason for the decline of the field of neural networks?", "options": ["The discovery of the limitations of single perceptron units.", "The complexity of models required exceeded computational capabilities.", "The lack of practical tools hindered further development.", "The focus on psychological applications overshadowed broader research."], "complexity": 2}, {"id": 109, "context": "By the 1990s larger neural networks began to be applied to many practical language processing tasks as well, like handwriting recognition (LeCun et al. 1989) and speech recognition (Morgan and Bourlard 1990). By the early 2000s, improvements in computer hardware and advances in optimization and training techniques made it possible to train even larger and deeper networks, leading to the modern term deep learning (Hinton et al. 2006, Bengio et al. 2007). We cover more related history in Chapter 8 and Chapter 16. ", "Bloom_type": "application", "question": "What was the name of the field that emerged with advancements in computer hardware and optimization techniques?", "options": ["Deep Learning", "Machine Learning", "Artificial Intelligence", "Neural Networks"], "complexity": 2}, {"id": 110, "context": "There are a number of excellent books on the subject. Goldberg (2017) has superb coverage of neural networks for natural language processing. For neural networks in general see Goodfellow et al. (2016) and Nielsen (2015). ", "Bloom_type": "application", "question": "Which book provides an excellent overview of neural networks specifically designed for image recognition?", "options": ["Nielsen (2015)", "Goodfellow et al. (2016)", "Goldberg (2017)", "None of the above"], "complexity": 2}, {"id": 111, "context": "This chapter introduces a deep learning architecture that offers an alternative way of representing time: recurrent neural networks (RNNs), and their variants like LSTMs. RNNs have a mechanism that deals directly with the sequential nature of language, allowing them to handle the temporal nature of language without the use of arbitrary fixed-sized windows. The recurrent network offers a new way to represent the prior context, in its recurrent connections, allowing the model`s decision to depend on information from hundreds of words in the past. We`ll see how to apply the model to the task of language modeling, to text classification tasks like sentiment analysis, and to sequence modeling tasks like part-of-speech tagging (a task we`ll return to in detail in Chapter 17). ", "Bloom_type": "application", "question": "What is the primary advantage of using Recurrent Neural Networks (RNNs) over traditional methods for handling sequential data?", "options": ["They allow capturing long-term dependencies.", "They can only process static input.", "They require large amounts of memory.", "They cannot capture long-term dependencies."], "complexity": 2}, {"id": 112, "context": "A recurrent neural network (RNN) is any network that contains a cycle within its network connections, meaning that the value of some unit is directly, or indirectly, dependent on its own earlier outputs as an input. While powerful, such networks are difficult to reason about and to train. However, within the general class of recurrent networks there are constrained architectures that have proven to be extremely effective when applied to language. In this section, we consider a class of recurrent networks referred to as Elman Networks (Elman, 1990) or simple recurrent networks. These networks are useful in their own right and serve as the basis for more complex approaches like the Long Short-Term Memory (LSTM) networks discussed ", "Bloom_type": "application", "question": "Which type of recurrent neural network is characterized by having cycles within its network connections?", "options": ["Recurrent Neural Network (RNN)", "Convolutional Neural Network (CNN)", "Deep Belief Network (DBN)", "Generative Adversarial Network (GAN)"], "complexity": 2}, {"id": 113, "context": "This chapter has introduced the concepts of recurrent neural networks and how they can be applied to language problems. Here`s a summary of the main points that we ", "Bloom_type": "application", "question": "What is the primary application of Recurrent Neural Networks (RNNs) as discussed in this chapter?", "options": ["They are utilized for natural language processing tasks.", "They are used for image recognition.", "They are employed for speech synthesis.", "They are applied to financial forecasting."], "complexity": 2}, {"id": 114, "context": " In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t ", "Bloom_type": "application", "question": "What is the primary difference between RNNs and other types of neural networks?", "options": ["RNNs process entire sequences sequentially.", "RNNs can only handle static inputs.", "RNNs require more computational resources than other types.", "RNNs cannot learn long-term dependencies."], "complexity": 2}, {"id": 115, "context": "While theoretically interesting, the difficulty with training RNNs and managing context over long sequences impeded progress on practical applications. This situation changed with the introduction of LSTMs in Hochreiter and Schmidhuber (1997) and Gers et al. (2000). Impressive performance gains were demonstrated on tasks at the boundary of signal processing and language processing including phoneme recognition (Graves and Schmidhuber, 2005), handwriting recognition (Graves et al., 2007) and most significantly speech recognition (Graves et al., 2013). Interest in applying neural networks to practical NLP problems surged with the work of Collobert and Weston (2008) and Collobert et al. (2011). These efforts made use of learned word embeddings, convolutional networks, and end-to-end training. ", "Bloom_type": "application", "question": "What was a significant factor contributing to the resurgence of interest in using neural networks for natural language processing?", "options": ["Development of specialized libraries and frameworks", "Theoretical advancements in machine learning algorithms", "Improved computational power and hardware capabilities", "Increased availability of large datasets for training"], "complexity": 2}, {"id": 116, "context": "The transformer is a neural network with a specific structure that includes a mechanism called self-attention or multi-head attention.1 Attention can be thought of as a way to build contextual representations of a token`s meaning by attending to and integrating information from surrounding tokens, helping the model learn how tokens relate to each other over large spans. ", "Bloom_type": "application", "question": "What is the primary function of self-attention in a transformer?", "options": ["To enhance the model's understanding of long-range dependencies", "To increase computational speed", "To reduce memory usage", "To decrease the number of parameters"], "complexity": 2}, {"id": 117, "context": "Layer Norm At two stages in the transformer block we normalize the vector (Ba et al., 2016). This process, called layer norm (short for layer normalization), is one of many forms of normalization that can be used to improve training performance in deep neural networks by keeping the values of a hidden layer in a range that facilitates gradient-based training. ", "Bloom_type": "application", "question": "What does layer norm specifically refer to in the context of transformers?", "options": ["It refers to standardizing the input data across different layers.", "It refers to normalizing the weights of the layers.", "It refers to adjusting the learning rate during training.", "It refers to applying activation functions before each layer."], "complexity": 2}, {"id": 118, "context": "In a very influential series of papers developing the idea of neural language models, (Bengio et al. 2000; Bengio et al. 2003; Bengio et al. 2006), Yoshua Bengio and colleagues drew on the central ideas of both these lines of self-supervised language modeling work, (the discriminatively trained word predictor, and the pretrained embeddings). Like the maxent models of Rosenfeld, Bengio`s model used the next word in running text as its supervision signal. Like the LSA models, Bengio`s model learned an embedding, but unlike the LSA models did it as part of the process of language modeling. The Bengio et al. (2003) model was a neural language model: a neural network that learned to predict the next word from prior words, and did so via learning embeddings as part of the prediction process. ", "Bloom_type": "application", "question": "What type of neural network is described in the passage?", "options": ["Recurrent Neural Network", "Feedforward Neural Network", "Convolutional Neural Network", "Transformer Model"], "complexity": 2}, {"id": 119, "context": "For sequence classification we represent the entire input to be classified by a single vector. We can represent a sequence in various ways. One way is to take the sum or the mean of the last output vector from each token in the sequence. For BERT, we instead add a new unique token to the vocabulary called [CLS], and prepended it to the start of all input sequences, both during pretraining and encoding. The output vector in the final layer of the model for the [CLS] input represents the entire input sequence and serves as the input to a classifier head, a logistic regression or neural network classifier that makes the relevant decision. ", "Bloom_type": "application", "question": "What does the output vector from the final layer of the BERT model represent?", "options": ["The representation of the entire input sequence", "The average of all tokens in the sequence", "The sum of all tokens in the sequence", "The probability distribution over classes"], "complexity": 2}, {"id": 120, "context": "improvement searches typically use a combination of a fixed number of iterations in combination with a failure to improve after some period to time as stopping criteria. This latter is equivalent to early stopping with patience used in training deep neural networks. ", "Bloom_type": "application", "question": "What does the response suggest about the stopping criteria for improving neural networks?", "options": ["It implies applying early stopping with patience during training.", "It suggests using a fixed number of iterations.", "It recommends increasing the number of iterations indefinitely.", "It proposes removing any form of stopping criteria."], "complexity": 2}, {"id": 121, "context": "Neural networks had been applied at various times to various aspects of machine translation; for example Schwenk et al. (2006) showed how to use neural language models to replace n-gram language models in a Spanish-English system based on IBM Model 4. The modern neural encoder-decoder approach was pioneered by Kalchbrenner and Blunsom (2013), who used a CNN encoder and an RNN decoder, and was first applied to MT by Bahdanau et al. (2015). The transformer encoderdecoder was proposed by Vaswani et al. (2017) (see the History section of Chapter 9). ", "Bloom_type": "application", "question": "What is the key difference between the traditional neural network approaches and the modern transformer-based methods?", "options": ["Traditional methods rely on sequence-to-sequence learning whereas transformers focus on self-attention mechanisms.", "Traditional methods use recurrent neural networks while transformers use convolutional neural networks.", "Traditional methods require more computational resources than transformers due to their reliance on memory.", "Traditional methods are less accurate than transformers because they lack parallel processing capabilities."], "complexity": 2}, {"id": 122, "context": "By around 1990 neural alternatives to the HMM/GMM architecture for ASR arose, based on a number of earlier experiments with neural networks for phoneme recognition and other speech tasks. Architectures included the time-delay neural network (TDNN)the first use of convolutional networks for speech (Waibel et al. 1989, Lang et al. 1990), RNNs (Robinson and Fallside, 1991), and the hybrid HMM/MLP architecture in which a feedforward neural network is trained as a phonetic classifier whose outputs are used as probability estimates for an HMM-based architecture (Morgan and Bourlard 1990, Bourlard and Morgan 1994, Morgan and Bourlard 1995). ", "Bloom_type": "application", "question": "What was the first application of convolutional networks in speech processing?", "options": ["Time-Delay Neural Network (TDNN)", "Recurrent Neural Networks (RNNs)", "Hybrid HMM/MLP Architecture", "All of the above"], "complexity": 2}, {"id": 123, "context": "Over the next two decades a combination of Moore`s law and the rise of GPUs allowed deep neural networks with many layers. Performance was getting close to traditional systems on smaller tasks like TIMIT phone recognition by 2009 (Mohamed et al., 2009), and by 2012, the performance of hybrid systems had surpassed traditional HMM/GMM systems (Jaitly et al. 2012, Dahl et al. 2012, inter alia). Originally it seemed that unsupervised pretraining of the networks using a technique like deep belief networks was important, but by 2013, it was clear that for hybrid HMM/GMM feedforward networks, all that mattered was to use a lot of data and enough layers, although a few other components did improve performance: using log mel features instead of MFCCs, using dropout, and using rectified linear units (Deng et al. 2013, Maas et al. 2013, Dahl et al. 2013). ", "Bloom_type": "application", "question": "What is the key factor determining the success of hybrid HMM/GMM feedforward networks?", "options": ["All of the above", "The number of layers in the neural network", "The type of activation function used", "The amount of training data available"], "complexity": 2}, {"id": 124, "context": "Instead linguistic structure plays a number of new roles. One important role is for interpretability: to provide a useful interpretive lens on neural networks. Knowing that a particular layer or neuron may be computing something related to a particular kind of structure can help us break open the black box` and understand what the components of our language models are doing. ", "Bloom_type": "application", "question": "What does it mean when we say knowing about the inner workings of neural networks helps with understanding their functionality?", "options": ["It provides insight into how different parts of the model contribute to the overall output.", "It allows us to predict future outcomes more accurately.", "It enables us to visualize the data before processing.", "It helps us reduce the computational complexity of the model."], "complexity": 2}, {"id": 125, "context": "The earliest disambiguation algorithms for parsing were based on probabilistic context-free grammars, first worked out by Booth (1969) and Salomaa (1969); see Appendix C for more history. Neural methods were first applied to parsing at around the same time as statistical parsing methods were developed (Henderson, 1994). In the earliest work neural networks were used to estimate some of the probabilities for statistical constituency parsers (Henderson, 2003, 2004; Emami and Jelinek, 2005) . The next decades saw a wide variety of neural parsing algorithms, including recursive neural architectures (Socher et al., 2011, 2013), encoder-decoder models (Vinyals et al., 2015; Choe and Charniak, 2016), and the idea of focusing on spans (Cross and Huang, 2016). For more on the span-based self-attention approach we describe in this chapter see Stern et al. (2017), Gaddy et al. (2018), Kitaev and Klein (2018), and Kitaev et al. (2019). See Chapter 20 for the parallel history of neural dependency parsing. ", "Bloom_type": "application", "question": "What was the initial application of neural networks in parsing?", "options": ["To model syntactic dependencies", "To predict sentence structure", "To analyze word frequencies", "To classify parts of speech"], "complexity": 2}, {"id": 126, "context": "State-of-the-art graph-based multilingual parsers are based on neural networks. Instead of extracting hand-designed features to represent each edge between words wi and w j, these parsers run the sentence through an encoder, and then pass the encoded representation of the two words wi and w j through a network that estimates a score for the edge i ", "Bloom_type": "application", "question": "What is the primary difference between traditional feature extraction methods and modern neural network approaches in parsing sentences?", "options": ["Both A) and B)", "Traditional methods require manual design of features.", "Neural network approaches eliminate the need for any manual feature engineering.", "Modern approaches focus solely on increasing computational speed."], "complexity": 2}, {"id": 127, "context": "The modern approach is instead to perform mention detection, anaphoricity, and coreference jointly in a single end-to-end model (Ng 2005b, Denis and Baldridge 2007, Rahman and Ng 2009). For example mention detection in the Lee et al. (2017b),2018 system is based on a single end-to-end neural network that computes a score for each mention being referential, a score for two mentions being coreference, and combines them to make a decision, training all these scores with a single end-to-end loss. We`ll describe this method in detail in Section 23.6. 7 ", "Bloom_type": "application", "question": "What is the main approach used for detecting mentions, anaphora, and coreference together?", "options": ["Hybrid approaches combining both", "Traditional rule-based methods", "Machine learning algorithms", "Manual annotation"], "complexity": 2}]}, "question answering": {"max_id": 148, "Questions": [{"id": 0, "context": "The notion of word similarity is very useful in larger semantic tasks. Knowing how similar two words are can help in computing how similar the meaning of two phrases or sentences are, a very important component of tasks like question answering, paraphrasing, and summarization. One way of getting values for word similarity is to ask humans to judge how similar one word is to another. A number of datasets have resulted from such experiments. For example the SimLex-999 dataset (Hill et al., 2015) gives values on a scale from 0 to 10, like the examples below, which range from near-synonyms (vanish, disappear) to pairs that scarcely seem to have anything in common (hole, agreement): ", "Bloom_type": "remember", "question": "In the context of tasking with word similarity, what method involves asking humans to rate how similar two words are?", "options": ["Gathering data through human judgments", "Using machine learning algorithms", "Analyzing natural language corpora", "Employing sentiment analysis techniques"], "complexity": 0}, {"id": 1, "context": "Semantic Frames and Roles Closely related to semantic fields is the idea of a semantic frame. A semantic frame is a set of words that denote perspectives or participants in a particular type of event. A commercial transaction, for example, is a kind of event in which one entity trades money to another entity in return for some good or service, after which the good changes hands or perhaps the service is performed. This event can be encoded lexically by using verbs like buy (the event from the perspective of the buyer), sell (from the perspective of the seller), pay (focusing on the monetary aspect), or nouns like buyer. Frames have semantic roles (like buyer, seller, goods, money), and words in a sentence can take on these roles. Knowing that buy and sell have this relation makes it possible for a system to know that a sentence like Sam bought the book from Ling could be paraphrased as Ling sold the book to Sam, and that Sam has the role of the buyer in the frame and Ling the seller. Being able to recognize such paraphrases is important for question answering, and can help in shifting perspective for machine translation. ", "Bloom_type": "remember", "question": "In what way does understanding semantic frames aid in question answering?", "options": ["It enables better comprehension of financial transactions.", "It helps in identifying synonyms.", "It aids in recognizing word relationships within sentences.", "It assists in translating between different languages."], "complexity": 0}, {"id": 2, "context": "In this chapter we`ll introduce the two most commonly used models. In the tf-idf model, an important baseline, the meaning of a word is defined by a simple function of the counts of nearby words. We will see that this method results in very long vectors that are sparse, i.e. mostly zeros (since most words simply never occur in the context of others). We`ll introduce the word2vec model family for constructing short, dense vectors that have useful semantic properties. We`ll also introduce the cosine, the standard way to use embeddings to compute semantic similarity, between two words, two sentences, or two documents, an important tool in practical applications like question answering, summarization, or automatic essay grading. ", "Bloom_type": "remember", "question": "In what application does the word2vec model family particularly excel?", "options": ["Question answering", "Image recognition", "Music recommendation", "Speech synthesis"], "complexity": 0}, {"id": 3, "context": "RNN-based language models can also be used to generate text. Text generation is of enormous practical importance, part of tasks like question answering, machine translation, text summarization, grammar correction, story generation, and conversational dialogue; any task where a system needs to produce text, conditioned on some other text. This use of a language model to generate text is one of the areas in which the impact of neural language models on NLP has been the largest. Text generation, along with image generation and code generation, constitute a new area of AI that is often called generative AI. ", "Bloom_type": "remember", "question": "Which type of AI involves generating text based on input text?", "options": ["Generative AI", "Reinforcement Learning", "Symbolic AI", "Rule-Based AI"], "complexity": 0}, {"id": 4, "context": "Technically an autoregressive model is a model that predicts a value at time t based on a linear function of the previous values at times t 2, and so on. Although language models are not linear (since they have many layers of non-linearities), we loosely refer to this generation technique as autoregressive generation since the word generated at each time step is conditioned on the word selected by the network from the previous step. Fig. 8.9 illustrates this approach. In this figure, the details of the RNN`s hidden layers and recurrent connections are hidden within the blue block. This simple architecture underlies state-of-the-art approaches to applications such as machine translation, summarization, and question answering. The key to these approaches is to prime the generation component with an appropriate context. That is, instead of simply using <s> to get things started we can provide a richer task-appropriate context; for translation the context is the sentence in the source language; for summarization it`s the long text we want to summarize. ", "Bloom_type": "remember", "question": "In what way does the autoregressive model differ from traditional linear models?", "options": ["It uses fewer layers.", "It generates more complex sentences.", "It requires less data for training.", "It relies solely on external inputs."], "complexity": 0}, {"id": 5, "context": "Encoder-decoder networks, sometimes called sequence-to-sequence networks, are models capable of generating contextually appropriate, arbitrary length, output sequences given an input sequence. Encoder-decoder networks have been applied to a very wide range of applications including summarization, question answering, and dialogue, but they are particularly popular for machine translation. ", "Bloom_type": "remember", "question": "In which application area are encoder-decoder networks most commonly used?", "options": ["All of the above", "Summarization", "Machine Translation", "Dialogue Systems"], "complexity": 0}, {"id": 6, "context": "able performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots. ", "Bloom_type": "remember", "question": "In what way has the ability to perform various natural language tasks improved due to the learning done during pretraining?", "options": ["It greatly improves their skill at generating coherent responses to questions.", "It enhances their capacity to understand complex scientific concepts.", "It significantly boosts their capability to solve mathematical problems.", "It increases their proficiency in identifying patterns in data."], "complexity": 0}, {"id": 7, "context": "We can also cast more complex tasks as word prediction. Consider question answering, in which the system is given a question (for example a question with a simple factual answer) and must give a textual answer; we introduce this task in detail in Chapter 14. We can cast the task of question answering as word prediction by giving a language model a question and a token like A: suggesting that an answer should come next: ", "Bloom_type": "remember", "question": "In what way does question answering relate to word prediction?", "options": ["It focuses on generating answers based on previous questions.", "It involves predicting the next word in a sentence.", "It requires understanding the meaning of each word individually.", "It uses a machine learning algorithm for natural language processing."], "complexity": 0}, {"id": 8, "context": "Other factors While the predictive accuracy of a language model, as measured by perplexity, is a very useful metric, we also care about different kinds of accuracy, for the downstream tasks we apply our language model to. For each task like machine translation, summarization, question answering, speech recognition, and dialogue, we can measure the accuracy at those tasks. Future chapters will introduce taskspecific metrics that allow us to evaluate how accuracy or correct language models are at these downstream tasks. ", "Bloom_type": "remember", "question": "In the context provided, what does the author mention as other factors besides perplexity?", "options": ["Accuracy", "Perplexity", "Metrics", "Tasks"], "complexity": 0}, {"id": 9, "context": "Large pretrained neural language models exhibit many of the potential harms discussed in Chapter 4 and Chapter 6. Many of these harms become realized when pretrained language models are used for any downstream task, particularly those involving text generation, whether question answering, machine translation, or in assistive technologies like writing aids or web search query completion, or predictive typing for email (Olteanu et al., 2020). ", "Bloom_type": "remember", "question": "Which type of task can large pretrained neural language models be utilized for?", "options": ["For all types of tasks including question answering", "Only for generating text", "Limited to specific domains only", "Not applicable"], "complexity": 0}, {"id": 10, "context": " Many NLP taskssuch as question answering, summarization, sentiment, and machine translationcan be cast as tasks of word prediction and hence addressed with Large language models. ", "Bloom_type": "remember", "question": "In which type of natural language processing (NLP) tasks can large language models address?", "options": ["All of the above", "Sentiment analysis", "Machine translation", "Summarization"], "complexity": 0}, {"id": 11, "context": "Transformers were first applied as encoder-decoders (Vaswani et al., 2017) and then to masked language modeling (Devlin et al., 2019) (as we`ll see in Chapter 13 and Chapter 11). Radford et al. (2019) then showed that the transformer-based autoregressive language model GPT2 could perform zero-shot on many NLP tasks like summarization and question answering. ", "Bloom_type": "remember", "question": "In which year did Radford et al. show that transformers can perform zero-shot on various NLP tasks?", "options": ["2017", "2015", "2016", "2018"], "complexity": 0}, {"id": 12, "context": "Let`s begin by introducing the bidirectional transformer encoder that underlies models like BERT and its descendants like RoBERTa (Liu et al., 2019) or SpanBERT (Joshi et al., 2020). In Chapter 9 we introduced causal (left-to-right) transformers and in Chapter 10 saw how they can serve as the basis for language models that can be applied to autoregressive contextual generation problems like question answering or summarization. But this left-to-right nature of these models is also a limitation, because there are tasks for which it would be useful, when processing a token, to be able to peak at future tokens. This is especially true for sequence labeling tasks ", "Bloom_type": "remember", "question": "In what way does the left-to-right nature of transformers limit their application?", "options": ["It prevents them from handling autoregressive tasks effectively.", "It restricts their ability to process long sequences.", "It makes them unsuitable for sequence labeling tasks.", "It limits their capacity for parallel processing."], "complexity": 0}, {"id": 13, "context": "Fig. 12.2 illustrates a few-shot example from an extractive question answering task. The context combines the task definition along with three gold-standard question and answer pairs from the training set. ", "Bloom_type": "remember", "question": "In the context, what is illustrated by Fig. 12.2?", "options": ["Instructions for extracting answers from texts", "The process of generating questions", "An example of a machine learning model", "Details about the training data used"], "complexity": 0}, {"id": 14, "context": "But more generally, the best way to select demonstrations from the training set is programmatically: choosing the set of demonstrations that most increases task performance of the prompt on a test set. Task performance for sentiment analysis or multiple-choice question answering can be measured in accuracy; for machine translation with chrF, and for summarization via Rouge. Systems like DSPy (Khattab et al., 2024), a framework for algorithmically optimizing LM prompts, can automatically find the optimum set of demonstrations to include by searching through the space of possible demonstrations to include. We`ll return to automatic prompt optimization in Section 12.5. ", "Bloom_type": "remember", "question": "In what type of tasks can systems like DSPy optimize the selection of demonstrations?", "options": ["All of the above", "Sentiment analysis", "Machine translation", "Summarization"], "complexity": 0}, {"id": 15, "context": "Many huge instruction tuning datasets have been created, covering many tasks and languages. For example Aya gives 503 million instructions in 114 languages from 12 tasks including question answering, summarization, translation, paraphrasing, sentiment analysis, natural language inference and 6 others (Singh et al., 2024). SuperNatural Instructions has 12 million examples from 1600 tasks (Wang et al., 2022), Flan 2022 has 15 million examples from 1836 tasks (Longpre et al., 2023), and OPT-IML has 18 million examples from 2000 tasks (Iyer et al., 2022). ", "Bloom_type": "remember", "question": "What type of data is used for training models in these large instruction tuning datasets?", "options": ["Text-based instructions", "Audio recordings", "Visual images", "Video clips"], "complexity": 0}, {"id": 16, "context": "Language models are evaluated in many ways. we introduced some evaluations for in Section 10.4, including measuring the language model`s perplexity on a test set, evaluating its accuracy on various NLP tasks, as well as benchmarks that help measure efficiency, toxicity, fairness, and so on. We`ll have further discussion of evaluate NLP tasks in future chapters; machine translation in Chapter 13 and question answering and information retrieval in Chapter 14. ", "Bloom_type": "remember", "question": "In what chapter will they discuss how to evaluate NLP tasks besides using perplexity and accuracy?", "options": ["Chapter 14", "Chapter 12", "Chapter 13", "Chapter 15"], "complexity": 0}, {"id": 17, "context": " One method for model alignment is instruction tuning, in which the model is finetuned (using the next-word-prediction language model objective) on a dataset of instructions together with correct responses. Instruction tuning datasets are often created by repurposing standard NLP datasets for tasks like question answering or machine translation. ", "Bloom_type": "remember", "question": "In what way does instruction tuning differ from traditional fine-tuning methods?", "options": ["Instruction tuning uses a different type of objective function.", "Instruction tuning requires more labeled data than traditional fine-tuning.", "Instruction tuning focuses solely on improving response accuracy.", "Instruction tuning can only be applied to question-answering models."], "complexity": 0}, {"id": 18, "context": "In this second part of the book we introduce fundamental NLP applications: machine translation, information retrieval, question answering, dialogue systems, and speech recognition. ", "Bloom_type": "remember", "question": "In which area of natural language processing are we introduced to fundamental applications such as machine translation, information retrieval, question answering, dialogue systems, and speech recognition?", "options": ["Natural Language Understanding", "Machine learning", "Text Generation", "Sentiment Analysis"], "complexity": 0}, {"id": 19, "context": "Question answering systems are designed to fill human information needs. Since a lot of information is present in text form (on the web or in other data like our email, or books), question answering is closely related to the task behind search engines. Indeed, the distinction is becoming ever more fuzzy, as modern search engines are integrated with large language models trained to do question answering. Question answering systems often focus on a useful subset of information needs: factoid questions, questions of fact or reasoning that can be answered with simple facts expressed in short or medium-length texts, like the following: ", "Bloom_type": "remember", "question": "In what way does question answering relate to search engines?", "options": ["It enhances the integration between search engines and natural language processing.", "It makes search engines less efficient.", "It complicates the understanding of search engine algorithms.", "It reduces the need for search engines."], "complexity": 0}, {"id": 20, "context": " QUESTION ANSWERING, INFORMATION RETRIEVAL, AND RAG ", "Bloom_type": "remember", "question": "In which field of computer science is the process of finding answers to questions primarily focused?", "options": ["Information Retrieval", "Machine Learning", "Artificial Intelligence", "Natural Language Processing"], "complexity": 0}, {"id": 21, "context": "A second problem is that simply prompting a large language model doesn`t allow us to ask questions about proprietary data. A common use of question answering is about data like our personal email or medical records. Or a company may have internal documents that contain answers for customer service or internal use. Or legal firms need to ask questions about legal discovery from proprietary documents. Finally, static large language models also have problems with questions about rapidly changing information (like questions about something that happened last week) since LLMs won`t have up-to-date information from after their release data. For this reason the most common way to do question-answering with LLMs is retrieval-augmented generation or RAG, and that is the method we will focus on in this chapter. In RAG we use information retrieval (IR) techniques to retrieve documents that are likely to have information that might help answer the question. Then we use a large language model to generate an answer given these documents. Basing our answers on retrieved documents can solve some of the problems with using simple prompting to answer questions. First, it helps ensure that the answer is grounded in facts from some curated dataset. And the system can give the user the answer accompanied by the context of the passage or document the answer came from. This information can help users have confidence in the accuracy of the answer (or help them spot when it is wrong!). And these retrieval techniques can be used on any proprietary data we want, such as legal or medical data for those applications. ", "Bloom_type": "remember", "question": "What is one benefit of using retrieval-augmented generation (RAG) over simple prompting for question answering?", "options": ["It allows users to see the source of the answer alongside the response.", "It ensures the answer is based on real-time data.", "It guarantees the answer is completely accurate.", "It eliminates the need for large language models."], "complexity": 0}, {"id": 22, "context": "We`ll begin by introducing information retrieval, the task of choosing the most relevant document from a document set given a user`s query expressing their information need. We`ll see the classic method based on cosines of sparse tf-idf vectors, a modern neural dense` retrievers based on instead representing queries and documents neurally with BERT or other language models. We then introduce retrieverbased question answering and the retrieval-augmented generation paradigm. ", "Bloom_type": "remember", "question": "In what field do we start by discussing how to choose the best document for a user?", "options": ["Information retrieval", "Machine learning", "Natural language processing", "Computer vision"], "complexity": 0}, {"id": 23, "context": "Finally, we`ll discuss various QA datasets. These are used for finetuning LLMs in instruction tuning, as we saw in Chapter 12. And they are also used as benchmarks, since question answering has an important function as a benchmark for measuring the abilities of language models. ", "Bloom_type": "remember", "question": "In what way do QA datasets serve as benchmarks for measuring the abilities of language models?", "options": ["They provide a standard set of questions for testing model performance.", "They help improve the accuracy of natural language processing.", "They offer real-world scenarios for training machine learning algorithms.", "They focus on improving the speed of data retrieval."], "complexity": 0}, {"id": 24, "context": " QUESTION ANSWERING, INFORMATION RETRIEVAL, AND RAG ", "Bloom_type": "remember", "question": "In which field is the process of finding answers to questions primarily focused?", "options": ["Natural Language Processing", "Machine Learning", "Computer Vision", "Robotics"], "complexity": 0}, {"id": 25, "context": " QUESTION ANSWERING, INFORMATION RETRIEVAL, AND RAG ", "Bloom_type": "remember", "question": "In what field is the process of finding answers to questions closely related to?", "options": ["Information Retrieval", "Physics", "Chemistry", "Rag"], "complexity": 0}, {"id": 26, "context": " QUESTION ANSWERING, INFORMATION RETRIEVAL, AND RAG ", "Bloom_type": "remember", "question": "In what field is the process of finding answers to questions primarily focused?", "options": ["Computer Science", "Physics", "Chemistry", "Biology"], "complexity": 0}, {"id": 27, "context": " QUESTION ANSWERING, INFORMATION RETRIEVAL, AND RAG ", "Bloom_type": "remember", "question": "In which field of study is the process of finding answers to questions primarily focused on?", "options": ["Information retrieval", "RAG", "Machine learning", "Natural language processing"], "complexity": 0}, {"id": 28, "context": " QUESTION ANSWERING, INFORMATION RETRIEVAL, AND RAG ", "Bloom_type": "remember", "question": "In what field is the process of finding answers to questions primarily applied?", "options": ["Computer Science", "Artificial Intelligence", "Physics", "Chemistry"], "complexity": 0}, {"id": 29, "context": "The dominant paradigm for question answering is to answer a user`s question by first finding supportive text segments from the web or another other large collection of documents, and then generating an answer based on the documents. The method of generating based on retrieved documents is called retrieval-augmented generation or RAG, and the two components are sometimes called the retriever and the reader (Chen et al., 2017a). Fig. 14.9 sketches out this standard QA model. ", "Bloom_type": "remember", "question": "In the context of question answering, what does the term \"retrieval-augmented generation\" refer to?", "options": ["The process of directly answering questions with existing knowledge", "The use of AI models to predict answers before retrieving any data", "Combining machine learning techniques with human feedback to improve accuracy", "Generating answers using only pre-existing text without external sources"], "complexity": 0}, {"id": 30, "context": " QUESTION ANSWERING, INFORMATION RETRIEVAL, AND RAG ", "Bloom_type": "remember", "question": "In what field is the process of finding answers to questions primarily focused?", "options": ["Computer Science", "Mathematics", "Physics", "Chemistry"], "complexity": 0}, {"id": 31, "context": "And simple conditional generation for question answering adds a prompt like Q: , followed by a query q , and A:, all concatenated: ", "Bloom_type": "remember", "question": "In the process of generating responses, what is added after the prompt?", "options": ["The instruction \"Q:\"", "The word \"and\"", "The phrase \"simple conditional generation\"", "The symbol \":,\""], "complexity": 0}, {"id": 32, "context": "There are scores of question answering datasets, used both for instruction tuning and for evaluation of the question answering abilities of language models. ", "Bloom_type": "remember", "question": "What is a common use case for question answering datasets?", "options": ["Evaluating natural language processing systems", "Training machine learning algorithms", "Generating new questions", "Creating fictional stories"], "complexity": 0}, {"id": 33, "context": "A similar natural question set is the MS MARCO (Microsoft Machine Reading Comprehension) collection of datasets, including 1 million real anonymized English questions from Microsoft Bing query logs together with a human generated answer and 9 million passages (Bajaj et al., 2016), that can be used both to test retrieval ranking and question answering. ", "Bloom_type": "remember", "question": "In what type of dataset are the 1 million real anonymized English questions from Microsoft Bing query logs included?", "options": ["MS MARCO", "MS COQA", "MS TAC", "MS RACE"], "complexity": 0}, {"id": 34, "context": " QUESTION ANSWERING, INFORMATION RETRIEVAL, AND RAG ", "Bloom_type": "remember", "question": "In which field of study is the process of finding answers to questions primarily focused on?", "options": ["Information retrieval", "RAG", "Machine learning", "Natural language processing"], "complexity": 0}, {"id": 35, "context": "Some of the question datasets described above augment each question with passage(s) from which the answer can be extracted. These datasets were mainly created for an earlier QA task called reading comprehension in which a model is given a question and a document and is required to extract the answer from the given document. We sometimes call the task of question answering given one or more documents (for example via RAG), the open book QA task, while the task of answering directly from the LM with no retrieval component at all is the closed book QA task.5 Thus datasets like Natural Questions can be treated as open book if the solver uses each question`s attached document, or closed book if the documents are not used, while datasets like MMLU are solely closed book. ", "Bloom_type": "remember", "question": "In what type of QA tasks do models need to extract answers from passages?", "options": ["Reading comprehension", "Closed book QA", "Open book QA", "Natural Language Inference"], "complexity": 0}, {"id": 36, "context": " QUESTION ANSWERING, INFORMATION RETRIEVAL, AND RAG ", "Bloom_type": "remember", "question": "In what field is the process of finding answers to questions primarily applied?", "options": ["Computer Science", "Artificial Intelligence", "Psychology", "Physics"], "complexity": 0}, {"id": 37, "context": "This chapter introduced the tasks of question answering and information retrieval. ", "Bloom_type": "remember", "question": "In this chapter, what two main tasks were discussed?", "options": ["Question answering and information retrieval", "Information gathering and data analysis", "Data mining and machine learning", "Text summarization and natural language processing"], "complexity": 0}, {"id": 38, "context": " Question answering (QA) is the task of answering a user`s questions.  We focus in this chapter on the task of retrieval-based question answering, in which the user`s questions are intended to be answered by the material in some set of documents (which might be the web). ", "Bloom_type": "remember", "question": "In QA, what does the user ask for?", "options": ["The questions", "The answers", "The documents", "The users"], "complexity": 0}, {"id": 39, "context": " Question answering systems generally use the retriever/reader architecture. In the retriever stage, an IR system is given a query and returns a set of documents. ", "Bloom_type": "remember", "question": "In the retriever stage of question answering systems, what does an IR system do?", "options": ["It selects relevant documents from a database", "It generates new questions", "It translates queries into natural language", "It performs sentiment analysis on documents"], "complexity": 0}, {"id": 40, "context": " QUESTION ANSWERING, INFORMATION RETRIEVAL, AND RAG ", "Bloom_type": "remember", "question": "In which field of study is the process of finding answers to questions primarily focused on?", "options": ["Information Retrieval", "RAG", "Machine Learning", "Natural Language Processing"], "complexity": 0}, {"id": 41, "context": "By a decade later, neural models were applied to semantic parsing (Dong and Lapata 2016, Jia and Liang 2016), and then to knowledge-based question answering by mapping text to SQL (Iyer et al., 2017). ", "Bloom_type": "remember", "question": "In what application of neural models did they map text to SQL?", "options": ["Semantic parsing", "Natural language understanding", "Machine translation", "Text summarization"], "complexity": 0}, {"id": 42, "context": "Meanwhile, the information-retrieval paradigm for question answering was influenced by the rise of the web in the 1990s. The U.S. government-sponsored TREC (Text REtrieval Conference) evaluations, run annually since 1992, provide a testbed for evaluating information-retrieval tasks and techniques (Voorhees and Harman, 2005). TREC added an influential QA track in 1999, which led to a wide variety of factoid and non-factoid systems competing in annual evaluations. ", "Bloom_type": "remember", "question": "In what year did the U.S. government-sponsored TREC Text Retrieval Conferences start?", "options": ["1992", "1987", "1990", "1994"], "complexity": 0}, {"id": 43, "context": "Early work on large language models showed that they stored sufficient knowledge in the pretraining process to answer questions (Petroni et al., 2019; Raffel et al., 2020; Radford et al., 2019; Roberts et al., 2020), at first not competitively with special-purpose question answerers, but then surpassing them. Retrieval-augmented generation algorithms were first introduced as a way to improve language modeling (Khandelwal et al., 2019), but were quickly applied to question answering (Izacard et al., 2022; Ram et al., 2023; Shi et al., 2023). ", "Bloom_type": "remember", "question": "In early research on large language models, what did these models primarily use their pretraining for?", "options": ["To store enough knowledge to compete with human-level performance", "To create specialized question-answerers", "To enhance retrieval-based generation algorithms", "To develop advanced natural language understanding"], "complexity": 0}, {"id": 44, "context": "Chatbots are systems that can carry on extended conversations with the goal of mimicking the unstructured conversations or chats` characteristic of informal humanhuman interaction. While early systems like ELIZA (Weizenbaum, 1966) or PARRY (Colby et al., 1971) had theoretical goals like testing theories of psychological counseling, for most of the last 50 years chatbots have been designed for entertainment. That changed with the recent rise of neural chatbots like ChatGPT, which incorporate solutions to NLP tasks like question answering, writing tools, or machine translation into a conversational interface. A conversation with ChatGPT is shown in Fig. 15.12. In this section we describe neural chatbot architectures and datasets. ", "Bloom_type": "remember", "question": "What is the primary goal of chatbots?", "options": ["To entertain users", "To mimic formal human-human interactions", "To test theories of psychological counseling", "To perform complex mathematical calculations"], "complexity": 0}, {"id": 45, "context": "Parts of speech (also known as POS) and named entities are useful clues to sentence structure and meaning. Knowing whether a word is a noun or a verb tells us about likely neighboring words (nouns in English are preceded by determiners and adjectives, verbs by nouns) and syntactic structure (verbs have dependency links to nouns), making part-of-speech tagging a key aspect of parsing. Knowing if a named entity like Washington is a name of a person, a place, or a university is important to many natural language processing tasks like question answering, stance detection, or information extraction. ", "Bloom_type": "remember", "question": "In what way does knowing the part of speech help with parsing sentences?", "options": ["It aids in understanding the grammatical function of words.", "It helps identify the subject of the sentence.", "It assists in determining the tense of verbs.", "It predicts the next word in the sentence."], "complexity": 0}, {"id": 46, "context": "Named entity tagging is a useful first step in lots of natural language processing tasks. In sentiment analysis we might want to know a consumer`s sentiment toward a particular entity. Entities are a useful first stage in question answering, or for linking text to information in structured knowledge sources like Wikipedia. And named entity tagging is also central to tasks involving building semantic representations, like extracting events and the relationship between participants. ", "Bloom_type": "remember", "question": "In what type of task would you use named entity tagging as a first step?", "options": ["All of the above", "Sentiment analysis", "Linking text to information in structured knowledge sources", "Building semantic representations"], "complexity": 0}, {"id": 47, "context": " Dependency-based analysis provides information directly useful in further language processing tasks including information extraction, semantic parsing and question answering. ", "Bloom_type": "remember", "question": "In what type of analysis does dependency-based analysis provide direct use for language processing tasks such as information extraction, semantic parsing, and question answering?", "options": ["Dependency-based", "Contextual", "Semantic", "Syntactic"], "complexity": 0}, {"id": 48, "context": "Coreference is an important component of natural language processing. A dialogue system that has just told the user There is a 2pm flight on United and a 4pm one on Cathay Pacific must know which flight the user means by I`ll take the second one. A question answering system that uses Wikipedia to answer a question about Marie Curie must know who she was in the sentence She was born in Warsaw. And a machine translation system translating from a language like Spanish, in which pronouns can be dropped, must use coreference from the previous sentence to decide whether the Spanish sentence Me encanta el conocimiento, dice.` should be translated as I love knowledge, he says`, or I love knowledge, she says`. Indeed, this example comes from an actual news article in El Pas about a female professor and was mistranslated as he in machine translation because of inaccurate coreference resolution (Schiebinger, 2013). ", "Bloom_type": "remember", "question": "In what way does coreference play a crucial role in natural language processing?", "options": ["It helps machines understand the meaning of pronouns.", "It assists in identifying the subject of a sentence.", "It aids in recognizing synonyms within sentences.", "It enables machines to recognize the gender of speakers."], "complexity": 0}, {"id": 49, "context": "The notion of word similarity is very useful in larger semantic tasks. Knowing how similar two words are can help in computing how similar the meaning of two phrases or sentences are, a very important component of tasks like question answering, paraphrasing, and summarization. One way of getting values for word similarity is to ask humans to judge how similar one word is to another. A number of datasets have resulted from such experiments. For example the SimLex-999 dataset (Hill et al., 2015) gives values on a scale from 0 to 10, like the examples below, which range from near-synonyms (vanish, disappear) to pairs that scarcely seem to have anything in common (hole, agreement): ", "Bloom_type": "comprehension", "question": "Explain why understanding word similarity is crucial for tasks involving question answering?", "options": ["It assists in comparing the meanings of phrases or sentences.", "It helps in identifying synonyms.", "It aids in determining antonyms.", "It enables calculating the exact definition of each word."], "complexity": 1}, {"id": 50, "context": "Semantic Frames and Roles Closely related to semantic fields is the idea of a semantic frame. A semantic frame is a set of words that denote perspectives or participants in a particular type of event. A commercial transaction, for example, is a kind of event in which one entity trades money to another entity in return for some good or service, after which the good changes hands or perhaps the service is performed. This event can be encoded lexically by using verbs like buy (the event from the perspective of the buyer), sell (from the perspective of the seller), pay (focusing on the monetary aspect), or nouns like buyer. Frames have semantic roles (like buyer, seller, goods, money), and words in a sentence can take on these roles. Knowing that buy and sell have this relation makes it possible for a system to know that a sentence like Sam bought the book from Ling could be paraphrased as Ling sold the book to Sam, and that Sam has the role of the buyer in the frame and Ling the seller. Being able to recognize such paraphrases is important for question answering, and can help in shifting perspective for machine translation. ", "Bloom_type": "comprehension", "question": "What does the concept of semantic frames relate to in the context of question answering?", "options": ["Identifying the roles played by entities in an event", "Understanding the structure of sentences", "Recognizing synonyms within a sentence", "Determining the tense of a sentence"], "complexity": 1}, {"id": 51, "context": "In this chapter we`ll introduce the two most commonly used models. In the tf-idf model, an important baseline, the meaning of a word is defined by a simple function of the counts of nearby words. We will see that this method results in very long vectors that are sparse, i.e. mostly zeros (since most words simply never occur in the context of others). We`ll introduce the word2vec model family for constructing short, dense vectors that have useful semantic properties. We`ll also introduce the cosine, the standard way to use embeddings to compute semantic similarity, between two words, two sentences, or two documents, an important tool in practical applications like question answering, summarization, or automatic essay grading. ", "Bloom_type": "comprehension", "question": "What is the primary difference between the TF-IDF model and Word2Vec model in terms of their approach to vector representation?", "options": ["TF-IDF creates long vectors whereas Word2Vec generates short vectors.", "TF-IDF uses dense vectors while Word2Vec uses sparse vectors.", "Word2Vec focuses on semantic similarity, while TF-IDF emphasizes frequency-based measures.", "Both models produce similar types of vectors."], "complexity": 1}, {"id": 52, "context": "RNN-based language models can also be used to generate text. Text generation is of enormous practical importance, part of tasks like question answering, machine translation, text summarization, grammar correction, story generation, and conversational dialogue; any task where a system needs to produce text, conditioned on some other text. This use of a language model to generate text is one of the areas in which the impact of neural language models on NLP has been the largest. Text generation, along with image generation and code generation, constitute a new area of AI that is often called generative AI. ", "Bloom_type": "comprehension", "question": "What are some practical applications of using RNN-based language models for generating text?", "options": ["Text summarization, grammar correction, and conversational dialogue", "Image generation, code generation, and natural language processing (NLP)", "Natural language processing (NLP), image generation, and conversational dialogue", "Grammar correction, natural language processing (NLP), and code generation"], "complexity": 1}, {"id": 53, "context": "Technically an autoregressive model is a model that predicts a value at time t based on a linear function of the previous values at times t 2, and so on. Although language models are not linear (since they have many layers of non-linearities), we loosely refer to this generation technique as autoregressive generation since the word generated at each time step is conditioned on the word selected by the network from the previous step. Fig. 8.9 illustrates this approach. In this figure, the details of the RNN`s hidden layers and recurrent connections are hidden within the blue block. This simple architecture underlies state-of-the-art approaches to applications such as machine translation, summarization, and question answering. The key to these approaches is to prime the generation component with an appropriate context. That is, instead of simply using <s> to get things started we can provide a richer task-appropriate context; for translation the context is the sentence in the source language; for summarization it`s the long text we want to summarize. ", "Bloom_type": "comprehension", "question": "What does the autoregressive generation technique involve?", "options": ["Predicting future values based on past values", "Generating random sequences", "Using fixed templates", "Ignoring previous steps"], "complexity": 1}, {"id": 54, "context": "Encoder-decoder networks, sometimes called sequence-to-sequence networks, are models capable of generating contextually appropriate, arbitrary length, output sequences given an input sequence. Encoder-decoder networks have been applied to a very wide range of applications including summarization, question answering, and dialogue, but they are particularly popular for machine translation. ", "Bloom_type": "comprehension", "question": "Explain how encoder-decoder networks are utilized in various applications such as summarization, question answering, and dialogue, focusing on their primary use case for machine translation.", "options": ["They excel at generating coherent responses to questions based on the input context.", "Encoder-decoder networks are best suited for summarizing large amounts of data.", "Encoder-decoder networks are less effective than other methods for translating languages.", "Their primary function is to translate sentences from one language into another."], "complexity": 1}, {"id": 55, "context": "able performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots. ", "Bloom_type": "comprehension", "question": "What aspect of AI has seen significant advancements due to its ability to perform various natural language tasks?", "options": ["Natural Language Processing (NLP)", "Image recognition", "Speech synthesis", "Robotics"], "complexity": 1}, {"id": 56, "context": "We can also cast more complex tasks as word prediction. Consider question answering, in which the system is given a question (for example a question with a simple factual answer) and must give a textual answer; we introduce this task in detail in Chapter 14. We can cast the task of question answering as word prediction by giving a language model a question and a token like A: suggesting that an answer should come next: ", "Bloom_type": "comprehension", "question": "How does question answering relate to word prediction?", "options": ["Question answering uses word prediction to find the most probable answer.", "Word prediction is only used for generating answers.", "Question answering involves predicting the next word in a sentence.", "Word prediction helps in understanding the meaning of questions."], "complexity": 1}, {"id": 57, "context": "Other factors While the predictive accuracy of a language model, as measured by perplexity, is a very useful metric, we also care about different kinds of accuracy, for the downstream tasks we apply our language model to. For each task like machine translation, summarization, question answering, speech recognition, and dialogue, we can measure the accuracy at those tasks. Future chapters will introduce taskspecific metrics that allow us to evaluate how accuracy or correct language models are at these downstream tasks. ", "Bloom_type": "comprehension", "question": "What type of accuracy does the context mention when discussing the predictive accuracy of a language model?", "options": ["Accuracy for question answering", "Overall accuracy across all tasks", "Accuracy specific to machine translation", "Accuracy for speech recognition and dialogue"], "complexity": 1}, {"id": 58, "context": "Large pretrained neural language models exhibit many of the potential harms discussed in Chapter 4 and Chapter 6. Many of these harms become realized when pretrained language models are used for any downstream task, particularly those involving text generation, whether question answering, machine translation, or in assistive technologies like writing aids or web search query completion, or predictive typing for email (Olteanu et al., 2020). ", "Bloom_type": "comprehension", "question": "Explain how large pretrained neural language models can lead to potential harms in various applications?", "options": ["They can cause ethical issues due to their ability to generate harmful content.", "Their use in question answering leads to increased accuracy but no other significant impacts.", "Pretrained models have no impact on harm since they are just learning tools.", "The primary concern is the increase in computational power needed."], "complexity": 1}, {"id": 59, "context": " Many NLP taskssuch as question answering, summarization, sentiment, and machine translationcan be cast as tasks of word prediction and hence addressed with Large language models. ", "Bloom_type": "comprehension", "question": "Explain how many NLP tasks can be transformed into word prediction problems for addressing with large language models?", "options": ["Three", "Only one", "Two", "Four"], "complexity": 1}, {"id": 60, "context": "Transformers were first applied as encoder-decoders (Vaswani et al., 2017) and then to masked language modeling (Devlin et al., 2019) (as we`ll see in Chapter 13 and Chapter 11). Radford et al. (2019) then showed that the transformer-based autoregressive language model GPT2 could perform zero-shot on many NLP tasks like summarization and question answering. ", "Bloom_type": "comprehension", "question": "What was one of the early applications of transformers before they were adapted for question answering?", "options": ["Masked language modeling", "Sequence-to-sequence translation", "Zero-shot learning", "Text classification"], "complexity": 1}, {"id": 61, "context": "Let`s begin by introducing the bidirectional transformer encoder that underlies models like BERT and its descendants like RoBERTa (Liu et al., 2019) or SpanBERT (Joshi et al., 2020). In Chapter 9 we introduced causal (left-to-right) transformers and in Chapter 10 saw how they can serve as the basis for language models that can be applied to autoregressive contextual generation problems like question answering or summarization. But this left-to-right nature of these models is also a limitation, because there are tasks for which it would be useful, when processing a token, to be able to peak at future tokens. This is especially true for sequence labeling tasks ", "Bloom_type": "comprehension", "question": "Explain why the left-to-right nature of transformers might limit their usefulness for certain types of tasks?", "options": ["Because they do not allow peeking at future tokens during processing.", "Because they cannot handle long sequences efficiently.", "Because they require more computational resources than right-to-left models.", "Both B and C are correct."], "complexity": 1}, {"id": 62, "context": "Fig. 12.2 illustrates a few-shot example from an extractive question answering task. The context combines the task definition along with three gold-standard question and answer pairs from the training set. ", "Bloom_type": "comprehension", "question": "Explain how Fig. 12.2 represents a few-shot example in an extractive question answering task.", "options": ["Fig. 12.2 presents a visual representation of the relationship between questions and their corresponding answers.", "Fig. 12.2 shows examples of questions asked during training.", "Fig. 12.2 demonstrates the use of machine learning algorithms for extracting answers.", "Fig. 12.2 illustrates the process of creating new questions based on existing ones."], "complexity": 1}, {"id": 63, "context": "But more generally, the best way to select demonstrations from the training set is programmatically: choosing the set of demonstrations that most increases task performance of the prompt on a test set. Task performance for sentiment analysis or multiple-choice question answering can be measured in accuracy; for machine translation with chrF, and for summarization via Rouge. Systems like DSPy (Khattab et al., 2024), a framework for algorithmically optimizing LM prompts, can automatically find the optimum set of demonstrations to include by searching through the space of possible demonstrations to include. We`ll return to automatic prompt optimization in Section 12.5. ", "Bloom_type": "comprehension", "question": "What metric is commonly used to measure the performance of systems like DSPy in selecting demonstrations for different tasks?", "options": ["Accuracy", "Precision", "Recall", "F1 Score"], "complexity": 1}, {"id": 64, "context": "Many huge instruction tuning datasets have been created, covering many tasks and languages. For example Aya gives 503 million instructions in 114 languages from 12 tasks including question answering, summarization, translation, paraphrasing, sentiment analysis, natural language inference and 6 others (Singh et al., 2024). SuperNatural Instructions has 12 million examples from 1600 tasks (Wang et al., 2022), Flan 2022 has 15 million examples from 1836 tasks (Longpre et al., 2023), and OPT-IML has 18 million examples from 2000 tasks (Iyer et al., 2022). ", "Bloom_type": "comprehension", "question": "What type of task does SUPERNATURAL INSTRUCTIONS cover?", "options": ["Question Answering", "Translation", "Paraphrasing", "Sentiment Analysis"], "complexity": 1}, {"id": 65, "context": "Language models are evaluated in many ways. we introduced some evaluations for in Section 10.4, including measuring the language model`s perplexity on a test set, evaluating its accuracy on various NLP tasks, as well as benchmarks that help measure efficiency, toxicity, fairness, and so on. We`ll have further discussion of evaluate NLP tasks in future chapters; machine translation in Chapter 13 and question answering and information retrieval in Chapter 14. ", "Bloom_type": "comprehension", "question": "What aspect of NLP tasks does this response discuss?", "options": ["Evaluation methods", "Machine Translation", "Information Retrieval", "Toxicity Measurement"], "complexity": 1}, {"id": 66, "context": " One method for model alignment is instruction tuning, in which the model is finetuned (using the next-word-prediction language model objective) on a dataset of instructions together with correct responses. Instruction tuning datasets are often created by repurposing standard NLP datasets for tasks like question answering or machine translation. ", "Bloom_type": "comprehension", "question": "Explain how instruction tuning differs from traditional fine-tuning methods?", "options": ["Traditional fine-tuning focuses solely on instruction data whereas instruction tuning incorporates response data.", "Instruction tuning involves using only the response data while traditional fine-tuning uses both instruction and response data.", "Instruction tuning requires more computational resources than traditional fine-tuning.", "There is no significant difference between instruction tuning and traditional fine-tuning."], "complexity": 1}, {"id": 67, "context": "In this second part of the book we introduce fundamental NLP applications: machine translation, information retrieval, question answering, dialogue systems, and speech recognition. ", "Bloom_type": "comprehension", "question": "Which of the following is NOT one of the fundamental NLP applications introduced in the book?", "options": ["Dialogue Systems", "Machine Translation", "Information Retrieval", "Speech Recognition"], "complexity": 1}, {"id": 68, "context": "People need to know things. So pretty much as soon as there were computers we were asking them questions. Systems in the 1960s were answering questions about baseball statistics and scientific facts. Even fictional computers in the 1970s like Deep Thought, invented by Douglas Adams in The Hitchhiker`s Guide to the Galaxy, answered the Ultimate Question Of Life, The Universe, and Everything.1 And because so much knowledge is encoded in text, question answering (QA) systems were performing at human levels even before LLMs: IBM`s Watson system won the TV game-show Jeopardy! in 2011, surpassing humans at answering questions like: ", "Bloom_type": "comprehension", "question": "What was one of the earliest examples of a computer answering questions?", "options": ["Deep Thought from The Hitchhiker's Guide to the Galaxy", "IBM's Watson system winning Jeopardy!", "Systems in the 1960s answering baseball statistics", "Fictional characters inventing complex systems"], "complexity": 1}, {"id": 69, "context": "Question answering systems are designed to fill human information needs. Since a lot of information is present in text form (on the web or in other data like our email, or books), question answering is closely related to the task behind search engines. Indeed, the distinction is becoming ever more fuzzy, as modern search engines are integrated with large language models trained to do question answering. Question answering systems often focus on a useful subset of information needs: factoid questions, questions of fact or reasoning that can be answered with simple facts expressed in short or medium-length texts, like the following: ", "Bloom_type": "comprehension", "question": "How does the concept of question answering relate to search engine functionality?", "options": ["Modern search engines integrate question answering capabilities directly into their core functionalities.", "Search engines use question answering to replace traditional keyword searches.", "The relationship between question answering and search engines is unclear due to advancements in AI.", "There is no direct relation; they serve different purposes."], "complexity": 1}, {"id": 70, "context": " QUESTION ANSWERING, INFORMATION RETRIEVAL, AND RAG ", "Bloom_type": "comprehension", "question": "What are some key components involved in the process of question answering?", "options": ["Information retrieval techniques and machine learning algorithms", "Natural language processing and semantic understanding", "Text summarization and topic modeling", "Query expansion and ranking systems"], "complexity": 1}, {"id": 71, "context": "A second problem is that simply prompting a large language model doesn`t allow us to ask questions about proprietary data. A common use of question answering is about data like our personal email or medical records. Or a company may have internal documents that contain answers for customer service or internal use. Or legal firms need to ask questions about legal discovery from proprietary documents. Finally, static large language models also have problems with questions about rapidly changing information (like questions about something that happened last week) since LLMs won`t have up-to-date information from after their release data. For this reason the most common way to do question-answering with LLMs is retrieval-augmented generation or RAG, and that is the method we will focus on in this chapter. In RAG we use information retrieval (IR) techniques to retrieve documents that are likely to have information that might help answer the question. Then we use a large language model to generate an answer given these documents. Basing our answers on retrieved documents can solve some of the problems with using simple prompting to answer questions. First, it helps ensure that the answer is grounded in facts from some curated dataset. And the system can give the user the answer accompanied by the context of the passage or document the answer came from. This information can help users have confidence in the accuracy of the answer (or help them spot when it is wrong!). And these retrieval techniques can be used on any proprietary data we want, such as legal or medical data for those applications. ", "Bloom_type": "comprehension", "question": "What is one common application of question answering in the context of proprietary data?", "options": ["Using IR techniques to find relevant documents and generating answers based on them", "Prompting a large language model directly with proprietary data", "Generating answers through simple prompts only", "Retrieving documents from public datasets exclusively"], "complexity": 1}, {"id": 72, "context": "We`ll begin by introducing information retrieval, the task of choosing the most relevant document from a document set given a user`s query expressing their information need. We`ll see the classic method based on cosines of sparse tf-idf vectors, a modern neural dense` retrievers based on instead representing queries and documents neurally with BERT or other language models. We then introduce retrieverbased question answering and the retrieval-augmented generation paradigm. ", "Bloom_type": "comprehension", "question": "What are two methods discussed for information retrieval mentioned in the context?", "options": ["Sparse tf-idf vectors and neural dense retrievers", "Cosine similarity and BERT-based retrieval", "Query representation using language models and cosine similarity", "Document relevance and retrieval augmentation"], "complexity": 1}, {"id": 73, "context": "Finally, we`ll discuss various QA datasets. These are used for finetuning LLMs in instruction tuning, as we saw in Chapter 12. And they are also used as benchmarks, since question answering has an important function as a benchmark for measuring the abilities of language models. ", "Bloom_type": "comprehension", "question": "Explain how question answering functions as a benchmark for measuring the abilities of language models.", "options": ["QA datasets help evaluate the model's performance by assessing its ability to answer questions accurately.", "QA datasets provide training data for fine-tuning LLMs.", "QA datasets serve as examples for teaching new languages.", "QA datasets are only useful for academic research."], "complexity": 1}, {"id": 74, "context": " QUESTION ANSWERING, INFORMATION RETRIEVAL, AND RAG ", "Bloom_type": "comprehension", "question": "What are some key components involved in the process of question answering?", "options": ["Information retrieval techniques and machine learning algorithms", "Natural language processing and expert systems", "Database management and data mining", "Text summarization and speech recognition"], "complexity": 1}, {"id": 75, "context": " QUESTION ANSWERING, INFORMATION RETRIEVAL, AND RAG ", "Bloom_type": "comprehension", "question": "What are some key components involved in question answering systems?", "options": ["Natural Language Processing (NLP), Information Retrieval, and Reinforcement Learning (Rag)", "Information Retrieval, Natural Language Processing (NLP), and Machine Learning (ML)", "Machine Learning (ML), Information Retrieval, and Reinforcement Learning (Rag)", "Reinforcement Learning (Rag), Information Retrieval, and Natural Language Processing (NLP)"], "complexity": 1}, {"id": 76, "context": " QUESTION ANSWERING, INFORMATION RETRIEVAL, AND RAG ", "Bloom_type": "comprehension", "question": "What are some key components involved in question answering systems?", "options": ["Information retrieval techniques and machine learning algorithms", "Natural language processing and database management", "Text summarization and speech recognition", "Image classification and video analysis"], "complexity": 1}, {"id": 77, "context": " QUESTION ANSWERING, INFORMATION RETRIEVAL, AND RAG ", "Bloom_type": "comprehension", "question": "What does the response 'QUESTION ANSWERING, INFORMATION RETRIEVAL, AND RAG' indicate about the topic?", "options": ["It suggests that the topic involves retrieving information for answers.", "It indicates that the topic focuses on how questions are answered.", "It implies that the topic deals with reasoning algorithms.", "It means none of the above."], "complexity": 1}, {"id": 78, "context": " QUESTION ANSWERING, INFORMATION RETRIEVAL, AND RAG ", "Bloom_type": "comprehension", "question": "What are some key components involved in the process of question answering?", "options": ["All of the above", "Information retrieval techniques and machine learning algorithms", "Natural language processing and semantic understanding", "Text summarization and topic modeling"], "complexity": 1}, {"id": 79, "context": "The dominant paradigm for question answering is to answer a user`s question by first finding supportive text segments from the web or another other large collection of documents, and then generating an answer based on the documents. The method of generating based on retrieved documents is called retrieval-augmented generation or RAG, and the two components are sometimes called the retriever and the reader (Chen et al., 2017a). Fig. 14.9 sketches out this standard QA model. ", "Bloom_type": "comprehension", "question": "What does the retrieval-augmented generation method involve in the context of question answering?", "options": ["Finding supporting text segments from the web or other large collections of documents and generating an answer based on those documents.", "Using machine learning algorithms to directly generate answers without any support from external documents.", "Combining human input with AI-generated responses to improve accuracy.", "Analyzing existing questions to predict future ones."], "complexity": 1}, {"id": 80, "context": " QUESTION ANSWERING, INFORMATION RETRIEVAL, AND RAG ", "Bloom_type": "comprehension", "question": "What are some key components involved in question answering systems?", "options": ["Information retrieval techniques and machine learning algorithms", "Natural language processing and database management", "Text summarization and speech recognition", "Image classification and video analysis"], "complexity": 1}, {"id": 81, "context": "And simple conditional generation for question answering adds a prompt like Q: , followed by a query q , and A:, all concatenated: ", "Bloom_type": "comprehension", "question": "What does adding a prompt like Q:, followed by a query q, and A:, all concatenated mean in the context of question answering?", "options": ["It involves generating a response using a specific format including a question and an answer.", "It means creating a response based solely on the query.", "It suggests focusing only on the answer part.", "It implies ignoring the question."], "complexity": 1}, {"id": 82, "context": "There are scores of question answering datasets, used both for instruction tuning and for evaluation of the question answering abilities of language models. ", "Bloom_type": "comprehension", "question": "What type of datasets are commonly used for training and testing question answering systems?", "options": ["Both instruction tuning and evaluation datasets", "Only instruction tuning datasets", "Only evaluation datasets", "None of the above"], "complexity": 1}, {"id": 83, "context": "A similar natural question set is the MS MARCO (Microsoft Machine Reading Comprehension) collection of datasets, including 1 million real anonymized English questions from Microsoft Bing query logs together with a human generated answer and 9 million passages (Bajaj et al., 2016), that can be used both to test retrieval ranking and question answering. ", "Bloom_type": "comprehension", "question": "What type of dataset does the MS MARCO collection include?", "options": ["Natural language processing data", "Machine learning algorithms", "Text-based datasets", "Image recognition databases"], "complexity": 1}, {"id": 84, "context": " QUESTION ANSWERING, INFORMATION RETRIEVAL, AND RAG ", "Bloom_type": "comprehension", "question": "What are some key components involved in question answering systems?", "options": ["Natural language processing (NLP), machine learning algorithms, and retrieval techniques.", "Machine learning algorithms only, no NLP or retrieval techniques.", "Retrieval techniques only, no NLP or machine learning algorithms.", "Information retrieval methods exclusively."], "complexity": 1}, {"id": 85, "context": "Some of the question datasets described above augment each question with passage(s) from which the answer can be extracted. These datasets were mainly created for an earlier QA task called reading comprehension in which a model is given a question and a document and is required to extract the answer from the given document. We sometimes call the task of question answering given one or more documents (for example via RAG), the open book QA task, while the task of answering directly from the LM with no retrieval component at all is the closed book QA task.5 Thus datasets like Natural Questions can be treated as open book if the solver uses each question`s attached document, or closed book if the documents are not used, while datasets like MMLU are solely closed book. ", "Bloom_type": "comprehension", "question": "What distinguishes the open book QA task from the closed book QA task?", "options": ["Open book requires using the document attached to the question, whereas closed book does not.", "Closed book involves direct answers from language models without any external resources.", "Open book allows access to additional passages beyond those in the original document.", "The distinction between these two types of questions depends entirely on the dataset."], "complexity": 1}, {"id": 86, "context": " QUESTION ANSWERING, INFORMATION RETRIEVAL, AND RAG ", "Bloom_type": "comprehension", "question": "What are some key components involved in the process of question answering?", "options": ["Information retrieval techniques and machine learning algorithms", "Natural language processing and expert systems", "Text summarization and knowledge graph construction", "Query formulation and response generation"], "complexity": 1}, {"id": 87, "context": "This chapter introduced the tasks of question answering and information retrieval. ", "Bloom_type": "comprehension", "question": "What are two primary functions covered in this chapter regarding question answering?", "options": ["Information retrieval and machine learning", "Machine learning and natural language processing", "Natural language processing and data mining", "Data mining and information retrieval"], "complexity": 1}, {"id": 88, "context": " Question answering (QA) is the task of answering a user`s questions.  We focus in this chapter on the task of retrieval-based question answering, in which the user`s questions are intended to be answered by the material in some set of documents (which might be the web). ", "Bloom_type": "comprehension", "question": "What does retrieval-based question answering entail?", "options": ["It focuses on retrieving relevant information from a specific document for each question.", "It involves directly answering the user's questions using knowledge from external sources.", "It requires understanding the user's intent before providing an answer.", "It aims to provide answers based solely on the content within the documents."], "complexity": 1}, {"id": 89, "context": " Question answering systems generally use the retriever/reader architecture. In the retriever stage, an IR system is given a query and returns a set of documents. ", "Bloom_type": "comprehension", "question": "In the retriever stage of question answering systems, what does an IR system typically do?", "options": ["Finds relevant documents", "Returns a set of questions", "Provides answers directly", "Updates user preferences"], "complexity": 1}, {"id": 90, "context": " QUESTION ANSWERING, INFORMATION RETRIEVAL, AND RAG ", "Bloom_type": "comprehension", "question": "What does the field of study encompassed by 'QUESTION ANSWERING, INFORMATION RETRIEVAL, AND RAG' include?", "options": ["Both Information Retrieval and Question Answering", "Information retrieval only", "RAG only", "None of the above"], "complexity": 1}, {"id": 91, "context": "By a decade later, neural models were applied to semantic parsing (Dong and Lapata 2016, Jia and Liang 2016), and then to knowledge-based question answering by mapping text to SQL (Iyer et al., 2017). ", "Bloom_type": "comprehension", "question": "Explain how neural models have been utilized for semantic parsing over time.", "options": ["Neural models were first used for semantic parsing before being applied to knowledge-based question answering.", "Semantic parsing was initially applied to neural models.", "Knowledge-based question answering was the initial use case for neural models.", "Semantic parsing was only applied after neural models had already been used for knowledge-based question answering."], "complexity": 1}, {"id": 92, "context": "Meanwhile, the information-retrieval paradigm for question answering was influenced by the rise of the web in the 1990s. The U.S. government-sponsored TREC (Text REtrieval Conference) evaluations, run annually since 1992, provide a testbed for evaluating information-retrieval tasks and techniques (Voorhees and Harman, 2005). TREC added an influential QA track in 1999, which led to a wide variety of factoid and non-factoid systems competing in annual evaluations. ", "Bloom_type": "comprehension", "question": "Explain how the information-retrieval paradigm for question answering evolved due to the rise of the web in the 1990s.", "options": ["The paradigm shifted from retrieval-based methods to more advanced AI techniques.", "It became less relevant because of the decline of the internet.", "There was no significant change; it remained largely unchanged.", "The paradigm transitioned towards more interactive user interfaces."], "complexity": 1}, {"id": 93, "context": "Early work on large language models showed that they stored sufficient knowledge in the pretraining process to answer questions (Petroni et al., 2019; Raffel et al., 2020; Radford et al., 2019; Roberts et al., 2020), at first not competitively with special-purpose question answerers, but then surpassing them. Retrieval-augmented generation algorithms were first introduced as a way to improve language modeling (Khandelwal et al., 2019), but were quickly applied to question answering (Izacard et al., 2022; Ram et al., 2023; Shi et al., 2023). ", "Bloom_type": "comprehension", "question": "Explain how retrieval-augmented generation algorithms improved upon traditional question answering methods?", "options": ["Both A) and B)", "By enhancing the accuracy of retrieved answers through machine learning techniques.", "Through the integration of external databases for faster search results.", "By reducing the need for human intervention in generating responses."], "complexity": 1}, {"id": 94, "context": "Chatbots are systems that can carry on extended conversations with the goal of mimicking the unstructured conversations or chats` characteristic of informal humanhuman interaction. While early systems like ELIZA (Weizenbaum, 1966) or PARRY (Colby et al., 1971) had theoretical goals like testing theories of psychological counseling, for most of the last 50 years chatbots have been designed for entertainment. That changed with the recent rise of neural chatbots like ChatGPT, which incorporate solutions to NLP tasks like question answering, writing tools, or machine translation into a conversational interface. A conversation with ChatGPT is shown in Fig. 15.12. In this section we describe neural chatbot architectures and datasets. ", "Bloom_type": "comprehension", "question": "What was one significant change in the design of chatbots over the past few decades?", "options": ["The primary purpose shifted from entertainment to practical applications.", "Chatbots were initially created purely for academic research.", "Early chatbots aimed at simulating human-like interactions.", "Neural chatbots replaced traditional methods entirely."], "complexity": 1}, {"id": 95, "context": "Parts of speech (also known as POS) and named entities are useful clues to sentence structure and meaning. Knowing whether a word is a noun or a verb tells us about likely neighboring words (nouns in English are preceded by determiners and adjectives, verbs by nouns) and syntactic structure (verbs have dependency links to nouns), making part-of-speech tagging a key aspect of parsing. Knowing if a named entity like Washington is a name of a person, a place, or a university is important to many natural language processing tasks like question answering, stance detection, or information extraction. ", "Bloom_type": "comprehension", "question": "What is an example of how knowing parts of speech can help in understanding sentence structure?", "options": ["Determiners precede nouns in sentences.", "Verbs follow nouns in sentences.", "Nouns are always followed by adjectives.", "Named entities indicate the type of word."], "complexity": 1}, {"id": 96, "context": "Named entity tagging is a useful first step in lots of natural language processing tasks. In sentiment analysis we might want to know a consumer`s sentiment toward a particular entity. Entities are a useful first stage in question answering, or for linking text to information in structured knowledge sources like Wikipedia. And named entity tagging is also central to tasks involving building semantic representations, like extracting events and the relationship between participants. ", "Bloom_type": "comprehension", "question": "What is the primary purpose of named entity tagging in relation to question answering?", "options": ["Both A and C", "To identify entities within a sentence", "To determine the sentiment towards an entity", "To link text to structured data sources"], "complexity": 1}, {"id": 97, "context": " Dependency-based analysis provides information directly useful in further language processing tasks including information extraction, semantic parsing and question answering. ", "Bloom_type": "comprehension", "question": "What does dependency-based analysis provide for language processing tasks?", "options": ["Directly useful information for all tasks", "Information only relevant for semantic parsing", "Useful information for extracting facts from texts", "No direct use for any task"], "complexity": 1}, {"id": 98, "context": "Coreference is an important component of natural language processing. A dialogue system that has just told the user There is a 2pm flight on United and a 4pm one on Cathay Pacific must know which flight the user means by I`ll take the second one. A question answering system that uses Wikipedia to answer a question about Marie Curie must know who she was in the sentence She was born in Warsaw. And a machine translation system translating from a language like Spanish, in which pronouns can be dropped, must use coreference from the previous sentence to decide whether the Spanish sentence Me encanta el conocimiento, dice.` should be translated as I love knowledge, he says`, or I love knowledge, she says`. Indeed, this example comes from an actual news article in El Pas about a female professor and was mistranslated as he in machine translation because of inaccurate coreference resolution (Schiebinger, 2013). ", "Bloom_type": "comprehension", "question": "Explain how coreference resolution affects the accuracy of machine translations?", "options": ["It prevents the misuse of pronouns leading to incorrect translations.", "It ensures that pronouns are correctly replaced with their antecedents.", "It guarantees that all sentences are grammatically correct.", "It eliminates the need for human translators."], "complexity": 1}, {"id": 99, "context": "The notion of word similarity is very useful in larger semantic tasks. Knowing how similar two words are can help in computing how similar the meaning of two phrases or sentences are, a very important component of tasks like question answering, paraphrasing, and summarization. One way of getting values for word similarity is to ask humans to judge how similar one word is to another. A number of datasets have resulted from such experiments. For example the SimLex-999 dataset (Hill et al., 2015) gives values on a scale from 0 to 10, like the examples below, which range from near-synonyms (vanish, disappear) to pairs that scarcely seem to have anything in common (hole, agreement): ", "Bloom_type": "application", "question": "What method could be used to measure the similarity between two words?", "options": ["Using a human judgment based on their understanding of synonyms.", "By comparing the frequency of occurrence of each word in a corpus.", "Analyzing the length of the words themselves.", "Calculating the distance between the vectors representing the words."], "complexity": 2}, {"id": 100, "context": "Semantic Frames and Roles Closely related to semantic fields is the idea of a semantic frame. A semantic frame is a set of words that denote perspectives or participants in a particular type of event. A commercial transaction, for example, is a kind of event in which one entity trades money to another entity in return for some good or service, after which the good changes hands or perhaps the service is performed. This event can be encoded lexically by using verbs like buy (the event from the perspective of the buyer), sell (from the perspective of the seller), pay (focusing on the monetary aspect), or nouns like buyer. Frames have semantic roles (like buyer, seller, goods, money), and words in a sentence can take on these roles. Knowing that buy and sell have this relation makes it possible for a system to know that a sentence like Sam bought the book from Ling could be paraphrased as Ling sold the book to Sam, and that Sam has the role of the buyer in the frame and Ling the seller. Being able to recognize such paraphrases is important for question answering, and can help in shifting perspective for machine translation. ", "Bloom_type": "application", "question": "What is an essential component for understanding how sentences relate to each other?", "options": ["Semantic frames", "Lexical items", "Grammatical structures", "Contextual clues"], "complexity": 2}, {"id": 101, "context": "In this chapter we`ll introduce the two most commonly used models. In the tf-idf model, an important baseline, the meaning of a word is defined by a simple function of the counts of nearby words. We will see that this method results in very long vectors that are sparse, i.e. mostly zeros (since most words simply never occur in the context of others). We`ll introduce the word2vec model family for constructing short, dense vectors that have useful semantic properties. We`ll also introduce the cosine, the standard way to use embeddings to compute semantic similarity, between two words, two sentences, or two documents, an important tool in practical applications like question answering, summarization, or automatic essay grading. ", "Bloom_type": "application", "question": "Which model introduces the cosine as a standard method for computing semantic similarity?", "options": ["Word2Vec Model Family", "TF-IDF Model", "Cosine Method", "None of the above"], "complexity": 2}, {"id": 102, "context": "RNN-based language models can also be used to generate text. Text generation is of enormous practical importance, part of tasks like question answering, machine translation, text summarization, grammar correction, story generation, and conversational dialogue; any task where a system needs to produce text, conditioned on some other text. This use of a language model to generate text is one of the areas in which the impact of neural language models on NLP has been the largest. Text generation, along with image generation and code generation, constitute a new area of AI that is often called generative AI. ", "Bloom_type": "application", "question": "What is an example of a task where a language model can generate text?", "options": ["Generating stories", "Summarizing news articles", "Translating between languages", "Writing poetry"], "complexity": 2}, {"id": 103, "context": "Technically an autoregressive model is a model that predicts a value at time t based on a linear function of the previous values at times t 2, and so on. Although language models are not linear (since they have many layers of non-linearities), we loosely refer to this generation technique as autoregressive generation since the word generated at each time step is conditioned on the word selected by the network from the previous step. Fig. 8.9 illustrates this approach. In this figure, the details of the RNN`s hidden layers and recurrent connections are hidden within the blue block. This simple architecture underlies state-of-the-art approaches to applications such as machine translation, summarization, and question answering. The key to these approaches is to prime the generation component with an appropriate context. That is, instead of simply using <s> to get things started we can provide a richer task-appropriate context; for translation the context is the sentence in the source language; for summarization it`s the long text we want to summarize. ", "Bloom_type": "application", "question": "In question answering, how does the initial context affect the model's performance?", "options": ["It sets the starting point for generating the response.", "It provides the first few characters of the input.", "It determines the length of the generated response.", "It influences the type of questions asked."], "complexity": 2}, {"id": 104, "context": "Encoder-decoder networks, sometimes called sequence-to-sequence networks, are models capable of generating contextually appropriate, arbitrary length, output sequences given an input sequence. Encoder-decoder networks have been applied to a very wide range of applications including summarization, question answering, and dialogue, but they are particularly popular for machine translation. ", "Bloom_type": "application", "question": "What is the primary function of encoder-decoder networks?", "options": ["To translate between languages", "To encode textual data only", "To decode arbitrary length sequences", "To summarize large datasets"], "complexity": 2}, {"id": 105, "context": "able performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots. ", "Bloom_type": "application", "question": "What is the primary benefit of using large language models in various NLP tasks?", "options": ["They excel at understanding and generating human-like text.", "They can only perform numerical calculations.", "They are limited to processing images.", "They cannot handle any type of input."], "complexity": 2}, {"id": 106, "context": "We can also cast more complex tasks as word prediction. Consider question answering, in which the system is given a question (for example a question with a simple factual answer) and must give a textual answer; we introduce this task in detail in Chapter 14. We can cast the task of question answering as word prediction by giving a language model a question and a token like A: suggesting that an answer should come next: ", "Bloom_type": "application", "question": "What is the primary method used for casting question answering tasks into word prediction?", "options": ["By generating tokens", "Using pre-trained models", "By providing a specific type of input", "Through direct instruction"], "complexity": 2}, {"id": 107, "context": "Other factors While the predictive accuracy of a language model, as measured by perplexity, is a very useful metric, we also care about different kinds of accuracy, for the downstream tasks we apply our language model to. For each task like machine translation, summarization, question answering, speech recognition, and dialogue, we can measure the accuracy at those tasks. Future chapters will introduce taskspecific metrics that allow us to evaluate how accuracy or correct language models are at these downstream tasks. ", "Bloom_type": "application", "question": "What type of accuracy does not directly measure the performance of a language model on specific tasks such as machine translation, summarization, question answering, speech recognition, and dialogue?", "options": ["Perplexity", "BLEU score", "ROUGE score", "F1 score"], "complexity": 2}, {"id": 108, "context": "Large pretrained neural language models exhibit many of the potential harms discussed in Chapter 4 and Chapter 6. Many of these harms become realized when pretrained language models are used for any downstream task, particularly those involving text generation, whether question answering, machine translation, or in assistive technologies like writing aids or web search query completion, or predictive typing for email (Olteanu et al., 2020). ", "Bloom_type": "application", "question": "What is an example of how large pretrained neural language models can lead to harmful outcomes?", "options": ["All of the above", "They may produce biased results.", "They might cause privacy violations.", "They could lead to misinformation."], "complexity": 2}, {"id": 109, "context": " Many NLP taskssuch as question answering, summarization, sentiment, and machine translationcan be cast as tasks of word prediction and hence addressed with Large language models. ", "Bloom_type": "application", "question": "Which type of NLP task can be effectively handled using large language models?", "options": ["All of the above", "Machine Translation", "Sentiment Analysis", "Summarization"], "complexity": 2}, {"id": 110, "context": "Transformers were first applied as encoder-decoders (Vaswani et al., 2017) and then to masked language modeling (Devlin et al., 2019) (as we`ll see in Chapter 13 and Chapter 11). Radford et al. (2019) then showed that the transformer-based autoregressive language model GPT2 could perform zero-shot on many NLP tasks like summarization and question answering. ", "Bloom_type": "application", "question": "What is an example of how transformers are used for question answering?", "options": ["Transformers were initially used for sequence-to-sequence models but later extended to include question answering.", "Transformers are only used for image classification.", "Transformers are not applicable to any task.", "Transformers can only be used for machine translation."], "complexity": 2}, {"id": 111, "context": "Let`s begin by introducing the bidirectional transformer encoder that underlies models like BERT and its descendants like RoBERTa (Liu et al., 2019) or SpanBERT (Joshi et al., 2020). In Chapter 9 we introduced causal (left-to-right) transformers and in Chapter 10 saw how they can serve as the basis for language models that can be applied to autoregressive contextual generation problems like question answering or summarization. But this left-to-right nature of these models is also a limitation, because there are tasks for which it would be useful, when processing a token, to be able to peak at future tokens. This is especially true for sequence labeling tasks ", "Bloom_type": "application", "question": "What is an example of a task where peeking at future tokens could be beneficial?", "options": ["Generating questions based on previous answers", "Summarizing long documents", "Classifying emails as spam or not spam", "Predicting stock market trends"], "complexity": 2}, {"id": 112, "context": "Fig. 12.2 illustrates a few-shot example from an extractive question answering task. The context combines the task definition along with three gold-standard question and answer pairs from the training set. ", "Bloom_type": "application", "question": "What does Fig. 12.2 show?", "options": ["Three examples of questions and answers from a question answering task", "An illustration of a machine learning model architecture", "A detailed explanation of the question answering method", "The structure of a dataset used for training models"], "complexity": 2}, {"id": 113, "context": "But more generally, the best way to select demonstrations from the training set is programmatically: choosing the set of demonstrations that most increases task performance of the prompt on a test set. Task performance for sentiment analysis or multiple-choice question answering can be measured in accuracy; for machine translation with chrF, and for summarization via Rouge. Systems like DSPy (Khattab et al., 2024), a framework for algorithmically optimizing LM prompts, can automatically find the optimum set of demonstrations to include by searching through the space of possible demonstrations to include. We`ll return to automatic prompt optimization in Section 12.5. ", "Bloom_type": "application", "question": "What metric is commonly used to measure the effectiveness of a demonstration set in improving task performance?", "options": ["Accuracy", "Precision", "Recall", "F1 Score"], "complexity": 2}, {"id": 114, "context": "Many huge instruction tuning datasets have been created, covering many tasks and languages. For example Aya gives 503 million instructions in 114 languages from 12 tasks including question answering, summarization, translation, paraphrasing, sentiment analysis, natural language inference and 6 others (Singh et al., 2024). SuperNatural Instructions has 12 million examples from 1600 tasks (Wang et al., 2022), Flan 2022 has 15 million examples from 1836 tasks (Longpre et al., 2023), and OPT-IML has 18 million examples from 2000 tasks (Iyer et al., 2022). ", "Bloom_type": "application", "question": "Which of the following is an example of a task covered by these instruction tuning datasets?", "options": ["Answering questions based on given texts", "Summarizing news articles", "Classifying images as cats or dogs", "Generating machine learning models"], "complexity": 2}, {"id": 115, "context": "Language models are evaluated in many ways. we introduced some evaluations for in Section 10.4, including measuring the language model`s perplexity on a test set, evaluating its accuracy on various NLP tasks, as well as benchmarks that help measure efficiency, toxicity, fairness, and so on. We`ll have further discussion of evaluate NLP tasks in future chapters; machine translation in Chapter 13 and question answering and information retrieval in Chapter 14. ", "Bloom_type": "application", "question": "What is an example of how language models can be evaluated?", "options": ["Evaluating the language model's performance on a specific task.", "Measuring the language model's perplexity on a training set.", "Benchmarking the language model's speed only.", "Assessing the language model's gender bias."], "complexity": 2}, {"id": 116, "context": " One method for model alignment is instruction tuning, in which the model is finetuned (using the next-word-prediction language model objective) on a dataset of instructions together with correct responses. Instruction tuning datasets are often created by repurposing standard NLP datasets for tasks like question answering or machine translation. ", "Bloom_type": "application", "question": "What is another common way to create an instruction tuning dataset?", "options": ["By modifying existing question answering datasets", "By using random prompts", "By directly asking humans for answers", "By creating entirely new datasets"], "complexity": 2}, {"id": 117, "context": "In this second part of the book we introduce fundamental NLP applications: machine translation, information retrieval, question answering, dialogue systems, and speech recognition. ", "Bloom_type": "application", "question": "Which application is not directly mentioned as being introduced in this section?", "options": ["Dialogue Systems", "Machine Translation", "Information Retrieval", "Speech Recognition"], "complexity": 2}, {"id": 118, "context": "People need to know things. So pretty much as soon as there were computers we were asking them questions. Systems in the 1960s were answering questions about baseball statistics and scientific facts. Even fictional computers in the 1970s like Deep Thought, invented by Douglas Adams in The Hitchhiker`s Guide to the Galaxy, answered the Ultimate Question Of Life, The Universe, and Everything.1 And because so much knowledge is encoded in text, question answering (QA) systems were performing at human levels even before LLMs: IBM`s Watson system won the TV game-show Jeopardy! in 2011, surpassing humans at answering questions like: ", "Bloom_type": "application", "question": "What was the first major application of question answering systems?", "options": ["Automated theorem proving", "Medical diagnosis", "Financial forecasting", "Space exploration"], "complexity": 2}, {"id": 119, "context": "Question answering systems are designed to fill human information needs. Since a lot of information is present in text form (on the web or in other data like our email, or books), question answering is closely related to the task behind search engines. Indeed, the distinction is becoming ever more fuzzy, as modern search engines are integrated with large language models trained to do question answering. Question answering systems often focus on a useful subset of information needs: factoid questions, questions of fact or reasoning that can be answered with simple facts expressed in short or medium-length texts, like the following: ", "Bloom_type": "application", "question": "What type of questions do question answering systems primarily address?", "options": ["Factoid questions based on short or medium-length texts", "Questions about personal experiences", "Questions requiring complex logical reasoning", "Questions asking for opinions and beliefs"], "complexity": 2}, {"id": 120, "context": " QUESTION ANSWERING, INFORMATION RETRIEVAL, AND RAG ", "Bloom_type": "application", "question": "What is the primary goal of using question answering systems?", "options": ["To provide users with direct answers to their questions", "To improve search engine performance", "To enhance user interaction through natural language processing", "To automate data retrieval tasks"], "complexity": 2}, {"id": 121, "context": "A second problem is that simply prompting a large language model doesn`t allow us to ask questions about proprietary data. A common use of question answering is about data like our personal email or medical records. Or a company may have internal documents that contain answers for customer service or internal use. Or legal firms need to ask questions about legal discovery from proprietary documents. Finally, static large language models also have problems with questions about rapidly changing information (like questions about something that happened last week) since LLMs won`t have up-to-date information from after their release data. For this reason the most common way to do question-answering with LLMs is retrieval-augmented generation or RAG, and that is the method we will focus on in this chapter. In RAG we use information retrieval (IR) techniques to retrieve documents that are likely to have information that might help answer the question. Then we use a large language model to generate an answer given these documents. Basing our answers on retrieved documents can solve some of the problems with using simple prompting to answer questions. First, it helps ensure that the answer is grounded in facts from some curated dataset. And the system can give the user the answer accompanied by the context of the passage or document the answer came from. This information can help users have confidence in the accuracy of the answer (or help them spot when it is wrong!). And these retrieval techniques can be used on any proprietary data we want, such as legal or medical data for those applications. ", "Bloom_type": "application", "question": "What is a common application of question answering?", "options": ["To enhance customer support through automated responses", "To predict stock prices based on historical data", "To analyze social media trends", "To improve search engine rankings"], "complexity": 2}, {"id": 122, "context": "We`ll begin by introducing information retrieval, the task of choosing the most relevant document from a document set given a user`s query expressing their information need. We`ll see the classic method based on cosines of sparse tf-idf vectors, a modern neural dense` retrievers based on instead representing queries and documents neurally with BERT or other language models. We then introduce retrieverbased question answering and the retrieval-augmented generation paradigm. ", "Bloom_type": "application", "question": "What is the first step in developing a retrieval-based question answering system?", "options": ["Choosing the most relevant document from a document set given a user\u2019s query expressing their information need.", "Representing queries and documents neurally using BERT or other language models.", "Introducing information retrieval as the task of selecting the best document for a query.", "Combining retrieval methods with generative models."], "complexity": 2}, {"id": 123, "context": "Finally, we`ll discuss various QA datasets. These are used for finetuning LLMs in instruction tuning, as we saw in Chapter 12. And they are also used as benchmarks, since question answering has an important function as a benchmark for measuring the abilities of language models. ", "Bloom_type": "application", "question": "What is the primary purpose of using QA datasets in the context of fine-tuning Large Language Models (LLMs)?", "options": ["To assess the model's effectiveness in processing and answering factual queries.", "To evaluate the model's performance on complex tasks beyond natural language understanding.", "To train the model with large amounts of data efficiently.", "To measure the model's capabilities in generating coherent responses to questions."], "complexity": 2}, {"id": 124, "context": " QUESTION ANSWERING, INFORMATION RETRIEVAL, AND RAG ", "Bloom_type": "application", "question": "What is the primary goal of using question answering systems?", "options": ["To provide users with direct answers to their questions", "To improve search engine performance", "To enhance user interaction with web pages", "To automate data retrieval tasks"], "complexity": 2}, {"id": 125, "context": " QUESTION ANSWERING, INFORMATION RETRIEVAL, AND RAG ", "Bloom_type": "application", "question": "What is the primary goal of using question answering systems?", "options": ["To provide users with direct answers to their questions", "To improve search engine rankings", "To enhance user experience through personalized recommendations", "To increase website traffic"], "complexity": 2}, {"id": 126, "context": " QUESTION ANSWERING, INFORMATION RETRIEVAL, AND RAG ", "Bloom_type": "application", "question": "What is the primary goal of using question answering systems?", "options": ["To provide users with direct answers to their questions", "To improve search engine rankings", "To enhance user experience through personalized recommendations", "To increase website traffic"], "complexity": 2}, {"id": 127, "context": " QUESTION ANSWERING, INFORMATION RETRIEVAL, AND RAG ", "Bloom_type": "application", "question": "What is the primary focus of the field that combines question answering with information retrieval?", "options": ["Natural Language Processing", "Machine Learning", "Artificial Intelligence", "Computer Vision"], "complexity": 2}, {"id": 128, "context": " QUESTION ANSWERING, INFORMATION RETRIEVAL, AND RAG ", "Bloom_type": "application", "question": "What is the primary goal of using question answering systems?", "options": ["To enable users to find answers to their questions efficiently.", "To provide users with detailed explanations about complex concepts.", "To store large amounts of unstructured data for future retrieval.", "To automatically summarize long documents based on user queries."], "complexity": 2}, {"id": 129, "context": "The dominant paradigm for question answering is to answer a user`s question by first finding supportive text segments from the web or another other large collection of documents, and then generating an answer based on the documents. The method of generating based on retrieved documents is called retrieval-augmented generation or RAG, and the two components are sometimes called the retriever and the reader (Chen et al., 2017a). Fig. 14.9 sketches out this standard QA model. ", "Bloom_type": "application", "question": "What does retrieval-augmented generation refer to?", "options": ["The approach of retrieving relevant documents and using them to generate an answer.", "The process of directly answering questions with existing knowledge.", "The technique of combining human reasoning with machine learning models.", "The method of using pre-trained language models to generate answers."], "complexity": 2}, {"id": 130, "context": " QUESTION ANSWERING, INFORMATION RETRIEVAL, AND RAG ", "Bloom_type": "application", "question": "What is the primary focus of the field that combines question answering with other AI techniques?", "options": ["Natural Language Processing", "Machine Learning", "Computer Vision", "Robotics"], "complexity": 2}, {"id": 131, "context": "And simple conditional generation for question answering adds a prompt like Q: , followed by a query q , and A:, all concatenated: ", "Bloom_type": "application", "question": "What is the response when generating a simple conditional generation for question answering?", "options": ["The response is just the concatenation of the prompt, query, and answer.", "The response is the concatenation of the prompt and the answer only.", "The response is the concatenation of the query and the answer only.", "The response is the concatenation of the prompt and the query."], "complexity": 2}, {"id": 132, "context": "There are scores of question answering datasets, used both for instruction tuning and for evaluation of the question answering abilities of language models. ", "Bloom_type": "application", "question": "Which type of dataset is primarily used for evaluating the question answering abilities of language models?", "options": ["QA (Question Answering) datasets", "Instruction tuning datasets", "Translation datasets", "Summarization datasets"], "complexity": 2}, {"id": 133, "context": "A similar natural question set is the MS MARCO (Microsoft Machine Reading Comprehension) collection of datasets, including 1 million real anonymized English questions from Microsoft Bing query logs together with a human generated answer and 9 million passages (Bajaj et al., 2016), that can be used both to test retrieval ranking and question answering. ", "Bloom_type": "application", "question": "What type of dataset does MS MARCO provide?", "options": ["Question answering data", "Natural language processing data", "Machine learning algorithms", "Text classification data"], "complexity": 2}, {"id": 134, "context": " QUESTION ANSWERING, INFORMATION RETRIEVAL, AND RAG ", "Bloom_type": "application", "question": "What is the primary goal of using question answering systems?", "options": ["To provide users with direct answers to their questions", "To improve search engine performance", "To enhance user interaction through natural language processing", "To automate data retrieval tasks"], "complexity": 2}, {"id": 135, "context": "Some of the question datasets described above augment each question with passage(s) from which the answer can be extracted. These datasets were mainly created for an earlier QA task called reading comprehension in which a model is given a question and a document and is required to extract the answer from the given document. We sometimes call the task of question answering given one or more documents (for example via RAG), the open book QA task, while the task of answering directly from the LM with no retrieval component at all is the closed book QA task.5 Thus datasets like Natural Questions can be treated as open book if the solver uses each question`s attached document, or closed book if the documents are not used, while datasets like MMLU are solely closed book. ", "Bloom_type": "application", "question": "What type of dataset does the Natural Questions dataset belong to?", "options": ["Open Book QA", "Closed Book QA", "Both Open Book and Closed Book QA", "None of the Above"], "complexity": 2}, {"id": 136, "context": " QUESTION ANSWERING, INFORMATION RETRIEVAL, AND RAG ", "Bloom_type": "application", "question": "What is the primary goal of using question answering systems?", "options": ["To provide users with direct answers to their questions", "To improve search engine performance", "To enhance user interaction through natural language processing", "To automate data retrieval tasks"], "complexity": 2}, {"id": 137, "context": "This chapter introduced the tasks of question answering and information retrieval. ", "Bloom_type": "application", "question": "What is the primary focus of this chapter?", "options": ["The introduction of question answering and information retrieval", "The development of new algorithms for machine learning", "The study of natural language processing techniques", "The exploration of deep learning models"], "complexity": 2}, {"id": 138, "context": " Question answering (QA) is the task of answering a user`s questions.  We focus in this chapter on the task of retrieval-based question answering, in which the user`s questions are intended to be answered by the material in some set of documents (which might be the web). ", "Bloom_type": "application", "question": "What does retrieval-based question answering involve?", "options": ["It relies on searching through a database of pre-existing knowledge for relevant information.", "It involves using machine learning algorithms to automatically create answers.", "It focuses on generating human-like responses based on the input.", "It combines both A and B."], "complexity": 2}, {"id": 139, "context": " Question answering systems generally use the retriever/reader architecture. In the retriever stage, an IR system is given a query and returns a set of documents. ", "Bloom_type": "application", "question": "What architectural component does not directly interact with the user input for retrieval?", "options": ["Retriever", "Reader", "Query Processor", "Indexer"], "complexity": 2}, {"id": 140, "context": " QUESTION ANSWERING, INFORMATION RETRIEVAL, AND RAG ", "Bloom_type": "application", "question": "What is the primary goal of using question answering systems?", "options": ["To provide users with direct answers to their questions", "To improve search engine rankings", "To enhance user experience through personalized content", "To increase website traffic"], "complexity": 2}, {"id": 141, "context": "By a decade later, neural models were applied to semantic parsing (Dong and Lapata 2016, Jia and Liang 2016), and then to knowledge-based question answering by mapping text to SQL (Iyer et al., 2017). ", "Bloom_type": "application", "question": "What was the next step after applying neural models for semantic parsing?", "options": ["Using SQL queries for data retrieval", "Mapping text to SQL directly", "Developing more advanced semantic understanding algorithms", "Implementing natural language processing techniques"], "complexity": 2}, {"id": 142, "context": "Meanwhile, the information-retrieval paradigm for question answering was influenced by the rise of the web in the 1990s. The U.S. government-sponsored TREC (Text REtrieval Conference) evaluations, run annually since 1992, provide a testbed for evaluating information-retrieval tasks and techniques (Voorhees and Harman, 2005). TREC added an influential QA track in 1999, which led to a wide variety of factoid and non-factoid systems competing in annual evaluations. ", "Bloom_type": "application", "question": "What event marked the beginning of significant advancements in question answering?", "options": ["The introduction of the World Wide Web", "The invention of the internet", "The launch of the first search engine", "The establishment of the Text Retrieval Conference"], "complexity": 2}, {"id": 143, "context": "Early work on large language models showed that they stored sufficient knowledge in the pretraining process to answer questions (Petroni et al., 2019; Raffel et al., 2020; Radford et al., 2019; Roberts et al., 2020), at first not competitively with special-purpose question answerers, but then surpassing them. Retrieval-augmented generation algorithms were first introduced as a way to improve language modeling (Khandelwal et al., 2019), but were quickly applied to question answering (Izacard et al., 2022; Ram et al., 2023; Shi et al., 2023). ", "Bloom_type": "application", "question": "Which of the following is an early application of retrieval-augmented generation algorithms for question answering?", "options": ["Ram et al., 2023", "Rafaela et al., 2019", "Shi et al., 2023", "Khandelwal et al., 2019"], "complexity": 2}, {"id": 144, "context": "Chatbots are systems that can carry on extended conversations with the goal of mimicking the unstructured conversations or chats` characteristic of informal humanhuman interaction. While early systems like ELIZA (Weizenbaum, 1966) or PARRY (Colby et al., 1971) had theoretical goals like testing theories of psychological counseling, for most of the last 50 years chatbots have been designed for entertainment. That changed with the recent rise of neural chatbots like ChatGPT, which incorporate solutions to NLP tasks like question answering, writing tools, or machine translation into a conversational interface. A conversation with ChatGPT is shown in Fig. 15.12. In this section we describe neural chatbot architectures and datasets. ", "Bloom_type": "application", "question": "What is the primary purpose of using chatbots?", "options": ["To enhance user experience through natural language processing", "To replace human customer service entirely", "To automate repetitive tasks", "To provide personalized recommendations"], "complexity": 2}, {"id": 145, "context": "Parts of speech (also known as POS) and named entities are useful clues to sentence structure and meaning. Knowing whether a word is a noun or a verb tells us about likely neighboring words (nouns in English are preceded by determiners and adjectives, verbs by nouns) and syntactic structure (verbs have dependency links to nouns), making part-of-speech tagging a key aspect of parsing. Knowing if a named entity like Washington is a name of a person, a place, or a university is important to many natural language processing tasks like question answering, stance detection, or information extraction. ", "Bloom_type": "application", "question": "What is an essential step in understanding how parts of speech affect sentence structure?", "options": ["Analyzing the function of named entities", "Identifying the tense of verbs", "Determining the gender of nouns", "Recognizing the role of adjectives before nouns"], "complexity": 2}, {"id": 146, "context": "Named entity tagging is a useful first step in lots of natural language processing tasks. In sentiment analysis we might want to know a consumer`s sentiment toward a particular entity. Entities are a useful first stage in question answering, or for linking text to information in structured knowledge sources like Wikipedia. And named entity tagging is also central to tasks involving building semantic representations, like extracting events and the relationship between participants. ", "Bloom_type": "application", "question": "What is an important initial step in question answering?", "options": ["Named entity tagging", "Building semantic representations", "Extracting events and relationships", "Sentiment analysis"], "complexity": 2}, {"id": 147, "context": " Dependency-based analysis provides information directly useful in further language processing tasks including information extraction, semantic parsing and question answering. ", "Bloom_type": "application", "question": "What is the primary purpose of using dependency-based analysis?", "options": ["To improve the accuracy of natural language understanding systems.", "To enhance the efficiency of data storage for large datasets.", "To increase the speed of internet connectivity globally.", "To reduce the carbon footprint of electronic devices."], "complexity": 2}, {"id": 148, "context": "Coreference is an important component of natural language processing. A dialogue system that has just told the user There is a 2pm flight on United and a 4pm one on Cathay Pacific must know which flight the user means by I`ll take the second one. A question answering system that uses Wikipedia to answer a question about Marie Curie must know who she was in the sentence She was born in Warsaw. And a machine translation system translating from a language like Spanish, in which pronouns can be dropped, must use coreference from the previous sentence to decide whether the Spanish sentence Me encanta el conocimiento, dice.` should be translated as I love knowledge, he says`, or I love knowledge, she says`. Indeed, this example comes from an actual news article in El Pas about a female professor and was mistranslated as he in machine translation because of inaccurate coreference resolution (Schiebinger, 2013). ", "Bloom_type": "application", "question": "In a dialogue system, how does it determine which flight the user means when they say `I'll take the second one'? ", "options": ["By using coreference resolution techniques to understand the reference of the pronoun.", "By asking the user for clarification immediately after.", "By ignoring all previous sentences and starting fresh with new questions.", "By applying a simple rule based on the time mentioned."], "complexity": 2}]}, "regular expression": {"max_id": 38, "Questions": [{"id": 0, "context": "We`ll begin with the most important tool for describing text patterns: the regular expression. Regular expressions can be used to specify strings we might want to extract from a document, from transforming I need X in ELIZA above, to defining strings like $199 or $24.99 for extracting tables of prices from a document. ", "Bloom_type": "remember", "question": "What is the primary purpose of using regular expressions?", "options": ["To define specific string patterns within documents", "To transform text into machine-readable code", "To encrypt sensitive data", "To create complex mathematical equations"], "complexity": 0}, {"id": 1, "context": "We`ll then turn to a set of tasks collectively called text normalization, in which regular expressions play an important part. Normalizing text means converting it to a more convenient, standard form. For example, most of what we are going to do with language relies on first separating out or tokenizing words or word parts from running text, the task of tokenization. English words are often separated from each other by whitespace, but whitespace is not always sufficient. New York and rock `n` roll are sometimes treated as large words despite the fact that they contain spaces, while sometimes we`ll need to separate I`m into the two words I and am. For processing tweets or texts we`ll need to tokenize emoticons like :) or hashtags ", "Bloom_type": "remember", "question": "In text normalization, why does regular expression play an important role?", "options": ["To identify patterns in text", "To convert text to binary code", "To remove all punctuation marks", "To increase text length"], "complexity": 0}, {"id": 2, "context": "One of the most useful tools for text processing in computer science has been the regular expression (often shortened to regex), a language for specifying text search strings. This practical language is used in every computer language, in text processing tools like the Unix tools grep, and in editors like vim or Emacs. Formally, a regular expression is an algebraic notation for characterizing a set of strings. Regular expressions are particularly useful for searching in texts, when we have a pattern to search for and a corpus of texts to search through. A regular expression search function will search through the corpus, returning all texts that match the pattern. The corpus can be a single document or a collection. For example, the Unix command-line tool grep takes a regular expression and returns every line of the input document that matches the expression. ", "Bloom_type": "remember", "question": "In what way does regular expression help in text processing?", "options": ["It enables precise matching and searching within large datasets.", "It simplifies complex programming languages.", "It allows direct access to hardware components.", "It enhances the efficiency of database queries."], "complexity": 0}, {"id": 3, "context": "Regular expressions are case sensitive; lower case /s/ is distinct from upper case /S/ (/s/ matches a lower case s but not an upper case S). This means that the pattern /woodchucks/ will not match the string Woodchucks. We can solve this problem with the use of the square braces [ and ]. The string of characters inside the braces specifies a disjunction of characters to match. For example, Fig. 2.2 shows that the pattern /[wW]/ matches patterns containing either w or W. ", "Bloom_type": "remember", "question": "Why does using square brackets `[ ]` help resolve the issue when matching strings with different cases?", "options": ["To indicate that both lowercase and uppercase letters should be matched.", "To change the case sensitivity of regular expressions.", "To specify a range of characters within the string.", "To exclude certain characters from being matched."], "complexity": 0}, {"id": 4, "context": "word, for example, aardvark, appears twice. We can specify this with the regular expression /aardvark.*aardvark/. ", "Bloom_type": "remember", "question": "What does the regular expression /aardvark.*aardvark/ match?", "options": ["All occurrences of \"aardvark\" including duplicates", "The first occurrence of \"aardvark\"", "The second occurrence of \"aardvark\"", "None of the above"], "complexity": 0}, {"id": 5, "context": "There are also two other anchors: \\b matches a word boundary, and \\B matches a non word-boundary. Thus, /\\bthe\\b/ matches the word the but not the word other. A word for the purposes of a regular expression is defined based on the definition of words in programming languages as a sequence of digits, underscores, or letters. Thus /\\b99\\b/ will match the string 99 in There are 99 bottles of beer on the wall (because 99 follows a space) but not 99 in There are 299 bottles of beer on the wall (since 99 follows a number). But it will match 99 in $99 (since 99 follows a dollar sign ($), which is not a digit, underscore, or letter). ", "Bloom_type": "remember", "question": "What does the character \"\b\" do in regular expressions?", "options": ["It matches a word boundary.", "It matches any whitespace character.", "It matches an uppercase letter.", "It matches a lowercase letter."], "complexity": 0}, {"id": 6, "context": "able to look for expressions like 6 GHz or 500 GB or $999.99. Let`s work out some regular expressions for this task. ", "Bloom_type": "remember", "question": "What is a regular expression used for?", "options": ["To find patterns in strings", "To calculate mathematical equations", "To encrypt messages", "To create new languages"], "complexity": 0}, {"id": 7, "context": "An important use of regular expressions is in substitutions. For example, the substitution operator s/regexp1/pattern/ used in Python and in Unix commands like vim or sed allows a string characterized by a regular expression to be replaced by another string: ", "Bloom_type": "remember", "question": "In programming, what does the `s` command do when using regular expressions?", "options": ["It replaces all occurrences of a pattern with another string.", "It checks if a string matches a specific pattern.", "It converts all uppercase letters to lowercase.", "It deletes all characters from a string."], "complexity": 0}, {"id": 8, "context": "In practice, since tokenization is run before any other language processing, it needs to be very fast. For word tokenization we generally use deterministic algorithms based on regular expressions compiled into efficient finite state automata. For example, Fig. 2.12 shows a basic regular expression that can be used to tokenize English with the nltk.regexp tokenize function of the Python-based Natural Language Toolkit (NLTK) (Bird et al. 2009; https://www.nltk.org). ", "Bloom_type": "remember", "question": "In what way does using regular expressions improve the efficiency of word tokenization compared to other methods?", "options": ["It allows for more complex patterns than deterministic algorithms.", "It eliminates the need for compilation.", "It compiles patterns directly into machine learning models.", "It uses less memory than deterministic algorithms."], "complexity": 0}, {"id": 9, "context": " The regular expression language is a powerful tool for pattern-matching.  Basic operations in regular expressions include concatenation of symbols, disjunction of symbols ([], |), counters (*, +, and {n,m}), anchors (, $) and precedence operators ((,)). ", "Bloom_type": "remember", "question": "In what way does the regular expression language facilitate pattern matching?", "options": ["It enables precise control over string manipulation tasks.", "It simplifies complex mathematical equations.", "It allows users to create custom programming languages.", "It provides a graphical interface for data visualization."], "complexity": 0}, {"id": 10, "context": "Kleene 1951; 1956 first defined regular expressions and the finite automaton, based on the McCulloch-Pitts neuron. Ken Thompson was one of the first to build regular expressions compilers into editors for text searching (Thompson, 1968). His editor ed included a command g/regular expression/p, or Global Regular Expression Print, which later became the Unix grep utility. ", "Bloom_type": "remember", "question": "Who among the following is credited with defining regular expressions?", "options": ["Ken Thompson", "Kleene", "McCulloch-Pitts", "Edsger W. Dijkstra"], "complexity": 0}, {"id": 11, "context": "Weizenbaum made use of this property of Rogerian psychiatric conversations, along with clever regular expressions, to allow ELIZA to interact in ways that seemed deceptively human-like, as in the sample conversational fragment in Fig. 15.1. ", "Bloom_type": "remember", "question": "What did Weizenbaum use to make ELIZA seem more human-like?", "options": ["clever regular expressions", "Rogerian psychiatric conversations", "sample conversational fragments", "Fig. 15.1"], "complexity": 0}, {"id": 12, "context": "While machine learned (neural or CRF) sequence models are the norm in academic research, commercial approaches to NER are often based on pragmatic combinations of lists and rules, with some smaller amount of supervised machine learning (Chiticariu et al., 2013). For example in the IBM System T architecture, a user specifies declarative constraints for tagging tasks in a formal query language that includes regular expressions, dictionaries, semantic constraints, and other operators, which the system compiles into an efficient extractor (Chiticariu et al., 2018). ", "Bloom_type": "remember", "question": "In what way do commercial approaches to Named Entity Recognition differ from academic ones?", "options": ["Commercial approaches incorporate both lists and rules along with machine learning techniques.", "Commercial approaches use neural networks exclusively.", "Commercial approaches rely solely on rule-based systems.", "Commercial approaches avoid any form of machine learning."], "complexity": 0}, {"id": 13, "context": "We`ll begin with the most important tool for describing text patterns: the regular expression. Regular expressions can be used to specify strings we might want to extract from a document, from transforming I need X in ELIZA above, to defining strings like $199 or $24.99 for extracting tables of prices from a document. ", "Bloom_type": "comprehension", "question": "What are regular expressions primarily used for in text processing?", "options": ["Extracting specific strings from documents", "Transforming sentences into different forms", "Defining complex mathematical equations", "Creating new languages"], "complexity": 1}, {"id": 14, "context": "We`ll then turn to a set of tasks collectively called text normalization, in which regular expressions play an important part. Normalizing text means converting it to a more convenient, standard form. For example, most of what we are going to do with language relies on first separating out or tokenizing words or word parts from running text, the task of tokenization. English words are often separated from each other by whitespace, but whitespace is not always sufficient. New York and rock `n` roll are sometimes treated as large words despite the fact that they contain spaces, while sometimes we`ll need to separate I`m into the two words I and am. For processing tweets or texts we`ll need to tokenize emoticons like :) or hashtags ", "Bloom_type": "comprehension", "question": "What does text normalization involve?", "options": ["Both A) and B)", "Converting text to a more convenient, standard form", "Separating out or tokenizing words or word parts from running text", "Processing tweets or texts"], "complexity": 1}, {"id": 15, "context": "One of the most useful tools for text processing in computer science has been the regular expression (often shortened to regex), a language for specifying text search strings. This practical language is used in every computer language, in text processing tools like the Unix tools grep, and in editors like vim or Emacs. Formally, a regular expression is an algebraic notation for characterizing a set of strings. Regular expressions are particularly useful for searching in texts, when we have a pattern to search for and a corpus of texts to search through. A regular expression search function will search through the corpus, returning all texts that match the pattern. The corpus can be a single document or a collection. For example, the Unix command-line tool grep takes a regular expression and returns every line of the input document that matches the expression. ", "Bloom_type": "comprehension", "question": "What does the regular expression (regex) serve as a language for in computer science?", "options": ["Algebraic notation for characterizing sets of strings", "Formal notation for mathematical operations", "Syntax for programming languages", "Encoding scheme for binary data"], "complexity": 1}, {"id": 16, "context": "Regular expressions are case sensitive; lower case /s/ is distinct from upper case /S/ (/s/ matches a lower case s but not an upper case S). This means that the pattern /woodchucks/ will not match the string Woodchucks. We can solve this problem with the use of the square braces [ and ]. The string of characters inside the braces specifies a disjunction of characters to match. For example, Fig. 2.2 shows that the pattern /[wW]/ matches patterns containing either w or W. ", "Bloom_type": "comprehension", "question": "What does the square brackets [ ] do in regular expressions?", "options": ["They define a set of characters to match.", "They specify a range of numbers.", "They indicate repetition of a character sequence.", "They represent special symbols."], "complexity": 1}, {"id": 17, "context": "word, for example, aardvark, appears twice. We can specify this with the regular expression /aardvark.*aardvark/. ", "Bloom_type": "comprehension", "question": "What does the regular expression /aardvark.*aardvark/ match?", "options": ["Any occurrence of the word 'aardvark' followed by any characters (including none) and then another occurrence of the word 'aardvark'", "The exact phrase 'aardvark aardvark'", "The first instance of the word 'aardvark' only", "The second instance of the word 'aardvark'"], "complexity": 1}, {"id": 18, "context": "There are also two other anchors: \\b matches a word boundary, and \\B matches a non word-boundary. Thus, /\\bthe\\b/ matches the word the but not the word other. A word for the purposes of a regular expression is defined based on the definition of words in programming languages as a sequence of digits, underscores, or letters. Thus /\\b99\\b/ will match the string 99 in There are 99 bottles of beer on the wall (because 99 follows a space) but not 99 in There are 299 bottles of beer on the wall (since 99 follows a number). But it will match 99 in $99 (since 99 follows a dollar sign ($), which is not a digit, underscore, or letter). ", "Bloom_type": "comprehension", "question": "What does the character '\b' do in regular expressions?", "options": ["It matches any word boundary.", "It matches any non-word boundary.", "It matches any digit.", "It matches any underscore."], "complexity": 1}, {"id": 19, "context": "able to look for expressions like 6 GHz or 500 GB or $999.99. Let`s work out some regular expressions for this task. ", "Bloom_type": "comprehension", "question": "What are examples of regular expressions mentioned in the context?", "options": ["6 GHz, 500 GB, $999.99", "Regular expressions themselves", "The response itself", "No examples were mentioned"], "complexity": 1}, {"id": 20, "context": "An important use of regular expressions is in substitutions. For example, the substitution operator s/regexp1/pattern/ used in Python and in Unix commands like vim or sed allows a string characterized by a regular expression to be replaced by another string: ", "Bloom_type": "comprehension", "question": "What does the substitution operator do using regular expressions?", "options": ["It replaces all occurrences of a pattern within a string.", "It converts every character in a string into its ASCII code.", "It finds and prints all matches of a regular expression in a string.", "It changes the case of each letter in a string."], "complexity": 1}, {"id": 21, "context": "In practice, since tokenization is run before any other language processing, it needs to be very fast. For word tokenization we generally use deterministic algorithms based on regular expressions compiled into efficient finite state automata. For example, Fig. 2.12 shows a basic regular expression that can be used to tokenize English with the nltk.regexp tokenize function of the Python-based Natural Language Toolkit (NLTK) (Bird et al. 2009; https://www.nltk.org). ", "Bloom_type": "comprehension", "question": "What are some examples of regular expressions used for tokenizing languages?", "options": ["Regular expressions are used for tokenizing sentences in natural language processing.", "Regular expressions are only used for programming languages.", "Regular expressions are not commonly used for tokenizing languages.", "Regular expressions are exclusively used for mathematical calculations."], "complexity": 1}, {"id": 22, "context": " The regular expression language is a powerful tool for pattern-matching.  Basic operations in regular expressions include concatenation of symbols, disjunction of symbols ([], |), counters (*, +, and {n,m}), anchors (, $) and precedence operators ((,)). ", "Bloom_type": "comprehension", "question": "What are some basic operations included in the regular expression language?", "options": ["Concatenation of symbols, disjunction of symbols [], |, counters *, +, and {n,m}, and precedence operators ()", "Concatenation of symbols, disjunction of symbols [ ], counters *, +, and {n,m}, and precedence operators ()", "Concatenation of symbols, disjunction of symbols []|, counters *+, and {n,m}, and precedence operators ()", "Concatenation of symbols, disjunction of symbols [ ]*, counters +, and {n,m}, and precedence operators ()"], "complexity": 1}, {"id": 23, "context": "Kleene 1951; 1956 first defined regular expressions and the finite automaton, based on the McCulloch-Pitts neuron. Ken Thompson was one of the first to build regular expressions compilers into editors for text searching (Thompson, 1968). His editor ed included a command g/regular expression/p, or Global Regular Expression Print, which later became the Unix grep utility. ", "Bloom_type": "comprehension", "question": "What did Ken Thompson do that contributed significantly to the development of regular expressions?", "options": ["Built regular expressions compilers into editors", "Defined regular expressions", "Invented the McCulloch-Pitts neuron", "Created the Unix grep utility"], "complexity": 1}, {"id": 24, "context": "Weizenbaum made use of this property of Rogerian psychiatric conversations, along with clever regular expressions, to allow ELIZA to interact in ways that seemed deceptively human-like, as in the sample conversational fragment in Fig. 15.1. ", "Bloom_type": "comprehension", "question": "How did Weizenbaum incorporate regular expressions into his work with Rogerian psychiatric conversations?", "options": ["Weizenbaum employed regular expressions to create more realistic dialogue patterns for ELIZA.", "Weizenbaum used regular expressions to analyze patient responses.", "Weizenbaum integrated regular expressions to enhance the complexity of conversation rules.", "Weizenbaum utilized regular expressions to improve the accuracy of patient diagnoses."], "complexity": 1}, {"id": 25, "context": "While machine learned (neural or CRF) sequence models are the norm in academic research, commercial approaches to NER are often based on pragmatic combinations of lists and rules, with some smaller amount of supervised machine learning (Chiticariu et al., 2013). For example in the IBM System T architecture, a user specifies declarative constraints for tagging tasks in a formal query language that includes regular expressions, dictionaries, semantic constraints, and other operators, which the system compiles into an efficient extractor (Chiticariu et al., 2018). ", "Bloom_type": "comprehension", "question": "What type of approach is commonly used in commercial natural language processing (NLP) systems?", "options": ["Pragmatic combinations of lists and rules", "Machine learning algorithms alone", "Neural network models exclusively", "Both B) and C)"], "complexity": 1}, {"id": 26, "context": "We`ll begin with the most important tool for describing text patterns: the regular expression. Regular expressions can be used to specify strings we might want to extract from a document, from transforming I need X in ELIZA above, to defining strings like $199 or $24.99 for extracting tables of prices from a document. ", "Bloom_type": "application", "question": "What is the first step when using regular expressions to manipulate text?", "options": ["Identify the specific pattern you want to match", "Write down all possible patterns in your document", "Count the number of occurrences of each pattern", "Choose a random string from your document"], "complexity": 2}, {"id": 27, "context": "We`ll then turn to a set of tasks collectively called text normalization, in which regular expressions play an important part. Normalizing text means converting it to a more convenient, standard form. For example, most of what we are going to do with language relies on first separating out or tokenizing words or word parts from running text, the task of tokenization. English words are often separated from each other by whitespace, but whitespace is not always sufficient. New York and rock `n` roll are sometimes treated as large words despite the fact that they contain spaces, while sometimes we`ll need to separate I`m into the two words I and am. For processing tweets or texts we`ll need to tokenize emoticons like :) or hashtags ", "Bloom_type": "application", "question": "What is the primary purpose of using regular expressions in text normalization?", "options": ["To convert text into a standardized format", "To identify patterns in text data", "To remove all punctuation marks from text", "To increase the speed of text processing"], "complexity": 2}, {"id": 28, "context": "One of the most useful tools for text processing in computer science has been the regular expression (often shortened to regex), a language for specifying text search strings. This practical language is used in every computer language, in text processing tools like the Unix tools grep, and in editors like vim or Emacs. Formally, a regular expression is an algebraic notation for characterizing a set of strings. Regular expressions are particularly useful for searching in texts, when we have a pattern to search for and a corpus of texts to search through. A regular expression search function will search through the corpus, returning all texts that match the pattern. The corpus can be a single document or a collection. For example, the Unix command-line tool grep takes a regular expression and returns every line of the input document that matches the expression. ", "Bloom_type": "application", "question": "Which of the following best describes how regular expressions work?", "options": ["Regular expressions are searched within a corpus of documents.", "Regular expressions are used only in programming languages.", "Regular expressions are applied directly on the text without any preprocessing.", "Regular expressions are processed using a predefined algorithm."], "complexity": 2}, {"id": 29, "context": "Regular expressions are case sensitive; lower case /s/ is distinct from upper case /S/ (/s/ matches a lower case s but not an upper case S). This means that the pattern /woodchucks/ will not match the string Woodchucks. We can solve this problem with the use of the square braces [ and ]. The string of characters inside the braces specifies a disjunction of characters to match. For example, Fig. 2.2 shows that the pattern /[wW]/ matches patterns containing either w or W. ", "Bloom_type": "application", "question": "What is the purpose of using square brackets [ ] in regular expressions?", "options": ["To denote a disjunction of characters", "To indicate repetition of a character sequence", "To specify a range of characters", "To define a set of characters"], "complexity": 2}, {"id": 30, "context": "word, for example, aardvark, appears twice. We can specify this with the regular expression /aardvark.*aardvark/. ", "Bloom_type": "application", "question": "What is the regular expression used to match the phrase 'aardvark' appearing twice consecutively?", "options": ["/aardvark.*aardvark/", "/aardvark/aardvark/", "/aardvark\\saardvark/", "/aardvark[a-z]*aardvark/"], "complexity": 2}, {"id": 31, "context": "There are also two other anchors: \\b matches a word boundary, and \\B matches a non word-boundary. Thus, /\\bthe\\b/ matches the word the but not the word other. A word for the purposes of a regular expression is defined based on the definition of words in programming languages as a sequence of digits, underscores, or letters. Thus /\\b99\\b/ will match the string 99 in There are 99 bottles of beer on the wall (because 99 follows a space) but not 99 in There are 299 bottles of beer on the wall (since 99 follows a number). But it will match 99 in $99 (since 99 follows a dollar sign ($), which is not a digit, underscore, or letter). ", "Bloom_type": "application", "question": "Which anchor should you use if you want to match the exact word 'the'?", "options": ["Use \\b before 'the'", "Use \\B after 'the'", "Use \\b after 'the'", "Use \\B before 'the'"], "complexity": 2}, {"id": 32, "context": "able to look for expressions like 6 GHz or 500 GB or $999.99. Let`s work out some regular expressions for this task. ", "Bloom_type": "application", "question": "What is the first step in creating a regular expression to match numbers with decimal points?", "options": ["Choose the character class for digits including decimals.", "Define the pattern for whole numbers only.", "Identify the delimiter between different parts of the number.", "Specify the exact format of the number (e.g., 6.0GHz)."], "complexity": 2}, {"id": 33, "context": "An important use of regular expressions is in substitutions. For example, the substitution operator s/regexp1/pattern/ used in Python and in Unix commands like vim or sed allows a string characterized by a regular expression to be replaced by another string: ", "Bloom_type": "application", "question": "In programming, what does the substitution operator `s` with a regular expression as the first argument replace?", "options": ["The substring matching the regular expression", "The entire string", "The pattern specified after the slash", "None of the above"], "complexity": 2}, {"id": 34, "context": "In practice, since tokenization is run before any other language processing, it needs to be very fast. For word tokenization we generally use deterministic algorithms based on regular expressions compiled into efficient finite state automata. For example, Fig. 2.12 shows a basic regular expression that can be used to tokenize English with the nltk.regexp tokenize function of the Python-based Natural Language Toolkit (NLTK) (Bird et al. 2009; https://www.nltk.org). ", "Bloom_type": "application", "question": "Which method should be applied first for word tokenization using NLTK?", "options": ["Compile the regular expression", "Use the tokenize function directly", "Create an efficient finite state automaton", "Identify the tokens"], "complexity": 2}, {"id": 35, "context": " The regular expression language is a powerful tool for pattern-matching.  Basic operations in regular expressions include concatenation of symbols, disjunction of symbols ([], |), counters (*, +, and {n,m}), anchors (, $) and precedence operators ((,)). ", "Bloom_type": "application", "question": "What is the first step when using regular expressions to match patterns?", "options": ["Identify the pattern to be matched", "Define the set of characters to search within", "Choose the appropriate regular expression syntax", "Combine different regular expression features"], "complexity": 2}, {"id": 36, "context": "Kleene 1951; 1956 first defined regular expressions and the finite automaton, based on the McCulloch-Pitts neuron. Ken Thompson was one of the first to build regular expressions compilers into editors for text searching (Thompson, 1968). His editor ed included a command g/regular expression/p, or Global Regular Expression Print, which later became the Unix grep utility. ", "Bloom_type": "application", "question": "What is an example of how regular expressions were used before their widespread adoption?", "options": ["Ken Thompson built regular expressions into his editor ed.", "Regular expressions were not used before their widespread adoption.", "The McCulloch-Pitts neuron was the basis for defining regular expressions.", "Kleene 1951 introduced the concept of regular expressions."], "complexity": 2}, {"id": 37, "context": "Weizenbaum made use of this property of Rogerian psychiatric conversations, along with clever regular expressions, to allow ELIZA to interact in ways that seemed deceptively human-like, as in the sample conversational fragment in Fig. 15.1. ", "Bloom_type": "application", "question": "What technique did Weizenbaum use to make ELIZA appear more human-like?", "options": ["clever regular expressions", "Rogerian psychiatric conversations", "machine learning algorithms", "natural language processing"], "complexity": 2}, {"id": 38, "context": "While machine learned (neural or CRF) sequence models are the norm in academic research, commercial approaches to NER are often based on pragmatic combinations of lists and rules, with some smaller amount of supervised machine learning (Chiticariu et al., 2013). For example in the IBM System T architecture, a user specifies declarative constraints for tagging tasks in a formal query language that includes regular expressions, dictionaries, semantic constraints, and other operators, which the system compiles into an efficient extractor (Chiticariu et al., 2018). ", "Bloom_type": "application", "question": "What is a key component used in the compilation phase of the IBM System T architecture?", "options": ["Regular expressions", "Machine learning algorithms", "Neural networks", "CRF models"], "complexity": 2}]}, "naive bayes": {"max_id": 47, "Questions": [{"id": 0, "context": "are much easier to define than those of Borges. In this chapter we introduce the naive Bayes algorithm and apply it to text categorization, the task of assigning a label or category to an entire text or document. ", "Bloom_type": "remember", "question": "What is introduced in this chapter regarding text categorization?", "options": ["Naive Bayes algorithm", "Text summarization techniques", "Machine learning models", "Natural language processing algorithms"], "complexity": 0}, {"id": 1, "context": "Finally, one of the oldest tasks in text classification is assigning a library subject category or topic label to a text. Deciding whether a research paper concerns epidemiology or instead, perhaps, embryology, is an important component of information retrieval. Various sets of subject categories exist, such as the MeSH (Medical Subject Headings) thesaurus. In fact, as we will see, subject category classification is the task for which the naive Bayes algorithm was invented in 1961 (Maron, 1961). Classification is essential for tasks below the level of the document as well. We`ve already seen period disambiguation (deciding if a period is the end of a sentence or part of a word), and word tokenization (deciding if a character should be a word boundary). Even language modeling can be viewed as classification: each word can be thought of as a class, and so predicting the next word is classifying the context-so-far into a class for each next word. A part-of-speech tagger (Chapter 17) classifies each occurrence of a word in a sentence as, e.g., a noun or a verb. ", "Bloom_type": "remember", "question": "In what year did the naive Bayes algorithm first become popular for its invention?", "options": ["1961", "1950", "1940", "1930"], "complexity": 0}, {"id": 2, "context": "Many kinds of machine learning algorithms are used to build classifiers. This chapter introduces naive Bayes; the following one introduces logistic regression. These exemplify two ways of doing classification. Generative classifiers like naive Bayes build a model of how a class could generate some input data. Given an observation, they return the class most likely to have generated the observation. Discriminative classifiers like logistic regression instead learn what features from the input are most useful to discriminate between the different possible classes. While discriminative systems are often more accurate and hence more commonly used, generative classifiers still have a role. ", "Bloom_type": "remember", "question": "Which algorithm is primarily concerned with modeling the probability distribution of each class?", "options": ["Naive Bayes", "Logistic Regression", "Support Vector Machines", "K-Nearest Neighbors"], "complexity": 0}, {"id": 3, "context": "In this section we introduce the multinomial naive Bayes classifier, so called because it is a Bayesian classifier that makes a simplifying (naive) assumption about ", "Bloom_type": "remember", "question": "In the context of machine learning, what does the term `naive bayes` refer to?", "options": ["A classification method based on Bayes theorem with strong independence assumptions among predictors", "A type of linear regression model", "An algorithm for unsupervised learning", "A clustering technique using k-means"], "complexity": 0}, {"id": 4, "context": "The simplest solution is the add-one (Laplace) smoothing introduced in Chapter 3. While Laplace smoothing is usually replaced by more sophisticated smoothing algorithms in language modeling, it is commonly used in naive Bayes text categorization: ", "Bloom_type": "remember", "question": "In which application of naive Bayes is Laplace smoothing most frequently utilized?", "options": ["Text categorization", "Language modeling", "Both A and B", "Neither A nor B"], "complexity": 0}, {"id": 5, "context": "Let`s walk through an example of training and testing naive Bayes with add-one smoothing. We`ll use a sentiment analysis domain with the two classes positive (+) and negative (-), and take the following miniature training and test documents simplified from actual movie reviews. ", "Bloom_type": "remember", "question": "In the context of training and testing naive Bayes, what is the purpose of using add-one smoothing?", "options": ["To increase the accuracy of predictions by penalizing rare events", "To reduce the computational complexity of calculations", "To ensure all features are equally important", "To prevent overfitting on small datasets"], "complexity": 0}, {"id": 6, "context": "While standard naive Bayes text classification can work well for sentiment analysis, some small changes are generally employed that improve performance. ", "Bloom_type": "remember", "question": "In sentiment analysis using naive Bayes, what is a common technique to enhance model performance?", "options": ["Implementing minor modifications", "Increasing the dataset size", "Applying more complex algorithms", "Using larger vocabulary sizes"], "complexity": 0}, {"id": 7, "context": "Consider the task of spam detection, deciding if a particular piece of email is an example of spam (unsolicited bulk email)one of the first applications of naive Bayes to text classification (Sahami et al., 1998). ", "Bloom_type": "remember", "question": "In which application did naive Bayes classifiers first appear?", "options": ["Spam detection", "Image recognition", "Sentiment analysis", "Speech recognition"], "complexity": 0}, {"id": 8, "context": "As we saw in the previous section, naive Bayes classifiers can use any sort of feature: dictionaries, URLs, email addresses, network features, phrases, and so on. But if, as in Section 4.3, we use only individual word features, and we use all of the words in the text (not a subset), then naive Bayes has an important similarity to language modeling. Specifically, a naive Bayes model can be viewed as a set of class-specific unigram language models, in which the model for each class instantiates a unigram language model. ", "Bloom_type": "remember", "question": "In what way does using individual word features instead of a subset improve naive Bayes classifier performance?", "options": ["It simplifies the computation by reducing the number of parameters.", "It enhances the accuracy by capturing more complex patterns in the data.", "It reduces overfitting by limiting the complexity of the model.", "It increases the interpretability of the model by providing clear linguistic rules."], "complexity": 0}, {"id": 9, "context": "Up to now we have been describing text classification tasks with only two classes. But lots of classification tasks in language processing have more than two classes. For sentiment analysis we generally have 3 classes (positive, negative, neutral) and even more classes are common for tasks like part-of-speech tagging, word sense disambiguation, semantic role labeling, emotion detection, and so on. Luckily the naive Bayes algorithm is already a multi-class classification algorithm. ", "Bloom_type": "remember", "question": "Which algorithm can handle multi-class classification tasks?", "options": ["Naive Bayes", "Logistic Regression", "Support Vector Machines", "K-Nearest Neighbors"], "complexity": 0}, {"id": 10, "context": "This chapter introduced the naive Bayes model for classification and applied it to the text categorization task of sentiment analysis. ", "Bloom_type": "remember", "question": "In which application was the naive Bayes model primarily used?", "options": ["Sentiment analysis", "Image recognition", "Speech recognition", "Music recommendation"], "complexity": 0}, {"id": 11, "context": "Multinomial naive Bayes text classification was proposed by Maron (1961) at the RAND Corporation for the task of assigning subject categories to journal abstracts. His model introduced most of the features of the modern form presented here, approximating the classification task with one-of categorization, and implementing add- smoothing and information-based feature selection. ", "Bloom_type": "remember", "question": "Who proposed multinomial naive Bayes for text classification?", "options": ["Maron", "RAND Corporation", "Maron (1961)", "Information-based feature selection"], "complexity": 0}, {"id": 12, "context": "The conditional independence assumptions of naive Bayes and the idea of Bayesian analysis of text seems to have arisen multiple times. The same year as Maron`s paper, Minsky (1961) proposed a naive Bayes classifier for vision and other artificial intelligence problems, and Bayesian techniques were also applied to the text classification task of authorship attribution by Mosteller and Wallace (1963). It had long been known that Alexander Hamilton, John Jay, and James Madison wrote the anonymously-published Federalist papers in 17871788 to persuade New York to ratify the United States Constitution. Yet although some of the 85 essays were clearly attributable to one author or another, the authorship of 12 were in dispute between Hamilton and Madison. Mosteller and Wallace (1963) trained a Bayesian probabilistic model of the writing of Hamilton and another model on the writings of Madison, then computed the maximum-likelihood author for each of the disputed essays. Naive Bayes was first applied to spam detection in Heckerman et al. (1998). Metsis et al. (2006), Pang et al. (2002), and Wang and Manning (2012) show that using boolean attributes with multinomial naive Bayes works better than full counts. Binary multinomial naive Bayes is sometimes confused with another variant of naive Bayes that also uses a binary representation of whether a term occurs in a document: Multivariate Bernoulli naive Bayes. The Bernoulli variant instead estimates P(w c) as the fraction of documents that contain a term, and includes a | probability for whether a term is not in a document. McCallum and Nigam (1998) and Wang and Manning (2012) show that the multivariate Bernoulli variant of naive Bayes doesn`t work as well as the multinomial algorithm for sentiment or other text tasks. ", "Bloom_type": "remember", "question": "In what year did Minsky propose a naive Bayes classifier for vision and other artificial intelligence problems?", "options": ["1961", "1963", "1998", "2006"], "complexity": 0}, {"id": 13, "context": "Logistic regression has a number of advantages over naive Bayes. Naive Bayes has overly strong conditional independence assumptions. Consider two features which are strongly correlated; in fact, imagine that we just add the same feature f1 twice. Naive Bayes will treat both copies of f1 as if they were separate, multiplying them both in, overestimating the evidence. By contrast, logistic regression is much more robust to correlated features; if two features f1 and f2 are perfectly correlated, regression will simply assign part of the weight to w1 and part to w2. Thus when there are many correlated features, logistic regression will assign a more accurate probability than naive Bayes. So logistic regression generally works better on larger documents or datasets and is a common default. ", "Bloom_type": "remember", "question": "In what way does logistic regression outperform naive Bayes?", "options": ["Logistic regression can handle correlated features more effectively.", "Logistic regression assigns weights directly to each feature.", "Logistic regression requires less data for training.", "Logistic regression uses fewer parameters."], "complexity": 0}, {"id": 14, "context": "Despite the less accurate probabilities, naive Bayes still often makes the correct classification decision. Furthermore, naive Bayes can work extremely well (sometimes even better than logistic regression) on very small datasets (Ng and Jordan, 2002) or short documents (Wang and Manning, 2012). Furthermore, naive Bayes is easy to implement and very fast to train (there`s no optimization step). So it`s still a reasonable approach to use in some situations. ", "Bloom_type": "remember", "question": "Which of the following statements about Naive Bayes is true?", "options": ["Naive Bayes always provides the most accurate classifications.", "Naive Bayes works best with large datasets.", "Naive Bayes is not suitable for handling categorical data.", "Naive Bayes is computationally expensive and time-consuming."], "complexity": 0}, {"id": 15, "context": "Detecting different interpersonal stances can be useful when extracting information from human-human conversations. The goal here is to detect stances like friendliness or awkwardness in interviews or friendly conversations, for example for summarizing meetings or finding parts of a conversation where people are especially excited or engaged, conversational hot spots that can help in meeting summarization. Detecting the personality of a usersuch as whether the user is an extrovert or the extent to which they are open to experience can help improve conversational agents, which seem to work better if they match users` personality expectations (Mairesse and Walker, 2008). And affect is important for generation as well as recognition; synthesizing affect is important for conversational agents in various domains, including literacy tutors such as children`s storybooks, or computer games. In Chapter 4 we introduced the use of naive Bayes classification to classify a document`s sentiment. Various classifiers have been successfully applied to many of these tasks, using all the words in the training set as input to a classifier which then determines the affect status of the text. ", "Bloom_type": "remember", "question": "In what way does detecting the personality of a user benefit conversational agents?", "options": ["It enhances their matching with users' personality expectations.", "It helps them understand their own emotions.", "It improves their ability to predict future actions.", "It allows them to summarize more accurately."], "complexity": 0}, {"id": 16, "context": "are much easier to define than those of Borges. In this chapter we introduce the naive Bayes algorithm and apply it to text categorization, the task of assigning a label or category to an entire text or document. ", "Bloom_type": "comprehension", "question": "What does the naive Bayes algorithm simplify when compared to other algorithms?", "options": ["The complexity of defining algorithms", "The difficulty of understanding complex texts", "The time required for computation", "The need for extensive data analysis"], "complexity": 1}, {"id": 17, "context": "Finally, one of the oldest tasks in text classification is assigning a library subject category or topic label to a text. Deciding whether a research paper concerns epidemiology or instead, perhaps, embryology, is an important component of information retrieval. Various sets of subject categories exist, such as the MeSH (Medical Subject Headings) thesaurus. In fact, as we will see, subject category classification is the task for which the naive Bayes algorithm was invented in 1961 (Maron, 1961). Classification is essential for tasks below the level of the document as well. We`ve already seen period disambiguation (deciding if a period is the end of a sentence or part of a word), and word tokenization (deciding if a character should be a word boundary). Even language modeling can be viewed as classification: each word can be thought of as a class, and so predicting the next word is classifying the context-so-far into a class for each next word. A part-of-speech tagger (Chapter 17) classifies each occurrence of a word in a sentence as, e.g., a noun or a verb. ", "Bloom_type": "comprehension", "question": "What is the primary purpose of using the Naive Bayes algorithm in text classification?", "options": ["To classify documents based on their content", "To determine the author of a text", "To predict stock market trends", "To encrypt messages"], "complexity": 1}, {"id": 18, "context": "Many kinds of machine learning algorithms are used to build classifiers. This chapter introduces naive Bayes; the following one introduces logistic regression. These exemplify two ways of doing classification. Generative classifiers like naive Bayes build a model of how a class could generate some input data. Given an observation, they return the class most likely to have generated the observation. Discriminative classifiers like logistic regression instead learn what features from the input are most useful to discriminate between the different possible classes. While discriminative systems are often more accurate and hence more commonly used, generative classifiers still have a role. ", "Bloom_type": "comprehension", "question": "What distinguishes naive Bayes classifiers from discriminative classifiers?", "options": ["Naive Bayes assumes independence among predictors, unlike discriminative methods which consider interactions.", "Naive Bayes builds models based on observed data, while discriminative classifiers focus on feature selection.", "Discriminative classifiers use prior probabilities, whereas naive Bayes relies solely on likelihoods.", "Discriminative classifiers require less computational power than naive Bayes."], "complexity": 1}, {"id": 19, "context": "In this section we introduce the multinomial naive Bayes classifier, so called because it is a Bayesian classifier that makes a simplifying (naive) assumption about ", "Bloom_type": "comprehension", "question": "What does the term 'naive Bayes' refer to in the context of machine learning?", "options": ["An algorithm based on Bayes theorem for classification problems", "A type of neural network model", "A method for clustering data points", "A technique for optimizing algorithms"], "complexity": 1}, {"id": 20, "context": "The simplest solution is the add-one (Laplace) smoothing introduced in Chapter 3. While Laplace smoothing is usually replaced by more sophisticated smoothing algorithms in language modeling, it is commonly used in naive Bayes text categorization: ", "Bloom_type": "comprehension", "question": "What is the primary use case for Naive Bayes in text categorization?", "options": ["Using simple add-one (Laplace) smoothing", "Replacing more complex smoothing algorithms", "Avoiding any smoothing techniques", "Neither A nor B"], "complexity": 1}, {"id": 21, "context": "Let`s walk through an example of training and testing naive Bayes with add-one smoothing. We`ll use a sentiment analysis domain with the two classes positive (+) and negative (-), and take the following miniature training and test documents simplified from actual movie reviews. ", "Bloom_type": "comprehension", "question": "What does the naive Bayes algorithm do when it comes to sentiment analysis?", "options": ["Both A and B are correct.", "Classifies documents into predefined categories based on their content.", "Uses machine learning techniques to predict the likelihood of a document belonging to one class over another.", "Analyzes the frequency of specific words within each document to determine its sentiment."], "complexity": 1}, {"id": 22, "context": "While standard naive Bayes text classification can work well for sentiment analysis, some small changes are generally employed that improve performance. ", "Bloom_type": "comprehension", "question": "What type of improvements are typically made to naive Bayes for better sentiment analysis performance?", "options": ["Both A) and B)", "Adding more features to the model", "Increasing the number of training examples", "Using different algorithms instead of Naive Bayes"], "complexity": 1}, {"id": 23, "context": "Consider the task of spam detection, deciding if a particular piece of email is an example of spam (unsolicited bulk email)one of the first applications of naive Bayes to text classification (Sahami et al., 1998). ", "Bloom_type": "comprehension", "question": "What was one of the early applications of Naive Bayes algorithm for text classification?", "options": ["Spam detection", "Sentiment analysis", "Topic modeling", "Named entity recognition"], "complexity": 1}, {"id": 24, "context": "As we saw in the previous section, naive Bayes classifiers can use any sort of feature: dictionaries, URLs, email addresses, network features, phrases, and so on. But if, as in Section 4.3, we use only individual word features, and we use all of the words in the text (not a subset), then naive Bayes has an important similarity to language modeling. Specifically, a naive Bayes model can be viewed as a set of class-specific unigram language models, in which the model for each class instantiates a unigram language model. ", "Bloom_type": "comprehension", "question": "What does using only individual word features imply about naive Bayes classifiers?", "options": ["It implies that the classifier treats every word independently.", "It implies that the classifier uses all available data points.", "It implies that the classifier focuses solely on the most frequent words.", "It implies that the classifier ignores the order of words in sentences."], "complexity": 1}, {"id": 25, "context": "Up to now we have been describing text classification tasks with only two classes. But lots of classification tasks in language processing have more than two classes. For sentiment analysis we generally have 3 classes (positive, negative, neutral) and even more classes are common for tasks like part-of-speech tagging, word sense disambiguation, semantic role labeling, emotion detection, and so on. Luckily the naive Bayes algorithm is already a multi-class classification algorithm. ", "Bloom_type": "comprehension", "question": "What is the primary advantage of using the Naive Bayes algorithm for multi-class classification tasks?", "options": ["It can handle any number of classes without overfitting.", "It simplifies complex data structures.", "It requires less computational resources compared to other algorithms.", "It automatically handles missing values in the dataset."], "complexity": 1}, {"id": 26, "context": "This chapter introduced the naive Bayes model for classification and applied it to the text categorization task of sentiment analysis. ", "Bloom_type": "comprehension", "question": "What was the primary focus of this chapter regarding the naive Bayes model?", "options": ["The theoretical foundations behind the Naive Bayes model", "The implementation details of the Naive Bayes algorithm", "The applications of the Naive Bayes model beyond text categorization", "The limitations of the Naive Bayes model"], "complexity": 1}, {"id": 27, "context": "Multinomial naive Bayes text classification was proposed by Maron (1961) at the RAND Corporation for the task of assigning subject categories to journal abstracts. His model introduced most of the features of the modern form presented here, approximating the classification task with one-of categorization, and implementing add- smoothing and information-based feature selection. ", "Bloom_type": "comprehension", "question": "What did Maron propose for text classification using multinomial naive Bayes?", "options": ["Classifying journal abstracts into categories", "Assigning subjects to journals", "Implementing add-smoothing techniques", "Selecting informative features"], "complexity": 1}, {"id": 28, "context": "The conditional independence assumptions of naive Bayes and the idea of Bayesian analysis of text seems to have arisen multiple times. The same year as Maron`s paper, Minsky (1961) proposed a naive Bayes classifier for vision and other artificial intelligence problems, and Bayesian techniques were also applied to the text classification task of authorship attribution by Mosteller and Wallace (1963). It had long been known that Alexander Hamilton, John Jay, and James Madison wrote the anonymously-published Federalist papers in 17871788 to persuade New York to ratify the United States Constitution. Yet although some of the 85 essays were clearly attributable to one author or another, the authorship of 12 were in dispute between Hamilton and Madison. Mosteller and Wallace (1963) trained a Bayesian probabilistic model of the writing of Hamilton and another model on the writings of Madison, then computed the maximum-likelihood author for each of the disputed essays. Naive Bayes was first applied to spam detection in Heckerman et al. (1998). Metsis et al. (2006), Pang et al. (2002), and Wang and Manning (2012) show that using boolean attributes with multinomial naive Bayes works better than full counts. Binary multinomial naive Bayes is sometimes confused with another variant of naive Bayes that also uses a binary representation of whether a term occurs in a document: Multivariate Bernoulli naive Bayes. The Bernoulli variant instead estimates P(w c) as the fraction of documents that contain a term, and includes a | probability for whether a term is not in a document. McCallum and Nigam (1998) and Wang and Manning (2012) show that the multivariate Bernoulli variant of naive Bayes doesn`t work as well as the multinomial algorithm for sentiment or other text tasks. ", "Bloom_type": "comprehension", "question": "What are two key differences between multinomial naive Bayes and multivariate Bernoulli naive Bayes?", "options": ["Multinomial naive Bayes calculates probabilities based on word frequency whereas multivariate Bernoulli naive Bayes considers the presence or absence of terms.", "Multinomial naive Bayes assumes continuous features while multivariate Bernoulli naive Bayes assumes discrete features.", "Multinomial naive Bayes can handle categorical data directly, unlike multivariate Bernoulli naive Bayes which requires discretization.", "Multinomial naive Bayes is more computationally intensive than multivariate Bernoulli naive Bayes."], "complexity": 1}, {"id": 29, "context": "Logistic regression has a number of advantages over naive Bayes. Naive Bayes has overly strong conditional independence assumptions. Consider two features which are strongly correlated; in fact, imagine that we just add the same feature f1 twice. Naive Bayes will treat both copies of f1 as if they were separate, multiplying them both in, overestimating the evidence. By contrast, logistic regression is much more robust to correlated features; if two features f1 and f2 are perfectly correlated, regression will simply assign part of the weight to w1 and part to w2. Thus when there are many correlated features, logistic regression will assign a more accurate probability than naive Bayes. So logistic regression generally works better on larger documents or datasets and is a common default. ", "Bloom_type": "comprehension", "question": "How does logistic regression handle correlated features compared to naive Bayes?", "options": ["Logistic regression treats each copy of a correlated feature separately.", "Naive Bayes assigns weights based on the correlation between features.", "Logistic regression ignores correlated features entirely.", "Both A and B are true."], "complexity": 1}, {"id": 30, "context": "Despite the less accurate probabilities, naive Bayes still often makes the correct classification decision. Furthermore, naive Bayes can work extremely well (sometimes even better than logistic regression) on very small datasets (Ng and Jordan, 2002) or short documents (Wang and Manning, 2012). Furthermore, naive Bayes is easy to implement and very fast to train (there`s no optimization step). So it`s still a reasonable approach to use in some situations. ", "Bloom_type": "comprehension", "question": "Explain why naive Bayes is considered a reasonable approach despite its accuracy issues?", "options": ["It simplifies complex data by assuming independence between features.", "It has high accuracy rates.", "It performs exceptionally well on large datasets.", "It requires minimal computational resources for training."], "complexity": 1}, {"id": 31, "context": "Detecting different interpersonal stances can be useful when extracting information from human-human conversations. The goal here is to detect stances like friendliness or awkwardness in interviews or friendly conversations, for example for summarizing meetings or finding parts of a conversation where people are especially excited or engaged, conversational hot spots that can help in meeting summarization. Detecting the personality of a usersuch as whether the user is an extrovert or the extent to which they are open to experience can help improve conversational agents, which seem to work better if they match users` personality expectations (Mairesse and Walker, 2008). And affect is important for generation as well as recognition; synthesizing affect is important for conversational agents in various domains, including literacy tutors such as children`s storybooks, or computer games. In Chapter 4 we introduced the use of naive Bayes classification to classify a document`s sentiment. Various classifiers have been successfully applied to many of these tasks, using all the words in the training set as input to a classifier which then determines the affect status of the text. ", "Bloom_type": "comprehension", "question": "What is the primary purpose of applying naive Bayes classification in detecting interpersonal stances?", "options": ["To enhance the performance of conversational agents by matching their personalities", "To determine the personality traits of users", "To summarize meetings and find engaging parts of conversations", "To analyze the affective content of texts across different domains"], "complexity": 1}, {"id": 32, "context": "are much easier to define than those of Borges. In this chapter we introduce the naive Bayes algorithm and apply it to text categorization, the task of assigning a label or category to an entire text or document. ", "Bloom_type": "application", "question": "What is the first step in applying the Naive Bayes algorithm?", "options": ["Collect and preprocess data", "Define the problem statement", "Choose a suitable model", "Calculate probabilities"], "complexity": 2}, {"id": 33, "context": "Finally, one of the oldest tasks in text classification is assigning a library subject category or topic label to a text. Deciding whether a research paper concerns epidemiology or instead, perhaps, embryology, is an important component of information retrieval. Various sets of subject categories exist, such as the MeSH (Medical Subject Headings) thesaurus. In fact, as we will see, subject category classification is the task for which the naive Bayes algorithm was invented in 1961 (Maron, 1961). Classification is essential for tasks below the level of the document as well. We`ve already seen period disambiguation (deciding if a period is the end of a sentence or part of a word), and word tokenization (deciding if a character should be a word boundary). Even language modeling can be viewed as classification: each word can be thought of as a class, and so predicting the next word is classifying the context-so-far into a class for each next word. A part-of-speech tagger (Chapter 17) classifies each occurrence of a word in a sentence as, e.g., a noun or a verb. ", "Bloom_type": "application", "question": "What did Maron invent in 1961?", "options": ["The Naive Bayes Algorithm", "MeSH Thesaurus", "Epidemiology", "Embryology"], "complexity": 2}, {"id": 34, "context": "Many kinds of machine learning algorithms are used to build classifiers. This chapter introduces naive Bayes; the following one introduces logistic regression. These exemplify two ways of doing classification. Generative classifiers like naive Bayes build a model of how a class could generate some input data. Given an observation, they return the class most likely to have generated the observation. Discriminative classifiers like logistic regression instead learn what features from the input are most useful to discriminate between the different possible classes. While discriminative systems are often more accurate and hence more commonly used, generative classifiers still have a role. ", "Bloom_type": "application", "question": "Which type of classifier is primarily concerned with modeling the probability distribution of the target variable?", "options": ["Generative Classifier", "Discriminative Classifier", "Both", "Neither"], "complexity": 2}, {"id": 35, "context": "In this section we introduce the multinomial naive Bayes classifier, so called because it is a Bayesian classifier that makes a simplifying (naive) assumption about ", "Bloom_type": "application", "question": "What does the term 'naive Bayes' refer to in the context of machine learning?", "options": ["An algorithm based on Bayes' theorem with strong independence assumptions among predictors", "A type of neural network model", "A classification method using decision trees", "A clustering technique for unsupervised learning"], "complexity": 2}, {"id": 36, "context": "The simplest solution is the add-one (Laplace) smoothing introduced in Chapter 3. While Laplace smoothing is usually replaced by more sophisticated smoothing algorithms in language modeling, it is commonly used in naive Bayes text categorization: ", "Bloom_type": "application", "question": "What technique is often used as an alternative to Laplace smoothing in naive Bayes classification?", "options": ["Maximum likelihood estimation", "Bayesian inference", "K-means clustering", "Expectation-Maximization algorithm"], "complexity": 2}, {"id": 37, "context": "Let`s walk through an example of training and testing naive Bayes with add-one smoothing. We`ll use a sentiment analysis domain with the two classes positive (+) and negative (-), and take the following miniature training and test documents simplified from actual movie reviews. ", "Bloom_type": "application", "question": "What is the first step in applying Naive Bayes for sentiment analysis?", "options": ["Calculate the probability of each word in the document using unsmoothed frequencies.", "Count the frequency of each unique word across all documents.", "Use the smoothed probabilities directly without further calculations.", "Select the most frequent class label as the predicted sentiment."], "complexity": 2}, {"id": 38, "context": "While standard naive Bayes text classification can work well for sentiment analysis, some small changes are generally employed that improve performance. ", "Bloom_type": "application", "question": "What is a common approach to enhance the performance of standard naive Bayes for sentiment analysis?", "options": ["Implement minor modifications such as handling missing values", "Increase the dataset size", "Use more complex models instead of naive Bayes", "Add more features to the model"], "complexity": 2}, {"id": 39, "context": "Consider the task of spam detection, deciding if a particular piece of email is an example of spam (unsolicited bulk email)one of the first applications of naive Bayes to text classification (Sahami et al., 1998). ", "Bloom_type": "application", "question": "What method should be used for classifying emails as either spam or not spam?", "options": ["Naive Bayes Classifier", "Support Vector Machine", "K-Nearest Neighbors", "Decision Tree"], "complexity": 2}, {"id": 40, "context": "As we saw in the previous section, naive Bayes classifiers can use any sort of feature: dictionaries, URLs, email addresses, network features, phrases, and so on. But if, as in Section 4.3, we use only individual word features, and we use all of the words in the text (not a subset), then naive Bayes has an important similarity to language modeling. Specifically, a naive Bayes model can be viewed as a set of class-specific unigram language models, in which the model for each class instantiates a unigram language model. ", "Bloom_type": "application", "question": "What is the relationship between naive Bayes classifiers and language modeling?", "options": ["Naive Bayes classifiers are similar to language modeling because they both use individual words.", "Naive Bayes classifiers are unrelated to language modeling.", "Naive Bayes classifiers are just a type of language model.", "Naive Bayes classifiers are not used in language modeling."], "complexity": 2}, {"id": 41, "context": "Up to now we have been describing text classification tasks with only two classes. But lots of classification tasks in language processing have more than two classes. For sentiment analysis we generally have 3 classes (positive, negative, neutral) and even more classes are common for tasks like part-of-speech tagging, word sense disambiguation, semantic role labeling, emotion detection, and so on. Luckily the naive Bayes algorithm is already a multi-class classification algorithm. ", "Bloom_type": "application", "question": "What is an example of a multi-class classification task?", "options": ["Sentiment analysis", "Text summarization", "Machine translation", "Image recognition"], "complexity": 2}, {"id": 42, "context": "This chapter introduced the naive Bayes model for classification and applied it to the text categorization task of sentiment analysis. ", "Bloom_type": "application", "question": "What is an example of applying the naive Bayes model?", "options": ["Classifying movie reviews as positive or negative based on their content", "Using the model to predict stock market trends", "Predicting weather conditions using historical data", "Determining the popularity of new products"], "complexity": 2}, {"id": 43, "context": "Multinomial naive Bayes text classification was proposed by Maron (1961) at the RAND Corporation for the task of assigning subject categories to journal abstracts. His model introduced most of the features of the modern form presented here, approximating the classification task with one-of categorization, and implementing add- smoothing and information-based feature selection. ", "Bloom_type": "application", "question": "What did Maron propose as an approach to classify journal abstracts?", "options": ["Multinomial Naive Bayes classifier", "Naive Bayes algorithm", "Bayesian network", "Decision tree"], "complexity": 2}, {"id": 44, "context": "The conditional independence assumptions of naive Bayes and the idea of Bayesian analysis of text seems to have arisen multiple times. The same year as Maron`s paper, Minsky (1961) proposed a naive Bayes classifier for vision and other artificial intelligence problems, and Bayesian techniques were also applied to the text classification task of authorship attribution by Mosteller and Wallace (1963). It had long been known that Alexander Hamilton, John Jay, and James Madison wrote the anonymously-published Federalist papers in 17871788 to persuade New York to ratify the United States Constitution. Yet although some of the 85 essays were clearly attributable to one author or another, the authorship of 12 were in dispute between Hamilton and Madison. Mosteller and Wallace (1963) trained a Bayesian probabilistic model of the writing of Hamilton and another model on the writings of Madison, then computed the maximum-likelihood author for each of the disputed essays. Naive Bayes was first applied to spam detection in Heckerman et al. (1998). Metsis et al. (2006), Pang et al. (2002), and Wang and Manning (2012) show that using boolean attributes with multinomial naive Bayes works better than full counts. Binary multinomial naive Bayes is sometimes confused with another variant of naive Bayes that also uses a binary representation of whether a term occurs in a document: Multivariate Bernoulli naive Bayes. The Bernoulli variant instead estimates P(w c) as the fraction of documents that contain a term, and includes a | probability for whether a term is not in a document. McCallum and Nigam (1998) and Wang and Manning (2012) show that the multivariate Bernoulli variant of naive Bayes doesn`t work as well as the multinomial algorithm for sentiment or other text tasks. ", "Bloom_type": "application", "question": "Which method did most researchers initially apply to classify authors based on their writing style?", "options": ["Naive Bayes", "Multinomial Naive Bayes", "Bernoulli Naive Bayes", "Boolean Naive Bayes"], "complexity": 2}, {"id": 45, "context": "Logistic regression has a number of advantages over naive Bayes. Naive Bayes has overly strong conditional independence assumptions. Consider two features which are strongly correlated; in fact, imagine that we just add the same feature f1 twice. Naive Bayes will treat both copies of f1 as if they were separate, multiplying them both in, overestimating the evidence. By contrast, logistic regression is much more robust to correlated features; if two features f1 and f2 are perfectly correlated, regression will simply assign part of the weight to w1 and part to w2. Thus when there are many correlated features, logistic regression will assign a more accurate probability than naive Bayes. So logistic regression generally works better on larger documents or datasets and is a common default. ", "Bloom_type": "application", "question": "Which algorithm is generally considered more robust against correlated features?", "options": ["Logistic Regression", "Naive Bayes", "Both equally well", "Neither"], "complexity": 2}, {"id": 46, "context": "Despite the less accurate probabilities, naive Bayes still often makes the correct classification decision. Furthermore, naive Bayes can work extremely well (sometimes even better than logistic regression) on very small datasets (Ng and Jordan, 2002) or short documents (Wang and Manning, 2012). Furthermore, naive Bayes is easy to implement and very fast to train (there`s no optimization step). So it`s still a reasonable approach to use in some situations. ", "Bloom_type": "application", "question": "What advantage does naive Bayes have over other classifiers?", "options": ["It is easier to implement and faster to train.", "It requires more computational resources for training.", "It provides highly accurate probability estimates.", "It works best with large datasets."], "complexity": 2}, {"id": 47, "context": "Detecting different interpersonal stances can be useful when extracting information from human-human conversations. The goal here is to detect stances like friendliness or awkwardness in interviews or friendly conversations, for example for summarizing meetings or finding parts of a conversation where people are especially excited or engaged, conversational hot spots that can help in meeting summarization. Detecting the personality of a usersuch as whether the user is an extrovert or the extent to which they are open to experience can help improve conversational agents, which seem to work better if they match users` personality expectations (Mairesse and Walker, 2008). And affect is important for generation as well as recognition; synthesizing affect is important for conversational agents in various domains, including literacy tutors such as children`s storybooks, or computer games. In Chapter 4 we introduced the use of naive Bayes classification to classify a document`s sentiment. Various classifiers have been successfully applied to many of these tasks, using all the words in the training set as input to a classifier which then determines the affect status of the text. ", "Bloom_type": "application", "question": "Which algorithm was used to classify the sentiment of documents?", "options": ["Naive Bayes Classification", "Support Vector Machines", "Decision Trees", "Random Forests"], "complexity": 2}]}, "loss function": {"max_id": 94, "Questions": [{"id": 0, "context": "3. An objective function that we want to optimize for learning, usually involving minimizing a loss function corresponding to error on training examples. We will introduce the cross-entropy loss function. ", "Bloom_type": "remember", "question": "In machine learning, what is the purpose of an objective function?", "options": ["To minimize the loss function", "To measure the accuracy of predictions", "To maximize the model complexity", "To calculate the entropy of data"], "complexity": 0}, {"id": 1, "context": "label y. Rather than measure similarity, we usually talk about the opposite of this: the distance between the system output and the gold output, and we call this distance the loss function or the cost function. In the next section we`ll introduce the loss function that is commonly used for logistic regression and also for neural networks, the cross-entropy loss. ", "Bloom_type": "remember", "question": "In machine learning, what do we refer to as the distance between the system output and the gold output when measuring similarity?", "options": ["Loss function", "Accuracy", "Precision", "Recall"], "complexity": 0}, {"id": 2, "context": "The second thing we need is an optimization algorithm for iteratively updating the weights so as to minimize this loss function. The standard algorithm for this is gradient descent; we`ll introduce the stochastic gradient descent algorithm in the following section. ", "Bloom_type": "remember", "question": "In machine learning, what does a loss function measure?", "options": ["The distance between predicted values and actual outcomes", "The accuracy of predictions", "The number of iterations needed for convergence", "The speed at which data is processed"], "complexity": 0}, {"id": 3, "context": "Eq. 5.22 describes a log likelihood that should be maximized. In order to turn this into a loss function (something that we need to minimize), we`ll just flip the sign on Eq. 5.22. The result is the cross-entropy loss LCE: ", "Bloom_type": "remember", "question": "In machine learning, what mathematical operation is used to convert the log likelihood into a loss function?", "options": ["Subtraction", "Addition", "Multiplication", "Division"], "complexity": 0}, {"id": 4, "context": "Why does minimizing this negative log probability do what we want? A perfect classifier would assign probability 1 to the correct outcome (y = 1 or y = 0) and probability 0 to the incorrect outcome. That means if y equals 1, the higher y is (the closer it is to 1), the better the classifier; the lower y is (the closer it is to 0), the y is (closer to 1), the better worse the classifier. If y equals 0, instead, the higher 1 the classifier. The negative log of y (if the true y equals 1) or 1 y (if the true y equals 0) is a convenient loss metric since it goes from 0 (negative log of 1, no loss) to infinity (negative log of 0, infinite loss). This loss function also ensures that as the probability of the correct answer is maximized, the probability of the incorrect answer is minimized; since the two sum to one, any increase in the probability of the correct answer is coming at the expense of the incorrect answer. It`s called the crossentropy loss, because Eq. 5.22 is also the formula for the cross-entropy between the true probability distribution y and our estimated distribution y. ", "Bloom_type": "remember", "question": "In machine learning, why is minimizing the negative log probability useful?", "options": ["To ensure that the model assigns high probabilities only to the correct outcomes.", "To maximize the accuracy of predictions by assigning equal probabilities to all outcomes.", "To minimize the error rate by penalizing incorrect classifications more heavily than correct ones.", "To calculate the exact probability distribution of each class."], "complexity": 0}, {"id": 5, "context": "(cid:88)i=1 How shall we find the minimum of this (or any) loss function? Gradient descent is a method that finds a minimum of a function by figuring out in which direction (in the space of the parameters  ) the function`s slope is rising the most steeply, and moving in the opposite direction. The intuition is that if you are hiking in a canyon and trying to descend most quickly down to the river at the bottom, you might look around yourself in all directions, find the direction where the ground is sloping the steepest, and walk downhill in that direction. ", "Bloom_type": "remember", "question": "In gradient descent, what does it mean when we say the function's slope is rising the most steeply?", "options": ["The derivative of the function is positive", "The function value is increasing", "The function has no local minima", "The function is linear"], "complexity": 0}, {"id": 6, "context": "For logistic regression, this loss function is conveniently convex. A convex function has at most one minimum; there are no local minima to get stuck in, so gradient descent starting from any point is guaranteed to find the minimum. (By contrast, the loss for multi-layer neural networks is non-convex, and gradient descent may get stuck in local minima for neural network training and never find the global optimum.) ", "Bloom_type": "remember", "question": "Which type of function does not guarantee finding the global optimum using gradient descent?", "options": ["Non-convex", "Convex", "Linear", "Quadratic"], "complexity": 0}, {"id": 7, "context": "Given a random initialization of w at some value w1, and assuming the loss function L happened to have the shape in Fig. 5.4, we need the algorithm to tell us whether at the next iteration we should move left (making w2 smaller than w1) or right (making w2 bigger than w1) to reach the minimum. ", "Bloom_type": "remember", "question": "In the context described, what is the role of the loss function \\(L\\)?", "options": ["It guides the direction of weight updates.", "It determines the initial values of weights.", "It calculates the accuracy of predictions.", "It measures how well the model fits the data."], "complexity": 0}, {"id": 8, "context": "The gradient descent algorithm answers this question by finding the gradient of the loss function at the current point and moving in the opposite direction. The gradient of a function of many variables is a vector pointing in the direction of the greatest increase in a function. The gradient is a multi-variable generalization of the ", "Bloom_type": "remember", "question": "In machine learning, what does the gradient descent algorithm use to find the minimum value of a loss function?", "options": ["The derivative of the loss function with respect to each variable", "The sum of all variable values", "The average of all variable values", "The integral of the loss function over all variables"], "complexity": 0}, {"id": 9, "context": "slope, so for a function of one variable like the one in Fig. 5.4, we can informally think of the gradient as the slope. The dotted line in Fig. 5.4 shows the slope of this hypothetical loss function at point w = w1. You can see that the slope of this dotted line is negative. Thus to find the minimum, gradient descent tells us to go in the opposite direction: moving w in a positive direction. ", "Bloom_type": "remember", "question": "In the context of finding the minimum using gradient descent, what does the negative slope indicate about the direction of movement?", "options": ["The direction of maximum decrease", "The direction of steepest ascent", "The direction of maximum increase", "The direction of no change"], "complexity": 0}, {"id": 10, "context": "In order to update  , we need a definition for the gradient L( f (x;  ), y). Recall that for logistic regression, the cross-entropy loss function is: ", "Bloom_type": "remember", "question": "In machine learning, what does the term \"gradient\" refer to when updating parameters?", "options": ["The rate of change of a function at a particular point", "The difference between actual outcomes and expected outcomes", "The sum of all errors made by the model", "The measure of how well the model fits the data"], "complexity": 0}, {"id": 11, "context": "Stochastic gradient descent is an online algorithm that minimizes the loss function by computing its gradient after each training example, and nudging  in the right direction (the opposite direction of the gradient). (An online algorithm is one that processes its input example by example, rather than waiting until it sees the entire input.) Stochastic gradient descent is called stochastic because it chooses a single random example at a time; in Section 5.6.4 we`ll discuss other versions of gradient descent that batch many examples at once. Fig. 5.6 shows the algorithm. ", "Bloom_type": "remember", "question": "What does stochastic gradient descent do with respect to the loss function?", "options": ["It computes the loss function's gradient after each training example.", "It calculates the exact value of the loss function.", "It ignores the loss function entirely.", "It waits for all training examples before making any adjustments."], "complexity": 0}, {"id": 12, "context": "loss function, such as the cross-entropy loss, that must be minimized. ", "Bloom_type": "remember", "question": "In machine learning, what is a key component of training models?", "options": ["Loss function minimization", "Data preprocessing", "Feature selection", "Model evaluation"], "complexity": 0}, {"id": 13, "context": " Minimizing this loss function is a convex optimization problem, and iterative ", "Bloom_type": "remember", "question": "In machine learning, what does minimizing the loss function help achieve?", "options": ["It guarantees global optimality of the solution.", "It helps reduce computational complexity.", "It ensures the model parameters converge faster.", "It prevents overfitting on training data."], "complexity": 0}, {"id": 14, "context": "We minimize this loss function using stochastic gradient descent. Fig. 6.14 ", "Bloom_type": "remember", "question": "In machine learning, what is used to measure how well a model fits the data?", "options": ["Loss Function", "Accuracy", "Precision", "Recall"], "complexity": 0}, {"id": 15, "context": "First, we`ll need a loss function that models the distance between the system output and the gold output, and it`s common to use the loss function used for logistic regression, the cross-entropy loss. ", "Bloom_type": "remember", "question": "In machine learning, what is the purpose of using a loss function?", "options": ["To calculate the distance between the predicted and actual values", "To measure the accuracy of the model", "To determine the bias of the model", "To optimize the hyperparameters"], "complexity": 0}, {"id": 16, "context": "Third, gradient descent requires knowing the gradient of the loss function, the vector that contains the partial derivative of the loss function with respect to each of the parameters. In logistic regression, for each observation we could directly compute the derivative of the loss function with respect to an individual w or b. But for neural networks, with millions of parameters in many layers, it`s much harder to see how to compute the partial derivative of some weight in layer 1 when the loss is attached to some much later layer. How do we partial out the loss over all those intermediate layers? The answer is the algorithm called error backpropagation or backward differentiation. ", "Bloom_type": "remember", "question": "In machine learning, what technique is used to calculate the partial derivatives of a loss function with respect to its parameters?", "options": ["Backward propagation", "Gradient descent", "Forward propagation", "Error minimization"], "complexity": 0}, {"id": 17, "context": "For the backward pass we`ll also need to compute the loss L. The loss function ", "Bloom_type": "remember", "question": "In machine learning, what is used for computing the error between predicted values and actual values during the backward pass?", "options": ["Loss function", "Activation function", "Cost function", "Gradient descent"], "complexity": 0}, {"id": 18, "context": "is used to compute the gradients of the loss function for a network. ", "Bloom_type": "remember", "question": "In machine learning, what does the loss function primarily measure?", "options": ["The error between predicted values and actual outcomes", "The accuracy of predictions", "The efficiency of computation", "The complexity of algorithms"], "complexity": 0}, {"id": 19, "context": "To train an RNN as a language model, we use the same self-supervision (or selftraining) algorithm we saw in Section 7.7: we take a corpus of text as training material and at each time step t ask the model to predict the next word. We call such a model self-supervised because we don`t have to add any special gold labels to the data; the natural sequence of words is its own supervision! We simply train the model to minimize the error in predicting the true next word in the training sequence, using cross-entropy as the loss function. Recall that the cross-entropy loss measures the difference between a predicted probability distribution and the ", "Bloom_type": "remember", "question": "In the process of training an RNN for a language model, what does minimizing the error in predicting the true next word entail?", "options": ["Minimizing the discrepancy between the predicted probabilities and actual outcomes", "Calculating the entropy of the entire dataset", "Determining the accuracy of the current model", "Ensuring all predictions are within a certain threshold"], "complexity": 0}, {"id": 20, "context": "Note that in this approach we don`t need intermediate outputs for the words in the sequence preceding the last element. Therefore, there are no loss terms associated with those elements. Instead, the loss function used to train the weights in the network is based entirely on the final text classification task. The output from the softmax output from the feedforward classifier together with a cross-entropy loss drives the training. The error signal from the classification is backpropagated all the way through the weights in the feedforward classifier through, to its input, and then through to the three sets of weights in the RNN as described earlier in Section 8.1.2. The training regimen that uses the loss from a downstream application to adjust the weights all the way through the network is referred to as end-to-end training. ", "Bloom_type": "remember", "question": "In the context provided, what does the loss function primarily focus on during the training process?", "options": ["Final text classification task", "Intermediate outputs", "Cross-entropy loss", "Error signals"], "complexity": 0}, {"id": 21, "context": "To train a transformer as a language model, we use the same self-supervision (or self-training) algorithm we saw in Section 8.2.2: we take a corpus of text as training material and at each time step t ask the model to predict the next word. We call such a model self-supervised because we don`t have to add any special gold labels to the data; the natural sequence of words is its own supervision! We simply train the model to minimize the error in predicting the true next word in the training sequence, using cross-entropy as the loss function. ", "Bloom_type": "remember", "question": "In the process of training a transformer for a language model, what is used to measure how well the model predicts the next word in a sequence?", "options": ["Loss function", "Cross-correlation", "Mean squared error", "Root mean square error"], "complexity": 0}, {"id": 22, "context": "As a way of getting a model to do what we want, prompting is fundamentally different than pretraining. Learning via pretraining means updating the model`s parameters by using gradient descent according to some loss function. But prompting with demonstrations can teach a model to do a new task. The model is learning something as it processes the prompt. ", "Bloom_type": "remember", "question": "What does the loss function help the model learn?", "options": ["The model learns how to process prompts effectively.", "The model learns how to perform tasks efficiently.", "The model learns how to update its parameters correctly.", "The model learns how to generalize well."], "complexity": 0}, {"id": 23, "context": "In the task-based finetuning of Chapter 11, we adapt to a particular task by adding a new specialized classification head and updating its features via its own loss function (e.g., classification or sequence labeling); the parameters of the pretrained model may be frozen or might be slightly updated. ", "Bloom_type": "remember", "question": "In the process of adapting a model for a specific task, what is primarily used to update the features of the specialized classification head?", "options": ["A modified version of the loss function", "The original loss function", "The pre-trained model itself", "None of the above"], "complexity": 0}, {"id": 24, "context": "While humans produce the best evaluations of machine translation output, running a human evaluation can be time consuming and expensive. For this reason automatic metrics are often used as temporary proxies. Automatic metrics are less accurate than human evaluation, but can help test potential system improvements, and even be used as an automatic loss function for training. In this section we introduce two families of such metrics, those based on character- or word-overlap and those based on embedding similarity. ", "Bloom_type": "remember", "question": "Which of the following is NOT a family of automatic metrics mentioned in the context?", "options": ["Sentence-level metrics", "Character-overlap metrics", "Word-overlap metrics", "Embedding-similarity metrics"], "complexity": 0}, {"id": 25, "context": "In this section we briefly introduce an alternative to encoder-decoder: an algorithm and loss function called CTC, short for Connectionist Temporal Classification (Graves et al., 2006), that deals with these problems in a very different way. The intuition of CTC is to output a single character for every frame of the input, so that ", "Bloom_type": "remember", "question": "What does the CTC algorithm do differently from traditional methods?", "options": ["It outputs characters per frame", "It uses a decoder instead of an encoder", "It focuses on temporal classification only", "It calculates losses based on character accuracy"], "complexity": 0}, {"id": 26, "context": "To train a CTC-based ASR system, we use negative log-likelihood loss with a special CTC loss function. Thus the loss for an entire dataset D is the sum of the negative log-likelihoods of the correct output Y for each input X: ", "Bloom_type": "remember", "question": "In training a CTC-based ASR system, what mathematical operation is used as the loss function?", "options": ["Negative log-likelihood", "Cross entropy", "Softmax", "Mean squared error"], "complexity": 0}, {"id": 27, "context": "It`s also possible to combine the two architectures/loss functions we`ve described, the cross-entropy loss from the encoder-decoder architecture, and the CTC loss. Fig. 16.13 shows a sketch. For training, we can simply weight the two losses with a  tuned on a devset: ", "Bloom_type": "remember", "question": "In machine learning, what is the purpose of combining different types of loss functions?", "options": ["To improve model accuracy", "To increase computational efficiency", "To reduce overfitting", "To simplify data preprocessing"], "complexity": 0}, {"id": 28, "context": " Two common paradigms for speech recognition are the encoder-decoder with attention model, and models based on the CTC loss function. Attentionbased models have higher accuracies, but models based on CTC more easily adapt to streaming: outputting graphemes online instead of waiting until the acoustic input is complete. ", "Bloom_type": "remember", "question": "Which paradigm has a lower accuracy than the encoder-decoder with attention model?", "options": ["CTC loss function", "Encoder-decoder with attention model", "Both have equal accuracy", "None of the above"], "complexity": 0}, {"id": 29, "context": "Meanwhile early work had proposed the CTC loss function by 2006 (Graves et al., 2006), and by 2012 the RNN-Transducer was defined and applied to phone recognition (Graves 2012, Graves et al. 2013), and then to end-to-end speech recognition rescoring (Graves and Jaitly, 2014), and then recognition (Maas et al., 2015), (Our dewith advances such as specialized beam search (Hannun et al., 2014). scription of CTC in the chapter draws on Hannun (2017), which we encourage the interested reader to follow). ", "Bloom_type": "remember", "question": "In what year did researchers propose the CTC loss function?", "options": ["2006", "2012", "2014", "2015"], "complexity": 0}, {"id": 30, "context": "Training is trickier in the mention-ranking model than the mention-pair model, because for each anaphor we don`t know which of all the possible gold antecedents Instead, the best antecedent for each mention is latent; that to use for training. is, for each mention we have a whole cluster of legal gold antecedents to choose from. Early work used heuristics to choose an antecedent, for example choosing the closest antecedent as the gold antecedent and all non-antecedents in a window of two sentences as the negative examples (Denis and Baldridge, 2008). Various kinds of ways to model latent antecedents exist (Fernandes et al. 2012, Chang et al. 2013, Durrett and Klein 2013). The simplest way is to give credit to any legal antecedent by summing over all of them, with a loss function that optimizes the likelihood of all correct antecedents from the gold clustering (Lee et al., 2017b). We`ll see the details in Section 23.6. ", "Bloom_type": "remember", "question": "In the mention-ranking model, what does the loss function primarily aim to optimize?", "options": ["The probability of selecting the correct antecedent", "The accuracy of the predicted mentions", "The number of positive examples", "The distance between mentions and their antecedents"], "complexity": 0}, {"id": 31, "context": "3. An objective function that we want to optimize for learning, usually involving minimizing a loss function corresponding to error on training examples. We will introduce the cross-entropy loss function. ", "Bloom_type": "comprehension", "question": "What does the loss function help us do during the learning process?", "options": ["It helps us optimize the parameters of our model.", "It helps us minimize errors on test examples.", "It helps us maximize the accuracy of our model.", "It helps us understand the data better."], "complexity": 1}, {"id": 32, "context": "label y. Rather than measure similarity, we usually talk about the opposite of this: the distance between the system output and the gold output, and we call this distance the loss function or the cost function. In the next section we`ll introduce the loss function that is commonly used for logistic regression and also for neural networks, the cross-entropy loss. ", "Bloom_type": "comprehension", "question": "What does the term 'loss function' refer to when compared to measuring similarity?", "options": ["The difference between the predicted value and the actual value", "The accuracy rate of the model", "The precision of the algorithm", "The consistency of data input"], "complexity": 1}, {"id": 33, "context": "The second thing we need is an optimization algorithm for iteratively updating the weights so as to minimize this loss function. The standard algorithm for this is gradient descent; we`ll introduce the stochastic gradient descent algorithm in the following section. ", "Bloom_type": "comprehension", "question": "What does the loss function represent in machine learning algorithms?", "options": ["The error between predicted values and actual values", "The accuracy of the model", "The number of iterations required for convergence", "The rate at which the weights are updated"], "complexity": 1}, {"id": 34, "context": "Eq. 5.22 describes a log likelihood that should be maximized. In order to turn this into a loss function (something that we need to minimize), we`ll just flip the sign on Eq. 5.22. The result is the cross-entropy loss LCE: ", "Bloom_type": "comprehension", "question": "What is the purpose of flipping the sign of Eq. 5.22 when turning it into a loss function?", "options": ["To minimize the likelihood", "To maximize the likelihood", "To maximize the probability", "To minimize the probability"], "complexity": 1}, {"id": 35, "context": "Why does minimizing this negative log probability do what we want? A perfect classifier would assign probability 1 to the correct outcome (y = 1 or y = 0) and probability 0 to the incorrect outcome. That means if y equals 1, the higher y is (the closer it is to 1), the better the classifier; the lower y is (the closer it is to 0), the y is (closer to 1), the better worse the classifier. If y equals 0, instead, the higher 1 the classifier. The negative log of y (if the true y equals 1) or 1 y (if the true y equals 0) is a convenient loss metric since it goes from 0 (negative log of 1, no loss) to infinity (negative log of 0, infinite loss). This loss function also ensures that as the probability of the correct answer is maximized, the probability of the incorrect answer is minimized; since the two sum to one, any increase in the probability of the correct answer is coming at the expense of the incorrect answer. It`s called the crossentropy loss, because Eq. 5.22 is also the formula for the cross-entropy between the true probability distribution y and our estimated distribution y. ", "Bloom_type": "comprehension", "question": "Explain why minimizing the negative log probability is beneficial for achieving a good classifier?", "options": ["It measures the difference between the true and estimated probabilities using logarithms.", "It assigns probabilities based on how close the predicted value is to 1 or 0.", "It calculates the distance between the actual and predicted values directly.", "It penalizes classifiers more heavily when they predict incorrectly."], "complexity": 1}, {"id": 36, "context": "(cid:88)i=1 How shall we find the minimum of this (or any) loss function? Gradient descent is a method that finds a minimum of a function by figuring out in which direction (in the space of the parameters  ) the function`s slope is rising the most steeply, and moving in the opposite direction. The intuition is that if you are hiking in a canyon and trying to descend most quickly down to the river at the bottom, you might look around yourself in all directions, find the direction where the ground is sloping the steepest, and walk downhill in that direction. ", "Bloom_type": "comprehension", "question": "Explain how gradient descent works to minimize a loss function?", "options": ["By identifying the lowest point of the loss function", "By finding the highest point of the loss function", "By randomly selecting points until convergence", "By calculating the average value of the loss function"], "complexity": 1}, {"id": 37, "context": "For logistic regression, this loss function is conveniently convex. A convex function has at most one minimum; there are no local minima to get stuck in, so gradient descent starting from any point is guaranteed to find the minimum. (By contrast, the loss for multi-layer neural networks is non-convex, and gradient descent may get stuck in local minima for neural network training and never find the global optimum.) ", "Bloom_type": "comprehension", "question": "Explain how the convexity property of the loss function affects the optimization process in logistic regression compared to multi-layer neural networks?", "options": ["The convexity of the loss function in logistic regression guarantees that gradient descent will always converge to the global minimum.", "In logistic regression, gradient descent can easily get stuck in local minima due to its non-convex nature.", "Multi-layer neural networks benefit from their non-convex loss functions because they allow for more complex models.", "Both logistic regression and multi-layer neural networks use gradient descent for optimization."], "complexity": 1}, {"id": 38, "context": "Given a random initialization of w at some value w1, and assuming the loss function L happened to have the shape in Fig. 5.4, we need the algorithm to tell us whether at the next iteration we should move left (making w2 smaller than w1) or right (making w2 bigger than w1) to reach the minimum. ", "Bloom_type": "comprehension", "question": "Explain how the loss function determines the direction of movement towards the minimum?", "options": ["The loss function calculates the gradient of the loss surface at each point and decides based on its sign.", "The loss function always moves the weight w closer to zero.", "The loss function randomly selects directions for movement.", "The loss function does not influence the movement."], "complexity": 1}, {"id": 39, "context": "The gradient descent algorithm answers this question by finding the gradient of the loss function at the current point and moving in the opposite direction. The gradient of a function of many variables is a vector pointing in the direction of the greatest increase in a function. The gradient is a multi-variable generalization of the ", "Bloom_type": "comprehension", "question": "Explain how the gradient descent algorithm uses the gradient of the loss function to find the optimal solution?", "options": ["The algorithm finds the minimum of the loss function by moving in the direction of decreasing values.", "The algorithm calculates the average value of all variables involved.", "It moves in the direction of increasing values for each variable.", "The algorithm ignores the gradient entirely, focusing only on minimizing the loss function."], "complexity": 1}, {"id": 40, "context": "slope, so for a function of one variable like the one in Fig. 5.4, we can informally think of the gradient as the slope. The dotted line in Fig. 5.4 shows the slope of this hypothetical loss function at point w = w1. You can see that the slope of this dotted line is negative. Thus to find the minimum, gradient descent tells us to go in the opposite direction: moving w in a positive direction. ", "Bloom_type": "comprehension", "question": "Explain how the concept of slope relates to finding the minimum using gradient descent?", "options": ["The slope indicates the rate of change; going against it helps minimize the loss.", "Slope determines the direction but not the magnitude of movement towards the minimum.", "Gradient descent uses the slope directly to calculate the exact minimum value.", "Slope has no relation to minimizing loss functions."], "complexity": 1}, {"id": 41, "context": "In an actual logistic regression, the parameter vector w is much longer than 1 or 2, since the input feature vector x can be quite long, and we need a weight wi for each xi. For each dimension/variable wi in w (plus the bias b), the gradient will have a component that tells us the slope with respect to that variable. In each dimension wi, we express the slope as a partial derivative  of the loss function. Essentially  wi we`re asking: How much would a small change in that variable wi influence the total loss function L? ", "Bloom_type": "comprehension", "question": "Explain how the loss function contributes to determining the weights in logistic regression?", "options": ["The loss function provides a measure of how well the model fits the data, guiding adjustments to the weights.", "The loss function calculates the overall error between predicted values and actual outcomes.", "The loss function determines the optimal weights by minimizing the sum of squared errors.", "The loss function adjusts the weights based on the number of features in the dataset."], "complexity": 1}, {"id": 42, "context": "In order to update  , we need a definition for the gradient L( f (x;  ), y). Recall that for logistic regression, the cross-entropy loss function is: ", "Bloom_type": "comprehension", "question": "What is the primary purpose of defining the gradient in machine learning algorithms?", "options": ["To minimize error rates during training", "To increase computational speed", "To enhance model accuracy", "To reduce memory usage"], "complexity": 1}, {"id": 43, "context": "Stochastic gradient descent is an online algorithm that minimizes the loss function by computing its gradient after each training example, and nudging  in the right direction (the opposite direction of the gradient). (An online algorithm is one that processes its input example by example, rather than waiting until it sees the entire input.) Stochastic gradient descent is called stochastic because it chooses a single random example at a time; in Section 5.6.4 we`ll discuss other versions of gradient descent that batch many examples at once. Fig. 5.6 shows the algorithm. ", "Bloom_type": "comprehension", "question": "What does the loss function represent in the context of stochastic gradient descent?", "options": ["The total error over all training examples", "The average error per training example", "The sum of squared errors for each individual example", "The difference between predicted and actual values"], "complexity": 1}, {"id": 44, "context": "loss function, such as the cross-entropy loss, that must be minimized. ", "Bloom_type": "comprehension", "question": "What does the loss function aim to minimize?", "options": ["The error between predicted values and actual outcomes", "The accuracy of predictions", "The complexity of the model", "The performance metrics"], "complexity": 1}, {"id": 45, "context": " Minimizing this loss function is a convex optimization problem, and iterative ", "Bloom_type": "comprehension", "question": "What does minimizing the loss function involve?", "options": ["Finding the minimum value of the function", "Determining the maximum value of the function", "Calculating the average of all values", "Identifying points where the function changes direction"], "complexity": 1}, {"id": 46, "context": "We minimize this loss function using stochastic gradient descent. Fig. 6.14 ", "Bloom_type": "comprehension", "question": "What does minimizing the loss function involve?", "options": ["Adjusting parameters based on gradients", "Calculating the error between predicted values and actual outcomes", "Determining the accuracy of model predictions", "Increasing the complexity of the model"], "complexity": 1}, {"id": 47, "context": "First, we`ll need a loss function that models the distance between the system output and the gold output, and it`s common to use the loss function used for logistic regression, the cross-entropy loss. ", "Bloom_type": "comprehension", "question": "What type of loss function is commonly used when modeling the distance between the system output and the gold output?", "options": ["Cross-Entropy Loss", "Mean Squared Error (MSE)", "Huber Loss", "RMSprop Loss"], "complexity": 1}, {"id": 48, "context": "Third, gradient descent requires knowing the gradient of the loss function, the vector that contains the partial derivative of the loss function with respect to each of the parameters. In logistic regression, for each observation we could directly compute the derivative of the loss function with respect to an individual w or b. But for neural networks, with millions of parameters in many layers, it`s much harder to see how to compute the partial derivative of some weight in layer 1 when the loss is attached to some much later layer. How do we partial out the loss over all those intermediate layers? The answer is the algorithm called error backpropagation or backward differentiation. ", "Bloom_type": "comprehension", "question": "Explain how the error backpropagation algorithm addresses the challenge of computing gradients through multiple layers in neural networks?", "options": ["By using a differentiable approximation method to approximate the derivatives at each step.", "It simplifies the computation by reducing the number of weights involved.", "Error backpropagation calculates the exact gradient for every single parameter.", "The algorithm does not address this issue; instead, it introduces new parameters."], "complexity": 1}, {"id": 49, "context": "For the backward pass we`ll also need to compute the loss L. The loss function ", "Bloom_type": "comprehension", "question": "What does the loss function represent in the context of computing gradients for backpropagation?", "options": ["The error between predicted and actual outputs", "The sum of all input values", "The total number of output neurons", "The average activation level across all units"], "complexity": 1}, {"id": 50, "context": "is used to compute the gradients of the loss function for a network. ", "Bloom_type": "comprehension", "question": "What does the loss function help calculate?", "options": ["The gradient of the error with respect to the weights", "The accuracy of predictions made by the model", "The number of parameters in the model", "The total cost of training the model"], "complexity": 1}, {"id": 51, "context": "To train an RNN as a language model, we use the same self-supervision (or selftraining) algorithm we saw in Section 7.7: we take a corpus of text as training material and at each time step t ask the model to predict the next word. We call such a model self-supervised because we don`t have to add any special gold labels to the data; the natural sequence of words is its own supervision! We simply train the model to minimize the error in predicting the true next word in the training sequence, using cross-entropy as the loss function. Recall that the cross-entropy loss measures the difference between a predicted probability distribution and the ", "Bloom_type": "comprehension", "question": "What does the cross-entropy loss measure in the context of training an RNN for language modeling?", "options": ["The difference between the predicted probabilities and the true probabilities", "The accuracy of the model's predictions", "The number of correctly classified words", "The similarity between the predicted and actual sequences"], "complexity": 1}, {"id": 52, "context": "Note that in this approach we don`t need intermediate outputs for the words in the sequence preceding the last element. Therefore, there are no loss terms associated with those elements. Instead, the loss function used to train the weights in the network is based entirely on the final text classification task. The output from the softmax output from the feedforward classifier together with a cross-entropy loss drives the training. The error signal from the classification is backpropagated all the way through the weights in the feedforward classifier through, to its input, and then through to the three sets of weights in the RNN as described earlier in Section 8.1.2. The training regimen that uses the loss from a downstream application to adjust the weights all the way through the network is referred to as end-to-end training. ", "Bloom_type": "comprehension", "question": "What type of loss function is typically used during the training phase when using an end-to-end approach?", "options": ["Cross-Entropy Loss", "Mean Squared Error (MSE)", "Binary Cross-Entropy Loss", "Hinge Loss"], "complexity": 1}, {"id": 53, "context": "To train a transformer as a language model, we use the same self-supervision (or self-training) algorithm we saw in Section 8.2.2: we take a corpus of text as training material and at each time step t ask the model to predict the next word. We call such a model self-supervised because we don`t have to add any special gold labels to the data; the natural sequence of words is its own supervision! We simply train the model to minimize the error in predicting the true next word in the training sequence, using cross-entropy as the loss function. ", "Bloom_type": "comprehension", "question": "What does the loss function do in the context of training a transformer for a language model?", "options": ["It minimizes the error in predicting the true next word in the training sequence.", "It adds special gold labels to the data.", "It trains the model to maximize the accuracy of predictions.", "It doesn't play a role in the training process."], "complexity": 1}, {"id": 54, "context": "As a way of getting a model to do what we want, prompting is fundamentally different than pretraining. Learning via pretraining means updating the model`s parameters by using gradient descent according to some loss function. But prompting with demonstrations can teach a model to do a new task. The model is learning something as it processes the prompt. ", "Bloom_type": "comprehension", "question": "What distinguishes learning through pretraining from prompting with demonstrations?", "options": ["The primary difference lies in how the model updates its parameters.", "Pretraining involves updating model parameters using gradient descent based on a loss function.", "Prompting with demonstrations allows the model to learn directly without any loss function.", "Learning through pretraining requires more data than prompting with demonstrations."], "complexity": 1}, {"id": 55, "context": "In the task-based finetuning of Chapter 11, we adapt to a particular task by adding a new specialized classification head and updating its features via its own loss function (e.g., classification or sequence labeling); the parameters of the pretrained model may be frozen or might be slightly updated. ", "Bloom_type": "comprehension", "question": "In the context of adapting to a specific task during finetuning, which component is responsible for calculating the error between predicted outputs and actual labels?", "options": ["The loss function calculates the error between predictions and targets.", "The loss function updates the parameters of the pretrained model.", "The loss function freezes all parameters of the pretrained model.", "The loss function does not play any role in this process."], "complexity": 1}, {"id": 56, "context": "While humans produce the best evaluations of machine translation output, running a human evaluation can be time consuming and expensive. For this reason automatic metrics are often used as temporary proxies. Automatic metrics are less accurate than human evaluation, but can help test potential system improvements, and even be used as an automatic loss function for training. In this section we introduce two families of such metrics, those based on character- or word-overlap and those based on embedding similarity. ", "Bloom_type": "comprehension", "question": "What role do automatic metrics play in the context of machine translation?", "options": ["They act as temporary solutions until human evaluations become more efficient.", "They serve as permanent replacements for human evaluations.", "They are only used for testing potential system improvements.", "They replace all other methods entirely."], "complexity": 1}, {"id": 57, "context": "In this section we briefly introduce an alternative to encoder-decoder: an algorithm and loss function called CTC, short for Connectionist Temporal Classification (Graves et al., 2006), that deals with these problems in a very different way. The intuition of CTC is to output a single character for every frame of the input, so that ", "Bloom_type": "comprehension", "question": "What does the loss function CTC aim to achieve?", "options": ["To minimize the difference between predicted and actual characters", "To maximize the similarity between frames", "To reduce computational complexity", "To improve the accuracy of character prediction"], "complexity": 1}, {"id": 58, "context": "To train a CTC-based ASR system, we use negative log-likelihood loss with a special CTC loss function. Thus the loss for an entire dataset D is the sum of the negative log-likelihoods of the correct output Y for each input X: ", "Bloom_type": "comprehension", "question": "What does the loss function represent in training a CTC-based ASR system?", "options": ["The total error rate across all inputs and their corresponding outputs", "The accuracy of the model's predictions", "The number of incorrect outputs in the dataset", "The average length of the outputs compared to the inputs"], "complexity": 1}, {"id": 59, "context": "It`s also possible to combine the two architectures/loss functions we`ve described, the cross-entropy loss from the encoder-decoder architecture, and the CTC loss. Fig. 16.13 shows a sketch. For training, we can simply weight the two losses with a  tuned on a devset: ", "Bloom_type": "comprehension", "question": "What are the two types of loss functions mentioned in the context?", "options": ["Binary cross-entropy and cross-entropy loss", "Cross-entropy loss and mean squared error", "Mean squared error and CTC loss", "CTC loss and binary cross-entropy"], "complexity": 1}, {"id": 60, "context": " Two common paradigms for speech recognition are the encoder-decoder with attention model, and models based on the CTC loss function. Attentionbased models have higher accuracies, but models based on CTC more easily adapt to streaming: outputting graphemes online instead of waiting until the acoustic input is complete. ", "Bloom_type": "comprehension", "question": "Which paradigm typically allows for real-time processing while maintaining high accuracy?", "options": ["CTC Loss Function-based Models", "Encoder-Decoder with Attention Model", "Both Paradigms Have Equal Real-Time Processing Capabilities", "Neither Paradigm Allows Real-Time Processing"], "complexity": 1}, {"id": 61, "context": "Meanwhile early work had proposed the CTC loss function by 2006 (Graves et al., 2006), and by 2012 the RNN-Transducer was defined and applied to phone recognition (Graves 2012, Graves et al. 2013), and then to end-to-end speech recognition rescoring (Graves and Jaitly, 2014), and then recognition (Maas et al., 2015), (Our dewith advances such as specialized beam search (Hannun et al., 2014). scription of CTC in the chapter draws on Hannun (2017), which we encourage the interested reader to follow). ", "Bloom_type": "comprehension", "question": "Explain how the use of the CTC loss function evolved from 2006 to 2014?", "options": ["The CTC loss function was first introduced in 2006, then improved upon in 2012 for phone recognition, and further refined in 2014 for speech recognition.", "The CTC loss function was first introduced in 2006, followed by its development into an RNN-Transducer model in 2012.", "In 2006, the CTC loss function was developed into an RNN-Transducer model, while it was only introduced in 2012 for phone recognition.", "CTC loss function was first introduced in 2006, but did not evolve significantly until 2012 when it was applied to end-to-end speech recognition rescoring."], "complexity": 1}, {"id": 62, "context": "Training is trickier in the mention-ranking model than the mention-pair model, because for each anaphor we don`t know which of all the possible gold antecedents Instead, the best antecedent for each mention is latent; that to use for training. is, for each mention we have a whole cluster of legal gold antecedents to choose from. Early work used heuristics to choose an antecedent, for example choosing the closest antecedent as the gold antecedent and all non-antecedents in a window of two sentences as the negative examples (Denis and Baldridge, 2008). Various kinds of ways to model latent antecedents exist (Fernandes et al. 2012, Chang et al. 2013, Durrett and Klein 2013). The simplest way is to give credit to any legal antecedent by summing over all of them, with a loss function that optimizes the likelihood of all correct antecedents from the gold clustering (Lee et al., 2017b). We`ll see the details in Section 23.6. ", "Bloom_type": "comprehension", "question": "What type of loss function was initially used in early work when selecting antecedents?", "options": ["Choosing the closest antecedent", "Summing over all legal antecedents", "Using only non-antecedents in a window of two sentences", "Optimizing the likelihood of incorrect antecedents"], "complexity": 1}, {"id": 63, "context": "3. An objective function that we want to optimize for learning, usually involving minimizing a loss function corresponding to error on training examples. We will introduce the cross-entropy loss function. ", "Bloom_type": "application", "question": "What is the primary purpose of using a loss function in machine learning?", "options": ["To minimize the difference between predicted values and actual outcomes", "To increase the accuracy of the model", "To maximize the complexity of the model", "To ensure all features are equally important"], "complexity": 2}, {"id": 64, "context": "label y. Rather than measure similarity, we usually talk about the opposite of this: the distance between the system output and the gold output, and we call this distance the loss function or the cost function. In the next section we`ll introduce the loss function that is commonly used for logistic regression and also for neural networks, the cross-entropy loss. ", "Bloom_type": "application", "question": "What is the opposite of measuring similarity when comparing a system's output with the gold standard?", "options": ["Defining the loss function", "Measuring similarity directly", "Calculating the cosine similarity", "Determining the Euclidean distance"], "complexity": 2}, {"id": 65, "context": "The second thing we need is an optimization algorithm for iteratively updating the weights so as to minimize this loss function. The standard algorithm for this is gradient descent; we`ll introduce the stochastic gradient descent algorithm in the following section. ", "Bloom_type": "application", "question": "Which of the following best describes how we update our model parameters using the gradient descent method?", "options": ["We compute the partial derivative of the loss function with respect to each parameter.", "We calculate the average error over all training examples.", "We randomly select a subset of data points for each iteration.", "We sum up all errors across all iterations."], "complexity": 2}, {"id": 66, "context": "Eq. 5.22 describes a log likelihood that should be maximized. In order to turn this into a loss function (something that we need to minimize), we`ll just flip the sign on Eq. 5.22. The result is the cross-entropy loss LCE: ", "Bloom_type": "application", "question": "What is the next step after flipping the sign of Eq. 5.22?", "options": ["Calculate the derivative of LCE with respect to the parameters", "Compute the gradient of LCE using backpropagation", "Implement an optimizer to update the model parameters", "Train the model using stochastic gradient descent"], "complexity": 2}, {"id": 67, "context": "Why does minimizing this negative log probability do what we want? A perfect classifier would assign probability 1 to the correct outcome (y = 1 or y = 0) and probability 0 to the incorrect outcome. That means if y equals 1, the higher y is (the closer it is to 1), the better the classifier; the lower y is (the closer it is to 0), the y is (closer to 1), the better worse the classifier. If y equals 0, instead, the higher 1 the classifier. The negative log of y (if the true y equals 1) or 1 y (if the true y equals 0) is a convenient loss metric since it goes from 0 (negative log of 1, no loss) to infinity (negative log of 0, infinite loss). This loss function also ensures that as the probability of the correct answer is maximized, the probability of the incorrect answer is minimized; since the two sum to one, any increase in the probability of the correct answer is coming at the expense of the incorrect answer. It`s called the crossentropy loss, because Eq. 5.22 is also the formula for the cross-entropy between the true probability distribution y and our estimated distribution y. ", "Bloom_type": "application", "question": "What is the purpose of using the negative log probability as a loss function?", "options": ["To minimize the error rate of the model", "To maximize the accuracy of the model", "To ensure the probabilities add up to one", "To calculate the distance between the actual and predicted distributions"], "complexity": 2}, {"id": 68, "context": "(cid:88)i=1 How shall we find the minimum of this (or any) loss function? Gradient descent is a method that finds a minimum of a function by figuring out in which direction (in the space of the parameters  ) the function`s slope is rising the most steeply, and moving in the opposite direction. The intuition is that if you are hiking in a canyon and trying to descend most quickly down to the river at the bottom, you might look around yourself in all directions, find the direction where the ground is sloping the steepest, and walk downhill in that direction. ", "Bloom_type": "application", "question": "What is the first step in applying gradient descent to minimize a loss function?", "options": ["Calculate the derivative of the loss function with respect to each parameter.", "Start with an initial guess for the parameter values.", "Choose a learning rate and initialize it randomly.", "Compute the gradient of the loss function using numerical differentiation."], "complexity": 2}, {"id": 69, "context": "For logistic regression, this loss function is conveniently convex. A convex function has at most one minimum; there are no local minima to get stuck in, so gradient descent starting from any point is guaranteed to find the minimum. (By contrast, the loss for multi-layer neural networks is non-convex, and gradient descent may get stuck in local minima for neural network training and never find the global optimum.) ", "Bloom_type": "application", "question": "Which of the following statements about loss functions is true?", "options": ["Non-convex loss functions prevent gradient descent from finding the global minimum.", "The loss function for logistic regression can only have one minimum.", "Gradient descent will always find the global minimum for both logistic regression and neural networks.", "Multi-layer neural networks have more than one minimum."], "complexity": 2}, {"id": 70, "context": "Given a random initialization of w at some value w1, and assuming the loss function L happened to have the shape in Fig. 5.4, we need the algorithm to tell us whether at the next iteration we should move left (making w2 smaller than w1) or right (making w2 bigger than w1) to reach the minimum. ", "Bloom_type": "application", "question": "What is the primary goal when determining the direction to update weights \\( w \\) based on the loss function \\( L \\)?", "options": ["To minimize the loss function \\( L \\) as much as possible.", "To maximize the likelihood of finding the global minimum faster.", "To ensure the gradient descent moves towards the maximum of \\( L \\).", "To balance between maximizing likelihood and minimizing \\( L \\)."], "complexity": 2}, {"id": 71, "context": "The gradient descent algorithm answers this question by finding the gradient of the loss function at the current point and moving in the opposite direction. The gradient of a function of many variables is a vector pointing in the direction of the greatest increase in a function. The gradient is a multi-variable generalization of the ", "Bloom_type": "application", "question": "What does the gradient descent algorithm primarily aim to find when optimizing a model?", "options": ["The minimum value of the loss function", "The maximum value of the loss function", "The average value of the loss function", "The variance of the loss function"], "complexity": 2}, {"id": 72, "context": "slope, so for a function of one variable like the one in Fig. 5.4, we can informally think of the gradient as the slope. The dotted line in Fig. 5.4 shows the slope of this hypothetical loss function at point w = w1. You can see that the slope of this dotted line is negative. Thus to find the minimum, gradient descent tells us to go in the opposite direction: moving w in a positive direction. ", "Bloom_type": "application", "question": "What does the negative slope indicate about the path towards finding the minimum?", "options": ["The path will converge towards the minimum.", "The path will increase indefinitely.", "The path will decrease indefinitely.", "The path will oscillate between increasing and decreasing."], "complexity": 2}, {"id": 73, "context": "In an actual logistic regression, the parameter vector w is much longer than 1 or 2, since the input feature vector x can be quite long, and we need a weight wi for each xi. For each dimension/variable wi in w (plus the bias b), the gradient will have a component that tells us the slope with respect to that variable. In each dimension wi, we express the slope as a partial derivative  of the loss function. Essentially  wi we`re asking: How much would a small change in that variable wi influence the total loss function L? ", "Bloom_type": "application", "question": "What does the partial derivative of the loss function represent?", "options": ["The rate at which the loss changes if only the variable wi is changed while others remain constant.", "The rate at which the loss changes if all variables are increased equally.", "The sum of all partial derivatives across all dimensions.", "The average value of the loss over all data points."], "complexity": 2}, {"id": 74, "context": "In order to update  , we need a definition for the gradient L( f (x;  ), y). Recall that for logistic regression, the cross-entropy loss function is: ", "Bloom_type": "application", "question": "What should we define as the gradient of the cross-entropy loss function with respect to the parameters \\( \theta \\) in logistic regression?", "options": ["The derivative of the sigmoid function with respect to \\( x \\)", "The product of the negative log-likelihood and the input data \\( X \\)", "The sum of all elements in the output vector \\( y \\)", "The difference between the predicted probability and the actual label"], "complexity": 2}, {"id": 75, "context": "Stochastic gradient descent is an online algorithm that minimizes the loss function by computing its gradient after each training example, and nudging  in the right direction (the opposite direction of the gradient). (An online algorithm is one that processes its input example by example, rather than waiting until it sees the entire input.) Stochastic gradient descent is called stochastic because it chooses a single random example at a time; in Section 5.6.4 we`ll discuss other versions of gradient descent that batch many examples at once. Fig. 5.6 shows the algorithm. ", "Bloom_type": "application", "question": "What does the loss function represent in the context of stochastic gradient descent?", "options": ["The difference between predicted values and actual values for each training example.", "The total sum of all errors made during training.", "The average error over all training examples.", "The rate at which the model's performance improves with each iteration."], "complexity": 2}, {"id": 76, "context": "loss function, such as the cross-entropy loss, that must be minimized. ", "Bloom_type": "application", "question": "What is the primary goal of minimizing a loss function?", "options": ["To reduce error", "To increase accuracy", "To maximize performance", "To minimize cost"], "complexity": 2}, {"id": 77, "context": " Minimizing this loss function is a convex optimization problem, and iterative ", "Bloom_type": "application", "question": "What should be done iteratively to minimize the loss function?", "options": ["Use gradient descent method", "Calculate the derivative of the loss function at each iteration", "Find the global minimum of the loss function", "Implement backpropagation algorithm"], "complexity": 2}, {"id": 78, "context": "We minimize this loss function using stochastic gradient descent. Fig. 6.14 ", "Bloom_type": "application", "question": "What is the primary goal of minimizing the loss function?", "options": ["To reduce the error between predicted values and actual values", "To maximize the accuracy of the model", "To find the optimal parameters for the model", "To increase the complexity of the model"], "complexity": 2}, {"id": 79, "context": "First, we`ll need a loss function that models the distance between the system output and the gold output, and it`s common to use the loss function used for logistic regression, the cross-entropy loss. ", "Bloom_type": "application", "question": "What is the first step in defining a loss function?", "options": ["Choose a suitable loss function based on the problem type", "Select an appropriate metric for measuring error", "Define the model architecture", "Compute the actual outputs of the model"], "complexity": 2}, {"id": 80, "context": "Third, gradient descent requires knowing the gradient of the loss function, the vector that contains the partial derivative of the loss function with respect to each of the parameters. In logistic regression, for each observation we could directly compute the derivative of the loss function with respect to an individual w or b. But for neural networks, with millions of parameters in many layers, it`s much harder to see how to compute the partial derivative of some weight in layer 1 when the loss is attached to some much later layer. How do we partial out the loss over all those intermediate layers? The answer is the algorithm called error backpropagation or backward differentiation. ", "Bloom_type": "application", "question": "What technique does the algorithm called error backpropagation use to calculate the gradients of the loss function?", "options": ["Backward differentiation", "Direct computation using derivatives", "Forward propagation only", "Gradient ascent"], "complexity": 2}, {"id": 81, "context": "For the backward pass we`ll also need to compute the loss L. The loss function ", "Bloom_type": "application", "question": "What should we calculate first when performing the backward pass?", "options": ["The gradient of the loss function with respect to the input data", "The derivative of the loss function with respect to the weights", "The sum of all elements in the loss function", "The average value of the loss function"], "complexity": 2}, {"id": 82, "context": "is used to compute the gradients of the loss function for a network. ", "Bloom_type": "application", "question": "What should be done after computing the gradients using the loss function?", "options": ["Use the computed gradients to update the model parameters", "Store the gradients directly", "Compute the gradients again with different weights", "None of the above"], "complexity": 2}, {"id": 83, "context": "To train an RNN as a language model, we use the same self-supervision (or selftraining) algorithm we saw in Section 7.7: we take a corpus of text as training material and at each time step t ask the model to predict the next word. We call such a model self-supervised because we don`t have to add any special gold labels to the data; the natural sequence of words is its own supervision! We simply train the model to minimize the error in predicting the true next word in the training sequence, using cross-entropy as the loss function. Recall that the cross-entropy loss measures the difference between a predicted probability distribution and the ", "Bloom_type": "application", "question": "What is used to measure the difference between a predicted probability distribution and the actual distribution?", "options": ["Cross-Entropy Loss", "Mean Squared Error", "Root Mean Square Error", "Mean Absolute Error"], "complexity": 2}, {"id": 84, "context": "Note that in this approach we don`t need intermediate outputs for the words in the sequence preceding the last element. Therefore, there are no loss terms associated with those elements. Instead, the loss function used to train the weights in the network is based entirely on the final text classification task. The output from the softmax output from the feedforward classifier together with a cross-entropy loss drives the training. The error signal from the classification is backpropagated all the way through the weights in the feedforward classifier through, to its input, and then through to the three sets of weights in the RNN as described earlier in Section 8.1.2. The training regimen that uses the loss from a downstream application to adjust the weights all the way through the network is referred to as end-to-end training. ", "Bloom_type": "application", "question": "What type of training does not require intermediate outputs?", "options": ["End-to-end training", "Backpropagation", "Gradient descent", "Forward propagation"], "complexity": 2}, {"id": 85, "context": "To train a transformer as a language model, we use the same self-supervision (or self-training) algorithm we saw in Section 8.2.2: we take a corpus of text as training material and at each time step t ask the model to predict the next word. We call such a model self-supervised because we don`t have to add any special gold labels to the data; the natural sequence of words is its own supervision! We simply train the model to minimize the error in predicting the true next word in the training sequence, using cross-entropy as the loss function. ", "Bloom_type": "application", "question": "What is used to measure how well the model predicts the next word in the sequence?", "options": ["Cross-Entropy Loss Function", "Mean Squared Error", "Root Mean Square Error", "Binary Cross Entropy"], "complexity": 2}, {"id": 86, "context": "As a way of getting a model to do what we want, prompting is fundamentally different than pretraining. Learning via pretraining means updating the model`s parameters by using gradient descent according to some loss function. But prompting with demonstrations can teach a model to do a new task. The model is learning something as it processes the prompt. ", "Bloom_type": "application", "question": "Which method should be used to update the model's parameters when teaching a model through prompts?", "options": ["Use gradient descent based on a loss function", "Update parameters directly without any function", "Implement an entirely new algorithm for parameter updates", "None of the above"], "complexity": 2}, {"id": 87, "context": "In the task-based finetuning of Chapter 11, we adapt to a particular task by adding a new specialized classification head and updating its features via its own loss function (e.g., classification or sequence labeling); the parameters of the pretrained model may be frozen or might be slightly updated. ", "Bloom_type": "application", "question": "What is the role of the loss function during the adaptation phase?", "options": ["To train the entire model end-to-end", "To fine-tune the existing heads only", "To update all parameters simultaneously", "To evaluate the performance on a validation set"], "complexity": 2}, {"id": 88, "context": "While humans produce the best evaluations of machine translation output, running a human evaluation can be time consuming and expensive. For this reason automatic metrics are often used as temporary proxies. Automatic metrics are less accurate than human evaluation, but can help test potential system improvements, and even be used as an automatic loss function for training. In this section we introduce two families of such metrics, those based on character- or word-overlap and those based on embedding similarity. ", "Bloom_type": "application", "question": "Which type of metric is commonly used as a proxy for human evaluation in machine translation?", "options": ["Both A) and B)", "Character-overlap metrics", "Word-overlap metrics", "Embedding-similarity metrics"], "complexity": 2}, {"id": 89, "context": "In this section we briefly introduce an alternative to encoder-decoder: an algorithm and loss function called CTC, short for Connectionist Temporal Classification (Graves et al., 2006), that deals with these problems in a very different way. The intuition of CTC is to output a single character for every frame of the input, so that ", "Bloom_type": "application", "question": "What does the CTC algorithm aim to achieve?", "options": ["To classify characters across frames", "To predict the next word in a sentence", "To reduce computational complexity", "To increase accuracy through parallel processing"], "complexity": 2}, {"id": 90, "context": "To train a CTC-based ASR system, we use negative log-likelihood loss with a special CTC loss function. Thus the loss for an entire dataset D is the sum of the negative log-likelihoods of the correct output Y for each input X: ", "Bloom_type": "application", "question": "What is the formula for calculating the total loss when training a CTC-based ASR system?", "options": ["Total Loss = \u2211(Y * -log(L))", "Total Loss = \u2211(-log(Y) + L)", "Total Loss = \u2211(-log(X) + Y)", "Total Loss = \u2211(X * -log(Y))"], "complexity": 2}, {"id": 91, "context": "It`s also possible to combine the two architectures/loss functions we`ve described, the cross-entropy loss from the encoder-decoder architecture, and the CTC loss. Fig. 16.13 shows a sketch. For training, we can simply weight the two losses with a  tuned on a devset: ", "Bloom_type": "application", "question": "Which of the following is an example of combining two different types of loss functions?", "options": ["Combining the cross-entropy loss with the CTC loss", "Using only the cross-entropy loss for both models", "Applying the CTC loss as the primary loss function", "Selecting a random combination of the two loss functions"], "complexity": 2}, {"id": 92, "context": " Two common paradigms for speech recognition are the encoder-decoder with attention model, and models based on the CTC loss function. Attentionbased models have higher accuracies, but models based on CTC more easily adapt to streaming: outputting graphemes online instead of waiting until the acoustic input is complete. ", "Bloom_type": "application", "question": "Which paradigm allows for real-time processing of audio data during speech recognition?", "options": ["CTC Loss Function", "Encoder-Decoder with Attention Model", "Both Paradigms Have Equal Real-Time Capabilities", "None of the Above"], "complexity": 2}, {"id": 93, "context": "Meanwhile early work had proposed the CTC loss function by 2006 (Graves et al., 2006), and by 2012 the RNN-Transducer was defined and applied to phone recognition (Graves 2012, Graves et al. 2013), and then to end-to-end speech recognition rescoring (Graves and Jaitly, 2014), and then recognition (Maas et al., 2015), (Our dewith advances such as specialized beam search (Hannun et al., 2014). scription of CTC in the chapter draws on Hannun (2017), which we encourage the interested reader to follow). ", "Bloom_type": "application", "question": "What is the first step in applying the CTC loss function for speech recognition?", "options": ["Develop the CTC loss function", "Define the RNN-Transducer model", "Implement the specialized beam search algorithm", "Train the model using the CTC loss"], "complexity": 2}, {"id": 94, "context": "Training is trickier in the mention-ranking model than the mention-pair model, because for each anaphor we don`t know which of all the possible gold antecedents Instead, the best antecedent for each mention is latent; that to use for training. is, for each mention we have a whole cluster of legal gold antecedents to choose from. Early work used heuristics to choose an antecedent, for example choosing the closest antecedent as the gold antecedent and all non-antecedents in a window of two sentences as the negative examples (Denis and Baldridge, 2008). Various kinds of ways to model latent antecedents exist (Fernandes et al. 2012, Chang et al. 2013, Durrett and Klein 2013). The simplest way is to give credit to any legal antecedent by summing over all of them, with a loss function that optimizes the likelihood of all correct antecedents from the gold clustering (Lee et al., 2017b). We`ll see the details in Section 23.6. ", "Bloom_type": "application", "question": "What kind of loss function was initially used in early work for training mention-ranking models?", "options": ["Choosing the closest antecedent and using all non-antecedents in a window as negatives.", "Summing over all legal antecedents with a loss function optimizing their likelihoods.", "Using heuristics such as nearest neighbor selection.", "Calculating losses based on the number of incorrect antecedents."], "complexity": 2}]}, "coherence": {"max_id": 74, "Questions": [{"id": 0, "context": "The focus of mask-based learning is on predicting words from surrounding contexts with the goal of producing effective word-level representations. However, an important class of applications involves determining the relationship between pairs of sentences. These include tasks like paraphrase detection (detecting if two sentences have similar meanings), entailment (detecting if the meanings of two sentences entail or contradict each other) or discourse coherence (deciding if two neighboring sentences form a coherent discourse). ", "Bloom_type": "remember", "question": "In the field of machine learning, what aspect focuses on understanding how words are represented based on their surroundings?", "options": ["Coherence", "Entailment", "Paraphrase Detection", "Discourse Coherence"], "complexity": 0}, {"id": 1, "context": "As mentioned in Section 11.2.2, an important type of problem involves the classification of pairs of input sequences. Practical applications that fall into this class include paraphrase detection (are the two sentences paraphrases of each other?), logical entailment (does sentence A logically entail sentence B?), and discourse coherence (how coherent is sentence B as a follow-on to sentence A?). ", "Bloom_type": "remember", "question": "In practical applications involving sequence classification, which of the following scenarios does not directly relate to coherence?", "options": ["Word sense disambiguation", "Paraphrase detection", "Logical entailment", "Sentiment analysis"], "complexity": 0}, {"id": 2, "context": "Another influential line of research from that decade focused on modeling the hierarchical structure of dialogue. Grosz`s pioneering 1977b dissertation first showed that task-oriented dialogues have a structure that closely parallels the structure of the task being performed (p. 27), leading to her work with Sidner and others showing how to use similar notions of intention and plans to model discourse structure and coherence in dialogue. See, e.g., Lochbaum et al. (2000) for a summary of the role of intentional structure in dialogue. ", "Bloom_type": "remember", "question": "What did Grosz show about task-oriented dialogues in 1977b?", "options": ["She showed that task-oriented dialogues closely resemble the structure of the tasks they perform.", "She demonstrated that task-oriented dialogues are unrelated to their tasks.", "She found that task-oriented dialogues do not follow any specific structure.", "She discovered that task-oriented dialogues have no coherent structure."], "complexity": 0}, {"id": 3, "context": "But coherent structure is not just a fact about movies or works of art. Like movies, language does not normally consist of isolated, unrelated sentences, but instead of collocated, structured, coherent groups of sentences. We refer to such a coherent structured group of sentences as a discourse, and we use the word coherence to refer to the relationship between sentences that makes real discourses different than just random assemblages of sentences. The chapter you are now reading is an example of a discourse, as is a news article, a conversation, a thread on social media, a Wikipedia page, and your favorite novel. ", "Bloom_type": "remember", "question": "What does the author mean by saying that language does not normally consist of isolated, unrelated sentences?", "options": ["The author means that language consists only of isolated, unrelated sentences.", "The author means that language consists of many isolated, unrelated sentences.", "The author means that language consists of few isolated, unrelated sentences.", "The author means that language consists of all sentences."], "complexity": 0}, {"id": 4, "context": "Entity-based coherence models measure this kind of coherence by tracking salient entities across a discourse. For example Centering Theory (Grosz et al., 1995), the most influential theory of entity-based coherence, keeps track of which entities in the discourse model are salient at any point (salient entities are more likely to be pronominalized or to appear in prominent syntactic positions like subject or object). In Centering Theory, transitions between sentences that maintain the same salient entity are considered more coherent than ones that repeatedly shift between entities. The entity grid model of coherence (Barzilay and Lapata, 2008) is a commonly used model that realizes some of the intuitions of the Centering Theory framework. Entity-based coherence is introduced in Section 24.3. ", "Bloom_type": "remember", "question": "In the context of entity-based coherence models, what does the salience of an entity refer to?", "options": ["The prominence of an entity in terms of its position and use", "The frequency with which an entity appears in the discourse", "The grammatical role of an entity within a sentence", "The logical relationship between two entities"], "complexity": 0}, {"id": 5, "context": "In addition to the local coherence between adjacent or nearby sentences, discourses also exhibit global coherence. Many genres of text are associated with particular conventional discourse structures. Academic articles might have sections describing the Methodology or Results. Stories might follow conventional plotlines or motifs. Persuasive essays have a particular claim they are trying to argue for, and an essay might express this claim together with a structured set of premises that support the argument and demolish potential counterarguments. We`ll introduce versions of each of these kinds of global coherence. ", "Bloom_type": "remember", "question": "In academic writing, what is referred to as the coherent structure of ideas presented throughout the document?", "options": ["Genre Conventions", "Sentence Structure", "Paragraph Flow", "Topic Sentences"], "complexity": 0}, {"id": 6, "context": "Why do we care about the local or global coherence of a discourse? Since coherence is a property of a well-written text, coherence detection plays a part in any ", "Bloom_type": "remember", "question": "Why does it matter if a text has good coherence?", "options": ["To improve the overall structure and flow of the text", "To ensure the reader can understand the main idea", "To make sure the sentences are grammatically correct", "To increase the word count"], "complexity": 0}, {"id": 7, "context": "task that requires measuring the quality of a text. For example coherence can help in pedagogical tasks like essay grading or essay quality measurement that are trying to grade how well-written a human essay is (Somasundaran et al. 2014, Feng et al. 2014, Lai and Tetreault 2018). Coherence can also help for summarization; knowing the coherence relationship between sentences can help know how to select information from them. Finally, detecting incoherent text may even play a role in mental health tasks like measuring symptoms of schizophrenia or other kinds of disordered language (Ditman and Kuperberg 2010, Elvevag et al. 2007, Bedi et al. 2015, Iter et al. 2018). ", "Bloom_type": "remember", "question": "In what way does coherence contribute to the quality assessment of written texts?", "options": ["It aids in identifying logical connections within paragraphs.", "It helps measure the emotional tone of the text.", "It assists in determining the length of each sentence.", "It enables detection of grammatical errors."], "complexity": 0}, {"id": 8, "context": "Now that we`ve seen examples of coherence, we can see more clearly how a coherence relation can play a role in summarization or information extraction. For example, the nuclei of a text presumably express more important information than the satellites, which might be dropped in a summary. ", "Bloom_type": "remember", "question": "In the context provided, what is an example of coherence?", "options": ["The relationship between different parts of a text", "The importance of each sentence in a paragraph", "The order in which ideas are presented in a summary", "The use of synonyms to improve clarity"], "complexity": 0}, {"id": 9, "context": "Given a sequence of sentences, how can we automatically determine the coherence relations between them? This task is often called discourse parsing (even though for PDTB we are only assigning labels to leaf spans and not building a full parse ", "Bloom_type": "remember", "question": "In discourse parsing, what label is assigned to indicate the coherence relation between two consecutive sentences?", "options": ["Coherent", "Discourse", "Leaf", "Span"], "complexity": 0}, {"id": 10, "context": "Centering Theory (Grosz et al., 1995) is a theory of both discourse salience and discourse coherence. As a model of discourse salience, Centering proposes that at any given point in the discourse one of the entities in the discourse model is salient: it is being centered on. As a model of discourse coherence, Centering proposes that discourses in which adjacent sentences CONTINUE to maintain the same salient entity are more coherent than those which SHIFT back and forth between multiple entities (we will see that CONTINUE and SHIFT are technical terms in the theory). ", "Bloom_type": "remember", "question": "According to Centering Theory, what determines the coherence of a discourse?", "options": ["The continuity of entities maintaining their salience throughout the discourse", "The number of entities mentioned in the discourse", "The frequency with which entities shift between different states", "The complexity of the discourse structure"], "complexity": 0}, {"id": 11, "context": "The entity grid model of Barzilay and Lapata (2008) is an alternative way to capture entity-based coherence: instead of having a top-down theory, the entity-grid model using machine learning to induce the patterns of entity mentioning that make a discourse more coherent. ", "Bloom_type": "remember", "question": "In the entity grid model of Barzilay and Lapata (2008), what does the use of machine learning primarily enable?", "options": ["To improve the overall coherence of the discourse", "To create a bottom-up theory", "To enhance the accuracy of entity mention detection", "To reduce the computational complexity of the model"], "complexity": 0}, {"id": 12, "context": "Alternatively, since it`s very expensive to get human labels, and we might not yet have an end-task in mind, we can use natural texts to do self-supervision. In self-supervision we pair up a natural discourse with a pseudo-document created by changing the ordering. Since naturally-ordered discourses are more coherent than random permutation (Lin et al., 2011), a successful coherence algorithm should prefer the original ordering. ", "Bloom_type": "remember", "question": "Which of the following is true about natural discourse?", "options": ["It prefers the original ordering over random permutations.", "It has no coherence.", "It is always coherent.", "It is less coherent than random permutations."], "complexity": 0}, {"id": 13, "context": "The third kind of local coherence is topical or semantic field coherence. Discourses cohere by talking about the same topics and subtopics, and drawing on the same semantic fields in doing so. ", "Bloom_type": "remember", "question": "What does discourse cohesion refer to?", "options": ["The connection between different parts of a text", "The way sentences are connected", "The consistency of ideas within a paragraph", "The agreement between speakers"], "complexity": 0}, {"id": 14, "context": " Modern neural representation-learning coherence models, beginning with Li et al. (2014), draw on the intuitions of these early unsupervised models for learning sentence representations and measuring how they change between neighboring sentences. But the new models also draw on the idea pioneered by Barzilay and Lapata (2005) of self-supervision. That is, unlike say coherence relation models, which train on hand-labeled representations for RST or PDTB, these models are trained to distinguish natural discourses from unnatural discourses formed by scrambling the order of sentences, thus using representation learning to discover the features that matter for at least the ordering aspect of coherence. ", "Bloom_type": "remember", "question": "What does modern neural representation-learning coherence models primarily use to learn sentence representations?", "options": ["Representation learning through self-supervision", "Hand-labeled representations for RST or PDTB", "Unsupervised models for learning sentence representations", "Coherence relations for distinguishing natural discourses"], "complexity": 0}, {"id": 15, "context": "Here we present one such model, the local coherence discriminator (LCD) (Xu et al., 2019). Like early models, LCD computes the coherence of a text as the average of coherence scores between consecutive pairs of sentences. But unlike the early unsupervised models, LCD is a self-supervised model trained to discriminate consecutive sentence pairs (si, si+1) in the training documents (assumed to be coherent) from (constructed) incoherent pairs (si, s(cid:48)). All consecutive pairs are positive examples, and the negative (incoherent) partner for a sentence si is another sentence uniformly sampled from the same document as si. ", "Bloom_type": "remember", "question": "What does the LCD model do differently compared to early unsupervised models?", "options": ["It trains on both coherent and incoherent pairs.", "It uses supervised learning instead of unsupervised learning.", "It calculates coherence based on all pairs of sentences.", "It only considers the coherence score between consecutive pairs."], "complexity": 0}, {"id": 16, "context": "Fig. 24.11 describes the architecture of the model f , which takes a sentence pair and returns a score, higher scores for more coherent pairs. Given an input sentence pair s and t, the model computes sentence embeddings s and t (using any sentence embeddings algorithm), and then concatenates four features of the pair: (1) t; (3) the absolute value the concatenation of the two vectors (2) their difference s t. These are passed of their difference through a one-layer feedforward network to output the coherence score. ", "Bloom_type": "remember", "question": "In the described model, what is used as input to compute sentence embeddings?", "options": ["Sentence embeddings from another source", "The absolute value of the concatenation of the two vectors", "The difference between the two vectors", "None of the above"], "complexity": 0}, {"id": 17, "context": "Xu et al. (2019) also give a useful baseline algorithm that itself has quite high performance in measuring perplexity: train an RNN language model on the data, and compute the log likelihood of sentence si in two ways, once given the preceding context (conditional log likelihood) and once with no context (marginal log likelihood). The difference between these values tells us how much the preceding context improved the predictability of si, a predictability measure of coherence. ", "Bloom_type": "remember", "question": "In Xu et al. (2019), what method is used to evaluate the coherence of sentences?", "options": ["Computing conditional and marginal log likelihoods", "Using perplexity", "Analyzing word frequency distribution", "Measuring syntactic complexity"], "complexity": 0}, {"id": 18, "context": "Training models to predict longer contexts than just consecutive pairs of sentences can result in even stronger discourse representations. For example a Transformer language model trained with a contrastive sentence objective to predict text up to a distance of 2 sentences improves performance on various discourse coherence tasks (Iter et al., 2020). ", "Bloom_type": "remember", "question": "What does training models to predict longer contexts improve?", "options": ["Discourse representation strength", "Model accuracy", "Sentence length prediction", "Word frequency analysis"], "complexity": 0}, {"id": 19, "context": "In this chapter we introduced local and global models for discourse coherence. ", "Bloom_type": "remember", "question": "What did the author introduce regarding discourse coherence?", "options": ["They discussed both local and global models.", "The author focused on only local models.", "Global models were not mentioned at all.", "The author ignored both types of models."], "complexity": 0}, {"id": 20, "context": " Discourses are not arbitrary collections of sentences; they must be coherent. Among the factors that make a discourse coherent are coherence relations between the sentences, entity-based coherence, and topical coherence. ", "Bloom_type": "remember", "question": "What is essential for making a discourse coherent?", "options": ["All of the above", "Entity-based coherence", "Topical coherence", "Coherence relations between the sentences"], "complexity": 0}, {"id": 21, "context": " Entity-based coherence captures the intuition that discourses are about an entity, and continue mentioning the entity from sentence to sentence. Centering Theory is a family of models describing how salience is modeled for discourse entities, and hence how coherence is achieved by virtue of keeping the same discourse entities salient over the discourse. The entity grid model gives a more bottom-up way to compute which entity realization transitions lead to coherence. ", "Bloom_type": "remember", "question": "Which method focuses on computing which entity realizations transition leads to coherence?", "options": ["Entity Grid Model", "Centering Theory", "Coherence Intuition", "Salience Modeling"], "complexity": 0}, {"id": 22, "context": " Many different genres have different types of global coherence. Persuasive essays have claims and premises that are extracted in the field of argument mining, scientific articles have structure related to aims, methods, results, and comparisons. ", "Bloom_type": "remember", "question": "What does the term \"coherence\" refer to?", "options": ["The alignment of ideas within a piece of writing", "The consistency between different genres", "The orderliness of arguments in persuasive essays", "The complexity of scientific article structures"], "complexity": 0}, {"id": 23, "context": "Barzilay and Lapata (2005) pioneered the idea of self-supervision for coherence: training a coherence model to distinguish true orderings of sentences from random permutations. Li et al. (2014) first applied this paradigm to neural sentencerepresentation, and many neural self-supervised models followed (Li and Jurafsky 2017, Logeswaran et al. 2018, Lai and Tetreault 2018, Xu et al. 2019, Iter et al. 2020) ", "Bloom_type": "remember", "question": "In what year did Barzilay and Lapata pioneer the concept of self-supervision for sentence coherence?", "options": ["2000", "1990", "2010", "2020"], "complexity": 0}, {"id": 24, "context": "Another aspect of global coherence is the global topic structure of a text, the way the topics shift over the course of the document. Barzilay and Lee (2004) introduced an HMM model for capturing topics for coherence, and later work expanded this intuition (Soricut and Marcu 2006, Elsner et al. 2007, Louis and Nenkova 2012, Li and Jurafsky 2017). ", "Bloom_type": "remember", "question": "In what type of models does Barzilay and Lee introduce an HMM model for capturing topics?", "options": ["Sequence models", "Recurrent neural networks", "Convolutional neural networks", "Transformer models"], "complexity": 0}, {"id": 25, "context": "The focus of mask-based learning is on predicting words from surrounding contexts with the goal of producing effective word-level representations. However, an important class of applications involves determining the relationship between pairs of sentences. These include tasks like paraphrase detection (detecting if two sentences have similar meanings), entailment (detecting if the meanings of two sentences entail or contradict each other) or discourse coherence (deciding if two neighboring sentences form a coherent discourse). ", "Bloom_type": "comprehension", "question": "Explain how mask-based learning can be applied beyond predicting individual words for generating effective word-level representations?", "options": ["It assesses the coherence between adjacent sentences within a paragraph.", "It predicts the next sentence based on its immediate surroundings.", "It determines the similarity between two sentences by comparing their structures.", "It identifies whether two sentences are contradictory or consistent."], "complexity": 1}, {"id": 26, "context": "As mentioned in Section 11.2.2, an important type of problem involves the classification of pairs of input sequences. Practical applications that fall into this class include paraphrase detection (are the two sentences paraphrases of each other?), logical entailment (does sentence A logically entail sentence B?), and discourse coherence (how coherent is sentence B as a follow-on to sentence A?). ", "Bloom_type": "comprehension", "question": "Explain how coherence can be applied in practical scenarios described in the context?", "options": ["Discourse coherence assesses the relationship between sentences within a paragraph.", "Paraphrase detection checks if two sentences are exactly the same.", "Logical entailment determines whether one statement necessarily follows from another.", "Coherence analysis focuses solely on word order consistency."], "complexity": 1}, {"id": 27, "context": "Another influential line of research from that decade focused on modeling the hierarchical structure of dialogue. Grosz`s pioneering 1977b dissertation first showed that task-oriented dialogues have a structure that closely parallels the structure of the task being performed (p. 27), leading to her work with Sidner and others showing how to use similar notions of intention and plans to model discourse structure and coherence in dialogue. See, e.g., Lochbaum et al. (2000) for a summary of the role of intentional structure in dialogue. ", "Bloom_type": "comprehension", "question": "Explain how Grosz's pioneering work in 1977b dissertation contributed to the understanding of coherence in task-oriented dialogues?", "options": ["Grosz's study highlighted the importance of task-specific intentions in creating cohesive dialogue sequences.", "Grosz's work demonstrated that coherent dialogue structures mirror the complexity of real-world tasks.", "Grosz's research underscored the need for explicit planning in dialogue design to enhance coherence.", "Grosz's findings suggested that coherence in dialogue can only be achieved through random interaction patterns."], "complexity": 1}, {"id": 28, "context": "But coherent structure is not just a fact about movies or works of art. Like movies, language does not normally consist of isolated, unrelated sentences, but instead of collocated, structured, coherent groups of sentences. We refer to such a coherent structured group of sentences as a discourse, and we use the word coherence to refer to the relationship between sentences that makes real discourses different than just random assemblages of sentences. The chapter you are now reading is an example of a discourse, as is a news article, a conversation, a thread on social media, a Wikipedia page, and your favorite novel. ", "Bloom_type": "comprehension", "question": "What is an example of a discourse?", "options": ["A continuous flow of connected sentences", "An isolated sentence", "A collection of unrelated sentences", "A single paragraph"], "complexity": 1}, {"id": 29, "context": "Entity-based coherence models measure this kind of coherence by tracking salient entities across a discourse. For example Centering Theory (Grosz et al., 1995), the most influential theory of entity-based coherence, keeps track of which entities in the discourse model are salient at any point (salient entities are more likely to be pronominalized or to appear in prominent syntactic positions like subject or object). In Centering Theory, transitions between sentences that maintain the same salient entity are considered more coherent than ones that repeatedly shift between entities. The entity grid model of coherence (Barzilay and Lapata, 2008) is a commonly used model that realizes some of the intuitions of the Centering Theory framework. Entity-based coherence is introduced in Section 24.3. ", "Bloom_type": "comprehension", "question": "How do entity-based coherence models determine the coherence of a discourse?", "options": ["By identifying and tracking salient entities across the discourse.", "By analyzing the frequency of pronoun usage within the discourse.", "By comparing the length of sentences for consistency.", "Both B) and C)"], "complexity": 1}, {"id": 30, "context": "In addition to the local coherence between adjacent or nearby sentences, discourses also exhibit global coherence. Many genres of text are associated with particular conventional discourse structures. Academic articles might have sections describing the Methodology or Results. Stories might follow conventional plotlines or motifs. Persuasive essays have a particular claim they are trying to argue for, and an essay might express this claim together with a structured set of premises that support the argument and demolish potential counterarguments. We`ll introduce versions of each of these kinds of global coherence. ", "Bloom_type": "comprehension", "question": "What type of global coherence does persuasive essays typically exhibit?", "options": ["Global coherence based on arguments and supporting evidence", "Local coherence within paragraphs", "Coherence determined by the author's personal style", "No coherent structure exists"], "complexity": 1}, {"id": 31, "context": "Why do we care about the local or global coherence of a discourse? Since coherence is a property of a well-written text, coherence detection plays a part in any ", "Bloom_type": "comprehension", "question": "Explain why it is important to consider both local and global coherence when writing a coherent text?", "options": ["Both A) and B)", "Local coherence ensures that each sentence makes sense independently.", "Global coherence helps maintain the overall flow and meaning of the entire piece.", "Neither A) nor B)"], "complexity": 1}, {"id": 32, "context": "task that requires measuring the quality of a text. For example coherence can help in pedagogical tasks like essay grading or essay quality measurement that are trying to grade how well-written a human essay is (Somasundaran et al. 2014, Feng et al. 2014, Lai and Tetreault 2018). Coherence can also help for summarization; knowing the coherence relationship between sentences can help know how to select information from them. Finally, detecting incoherent text may even play a role in mental health tasks like measuring symptoms of schizophrenia or other kinds of disordered language (Ditman and Kuperberg 2010, Elvevag et al. 2007, Bedi et al. 2015, Iter et al. 2018). ", "Bloom_type": "comprehension", "question": "Explain how coherence relates to different types of tasks mentioned in the context?", "options": ["Coherence helps measure the quality of texts by ensuring they flow logically.", "Coherence assists in identifying patterns within a text.", "Coherence aids in determining the overall structure of an essay.", "Coherence is irrelevant to any type of text analysis."], "complexity": 1}, {"id": 33, "context": "Now that we`ve seen examples of coherence, we can see more clearly how a coherence relation can play a role in summarization or information extraction. For example, the nuclei of a text presumably express more important information than the satellites, which might be dropped in a summary. ", "Bloom_type": "comprehension", "question": "Explain how coherence plays a role in summarization or information extraction?", "options": ["Coherence helps identify the most important parts of a text by focusing on the central ideas.", "Coherence refers to the order in which sentences are written, ensuring they flow logically.", "Coherence means connecting different pieces of information together to make sense.", "Coherence involves using complex vocabulary to improve understanding."], "complexity": 1}, {"id": 34, "context": "Given a sequence of sentences, how can we automatically determine the coherence relations between them? This task is often called discourse parsing (even though for PDTB we are only assigning labels to leaf spans and not building a full parse ", "Bloom_type": "comprehension", "question": "What is the primary goal of discourse parsing when analyzing a sequence of sentences?", "options": ["To assign labels to all span levels in the sequence", "To identify coherent relationships within each sentence", "To build a complete parse tree from the sequence", "To understand the overall structure of the entire document"], "complexity": 1}, {"id": 35, "context": "Centering Theory (Grosz et al., 1995) is a theory of both discourse salience and discourse coherence. As a model of discourse salience, Centering proposes that at any given point in the discourse one of the entities in the discourse model is salient: it is being centered on. As a model of discourse coherence, Centering proposes that discourses in which adjacent sentences CONTINUE to maintain the same salient entity are more coherent than those which SHIFT back and forth between multiple entities (we will see that CONTINUE and SHIFT are technical terms in the theory). ", "Bloom_type": "comprehension", "question": "Explain how Centering Theory distinguishes discourse coherence based on the movement of salient entities?", "options": ["Centering Theory asserts that discourse coherence improves when entities remain consistently centered throughout the discourse.", "Centering Theory states that discourse coherence depends solely on the number of entities involved.", "Centering Theory suggests that discourse coherence increases when entities shift frequently between sentences.", "Centering Theory claims that discourse coherence decreases when entities move from sentence to sentence."], "complexity": 1}, {"id": 36, "context": "The entity grid model of Barzilay and Lapata (2008) is an alternative way to capture entity-based coherence: instead of having a top-down theory, the entity-grid model using machine learning to induce the patterns of entity mentioning that make a discourse more coherent. ", "Bloom_type": "comprehension", "question": "How does the entity grid model differ from traditional theories in capturing discourse coherence?", "options": ["The entity grid model uses machine learning algorithms to identify consistent mentions of entities.", "The entity grid model relies on human reasoning for pattern recognition.", "Traditional theories focus solely on bottom-up approaches.", "Both models are equally effective at identifying coherent discourses."], "complexity": 1}, {"id": 37, "context": "Alternatively, since it`s very expensive to get human labels, and we might not yet have an end-task in mind, we can use natural texts to do self-supervision. In self-supervision we pair up a natural discourse with a pseudo-document created by changing the ordering. Since naturally-ordered discourses are more coherent than random permutation (Lin et al., 2011), a successful coherence algorithm should prefer the original ordering. ", "Bloom_type": "comprehension", "question": "Explain why a successful coherence algorithm would prefer the original ordering over a randomly shuffled one?", "options": ["Because humans find naturally ordered discourses easier to understand.", "Because randomly shuffled orders make the discourse less coherent.", "Because coherent discourses require more computational resources.", "Because coherent discourses are always preferred by algorithms."], "complexity": 1}, {"id": 38, "context": "The third kind of local coherence is topical or semantic field coherence. Discourses cohere by talking about the same topics and subtopics, and drawing on the same semantic fields in doing so. ", "Bloom_type": "comprehension", "question": "Explain how discourses can cohere through topical or semantic field coherence?", "options": ["Discourses cohere by discussing the same topics and utilizing common semantic fields.", "Discourses cohere by discussing different topics and using unrelated semantic fields.", "Discourses cohere by focusing solely on one topic and ignoring other aspects.", "Discourses do not cohere based on their content but rather on their structure."], "complexity": 1}, {"id": 39, "context": " Modern neural representation-learning coherence models, beginning with Li et al. (2014), draw on the intuitions of these early unsupervised models for learning sentence representations and measuring how they change between neighboring sentences. But the new models also draw on the idea pioneered by Barzilay and Lapata (2005) of self-supervision. That is, unlike say coherence relation models, which train on hand-labeled representations for RST or PDTB, these models are trained to distinguish natural discourses from unnatural discourses formed by scrambling the order of sentences, thus using representation learning to discover the features that matter for at least the ordering aspect of coherence. ", "Bloom_type": "comprehension", "question": "How do modern neural representation-learning coherence models differ from earlier unsupervised models in their approach to discovering coherence?", "options": ["Both A and C are true.", "They use hand-labeled data for training.", "They focus solely on the ordering aspect of coherence.", "They rely on self-supervision through scrambled sentence orderings."], "complexity": 1}, {"id": 40, "context": "Here we present one such model, the local coherence discriminator (LCD) (Xu et al., 2019). Like early models, LCD computes the coherence of a text as the average of coherence scores between consecutive pairs of sentences. But unlike the early unsupervised models, LCD is a self-supervised model trained to discriminate consecutive sentence pairs (si, si+1) in the training documents (assumed to be coherent) from (constructed) incoherent pairs (si, s(cid:48)). All consecutive pairs are positive examples, and the negative (incoherent) partner for a sentence si is another sentence uniformly sampled from the same document as si. ", "Bloom_type": "comprehension", "question": "How does the Local Coherence Discriminator (LCD) differ from earlier unsupervised models in its approach to computing coherence?", "options": ["The LCD uses a global coherence score instead of an average of pairwise scores.", "The LCD calculates coherence based solely on the first sentence in each pair.", "The LCD relies on human annotations for all coherence assessments.", "The LCD only considers the coherence of sentences within the same paragraph."], "complexity": 1}, {"id": 41, "context": "Fig. 24.11 describes the architecture of the model f , which takes a sentence pair and returns a score, higher scores for more coherent pairs. Given an input sentence pair s and t, the model computes sentence embeddings s and t (using any sentence embeddings algorithm), and then concatenates four features of the pair: (1) t; (3) the absolute value the concatenation of the two vectors (2) their difference s t. These are passed of their difference through a one-layer feedforward network to output the coherence score. ", "Bloom_type": "comprehension", "question": "What feature does the model use to calculate the coherence score between two sentences?", "options": ["The absolute value of the sum of the two sentence embeddings", "The cosine similarity between the two sentence embeddings", "The dot product of the two sentence embeddings", "The Euclidean distance between the two sentence embeddings"], "complexity": 1}, {"id": 42, "context": "Xu et al. (2019) also give a useful baseline algorithm that itself has quite high performance in measuring perplexity: train an RNN language model on the data, and compute the log likelihood of sentence si in two ways, once given the preceding context (conditional log likelihood) and once with no context (marginal log likelihood). The difference between these values tells us how much the preceding context improved the predictability of si, a predictability measure of coherence. ", "Bloom_type": "comprehension", "question": "Explain how Xu et al.'s method measures coherence using conditional and marginal log likelihood?", "options": ["The method combines both conditional and marginal log likelihood for a more comprehensive measure of coherence.", "Xu et al. uses only the conditional log likelihood to measure coherence.", "Xu et al. focuses solely on the marginal log likelihood to assess coherence.", "The approach does not use either conditional or marginal log likelihood."], "complexity": 1}, {"id": 43, "context": "Training models to predict longer contexts than just consecutive pairs of sentences can result in even stronger discourse representations. For example a Transformer language model trained with a contrastive sentence objective to predict text up to a distance of 2 sentences improves performance on various discourse coherence tasks (Iter et al., 2020). ", "Bloom_type": "comprehension", "question": "Explain how training models to predict longer contexts enhances discourse representation?", "options": ["It allows for more complex relationships between distant sentences.", "It increases the accuracy of predicting individual words.", "It reduces the need for human supervision during training.", "It simplifies the structure of the input data."], "complexity": 1}, {"id": 44, "context": "In this chapter we introduced local and global models for discourse coherence. ", "Bloom_type": "comprehension", "question": "What does the introduction of local and global models for discourse coherence suggest about the structure of this chapter?", "options": ["The chapter provides an overview of various models, including both local and global.", "The chapter focuses solely on local models.", "The chapter emphasizes the importance of global models over local ones.", "The chapter does not discuss any models."], "complexity": 1}, {"id": 45, "context": " Discourses are not arbitrary collections of sentences; they must be coherent. Among the factors that make a discourse coherent are coherence relations between the sentences, entity-based coherence, and topical coherence. ", "Bloom_type": "comprehension", "question": "What are some key elements that contribute to making a discourse coherent?", "options": ["Coherence relations between sentences, entity-based coherence, and topical coherence", "Entity-based coherence and topical coherence only", "Coherence relations between sentences and entity-based coherence only", "Topical coherence and coherence relations between sentences only"], "complexity": 1}, {"id": 46, "context": " Entity-based coherence captures the intuition that discourses are about an entity, and continue mentioning the entity from sentence to sentence. Centering Theory is a family of models describing how salience is modeled for discourse entities, and hence how coherence is achieved by virtue of keeping the same discourse entities salient over the discourse. The entity grid model gives a more bottom-up way to compute which entity realization transitions lead to coherence. ", "Bloom_type": "comprehension", "question": "Which method focuses on computing which entity realizations transition leads to coherence?", "options": ["Entity Grid Model", "Centering Theory", "Both A) and B)", "Neither A) nor B)"], "complexity": 1}, {"id": 47, "context": " Many different genres have different types of global coherence. Persuasive essays have claims and premises that are extracted in the field of argument mining, scientific articles have structure related to aims, methods, results, and comparisons. ", "Bloom_type": "comprehension", "question": "Which type of coherence is most commonly found in persuasive essays?", "options": ["Global coherence", "Local coherence", "Sentence-level coherence", "Paragraph-level coherence"], "complexity": 1}, {"id": 48, "context": "Barzilay and Lapata (2005) pioneered the idea of self-supervision for coherence: training a coherence model to distinguish true orderings of sentences from random permutations. Li et al. (2014) first applied this paradigm to neural sentencerepresentation, and many neural self-supervised models followed (Li and Jurafsky 2017, Logeswaran et al. 2018, Lai and Tetreault 2018, Xu et al. 2019, Iter et al. 2020) ", "Bloom_type": "comprehension", "question": "What was the initial application of the self-supervision method for coherence mentioned by Barzilay and Lapata (2005)?", "options": ["Sentences", "Phrases", "Words", "Sentences and phrases"], "complexity": 1}, {"id": 49, "context": "Another aspect of global coherence is the global topic structure of a text, the way the topics shift over the course of the document. Barzilay and Lee (2004) introduced an HMM model for capturing topics for coherence, and later work expanded this intuition (Soricut and Marcu 2006, Elsner et al. 2007, Louis and Nenkova 2012, Li and Jurafsky 2017). ", "Bloom_type": "comprehension", "question": "What does the introduction of an HMM model by Barzilay and Lee (2004) contribute to understanding about coherence?", "options": ["The importance of shifting topics within a document", "The complexity of global topic structures", "The effectiveness of using HMM models in all types of texts", "The necessity of expanding the scope of topic shifts"], "complexity": 1}, {"id": 50, "context": "The focus of mask-based learning is on predicting words from surrounding contexts with the goal of producing effective word-level representations. However, an important class of applications involves determining the relationship between pairs of sentences. These include tasks like paraphrase detection (detecting if two sentences have similar meanings), entailment (detecting if the meanings of two sentences entail or contradict each other) or discourse coherence (deciding if two neighboring sentences form a coherent discourse). ", "Bloom_type": "application", "question": "Which task falls under the category of determining the relationship between pairs of sentences?", "options": ["All of the above", "Paraphrase detection", "Entailment", "Discourse coherence"], "complexity": 2}, {"id": 51, "context": "As mentioned in Section 11.2.2, an important type of problem involves the classification of pairs of input sequences. Practical applications that fall into this class include paraphrase detection (are the two sentences paraphrases of each other?), logical entailment (does sentence A logically entail sentence B?), and discourse coherence (how coherent is sentence B as a follow-on to sentence A?). ", "Bloom_type": "application", "question": "What does it mean when we say that two sentences are coherent?", "options": ["The sentences flow together naturally and make sense.", "The sentences contain no errors.", "The sentences can be read out loud without stopping.", "The sentences share similar vocabulary."], "complexity": 2}, {"id": 52, "context": "Another influential line of research from that decade focused on modeling the hierarchical structure of dialogue. Grosz`s pioneering 1977b dissertation first showed that task-oriented dialogues have a structure that closely parallels the structure of the task being performed (p. 27), leading to her work with Sidner and others showing how to use similar notions of intention and plans to model discourse structure and coherence in dialogue. See, e.g., Lochbaum et al. (2000) for a summary of the role of intentional structure in dialogue. ", "Bloom_type": "application", "question": "What is an important aspect of dialogue coherence?", "options": ["The alignment of participants' intentions and actions.", "The chronological sequence of events in the conversation.", "The similarity between participants' speech patterns.", "The frequency of using technical jargon."], "complexity": 2}, {"id": 53, "context": "But coherent structure is not just a fact about movies or works of art. Like movies, language does not normally consist of isolated, unrelated sentences, but instead of collocated, structured, coherent groups of sentences. We refer to such a coherent structured group of sentences as a discourse, and we use the word coherence to refer to the relationship between sentences that makes real discourses different than just random assemblages of sentences. The chapter you are now reading is an example of a discourse, as is a news article, a conversation, a thread on social media, a Wikipedia page, and your favorite novel. ", "Bloom_type": "application", "question": "What is referred to when discussing the relationship between sentences that make up a coherent structured group?", "options": ["Coherence", "Syntax", "Semantics", "Syntactic Structure"], "complexity": 2}, {"id": 54, "context": "Entity-based coherence models measure this kind of coherence by tracking salient entities across a discourse. For example Centering Theory (Grosz et al., 1995), the most influential theory of entity-based coherence, keeps track of which entities in the discourse model are salient at any point (salient entities are more likely to be pronominalized or to appear in prominent syntactic positions like subject or object). In Centering Theory, transitions between sentences that maintain the same salient entity are considered more coherent than ones that repeatedly shift between entities. The entity grid model of coherence (Barzilay and Lapata, 2008) is a commonly used model that realizes some of the intuitions of the Centering Theory framework. Entity-based coherence is introduced in Section 24.3. ", "Bloom_type": "application", "question": "Which method is used to determine the coherence of a sentence based on the presence of salient entities?", "options": ["Both A and B", "Centering Theory", "Entity Grid Model", "None of the above"], "complexity": 2}, {"id": 55, "context": "In addition to the local coherence between adjacent or nearby sentences, discourses also exhibit global coherence. Many genres of text are associated with particular conventional discourse structures. Academic articles might have sections describing the Methodology or Results. Stories might follow conventional plotlines or motifs. Persuasive essays have a particular claim they are trying to argue for, and an essay might express this claim together with a structured set of premises that support the argument and demolish potential counterarguments. We`ll introduce versions of each of these kinds of global coherence. ", "Bloom_type": "application", "question": "What is another way to describe the global coherence found in academic articles?", "options": ["Global structure based on genre conventions", "Local coherence within paragraphs", "Sequential presentation of facts", "Chronological timeline of events"], "complexity": 2}, {"id": 56, "context": "Why do we care about the local or global coherence of a discourse? Since coherence is a property of a well-written text, coherence detection plays a part in any ", "Bloom_type": "application", "question": "What role does coherence play in writing?", "options": ["It helps readers understand the flow and meaning.", "It ensures all sentences are grammatically correct.", "It guarantees the writer uses complex vocabulary.", "It prevents the introduction of new ideas."], "complexity": 2}, {"id": 57, "context": "task that requires measuring the quality of a text. For example coherence can help in pedagogical tasks like essay grading or essay quality measurement that are trying to grade how well-written a human essay is (Somasundaran et al. 2014, Feng et al. 2014, Lai and Tetreault 2018). Coherence can also help for summarization; knowing the coherence relationship between sentences can help know how to select information from them. Finally, detecting incoherent text may even play a role in mental health tasks like measuring symptoms of schizophrenia or other kinds of disordered language (Ditman and Kuperberg 2010, Elvevag et al. 2007, Bedi et al. 2015, Iter et al. 2018). ", "Bloom_type": "application", "question": "What aspect of text analysis does coherence primarily focus on?", "options": ["Determining the overall structure and flow of ideas within a piece of writing", "Identifying errors in grammar and syntax", "Counting the number of words used per sentence", "Calculating the frequency of specific vocabulary terms"], "complexity": 2}, {"id": 58, "context": "Now that we`ve seen examples of coherence, we can see more clearly how a coherence relation can play a role in summarization or information extraction. For example, the nuclei of a text presumably express more important information than the satellites, which might be dropped in a summary. ", "Bloom_type": "application", "question": "What is an example of coherence in text summarization?", "options": ["Nuclei express more important information than satellites.", "Satellites are less important than nuclei.", "The satellites drop out in summaries.", "Summaries should include all details."], "complexity": 2}, {"id": 59, "context": "Given a sequence of sentences, how can we automatically determine the coherence relations between them? This task is often called discourse parsing (even though for PDTB we are only assigning labels to leaf spans and not building a full parse ", "Bloom_type": "application", "question": "What is the main goal when performing discourse parsing?", "options": ["To determine the coherence relations between consecutive sentences.", "To identify the logical connections between different parts of a sentence.", "To assign labels to each word in a sentence.", "To build a complete parse tree of the entire document."], "complexity": 2}, {"id": 60, "context": "Centering Theory (Grosz et al., 1995) is a theory of both discourse salience and discourse coherence. As a model of discourse salience, Centering proposes that at any given point in the discourse one of the entities in the discourse model is salient: it is being centered on. As a model of discourse coherence, Centering proposes that discourses in which adjacent sentences CONTINUE to maintain the same salient entity are more coherent than those which SHIFT back and forth between multiple entities (we will see that CONTINUE and SHIFT are technical terms in the theory). ", "Bloom_type": "application", "question": "In the theory of discourse coherence proposed by Centering, what does it mean when adjacent sentences continue to maintain the same salient entity?", "options": ["They remain focused on the same entity throughout.", "They shift their focus frequently.", "They alternate between different entities.", "Their focus changes unpredictably."], "complexity": 2}, {"id": 61, "context": "The entity grid model of Barzilay and Lapata (2008) is an alternative way to capture entity-based coherence: instead of having a top-down theory, the entity-grid model using machine learning to induce the patterns of entity mentioning that make a discourse more coherent. ", "Bloom_type": "application", "question": "What does the entity grid model aim to achieve?", "options": ["To improve the coherence of discourses based on pattern recognition", "To create a bottom-up approach for understanding discourse", "To develop a method for detecting entities in natural language", "To enhance the accuracy of entity mention prediction through machine learning"], "complexity": 2}, {"id": 62, "context": "Alternatively, since it`s very expensive to get human labels, and we might not yet have an end-task in mind, we can use natural texts to do self-supervision. In self-supervision we pair up a natural discourse with a pseudo-document created by changing the ordering. Since naturally-ordered discourses are more coherent than random permutation (Lin et al., 2011), a successful coherence algorithm should prefer the original ordering. ", "Bloom_type": "application", "question": "Which of the following best describes why a coherence algorithm prefers the original ordering over random permutations?", "options": ["Because naturally-ordered discourses are more coherent.", "Because randomly ordered discourses are less coherent.", "Because the pseudo-documents are always better than real documents.", "Because coherence algorithms cannot handle random permutations."], "complexity": 2}, {"id": 63, "context": "The third kind of local coherence is topical or semantic field coherence. Discourses cohere by talking about the same topics and subtopics, and drawing on the same semantic fields in doing so. ", "Bloom_type": "application", "question": "What type of coherence does discourse cohesion primarily rely on?", "options": ["Topical or semantic field coherence", "Syntactic coherence", "Lexical coherence", "Narrative coherence"], "complexity": 2}, {"id": 64, "context": " Modern neural representation-learning coherence models, beginning with Li et al. (2014), draw on the intuitions of these early unsupervised models for learning sentence representations and measuring how they change between neighboring sentences. But the new models also draw on the idea pioneered by Barzilay and Lapata (2005) of self-supervision. That is, unlike say coherence relation models, which train on hand-labeled representations for RST or PDTB, these models are trained to distinguish natural discourses from unnatural discourses formed by scrambling the order of sentences, thus using representation learning to discover the features that matter for at least the ordering aspect of coherence. ", "Bloom_type": "application", "question": "What technique does modern neural representation-learning coherence models use to identify coherent sequences?", "options": ["Self-supervision through scrambled sentence order", "Hand-labeling of sentences", "Training on supervised data", "Feature extraction from raw text"], "complexity": 2}, {"id": 65, "context": "Here we present one such model, the local coherence discriminator (LCD) (Xu et al., 2019). Like early models, LCD computes the coherence of a text as the average of coherence scores between consecutive pairs of sentences. But unlike the early unsupervised models, LCD is a self-supervised model trained to discriminate consecutive sentence pairs (si, si+1) in the training documents (assumed to be coherent) from (constructed) incoherent pairs (si, s(cid:48)). All consecutive pairs are positive examples, and the negative (incoherent) partner for a sentence si is another sentence uniformly sampled from the same document as si. ", "Bloom_type": "application", "question": "What distinguishes the Local Coherence Discriminator (LCD) from earlier unsupervised models?", "options": ["It trains to distinguish coherent from incoherent sentence pairs.", "It uses only positive examples.", "It relies on human annotations.", "It requires labeled data."], "complexity": 2}, {"id": 66, "context": "Fig. 24.11 describes the architecture of the model f , which takes a sentence pair and returns a score, higher scores for more coherent pairs. Given an input sentence pair s and t, the model computes sentence embeddings s and t (using any sentence embeddings algorithm), and then concatenates four features of the pair: (1) t; (3) the absolute value the concatenation of the two vectors (2) their difference s t. These are passed of their difference through a one-layer feedforward network to output the coherence score. ", "Bloom_type": "application", "question": "What is the first step in calculating the coherence score?", "options": ["Compute sentence embeddings using any sentence embeddings algorithm", "Concatenate the absolute values of the two vectors", "Pass the concatenated features through a one-layer feedforward network", "Calculate the difference between the two vectors"], "complexity": 2}, {"id": 67, "context": "Xu et al. (2019) also give a useful baseline algorithm that itself has quite high performance in measuring perplexity: train an RNN language model on the data, and compute the log likelihood of sentence si in two ways, once given the preceding context (conditional log likelihood) and once with no context (marginal log likelihood). The difference between these values tells us how much the preceding context improved the predictability of si, a predictability measure of coherence. ", "Bloom_type": "application", "question": "What is the key metric for evaluating coherence mentioned in the passage?", "options": ["Predictability Measure", "Perplexity", "Conditional Log Likelihood", "Marginal Log Likelihood"], "complexity": 2}, {"id": 68, "context": "Training models to predict longer contexts than just consecutive pairs of sentences can result in even stronger discourse representations. For example a Transformer language model trained with a contrastive sentence objective to predict text up to a distance of 2 sentences improves performance on various discourse coherence tasks (Iter et al., 2020). ", "Bloom_type": "application", "question": "What is an effective way to improve the coherence of discourse?", "options": ["Implement a contrastive sentence objective to predict longer sequences", "Increase the number of training examples only", "Use a larger dataset for training", "Focus solely on improving individual word-level accuracy"], "complexity": 2}, {"id": 69, "context": "In this chapter we introduced local and global models for discourse coherence. ", "Bloom_type": "application", "question": "What is the main difference between local and global models of discourse coherence?", "options": ["Local models prioritize logical connections within sentences, while global models emphasize thematic links across paragraphs.", "Local models focus on individual sentences, while global models consider the entire paragraph.", "Global models are used only in technical writing, whereas local models are used in creative writing.", "Global models deal with the structure of paragraphs, while local models handle the flow of ideas throughout the document."], "complexity": 2}, {"id": 70, "context": " Discourses are not arbitrary collections of sentences; they must be coherent. Among the factors that make a discourse coherent are coherence relations between the sentences, entity-based coherence, and topical coherence. ", "Bloom_type": "application", "question": "What is an important factor for making a discourse coherent?", "options": ["The consistency of ideas presented throughout the discourse.", "The number of sentences in the discourse.", "The length of each sentence within the discourse.", "The complexity of vocabulary used in the discourse."], "complexity": 2}, {"id": 71, "context": " Entity-based coherence captures the intuition that discourses are about an entity, and continue mentioning the entity from sentence to sentence. Centering Theory is a family of models describing how salience is modeled for discourse entities, and hence how coherence is achieved by virtue of keeping the same discourse entities salient over the discourse. The entity grid model gives a more bottom-up way to compute which entity realization transitions lead to coherence. ", "Bloom_type": "application", "question": "Which method focuses on computing which entity realizations transition leads to coherence?", "options": ["Entity Grid Model", "Centering Theory", "Response", "None of the above"], "complexity": 2}, {"id": 72, "context": " Many different genres have different types of global coherence. Persuasive essays have claims and premises that are extracted in the field of argument mining, scientific articles have structure related to aims, methods, results, and comparisons. ", "Bloom_type": "application", "question": "In which genre is it common for authors to focus on the structure of their work?", "options": ["Scientific articles", "Persuasive essays", "Argument mining", "Creative writing"], "complexity": 2}, {"id": 73, "context": "Barzilay and Lapata (2005) pioneered the idea of self-supervision for coherence: training a coherence model to distinguish true orderings of sentences from random permutations. Li et al. (2014) first applied this paradigm to neural sentencerepresentation, and many neural self-supervised models followed (Li and Jurafsky 2017, Logeswaran et al. 2018, Lai and Tetreault 2018, Xu et al. 2019, Iter et al. 2020) ", "Bloom_type": "application", "question": "What is the primary goal of applying self-supervision techniques in natural language processing?", "options": ["To enhance the generation of coherent sequences", "To improve the accuracy of word embeddings", "To increase the speed of data preprocessing", "To reduce the need for manual labeling"], "complexity": 2}, {"id": 74, "context": "Another aspect of global coherence is the global topic structure of a text, the way the topics shift over the course of the document. Barzilay and Lee (2004) introduced an HMM model for capturing topics for coherence, and later work expanded this intuition (Soricut and Marcu 2006, Elsner et al. 2007, Louis and Nenkova 2012, Li and Jurafsky 2017). ", "Bloom_type": "application", "question": "What method did Barzilay and Lee introduce for capturing topics in coherence?", "options": ["HMM model", "Latent Dirichlet Allocation", "Topic modeling", "Coherence analysis"], "complexity": 2}]}, "dependency": {"max_id": 62, "Questions": [{"id": 0, "context": "Parts of speech (also known as POS) and named entities are useful clues to sentence structure and meaning. Knowing whether a word is a noun or a verb tells us about likely neighboring words (nouns in English are preceded by determiners and adjectives, verbs by nouns) and syntactic structure (verbs have dependency links to nouns), making part-of-speech tagging a key aspect of parsing. Knowing if a named entity like Washington is a name of a person, a place, or a university is important to many natural language processing tasks like question answering, stance detection, or information extraction. ", "Bloom_type": "remember", "question": "In what way does knowing the part of speech help with sentence structure?", "options": ["It enables predicting the next word in the sentence.", "It helps determine the tense of the sentence.", "It aids in identifying the subject of the sentence.", "It assists in recognizing the main clause of the sentence."], "complexity": 0}, {"id": 1, "context": "Our focus in this chapter is context-free grammars and the CKY algorithm for parsing them. Context-free grammars are the backbone of many formal models of the syntax of natural language (and, for that matter, of computer languages). Syntactic parsing is the task of assigning a syntactic structure to a sentence. Parse trees (whether for context-free grammars or for the dependency or CCG formalisms we introduce in following chapters) can be used in applications such as grammar checking: sentence that cannot be parsed may have grammatical errors (or at least be hard to read). Parse trees can be an intermediate stage of representation for formal semantic analysis. And parsers and the grammatical structure they assign a sentence are a useful text analysis tool for text data science applications that require modeling the relationship of elements in sentences. ", "Bloom_type": "remember", "question": "In what way do parse trees help with text analysis tools?", "options": ["They enable the identification of grammatical structures within texts.", "They provide a visual representation of word order.", "They allow for statistical analysis of sentence length.", "They facilitate the automatic correction of spelling mistakes."], "complexity": 0}, {"id": 2, "context": "Figure 19.1 on the next page shows the dependency analysis from Eq. 19.1 but visualized as a tree, alongside its corresponding phrase-structure analysis of the kind given in the prior chapter. Note the absence of nodes corresponding to phrasal constituents or lexical categories in the dependency parse; the internal structure of the dependency parse consists solely of directed relations between words. These headdependent relationships directly encode important information that is often buried in the more complex phrase-structure parses. For example, the arguments to the verb prefer are directly linked to it in the dependency structure, while their connection to the main verb is more distant in the phrase-structure tree. Similarly, morning and Denver, modifiers of flight, are linked to it directly in the dependency structure. This fact that the head-dependent relations are a good proxy for the semantic relationship between predicates and their arguments is an important reason why dependency grammars are currently more common than constituency grammars in natural language processing. ", "Bloom_type": "remember", "question": "In what way does the dependency parse differ from the phrase-structure parse?", "options": ["The dependency parse focuses only on the direct relationships between verbs.", "The dependency parse uses fewer words.", "The dependency parse has no nodes.", "The dependency parse represents the internal structure of phrases."], "complexity": 0}, {"id": 3, "context": "The traditional linguistic notion of grammatical relation provides the basis for the binary relations that comprise these dependency structures. The arguments to these relations consist of a head and a dependent. The head plays the role of the central organizing word, and the dependent as a kind of modifier. The head-dependent relationship is made explicit by directly linking heads to the words that are immediately dependent on them. ", "Bloom_type": "remember", "question": "In linguistics, what does the term \"head\" refer to?", "options": ["The word that modifies another word", "The type of sentence structure", "The part of speech of a noun", "The main verb of a clause"], "complexity": 0}, {"id": 4, "context": "Treebanks play a critical role in the development and evaluation of dependency parsers. They are used for training parsers, they act as the gold labels for evaluating parsers, and they also provide useful information for corpus linguistics studies. ", "Bloom_type": "remember", "question": "What do treebanks primarily serve in the field of natural language processing?", "options": ["Training models", "Creating grammars", "Generating speech", "Analyzing syntax"], "complexity": 0}, {"id": 5, "context": "The largest open community project for building dependency trees is the Universal Dependencies project at https://universaldependencies.org/ introduced above, which currently has almost 200 dependency treebanks in more than 100 languages (de Marneffe et al., 2021). Here are a few UD examples showing dependency trees for sentences in Spanish, Basque, and Mandarin Chinese: ", "Bloom_type": "remember", "question": "Which of the following statements about the Universal Dependencies project is true?", "options": ["UD contains over 500 dependency treebanks from various languages.", "The project focuses on creating dependency trees exclusively for English.", "The project was initiated by de Marneffe alone.", "Spanish, Basque, and Mandarin Chinese are not included in any UD example."], "complexity": 0}, {"id": 6, "context": "The specification of a transition-based parser is quite simple, based on representing the current state of the parse as a configuration: the stack, an input buffer of words or tokens, and a set of relations representing a dependency tree. Parsing means making a sequence of transitions through the space of possible configurations. We start with an initial configuration in which the stack contains the ROOT node, the buffer has the tokens in the sentence, and an empty set of relations represents the parse. In the final goal state, the stack and the word list should be empty, and the set of relations will represent the final parse. Fig. 19.5 gives the algorithm. ", "Bloom_type": "remember", "question": "In parsing, what does the final goal state refer to?", "options": ["Both A) and C)", "The stack being empty", "The buffer containing only the ROOT token", "The set of relations representing the dependency tree"], "complexity": 0}, {"id": 7, "context": "The oracle for greedily selecting the appropriate transition is trained by supervised machine learning. As with all supervised machine learning methods, we will need training data: configurations annotated with the correct transition to take. We can draw these from dependency trees. And we need to extract features of the configuration. We`ll introduce neural classifiers that represent the configuration via embeddings, as well as classic systems that use hand-designed features. ", "Bloom_type": "remember", "question": "In the context provided, what does the term \"dependency\" refer to?", "options": ["The system that selects the most appropriate transition based on greedy selection", "The process of extracting features from a configuration", "The method used to train a model using labeled data", "The type of tree structure used in natural language processing"], "complexity": 0}, {"id": 8, "context": "Graph-based methods are the second important family of dependency parsing algorithms. Graph-based parsers are more accurate than transition-based parsers, especially on long sentences; transition-based methods have trouble when the heads are very far from the dependents (McDonald and Nivre, 2011). Graph-based methods avoid this difficulty by scoring entire trees, rather than relying on greedy local decisions. Furthermore, unlike transition-based approaches, graph-based parsers can produce non-projective trees. Although projectivity is not a significant issue for English, it is definitely a problem for many of the world`s languages. ", "Bloom_type": "remember", "question": "Which type of dependency parsing algorithm is less accurate compared to graph-based parsers?", "options": ["Transition-based", "Rule-based", "Dependency-based", "Statistical"], "complexity": 0}, {"id": 9, "context": "Graph-based dependency parsers search through the space of possible trees for a given sentence for a tree (or trees) that maximize some score. These methods encode the search space as directed graphs and employ methods drawn from graph theory to search the space for optimal solutions. More formally, given a sentence S we`re looking for the best dependency tree in Gs, the space of all possible trees for that sentence, that maximizes some score. ", "Bloom_type": "remember", "question": "In the context of graph-based dependency parsing, what is the primary goal when searching for an optimal solution?", "options": ["To maximize the overall score of the dependency tree", "To find the shortest path between two nodes", "To identify the most frequent word usage in the sentence", "To minimize the number of edges in the graph"], "complexity": 0}, {"id": 10, "context": "It turns out that finding the best dependency parse for S is equivalent to finding the maximum spanning tree over G. A spanning tree over a graph G is a subset of G that is a tree and covers all the vertices in G; a spanning tree over G that starts from the ROOT is a valid parse of S. A maximum spanning tree is the spanning tree with the highest score. Thus a maximum spanning tree of G emanating from the ROOT is the optimal dependency parse for the sentence. ", "Bloom_type": "remember", "question": "What does the maximum spanning tree represent in the context?", "options": ["The optimal dependency parse for the sentence", "The shortest path between two nodes", "The most efficient way to cover all nodes in a network", "The least number of edges needed to connect all nodes"], "complexity": 0}, {"id": 11, "context": "This chapter has introduced the concept of dependency grammars and dependency parsing. Here`s a summary of the main points that we covered: ", "Bloom_type": "remember", "question": "What is the main focus of this chapter?", "options": ["Both A) and B)", "Dependency grammars", "Dependency parsing", "Neither A) nor B)"], "complexity": 0}, {"id": 12, "context": " In dependency-based approaches to syntax, the structure of a sentence is described in terms of a set of binary relations that hold between the words in a sentence. Larger notions of constituency are not directly encoded in dependency analyses. ", "Bloom_type": "remember", "question": "In dependency-based approaches to syntax, what does the structure of a sentence describe?", "options": ["Binary relations that hold between words", "Larger notions of constituency", "The relationships between sentences", "The grammatical rules for word order"], "complexity": 0}, {"id": 13, "context": " Dependency-based analysis provides information directly useful in further language processing tasks including information extraction, semantic parsing and question answering. ", "Bloom_type": "remember", "question": "Dependency-based analysis is used for:", "options": ["Enhancing natural language understanding", "Improving computer vision", "Increasing internet speed", "Boosting video encoding efficiency"], "complexity": 0}, {"id": 14, "context": " Treebanks provide the data needed to train these systems. Dependency treebanks can be created directly by human annotators or via automatic transformation from phrase-structure treebanks. ", "Bloom_type": "remember", "question": "What are dependency treebanks used for?", "options": ["To train machine learning models", "To create new languages", "To annotate phrases", "To transform sentences into diagrams"], "complexity": 0}, {"id": 15, "context": "The dependency-based approach to grammar is much older than the relatively recent phrase-structure or constituency grammars, which date only to the 20th century. Dependency grammar dates back to the Indian grammarian Pan. ini sometime between the 7th and 4th centuries BCE, as well as the ancient Greek linguistic traditions. Contemporary theories of dependency grammar all draw heavily on the 20th century work of Tesni`ere (1959). ", "Bloom_type": "remember", "question": "When did the concept of dependency-based grammar first appear?", "options": ["The 7th century BCE", "The 20th century", "The 4th century BCE", "The 1st century CE"], "complexity": 0}, {"id": 16, "context": "Automatic parsing using dependency grammars was first introduced into computational linguistics by early work on machine translation at the RAND Corporation led by David Hays. This work on dependency parsing closely paralleled work on constituent parsing and made explicit use of grammars to guide the parsing process. After this early period, computational work on dependency parsing remained intermittent over the following decades. Notable implementations of dependency parsers for English during this period include Link Grammar (Sleator and Temperley, 1993), Constraint Grammar (Karlsson et al., 1995), and MINIPAR (Lin, 2003). ", "Bloom_type": "remember", "question": "What did David Hays lead early work on machine translation at the RAND Corporation focused on?", "options": ["Dependency Grammars", "Constituent Parsing", "Rule-Based Parsing", "Statistical Machine Translation"], "complexity": 0}, {"id": 17, "context": "Dependency parsing saw a major resurgence in the late 1990`s with the appearance of large dependency-based treebanks and the associated advent of data driven approaches described in this chapter. Eisner (1996) developed an efficient dynamic programming approach to dependency parsing based on bilexical grammars derived from the Penn Treebank. Covington (2001) introduced the deterministic word by word approach underlying current transition-based approaches. Yamada and Matsumoto (2003) and Kudo and Matsumoto (2002) introduced both the shift-reduce paradigm and the use of supervised machine learning in the form of support vector machines to dependency parsing. ", "Bloom_type": "remember", "question": "In which year did Eisner develop an efficient dynamic programming approach for dependency parsing?", "options": ["1996", "1985", "1990", "2000"], "complexity": 0}, {"id": 18, "context": "It`s also possible to use dependency parses instead of constituency parses as the basis of features, for example using dependency parse paths instead of constituency paths. ", "Bloom_type": "remember", "question": "What can be used instead of constituency parses?", "options": ["Dependency parses", "Constituency paths", "Feature paths", "Sentence structures"], "complexity": 0}, {"id": 19, "context": "The combination of rich linguistic annotation and corpus-based approach instantiated in FrameNet and PropBank led to a revival of automatic approaches to semantic role labeling, first on FrameNet (Gildea and Jurafsky, 2000) and then on PropBank data (Gildea and Palmer, 2002, inter alia). The problem first addressed in the 1970s by handwritten rules was thus now generally recast as one of supervised machine learning enabled by large and consistent databases. Many popular features used for role labeling are defined in Gildea and Jurafsky (2002), Surdeanu et al. (2003), Xue and Palmer (2004), Pradhan et al. (2005), Che et al. (2009), and Zhao et al. (2009). The use of dependency rather than constituency parses was introduced in the CoNLL-2008 shared task (Surdeanu et al., 2008). For surveys see Palmer et al. (2010) and M`arquez et al. (2008). ", "Bloom_type": "remember", "question": "In what way did the introduction of dependency parsing change the approach to semantic role labeling?", "options": ["It required less computational resources.", "It made it more difficult to define features.", "It shifted focus from supervised learning to rule-based methods.", "It eliminated the need for annotated corpora."], "complexity": 0}, {"id": 20, "context": "The use of neural approaches to semantic role labeling was pioneered by Collobert et al. (2011), who applied a CRF on top of a convolutional net. Early work like Foland, Jr. and Martin (2015) focused on using dependency features. Later work eschewed syntactic features altogether; Zhou and Xu (2015b) introduced the use of a stacked (6-8 layer) biLSTM architecture, and (He et al., 2017) showed how to augment the biLSTM architecture with highway networks and also replace the CRF with A* decoding that make it possible to apply a wide variety of global constraints in SRL decoding. ", "Bloom_type": "remember", "question": "In what way did early works differ from later ones regarding their approach to semantic role labeling?", "options": ["Early works used only syntactic features.", "Later works ignored dependency features entirely.", "Early works utilized a single-layer LSTM for feature extraction.", "Later works exclusively relied on CRFs for constraint application."], "complexity": 0}, {"id": 21, "context": "Parts of speech (also known as POS) and named entities are useful clues to sentence structure and meaning. Knowing whether a word is a noun or a verb tells us about likely neighboring words (nouns in English are preceded by determiners and adjectives, verbs by nouns) and syntactic structure (verbs have dependency links to nouns), making part-of-speech tagging a key aspect of parsing. Knowing if a named entity like Washington is a name of a person, a place, or a university is important to many natural language processing tasks like question answering, stance detection, or information extraction. ", "Bloom_type": "comprehension", "question": "What does knowing parts of speech tell us about sentences?", "options": ["It helps identify the grammatical function of each word.", "It aids in understanding the overall meaning of the sentence.", "It assists in determining the tense of the verb.", "It allows for easier translation into other languages."], "complexity": 1}, {"id": 22, "context": "Our focus in this chapter is context-free grammars and the CKY algorithm for parsing them. Context-free grammars are the backbone of many formal models of the syntax of natural language (and, for that matter, of computer languages). Syntactic parsing is the task of assigning a syntactic structure to a sentence. Parse trees (whether for context-free grammars or for the dependency or CCG formalisms we introduce in following chapters) can be used in applications such as grammar checking: sentence that cannot be parsed may have grammatical errors (or at least be hard to read). Parse trees can be an intermediate stage of representation for formal semantic analysis. And parsers and the grammatical structure they assign a sentence are a useful text analysis tool for text data science applications that require modeling the relationship of elements in sentences. ", "Bloom_type": "comprehension", "question": "What is the primary purpose of using parse trees in natural language processing?", "options": ["To analyze the grammatical structure of sentences", "To identify dependencies between words in a sentence", "To determine the meaning of individual words", "To classify different types of sentences"], "complexity": 1}, {"id": 23, "context": "Figure 19.1 on the next page shows the dependency analysis from Eq. 19.1 but visualized as a tree, alongside its corresponding phrase-structure analysis of the kind given in the prior chapter. Note the absence of nodes corresponding to phrasal constituents or lexical categories in the dependency parse; the internal structure of the dependency parse consists solely of directed relations between words. These headdependent relationships directly encode important information that is often buried in the more complex phrase-structure parses. For example, the arguments to the verb prefer are directly linked to it in the dependency structure, while their connection to the main verb is more distant in the phrase-structure tree. Similarly, morning and Denver, modifiers of flight, are linked to it directly in the dependency structure. This fact that the head-dependent relations are a good proxy for the semantic relationship between predicates and their arguments is an important reason why dependency grammars are currently more common than constituency grammars in natural language processing. ", "Bloom_type": "comprehension", "question": "What does the dependency parse show about the relationship between verbs and their arguments compared to the phrase-structure tree?", "options": ["The dependency parse shows direct links between verbs and their arguments.", "The dependency parse shows indirect connections between verbs and their arguments.", "The dependency parse shows no relation between verbs and their arguments.", "The dependency parse shows only lexical category connections."], "complexity": 1}, {"id": 24, "context": "The traditional linguistic notion of grammatical relation provides the basis for the binary relations that comprise these dependency structures. The arguments to these relations consist of a head and a dependent. The head plays the role of the central organizing word, and the dependent as a kind of modifier. The head-dependent relationship is made explicit by directly linking heads to the words that are immediately dependent on them. ", "Bloom_type": "comprehension", "question": "What does the head play in the head-dependent relationship described in the context?", "options": ["The head modifies the dependent.", "The head is modified by the dependent.", "The head has no specific function.", "The head and dependent have equal roles."], "complexity": 1}, {"id": 25, "context": "Treebanks play a critical role in the development and evaluation of dependency parsers. They are used for training parsers, they act as the gold labels for evaluating parsers, and they also provide useful information for corpus linguistics studies. ", "Bloom_type": "comprehension", "question": "What are some primary uses of treebanks in relation to dependency parsers?", "options": ["All of the above", "Training parsers only", "Acting as gold labels for evaluation only", "Providing useful information for corpus linguistics studies only"], "complexity": 1}, {"id": 26, "context": "The largest open community project for building dependency trees is the Universal Dependencies project at https://universaldependencies.org/ introduced above, which currently has almost 200 dependency treebanks in more than 100 languages (de Marneffe et al., 2021). Here are a few UD examples showing dependency trees for sentences in Spanish, Basque, and Mandarin Chinese: ", "Bloom_type": "comprehension", "question": "What does the Universal Dependencies project aim to achieve?", "options": ["To build dependency trees for all major world languages.", "To provide resources for teaching language classes.", "To create dictionaries for each language.", "To analyze the grammatical structures of various languages."], "complexity": 1}, {"id": 27, "context": "The specification of a transition-based parser is quite simple, based on representing the current state of the parse as a configuration: the stack, an input buffer of words or tokens, and a set of relations representing a dependency tree. Parsing means making a sequence of transitions through the space of possible configurations. We start with an initial configuration in which the stack contains the ROOT node, the buffer has the tokens in the sentence, and an empty set of relations represents the parse. In the final goal state, the stack and the word list should be empty, and the set of relations will represent the final parse. Fig. 19.5 gives the algorithm. ", "Bloom_type": "comprehension", "question": "What does parsing mean in the context of transition-based parsers?", "options": ["Making a sequence of transitions through the space of possible configurations", "Determining the dependencies between words in a sentence", "Building a dependency tree from a sentence", "Creating a new configuration for each token added to the stack"], "complexity": 1}, {"id": 28, "context": "The oracle for greedily selecting the appropriate transition is trained by supervised machine learning. As with all supervised machine learning methods, we will need training data: configurations annotated with the correct transition to take. We can draw these from dependency trees. And we need to extract features of the configuration. We`ll introduce neural classifiers that represent the configuration via embeddings, as well as classic systems that use hand-designed features. ", "Bloom_type": "comprehension", "question": "What are the two primary approaches mentioned for extracting features in the context of supervised machine learning for dependency selection?", "options": ["Neural classifiers and hand-designed features", "Hand-designed features and traditional algorithms", "Traditional algorithms and neural networks", "Machine learning models and rule-based systems"], "complexity": 1}, {"id": 29, "context": "Graph-based methods are the second important family of dependency parsing algorithms. Graph-based parsers are more accurate than transition-based parsers, especially on long sentences; transition-based methods have trouble when the heads are very far from the dependents (McDonald and Nivre, 2011). Graph-based methods avoid this difficulty by scoring entire trees, rather than relying on greedy local decisions. Furthermore, unlike transition-based approaches, graph-based parsers can produce non-projective trees. Although projectivity is not a significant issue for English, it is definitely a problem for many of the world`s languages. ", "Bloom_type": "comprehension", "question": "What distinguishes graph-based dependency parsing algorithms from transition-based ones?", "options": ["Graph-based methods score entire trees while transition-based methods rely on greedy local decisions.", "Transition-based methods are less accurate than graph-based methods.", "Graph-based methods cannot handle long sentences effectively.", "Transition-based methods do not produce non-projective trees."], "complexity": 1}, {"id": 30, "context": "Graph-based dependency parsers search through the space of possible trees for a given sentence for a tree (or trees) that maximize some score. These methods encode the search space as directed graphs and employ methods drawn from graph theory to search the space for optimal solutions. More formally, given a sentence S we`re looking for the best dependency tree in Gs, the space of all possible trees for that sentence, that maximizes some score. ", "Bloom_type": "comprehension", "question": "What does the dependency parser aim to find when searching through the space of possible trees for a given sentence?", "options": ["The best dependency tree in Gs", "The longest path in the sentence", "The most complex tree structure", "The shortest path between two nodes"], "complexity": 1}, {"id": 31, "context": "It turns out that finding the best dependency parse for S is equivalent to finding the maximum spanning tree over G. A spanning tree over a graph G is a subset of G that is a tree and covers all the vertices in G; a spanning tree over G that starts from the ROOT is a valid parse of S. A maximum spanning tree is the spanning tree with the highest score. Thus a maximum spanning tree of G emanating from the ROOT is the optimal dependency parse for the sentence. ", "Bloom_type": "comprehension", "question": "What does it mean when we say that finding the best dependency parse for S is equivalent to finding the maximum spanning tree over G?", "options": ["Finding the maximum spanning tree helps us find the best dependency parse, but they are not necessarily the same.", "It means that the best dependency parse will always have the same structure as the maximum spanning tree.", "The best dependency parse can only be found by using the maximum spanning tree method.", "There is no connection between finding the best dependency parse and finding the maximum spanning tree."], "complexity": 1}, {"id": 32, "context": "This chapter has introduced the concept of dependency grammars and dependency parsing. Here`s a summary of the main points that we covered: ", "Bloom_type": "comprehension", "question": "What does this chapter focus on regarding dependency grammars and parsing?", "options": ["Both A) and B)", "The history of dependency grammars", "Dependency grammars and their applications", "Parsing techniques for natural language processing"], "complexity": 1}, {"id": 33, "context": " In dependency-based approaches to syntax, the structure of a sentence is described in terms of a set of binary relations that hold between the words in a sentence. Larger notions of constituency are not directly encoded in dependency analyses. ", "Bloom_type": "comprehension", "question": "In dependency-based approaches to syntax, how are larger notions of constituency typically represented?", "options": ["By describing relationships between individual words", "Through direct encoding in the analysis", "Using complex grammatical structures", "Not applicable"], "complexity": 1}, {"id": 34, "context": " Dependency-based analysis provides information directly useful in further language processing tasks including information extraction, semantic parsing and question answering. ", "Bloom_type": "comprehension", "question": "What does dependency-based analysis provide for language processing tasks?", "options": ["Directly useful information for all types of language processing tasks", "Information only relevant for information extraction", "Information only relevant for semantic parsing", "Information only relevant for question answering"], "complexity": 1}, {"id": 35, "context": " Treebanks provide the data needed to train these systems. Dependency treebanks can be created directly by human annotators or via automatic transformation from phrase-structure treebanks. ", "Bloom_type": "comprehension", "question": "What are dependency trees used for?", "options": ["To analyze syntactic relationships between words", "To create direct human annotations", "To automatically transform phrase structure treebanks into dependency treebanks", "To predict future events based on past occurrences"], "complexity": 1}, {"id": 36, "context": "The dependency-based approach to grammar is much older than the relatively recent phrase-structure or constituency grammars, which date only to the 20th century. Dependency grammar dates back to the Indian grammarian Pan. ini sometime between the 7th and 4th centuries BCE, as well as the ancient Greek linguistic traditions. Contemporary theories of dependency grammar all draw heavily on the 20th century work of Tesni`ere (1959). ", "Bloom_type": "comprehension", "question": "Which theory of dependency grammar has its roots in the 20th century?", "options": ["Tesni\u00e8re\u2019s Theory", "Panini\u2019s Grammar", "Indian Linguistic Traditions", "Greek Linguistic Traditions"], "complexity": 1}, {"id": 37, "context": "Automatic parsing using dependency grammars was first introduced into computational linguistics by early work on machine translation at the RAND Corporation led by David Hays. This work on dependency parsing closely paralleled work on constituent parsing and made explicit use of grammars to guide the parsing process. After this early period, computational work on dependency parsing remained intermittent over the following decades. Notable implementations of dependency parsers for English during this period include Link Grammar (Sleator and Temperley, 1993), Constraint Grammar (Karlsson et al., 1995), and MINIPAR (Lin, 2003). ", "Bloom_type": "comprehension", "question": "What were some notable implementations of dependency parsers for English mentioned in the context?", "options": ["Link Grammar, Constraint Grammar, and MINIPAR", "Constraint Grammar, MINIPAR, and Dependency Parsing", "Dependency Parsing, Constraint Grammar, and MINIPAR", "MINIPAR, Constraint Grammar, and Dependency Parsing"], "complexity": 1}, {"id": 38, "context": "Dependency parsing saw a major resurgence in the late 1990`s with the appearance of large dependency-based treebanks and the associated advent of data driven approaches described in this chapter. Eisner (1996) developed an efficient dynamic programming approach to dependency parsing based on bilexical grammars derived from the Penn Treebank. Covington (2001) introduced the deterministic word by word approach underlying current transition-based approaches. Yamada and Matsumoto (2003) and Kudo and Matsumoto (2002) introduced both the shift-reduce paradigm and the use of supervised machine learning in the form of support vector machines to dependency parsing. ", "Bloom_type": "comprehension", "question": "What are some key methods for dependency parsing mentioned in the context?", "options": ["Dynamic programming, bilexical grammars, and support vector machines", "Efficient algorithms, probabilistic models, and neural networks", "Word-by-word parsing, shift-reduce paradigms, and unsupervised learning", "Dependency trees, constituency parsing, and rule-based systems"], "complexity": 1}, {"id": 39, "context": "It`s also possible to use dependency parses instead of constituency parses as the basis of features, for example using dependency parse paths instead of constituency paths. ", "Bloom_type": "comprehension", "question": "What are two types of parsing methods mentioned in the context?", "options": ["Dependency and constituency parses", "Constituency and syntax parses", "Syntax and dependency parses", "Syntactic and dependency parses"], "complexity": 1}, {"id": 40, "context": "The combination of rich linguistic annotation and corpus-based approach instantiated in FrameNet and PropBank led to a revival of automatic approaches to semantic role labeling, first on FrameNet (Gildea and Jurafsky, 2000) and then on PropBank data (Gildea and Palmer, 2002, inter alia). The problem first addressed in the 1970s by handwritten rules was thus now generally recast as one of supervised machine learning enabled by large and consistent databases. Many popular features used for role labeling are defined in Gildea and Jurafsky (2002), Surdeanu et al. (2003), Xue and Palmer (2004), Pradhan et al. (2005), Che et al. (2009), and Zhao et al. (2009). The use of dependency rather than constituency parses was introduced in the CoNLL-2008 shared task (Surdeanu et al., 2008). For surveys see Palmer et al. (2010) and M`arquez et al. (2008). ", "Bloom_type": "comprehension", "question": "What methodological shift occurred in the field of semantic role labeling due to advancements in technology?", "options": ["From constituency parsing to dependency parsing", "From manual rule-based systems to automated machine learning techniques", "From annotated corpora to unlabeled datasets", "From lexical semantics to syntactic analysis"], "complexity": 1}, {"id": 41, "context": "The use of neural approaches to semantic role labeling was pioneered by Collobert et al. (2011), who applied a CRF on top of a convolutional net. Early work like Foland, Jr. and Martin (2015) focused on using dependency features. Later work eschewed syntactic features altogether; Zhou and Xu (2015b) introduced the use of a stacked (6-8 layer) biLSTM architecture, and (He et al., 2017) showed how to augment the biLSTM architecture with highway networks and also replace the CRF with A* decoding that make it possible to apply a wide variety of global constraints in SRL decoding. ", "Bloom_type": "comprehension", "question": "What approach did early works take when dealing with dependencies?", "options": ["They focused solely on dependency features.", "They ignored syntactic features entirely.", "They combined syntactic and dependency features.", "They developed their own unique method."], "complexity": 1}, {"id": 42, "context": "Parts of speech (also known as POS) and named entities are useful clues to sentence structure and meaning. Knowing whether a word is a noun or a verb tells us about likely neighboring words (nouns in English are preceded by determiners and adjectives, verbs by nouns) and syntactic structure (verbs have dependency links to nouns), making part-of-speech tagging a key aspect of parsing. Knowing if a named entity like Washington is a name of a person, a place, or a university is important to many natural language processing tasks like question answering, stance detection, or information extraction. ", "Bloom_type": "application", "question": "What does knowing the type of a word help with?", "options": ["Identifying the parts of speech", "Determining the tense of the verb", "Predicting the next word in a sentence", "Recognizing the gender of the subject"], "complexity": 2}, {"id": 43, "context": "Our focus in this chapter is context-free grammars and the CKY algorithm for parsing them. Context-free grammars are the backbone of many formal models of the syntax of natural language (and, for that matter, of computer languages). Syntactic parsing is the task of assigning a syntactic structure to a sentence. Parse trees (whether for context-free grammars or for the dependency or CCG formalisms we introduce in following chapters) can be used in applications such as grammar checking: sentence that cannot be parsed may have grammatical errors (or at least be hard to read). Parse trees can be an intermediate stage of representation for formal semantic analysis. And parsers and the grammatical structure they assign a sentence are a useful text analysis tool for text data science applications that require modeling the relationship of elements in sentences. ", "Bloom_type": "application", "question": "What is the primary goal of syntactic parsing?", "options": ["To determine the grammatical structure of the sentence", "To understand the meaning of the sentence", "To identify the parts of speech in the sentence", "To translate the sentence into another language"], "complexity": 2}, {"id": 44, "context": "Figure 19.1 on the next page shows the dependency analysis from Eq. 19.1 but visualized as a tree, alongside its corresponding phrase-structure analysis of the kind given in the prior chapter. Note the absence of nodes corresponding to phrasal constituents or lexical categories in the dependency parse; the internal structure of the dependency parse consists solely of directed relations between words. These headdependent relationships directly encode important information that is often buried in the more complex phrase-structure parses. For example, the arguments to the verb prefer are directly linked to it in the dependency structure, while their connection to the main verb is more distant in the phrase-structure tree. Similarly, morning and Denver, modifiers of flight, are linked to it directly in the dependency structure. This fact that the head-dependent relations are a good proxy for the semantic relationship between predicates and their arguments is an important reason why dependency grammars are currently more common than constituency grammars in natural language processing. ", "Bloom_type": "application", "question": "What does the dependency parse primarily focus on?", "options": ["The graphical representation of word connections", "The syntactic structure of phrases", "The semantic meaning of sentences", "The hierarchical breakdown of clauses"], "complexity": 2}, {"id": 45, "context": "The traditional linguistic notion of grammatical relation provides the basis for the binary relations that comprise these dependency structures. The arguments to these relations consist of a head and a dependent. The head plays the role of the central organizing word, and the dependent as a kind of modifier. The head-dependent relationship is made explicit by directly linking heads to the words that are immediately dependent on them. ", "Bloom_type": "application", "question": "In a sentence structure, what does the head play?", "options": ["The role of the central organizing word", "The role of the dependent word", "The role of the direct object", "The role of the prepositional phrase"], "complexity": 2}, {"id": 46, "context": "Treebanks play a critical role in the development and evaluation of dependency parsers. They are used for training parsers, they act as the gold labels for evaluating parsers, and they also provide useful information for corpus linguistics studies. ", "Bloom_type": "application", "question": "In treebanking, what is the primary purpose of using trees?", "options": ["To represent syntactic structures", "To store lexical entries", "To categorize sentence types", "To calculate grammatical features"], "complexity": 2}, {"id": 47, "context": "The largest open community project for building dependency trees is the Universal Dependencies project at https://universaldependencies.org/ introduced above, which currently has almost 200 dependency treebanks in more than 100 languages (de Marneffe et al., 2021). Here are a few UD examples showing dependency trees for sentences in Spanish, Basque, and Mandarin Chinese: ", "Bloom_type": "application", "question": "What is the primary purpose of the Universal Dependencies project?", "options": ["To build dependency trees for various languages", "To provide a platform for sharing linguistic data", "To create a comprehensive dictionary of all human languages", "To develop algorithms for natural language processing"], "complexity": 2}, {"id": 48, "context": "The specification of a transition-based parser is quite simple, based on representing the current state of the parse as a configuration: the stack, an input buffer of words or tokens, and a set of relations representing a dependency tree. Parsing means making a sequence of transitions through the space of possible configurations. We start with an initial configuration in which the stack contains the ROOT node, the buffer has the tokens in the sentence, and an empty set of relations represents the parse. In the final goal state, the stack and the word list should be empty, and the set of relations will represent the final parse. Fig. 19.5 gives the algorithm. ", "Bloom_type": "application", "question": "What does parsing mean in the context of a transition-based parser?", "options": ["Making a sequence of transitions through the space of possible configurations.", "Determining the structure of a sentence based on grammatical rules.", "Creating a dependency tree using a set of relations.", "Storing the final parse in the stack."], "complexity": 2}, {"id": 49, "context": "The oracle for greedily selecting the appropriate transition is trained by supervised machine learning. As with all supervised machine learning methods, we will need training data: configurations annotated with the correct transition to take. We can draw these from dependency trees. And we need to extract features of the configuration. We`ll introduce neural classifiers that represent the configuration via embeddings, as well as classic systems that use hand-designed features. ", "Bloom_type": "application", "question": "What type of data should be used to train the oracle for greedy selection?", "options": ["Dependency trees", "Randomly generated data", "Unannotated data", "Hand-crafted features"], "complexity": 2}, {"id": 50, "context": "Graph-based methods are the second important family of dependency parsing algorithms. Graph-based parsers are more accurate than transition-based parsers, especially on long sentences; transition-based methods have trouble when the heads are very far from the dependents (McDonald and Nivre, 2011). Graph-based methods avoid this difficulty by scoring entire trees, rather than relying on greedy local decisions. Furthermore, unlike transition-based approaches, graph-based parsers can produce non-projective trees. Although projectivity is not a significant issue for English, it is definitely a problem for many of the world`s languages. ", "Bloom_type": "application", "question": "What distinguishes graph-based dependency parsing algorithms from transition-based ones?", "options": ["Graph-based methods score entire trees instead of making local decisions.", "Transition-based methods cannot handle long sentences accurately.", "Graph-based methods always produce projective trees.", "Transition-based methods struggle with distant head-dependency relations."], "complexity": 2}, {"id": 51, "context": "Graph-based dependency parsers search through the space of possible trees for a given sentence for a tree (or trees) that maximize some score. These methods encode the search space as directed graphs and employ methods drawn from graph theory to search the space for optimal solutions. More formally, given a sentence S we`re looking for the best dependency tree in Gs, the space of all possible trees for that sentence, that maximizes some score. ", "Bloom_type": "application", "question": "In which step of the process are the dependency trees generated?", "options": ["During the application of {method1} and {method2}", "After encoding the search space as directed graphs", "Before searching the space with methods from graph theory", "After applying {method1} and using its result in {method2}"], "complexity": 2}, {"id": 52, "context": "It turns out that finding the best dependency parse for S is equivalent to finding the maximum spanning tree over G. A spanning tree over a graph G is a subset of G that is a tree and covers all the vertices in G; a spanning tree over G that starts from the ROOT is a valid parse of S. A maximum spanning tree is the spanning tree with the highest score. Thus a maximum spanning tree of G emanating from the ROOT is the optimal dependency parse for the sentence. ", "Bloom_type": "application", "question": "What does it mean when we say that finding the best dependency parse for S is equivalent to finding the maximum spanning tree over G?", "options": ["It means that the solution to the first problem provides an upper bound on the solution to the second.", "It means that both problems are identical.", "It means that solving one problem will automatically solve the other.", "It means that the solutions to these two problems can be combined."], "complexity": 2}, {"id": 53, "context": "This chapter has introduced the concept of dependency grammars and dependency parsing. Here`s a summary of the main points that we covered: ", "Bloom_type": "application", "question": "What is the primary focus when discussing dependency grammars?", "options": ["The rules governing word relationships", "The structure of sentences", "The syntax of individual words", "The morphology of sentence structures"], "complexity": 2}, {"id": 54, "context": " In dependency-based approaches to syntax, the structure of a sentence is described in terms of a set of binary relations that hold between the words in a sentence. Larger notions of constituency are not directly encoded in dependency analyses. ", "Bloom_type": "application", "question": "Which of the following best describes how dependency-based approaches represent syntactic structures?", "options": ["Through a series of binary relations that connect word-level dependencies.", "By explicitly defining all possible constituents within sentences.", "Using complex rules for determining phrase structure.", "Encoding larger notions of constituency through hierarchical relationships."], "complexity": 2}, {"id": 55, "context": " Dependency-based analysis provides information directly useful in further language processing tasks including information extraction, semantic parsing and question answering. ", "Bloom_type": "application", "question": "What is the primary purpose of dependency-based analysis?", "options": ["To improve the accuracy of natural language understanding systems.", "To enhance the efficiency of data storage for large datasets.", "To increase the speed of internet connectivity worldwide.", "To reduce the carbon footprint of electronic devices."], "complexity": 2}, {"id": 56, "context": " Treebanks provide the data needed to train these systems. Dependency treebanks can be created directly by human annotators or via automatic transformation from phrase-structure treebanks. ", "Bloom_type": "application", "question": "What is the primary difference between creating dependency treebanks manually and automatically?", "options": ["Manual creation provides better control over the annotation process.", "Manual creation allows for more accurate results.", "Automatic creation requires less time than manual creation.", "Automatic creation cannot handle complex sentences."], "complexity": 2}, {"id": 57, "context": "The dependency-based approach to grammar is much older than the relatively recent phrase-structure or constituency grammars, which date only to the 20th century. Dependency grammar dates back to the Indian grammarian Pan. ini sometime between the 7th and 4th centuries BCE, as well as the ancient Greek linguistic traditions. Contemporary theories of dependency grammar all draw heavily on the 20th century work of Tesni`ere (1959). ", "Bloom_type": "application", "question": "Which theory of dependency grammar is most closely associated with the works of Tesni\u00e8re?", "options": ["The dependency-based approach", "The phrase-structure approach", "The constituency approach", "The generative approach"], "complexity": 2}, {"id": 58, "context": "Automatic parsing using dependency grammars was first introduced into computational linguistics by early work on machine translation at the RAND Corporation led by David Hays. This work on dependency parsing closely paralleled work on constituent parsing and made explicit use of grammars to guide the parsing process. After this early period, computational work on dependency parsing remained intermittent over the following decades. Notable implementations of dependency parsers for English during this period include Link Grammar (Sleator and Temperley, 1993), Constraint Grammar (Karlsson et al., 1995), and MINIPAR (Lin, 2003). ", "Bloom_type": "application", "question": "What is the primary method used in dependency parsing?", "options": ["Dependency Grammars", "Constituent Parsing", "Constraint Grammars", "Rule-Based Parsing"], "complexity": 2}, {"id": 59, "context": "Dependency parsing saw a major resurgence in the late 1990`s with the appearance of large dependency-based treebanks and the associated advent of data driven approaches described in this chapter. Eisner (1996) developed an efficient dynamic programming approach to dependency parsing based on bilexical grammars derived from the Penn Treebank. Covington (2001) introduced the deterministic word by word approach underlying current transition-based approaches. Yamada and Matsumoto (2003) and Kudo and Matsumoto (2002) introduced both the shift-reduce paradigm and the use of supervised machine learning in the form of support vector machines to dependency parsing. ", "Bloom_type": "application", "question": "What is a key method used for improving efficiency in dependency parsing?", "options": ["Adopting data-driven techniques", "Using static grammars only", "Implementing rule-based systems exclusively", "Employing heuristic algorithms"], "complexity": 2}, {"id": 60, "context": "It`s also possible to use dependency parses instead of constituency parses as the basis of features, for example using dependency parse paths instead of constituency paths. ", "Bloom_type": "application", "question": "Which method can be used to replace constituency paths with dependency paths?", "options": ["Transform constituency paths into dependency paths.", "Replace constituency paths with dependency paths directly.", "Use dependency parses as the basis of features.", "Create new dependency paths based on constituency paths."], "complexity": 2}, {"id": 61, "context": "The combination of rich linguistic annotation and corpus-based approach instantiated in FrameNet and PropBank led to a revival of automatic approaches to semantic role labeling, first on FrameNet (Gildea and Jurafsky, 2000) and then on PropBank data (Gildea and Palmer, 2002, inter alia). The problem first addressed in the 1970s by handwritten rules was thus now generally recast as one of supervised machine learning enabled by large and consistent databases. Many popular features used for role labeling are defined in Gildea and Jurafsky (2002), Surdeanu et al. (2003), Xue and Palmer (2004), Pradhan et al. (2005), Che et al. (2009), and Zhao et al. (2009). The use of dependency rather than constituency parses was introduced in the CoNLL-2008 shared task (Surdeanu et al., 2008). For surveys see Palmer et al. (2010) and M`arquez et al. (2008). ", "Bloom_type": "application", "question": "What is the primary method used for semantic role labeling?", "options": ["Machine learning", "Rule-based methods", "Dependency parsing", "Constituent parsing"], "complexity": 2}, {"id": 62, "context": "The use of neural approaches to semantic role labeling was pioneered by Collobert et al. (2011), who applied a CRF on top of a convolutional net. Early work like Foland, Jr. and Martin (2015) focused on using dependency features. Later work eschewed syntactic features altogether; Zhou and Xu (2015b) introduced the use of a stacked (6-8 layer) biLSTM architecture, and (He et al., 2017) showed how to augment the biLSTM architecture with highway networks and also replace the CRF with A* decoding that make it possible to apply a wide variety of global constraints in SRL decoding. ", "Bloom_type": "application", "question": "What is the main difference between early and later works in semantic role labeling?", "options": ["Later works replaced CRF with A* decoding.", "Early works used only syntactic features.", "Later works discarded all previous methods.", "Later works combined both syntactic and dependency features."], "complexity": 2}]}, "coreference": {"max_id": 96, "Questions": [{"id": 0, "context": "Finally, we`ll need to compare words and other strings. We`ll introduce a metric called edit distance that measures how similar two strings are based on the number of edits (insertions, deletions, substitutions) it takes to change one string into the other. Edit distance is an algorithm with applications throughout language processing, from spelling correction to speech recognition to coreference resolution. ", "Bloom_type": "remember", "question": "In what field does the concept of edit distance find application beyond language processing?", "options": ["Artificial Intelligence", "Computer Science", "Mathematics", "Physics"], "complexity": 0}, {"id": 1, "context": "At the same time in the early 1990s logistic regression was developed and applied to NLP at IBM Research under the name maximum entropy modeling or maxent (Berger et al., 1996), seemingly independent of the statistical literature. Under that name it was applied to language modeling (Rosenfeld, 1996), part-of-speech tagging (Ratnaparkhi, 1996), parsing (Ratnaparkhi, 1997), coreference resolution (Kehler, 1997b), and text classification (Nigam et al., 1999). ", "Bloom_type": "remember", "question": "In what year did logistic regression begin its application in natural language processing?", "options": ["1990", "1985", "1992", "1994"], "complexity": 0}, {"id": 2, "context": "conclude with linguistic structures that tend to be related to discourse and meaning over larger texts, including coreference and discourse coherence. In each case we`ll give algorithms for automatically annotating the relevant structure. ", "Bloom_type": "remember", "question": "In which type of linguistic structures are concluded methods focused on?", "options": ["semantic relations", "grammatical patterns", "syntactic rules", "lexical items"], "complexity": 0}, {"id": 3, "context": "One common approach is to make repeated rule-based passes over a text, starting with rules with very high precision but low recall, and, in subsequent stages, using machine learning methods that take the output of the first pass into account (an approach first worked out for coreference (Lee et al., 2017a)): ", "Bloom_type": "remember", "question": "In what method are repeated rule-based passes over a text made?", "options": ["Both A and B", "Machine learning", "Rule-based", "Neither A nor B"], "complexity": 0}, {"id": 4, "context": "Highly inflectional languages also have much more information than English coded in word morphology, like case (nominative, accusative, genitive) or gender (masculine, feminine). Because this information is important for tasks like parsing and coreference resolution, part-of-speech taggers for morphologically rich lan", "Bloom_type": "remember", "question": "In highly inflected languages, what additional linguistic feature besides case and gender provides crucial information for parsing and resolving coreference?", "options": ["Number", "Tense", "Person", "Voice"], "complexity": 0}, {"id": 5, "context": "The result of processing these two sentences is the five draft templates (Fig. 20.21) that must then be merged into the single hierarchical structure shown in Fig. 20.19. The merging algorithm, after performing coreference resolution, merges two activities that are likely to be describing the same events. ", "Bloom_type": "remember", "question": "What does the merging algorithm do after performing coreference resolution?", "options": ["It combines similar activities.", "It creates new templates.", "It deletes unnecessary activities.", "It separates different events."], "complexity": 0}, {"id": 6, "context": "This results in a (S,A,P) tuple for a given entity mention; To get scores for the representation of an entity in a complete document, we can run coreference resolution and average the (S,A,P) scores for all the mentions. Fig. 22.13 shows the scores from their algorithm for characters from the movie The Dark Knight when run on Wikipedia plot summary texts with gold coreference. ", "Bloom_type": "remember", "question": "In the context provided, what is the purpose of running coreference resolution?", "options": ["To score representations of entities in documents", "To identify synonyms", "To find antonyms", "To calculate sentence lengths"], "complexity": 0}, {"id": 7, "context": "Coreference is an important component of natural language processing. A dialogue system that has just told the user There is a 2pm flight on United and a 4pm one on Cathay Pacific must know which flight the user means by I`ll take the second one. A question answering system that uses Wikipedia to answer a question about Marie Curie must know who she was in the sentence She was born in Warsaw. And a machine translation system translating from a language like Spanish, in which pronouns can be dropped, must use coreference from the previous sentence to decide whether the Spanish sentence Me encanta el conocimiento, dice.` should be translated as I love knowledge, he says`, or I love knowledge, she says`. Indeed, this example comes from an actual news article in El Pas about a female professor and was mistranslated as he in machine translation because of inaccurate coreference resolution (Schiebinger, 2013). ", "Bloom_type": "remember", "question": "In what way does coreference play a crucial role in natural language processing?", "options": ["It assists machines in determining the referents of pronouns.", "It helps machines understand the meaning of sentences.", "It allows machines to recognize synonyms in different contexts.", "It enables machines to identify the gender of speakers in conversations."], "complexity": 0}, {"id": 8, "context": "In this chapter we focus on the task of coreference resolution. Coreference resolution is the task of determining whether two mentions corefer, by which we mean they refer to the same entity in the discourse model (the same discourse entity). The set of coreferring expressions is often called a coreference chain or a cluster. For example, in processing (23.1), a coreference resolution algorithm would need to find at least four coreference chains, corresponding to the four entities in the discourse model in Fig. 23.1. ", "Bloom_type": "remember", "question": "In the context of coreference resolution, what does it mean when two mentions are said to corefer?", "options": ["They are referring to the same entity in the discourse model.", "They are mentioned together in the same sentence.", "They are both pronouns used within the same clause.", "They are referring to different but similar entities."], "complexity": 0}, {"id": 9, "context": "Note that mentions can be nested; for example the mention her is syntactically part of another mention, her pay, referring to a completely different discourse entity. Coreference resolution thus comprises two tasks (although they are often performed jointly): (1) identifying the mentions, and (2) clustering them into coreference chains/discourse entities. ", "Bloom_type": "remember", "question": "In the context provided, what does the term \"coreference\" refer to?", "options": ["The clustering of mentions into groups based on their semantic relationship", "The process of resolving mentions within a sentence", "The identification of all possible mentions in a document", "The act of mentioning someone by name"], "complexity": 0}, {"id": 10, "context": "Before turning to algorithms, however, we mention some important tasks we will only touch on briefly at the end of this chapter. First are the famous Winograd Schema problems (so-called because they were first pointed out by Terry Winograd in his dissertation). These entity coreference resolution problems are designed to be too difficult to be solved by the resolution methods we describe in this chapter, and the kind of real-world knowledge they require has made them a kind of challenge task for natural language processing. For example, consider the task of determining the correct antecedent of the pronoun they in the following example: (23.2) The city council denied the demonstrators a permit because ", "Bloom_type": "remember", "question": "What is an example of an entity coreference resolution problem mentioned in the context?", "options": ["The city council denied the demonstrators a permit because.", "Terry Winograd in his dissertation", "entity coreference resolution problems", "real-world knowledge"], "complexity": 0}, {"id": 11, "context": "We can formulate the task of coreference resolution as follows: Given a text T , find all entities and the coreference links between them. We evaluate our task by comparing the links our system creates with those in human-created gold coreference annotations on T . ", "Bloom_type": "remember", "question": "In the context, what is the process of finding all entities and their coreference links within a text?", "options": ["Coreference resolution", "Entity linking", "Dependency parsing", "Named entity recognition"], "complexity": 0}, {"id": 12, "context": "For most coreference evaluation campaigns, the input to the system is the raw text of articles, and systems must detect mentions and then link them into clusters. Solving this task requires dealing with pronominal anaphora (figuring out that her refers to Victoria Chen), filtering out non-referential pronouns like the pleonastic It in It has been ten years), dealing with definite noun phrases to figure out that the 38-year-old is coreferent with Victoria Chen, and that the company is the same as Megabucks. And we need to deal with names, to realize that Megabucks is the same as Megabucks Banking. ", "Bloom_type": "remember", "question": "In a coreference evaluation campaign, what type of pronoun does not refer back to any previous mention?", "options": ["Indefinite pronouns", "Pronouns", "Definite nouns", "Names"], "complexity": 0}, {"id": 13, "context": "A number of corpora mark richer discourse phenomena. The ISNotes corpus annotates a portion of OntoNotes for information status, include bridging examples (Hou et al., 2018). The LitBank coreference corpus (Bamman et al., 2020) contains coreference annotations for 210,532 tokens from 100 different literary novels, including singletons and quantified and negated noun phrases. The AnCora-CO coreference corpus (Recasens and Mart, 2010) contains 400,000 words each of Spanish (AnCora-CO-Es) and Catalan (AnCora-CO-Ca) news data, and includes labels for complex phenomena like discourse deixis in both languages. The ARRAU corpus (Uryupina et al., 2020) contains 350,000 words of English marking all NPs, which means singleton clusters are available. ARRAU includes diverse genres like dialog (the TRAINS data) and fiction (the Pear Stories), and has labels for bridging references, discourse deixis, generics, and ambiguous anaphoric relations. ", "Bloom_type": "remember", "question": "Which corpus is annotated with rich discourse phenomena?", "options": ["LitBank", "ISNotes", "AnCora-CO", "ARRAU"], "complexity": 0}, {"id": 14, "context": "The modern approach is instead to perform mention detection, anaphoricity, and coreference jointly in a single end-to-end model (Ng 2005b, Denis and Baldridge 2007, Rahman and Ng 2009). For example mention detection in the Lee et al. (2017b),2018 system is based on a single end-to-end neural network that computes a score for each mention being referential, a score for two mentions being coreference, and combines them to make a decision, training all these scores with a single end-to-end loss. We`ll describe this method in detail in Section 23.6. 7 ", "Bloom_type": "remember", "question": "In what way are mention detection, anaphoricity, and coreference analyzed together in the modern approach?", "options": ["They are analyzed through a single end-to-end model.", "They are analyzed separately.", "They are analyzed using different models.", "They are analyzed by performing tasks individually."], "complexity": 0}, {"id": 15, "context": "We begin with the mention-pair architecture, the simplest and most influential coreference architecture, which introduces many of the features of more complex algorithms, even though other architectures perform better. The mention-pair architecture is based around a classifier that as its name suggestsis given a pair of mentions, a candidate anaphor and a candidate antecedent, and makes a binary classification decision: coreferring or not. ", "Bloom_type": "remember", "question": "Which architectural feature does the mention-pair architecture introduce?", "options": ["Coreference resolution", "Dependency parsing", "Named entity recognition", "Part-of-speech tagging"], "complexity": 0}, {"id": 16, "context": "In early formulations, for mention i, the classifier decides which of the prior mentions is the antecedent (Denis and Baldridge, 2008). But suppose i is 1 in fact not anaphoric, and none of the antecedents should be chosen? Such a model would need to run a separate anaphoricity classifier on i. Instead, it turns out to be better to jointly learn anaphoricity detection and coreference together with a single loss (Rahman and Ng, 2009). ", "Bloom_type": "remember", "question": "In the context provided, what does the term \"coreference\" refer to?", "options": ["The method used by models to decide which mention is the antecedent", "The process of assigning labels to mentions", "The ability of classifiers to detect anaphoric references", "The way in which mentions are grouped into clusters"], "complexity": 0}, {"id": 17, "context": "However, there are a wide variety of methods for doing this comparison. In fact, there are 5 common metrics used to evaluate coreference algorithms: the link based MUC (Vilain et al., 1995) and BLANC (Recasens and Hovy 2011, Luo et al. 2014) metrics, the mention based B3 metric (Bagga and Baldwin, 1998), the entity based CEAF metric (Luo, 2005), and the link based entity aware LEA metric (Moosavi and Strube, 2016). ", "Bloom_type": "remember", "question": "Which method is not mentioned as one of the five common metrics used to evaluate coreference algorithms?", "options": ["The link based MUC", "The mention based B3", "The entity based CEAF", "The link based entity aware LEA"], "complexity": 0}, {"id": 18, "context": "Alternative metrics have been proposed that deal with particular coreference domains or tasks. For example, consider the task of resolving mentions to named entities (persons, organizations, geopolitical entities), which might be useful for information extraction or knowledge base completion. A hypothesis chain that correctly contains all the pronouns referring to an entity, but has no version of the name itself, or is linked with a wrong name, is not useful for this task. We might instead want a metric that weights each mention by how informative it is (with names being most informative) (Chen and Ng, 2013) or a metric that considers a hypothesis to match a gold chain only if it contains at least one variant of a name (the NEC F1 metric of Agarwal et al. (2019)). ", "Bloom_type": "remember", "question": "Which alternative metric focuses on weighting mentions based on their informativeness?", "options": ["The weighted mention metric", "The NEC F1 metric", "The hypothesis chain metric", "The name-based metric"], "complexity": 0}, {"id": 19, "context": "One possible source of this bias is that female entities are significantly underrepresented in the OntoNotes dataset, used to train most coreference systems. Zhao et al. (2018a) propose a way to overcome this bias: they generate a second gender-swapped dataset in which all male entities in OntoNotes are replaced with female ones and vice versa, and retrain coreference systems on the combined original and swapped OntoNotes data, also using debiased GloVE embeddings (Bolukbasi et al., 2016). The resulting coreference systems no longer exhibit bias on the WinoBias dataset, without significantly impacting OntoNotes coreference accuracy. In a follow-up paper, Zhao et al. (2019) show that the same biases exist in ELMo contextualized word vector representations and coref systems that use them. They showed that retraining ELMo with data augmentation again reduces or removes bias in coreference systems on WinoBias. ", "Bloom_type": "remember", "question": "In what year did Zhao et al. publish their follow-up paper showing how retraining ELMo can reduce bias in coreference systems?", "options": ["2019", "2017", "2018", "2020"], "complexity": 0}, {"id": 20, "context": "Webster et al. (2018) introduces another dataset, GAP, and the task of Gendered Pronoun Resolution as a tool for developing improved coreference algorithms for gendered pronouns. GAP is a gender-balanced labeled corpus of 4,454 sentences with gendered ambiguous pronouns (by contrast, only 20% of the gendered pronouns in the English OntoNotes training data are feminine). The examples were created by drawing on naturally occurring sentences from Wikipedia pages to create hard to resolve cases with two named entities of the same gender and an ambiguous pronoun that may refer to either person (or neither), like the following: ", "Bloom_type": "remember", "question": "What does GAP stand for?", "options": ["Gender Balanced Corpus", "Gender Ambiguous Pronoun", "Generalized Argument Pair", "Glossary of Analytical Procedures"], "complexity": 0}, {"id": 21, "context": " This is the task of linking together mentions in text which corefer, i.e. refer to the same discourse entity in the discourse model, resulting in a set of coreference chains (also called clusters or entities). ", "Bloom_type": "remember", "question": "In the context provided, what does the term \"coreference\" refer to?", "options": ["The creation of coreference chains", "The process of linking mentions within sentences", "The identification of synonyms in a sentence", "The separation of different discourse entities"], "complexity": 0}, {"id": 22, "context": " Some NPs are not referring expressions, such as pleonastic it in It is raining.  Many corpora have human-labeled coreference annotations that can be used for supervised learning, including OntoNotes for English, Chinese, and Arabic, ARRAU for English, and AnCora for Spanish and Catalan. ", "Bloom_type": "remember", "question": "What do some NP examples of non-referencing expressions include?", "options": ["It", "The sun", "He", "She"], "complexity": 0}, {"id": 23, "context": " Mention detection can start with all nouns and named entities and then use anaphoricity classifiers or referentiality classifiers to filter out non-mentions.  Three common architectures for coreference are mention-pair, mention-rank, and entity-based, each of which can make use of feature-based or neural classifiers. ", "Bloom_type": "remember", "question": "Which architecture is not commonly used for detecting mentions?", "options": ["feature-based", "mention-pair", "entity-based", "mention-rank"], "complexity": 0}, {"id": 24, "context": "tection and coreference in a single end-to-end architecture. ", "Bloom_type": "remember", "question": "In an end-to-end architecture, what is the relationship between detection and coreference?", "options": ["Coreference helps improve detection accuracy.", "Detection improves coreference efficiency.", "Coreference enhances detection speed.", "Detection and coreference are unrelated processes."], "complexity": 0}, {"id": 25, "context": " The Winograd Schema Challenge problems are difficult coreference problems that seem to require world knowledge or sophisticated reasoning to solve.  Coreference systems exhibit gender bias which can be evaluated using datasets ", "Bloom_type": "remember", "question": "What is a characteristic of coreference systems as mentioned in the context?", "options": ["They show no gender bias.", "They only use basic logical reasoning.", "They do not require any external data.", "They cannot be evaluated with datasets."], "complexity": 0}, {"id": 26, "context": "Coreference competitions as part of the US DARPA-sponsored MUC conferences provided early labeled coreference datasets (the 1995 MUC-6 and 1998 MUC7 corpora), and set the tone for much later work, choosing to focus exclusively on the simplest cases of identity coreference (ignoring difficult cases like bridging, metonymy, and part-whole) and drawing the community toward supervised machine learning and metrics like the MUC metric (Vilain et al., 1995). The later ACE evaluations produced labeled coreference corpora in English, Chinese, and Arabic that were widely used for model training and evaluation. ", "Bloom_type": "remember", "question": "In which conference did the first labeled coreference dataset originate?", "options": ["NAACL", "ACL", "EMNLP", "IJCNLP"], "complexity": 0}, {"id": 27, "context": "The move from mention-pair to mention-ranking approaches was pioneered by Yang et al. (2003) and Iida et al. (2003) who proposed pairwise ranking methods, then extended by Denis and Baldridge (2008) who proposed to do ranking via a softmax over all prior mentions. The idea of doing mention detection, anaphoricity, and coreference jointly in a single end-to-end model grew out of the early proposal of Ng (2005b) to use a dummy antecedent for mention-ranking, allowing non-referential` to be a choice for coreference classifiers, Denis and Baldridge`s 2007 joint system combining anaphoricity classifier probabilities with coreference probabilities, the Denis and Baldridge (2008) ranking model, and the Rahman and Ng (2009) proposal to train the two models jointly with a single objective. ", "Bloom_type": "remember", "question": "In which year did Denis and Baldridge propose their ranking model?", "options": ["2008", "1995", "2004", "2006"], "complexity": 0}, {"id": 28, "context": "Coreference is also related to the task of entity linking discussed in Chapter 14. Coreference can help entity linking by giving more possible surface forms to help link to the right Wikipedia page, and conversely entity linking can help improve coreference resolution. Consider this example from Hajishirzi et al. (2013): ", "Bloom_type": "remember", "question": "In what way does coreference relate to entity linking?", "options": ["Coreference provides additional surface forms for linking entities to their Wikipedia pages.", "Coreference helps with the identification of entities on a Wikipedia page.", "Entity linking improves the accuracy of coreference resolution.", "Entity linking enhances the understanding of coreference rules."], "complexity": 0}, {"id": 29, "context": "Integrating entity linking into coreference can help draw encyclopedic knowledge (like the fact that Donald Tsang is a president) to help disambiguate the mention the President. Ponzetto and Strube (2006) 2007 and Ratinov and Roth (2012) showed that such attributes extracted from Wikipedia pages could be used to build richer models of entity mentions in coreference. More recent research shows how to do linking and coreference jointly (Hajishirzi et al. 2013, Zheng et al. 2013) or even jointly with named entity tagging as well (Durrett and Klein 2014). ", "Bloom_type": "remember", "question": "In what way does integrating entity linking into coreference improve the ability to disambiguate mentions?", "options": ["By using attributes extracted from Wikipedia pages to enhance coreference resolution", "By focusing solely on the textual similarity between entities", "By reducing the need for named entity tagging", "Both A and B are correct"], "complexity": 0}, {"id": 30, "context": "Fig. 24.8 from Barzilay and Lapata (2008) shows a grid for the text shown in Fig. 24.9. There is one row for each of the six sentences. The second column, for the entity trial`, is O    X, showing that the trial appears in the first sentence as direct object, in the last sentence as an oblique, and does not appear in the middle sentences. The third column, for the entity Microsoft, shows that it appears as subject in sentence 1 (it also appears as the object of the preposition against, but entities that appear multiple times are recorded with their highest-ranked grammatical function). Computing the entity grids requires extracting entities and doing coreference ", "Bloom_type": "remember", "question": "In the context provided, what process involves identifying which mentions refer to the same entity across different parts of speech?", "options": ["Coreference resolution", "Entity extraction", "Dependency parsing", "Part-of-speech tagging"], "complexity": 0}, {"id": 31, "context": "Finally, we`ll need to compare words and other strings. We`ll introduce a metric called edit distance that measures how similar two strings are based on the number of edits (insertions, deletions, substitutions) it takes to change one string into the other. Edit distance is an algorithm with applications throughout language processing, from spelling correction to speech recognition to coreference resolution. ", "Bloom_type": "comprehension", "question": "What does the term 'coreference resolution' refer to?", "options": ["The process of identifying and resolving references within sentences.", "The comparison of strings for similarity using edit distance.", "The measurement of how close two strings are by counting insertions, deletions, and substitutions.", "The introduction of a metric that calculates the number of changes needed between two strings."], "complexity": 1}, {"id": 32, "context": "Much of natural language processing is concerned with measuring how similar two strings are. For example in spelling correction, the user typed some erroneous stringlet`s say graffeand we want to know what the user meant. The user probably intended a word that is similar to graffe. Among candidate similar words, the word giraffe, which differs by only one letter from graffe, seems intuitively to be more similar than, say grail or graf, which differ in more letters. Another example comes from coreference, the task of deciding whether two strings such as the following refer to the same entity: ", "Bloom_type": "comprehension", "question": "Explain the concept of coreference in natural language processing?", "options": ["Coreference is about determining if two strings represent the same person or thing in a sentence.", "Coreference refers to the similarity between two strings based on their length.", "Coreference involves comparing the meanings of two strings using spell check tools.", "Coreference measures the frequency of occurrence of two strings within a document."], "complexity": 1}, {"id": 33, "context": "At the same time in the early 1990s logistic regression was developed and applied to NLP at IBM Research under the name maximum entropy modeling or maxent (Berger et al., 1996), seemingly independent of the statistical literature. Under that name it was applied to language modeling (Rosenfeld, 1996), part-of-speech tagging (Ratnaparkhi, 1996), parsing (Ratnaparkhi, 1997), coreference resolution (Kehler, 1997b), and text classification (Nigam et al., 1999). ", "Bloom_type": "comprehension", "question": "What did the development of logistic regression lead to being named after?", "options": ["Maximum Entropy Modeling", "Logistic Regression", "Coreference Resolution", "Text Classification"], "complexity": 1}, {"id": 34, "context": "conclude with linguistic structures that tend to be related to discourse and meaning over larger texts, including coreference and discourse coherence. In each case we`ll give algorithms for automatically annotating the relevant structure. ", "Bloom_type": "comprehension", "question": "Explain how coreference works within larger texts?", "options": ["Coreference involves the use of algorithms to identify and annotate relevant structures.", "Coreference refers to the automatic annotation of linguistic structures.", "Coreference is unrelated to discourse and meaning.", "Coreference deals with the automatic identification of all linguistic structures."], "complexity": 1}, {"id": 35, "context": "One common approach is to make repeated rule-based passes over a text, starting with rules with very high precision but low recall, and, in subsequent stages, using machine learning methods that take the output of the first pass into account (an approach first worked out for coreference (Lee et al., 2017a)): ", "Bloom_type": "comprehension", "question": "Explain how coreference resolution works in natural language processing?", "options": ["Coreference resolution involves identifying and linking together mentions of the same entity within a sentence.", "It refers to the process of finding synonyms for each word in a sentence.", "Coreference resolution uses statistical models only, ignoring previous iterations.", "Coreference resolution is unrelated to machine learning techniques."], "complexity": 1}, {"id": 36, "context": "Highly inflectional languages also have much more information than English coded in word morphology, like case (nominative, accusative, genitive) or gender (masculine, feminine). Because this information is important for tasks like parsing and coreference resolution, part-of-speech taggers for morphologically rich lan", "Bloom_type": "comprehension", "question": "What does the additional information in highly inflectional languages contribute to?", "options": ["Both A and B", "Parsing", "Coreference Resolution", "Neither A nor B"], "complexity": 1}, {"id": 37, "context": "The result of processing these two sentences is the five draft templates (Fig. 20.21) that must then be merged into the single hierarchical structure shown in Fig. 20.19. The merging algorithm, after performing coreference resolution, merges two activities that are likely to be describing the same events. ", "Bloom_type": "comprehension", "question": "What does the merging algorithm do after performing coreference resolution?", "options": ["It identifies and merges activities that refer to the same event.", "It combines all the activities described in the sentences.", "It separates activities that describe different events.", "It creates new activities based on the existing ones."], "complexity": 1}, {"id": 38, "context": "This results in a (S,A,P) tuple for a given entity mention; To get scores for the representation of an entity in a complete document, we can run coreference resolution and average the (S,A,P) scores for all the mentions. Fig. 22.13 shows the scores from their algorithm for characters from the movie The Dark Knight when run on Wikipedia plot summary texts with gold coreference. ", "Bloom_type": "comprehension", "question": "What does running coreference resolution do?", "options": ["It calculates the score for each mention.", "It identifies entities within sentences.", "It determines the relationship between different mentions.", "It predicts future events based on past occurrences."], "complexity": 1}, {"id": 39, "context": "Coreference is an important component of natural language processing. A dialogue system that has just told the user There is a 2pm flight on United and a 4pm one on Cathay Pacific must know which flight the user means by I`ll take the second one. A question answering system that uses Wikipedia to answer a question about Marie Curie must know who she was in the sentence She was born in Warsaw. And a machine translation system translating from a language like Spanish, in which pronouns can be dropped, must use coreference from the previous sentence to decide whether the Spanish sentence Me encanta el conocimiento, dice.` should be translated as I love knowledge, he says`, or I love knowledge, she says`. Indeed, this example comes from an actual news article in El Pas about a female professor and was mistranslated as he in machine translation because of inaccurate coreference resolution (Schiebinger, 2013). ", "Bloom_type": "comprehension", "question": "Explain how coreference plays a role in different types of AI systems mentioned in the context?", "options": ["Coreference helps machines understand the meaning of sentences better.", "Coreference allows machines to remember past conversations more effectively.", "Coreference enables machines to predict future events based on current data.", "Coreference ensures that all parts of speech are correctly identified."], "complexity": 1}, {"id": 40, "context": "In this chapter we focus on the task of coreference resolution. Coreference resolution is the task of determining whether two mentions corefer, by which we mean they refer to the same entity in the discourse model (the same discourse entity). The set of coreferring expressions is often called a coreference chain or a cluster. For example, in processing (23.1), a coreference resolution algorithm would need to find at least four coreference chains, corresponding to the four entities in the discourse model in Fig. 23.1. ", "Bloom_type": "comprehension", "question": "What does coreference resolution involve?", "options": ["Determining if two mentions are referring to the same entity in the discourse model", "Finding all possible entities in the discourse model", "Identifying clusters of corefering expressions", "Processing sentences to identify coreferences"], "complexity": 1}, {"id": 41, "context": "Note that mentions can be nested; for example the mention her is syntactically part of another mention, her pay, referring to a completely different discourse entity. Coreference resolution thus comprises two tasks (although they are often performed jointly): (1) identifying the mentions, and (2) clustering them into coreference chains/discourse entities. ", "Bloom_type": "comprehension", "question": "What does coreference resolution involve?", "options": ["Clustering mentions into groups based on their relationships", "Identifying mentions and resolving conflicts between them", "Determining the meaning of each individual word", "Finding synonyms for each mentioned entity"], "complexity": 1}, {"id": 42, "context": "Before turning to algorithms, however, we mention some important tasks we will only touch on briefly at the end of this chapter. First are the famous Winograd Schema problems (so-called because they were first pointed out by Terry Winograd in his dissertation). These entity coreference resolution problems are designed to be too difficult to be solved by the resolution methods we describe in this chapter, and the kind of real-world knowledge they require has made them a kind of challenge task for natural language processing. For example, consider the task of determining the correct antecedent of the pronoun they in the following example: (23.2) The city council denied the demonstrators a permit because ", "Bloom_type": "comprehension", "question": "What type of problem does the Winograd Schema belong to?", "options": ["Entity coreference resolution", "Syntax parsing", "Semantic role labeling", "Dependency parsing"], "complexity": 1}, {"id": 43, "context": "We can formulate the task of coreference resolution as follows: Given a text T , find all entities and the coreference links between them. We evaluate our task by comparing the links our system creates with those in human-created gold coreference annotations on T . ", "Bloom_type": "comprehension", "question": "What does the task of coreference resolution involve?", "options": ["Identifying entities within a text and linking them together based on their relationships.", "Determining which parts of speech are most frequently used in a sentence.", "Finding synonyms for each word in the text.", "Calculating the frequency of each entity in the text."], "complexity": 1}, {"id": 44, "context": "For most coreference evaluation campaigns, the input to the system is the raw text of articles, and systems must detect mentions and then link them into clusters. Solving this task requires dealing with pronominal anaphora (figuring out that her refers to Victoria Chen), filtering out non-referential pronouns like the pleonastic It in It has been ten years), dealing with definite noun phrases to figure out that the 38-year-old is coreferent with Victoria Chen, and that the company is the same as Megabucks. And we need to deal with names, to realize that Megabucks is the same as Megabucks Banking. ", "Bloom_type": "comprehension", "question": "What are some challenges faced when performing coreference evaluation?", "options": ["Dealing with pronominal anaphora, filtering out non-referential pronouns, and handling definite noun phrases.", "Solving for the input type, detecting mentions, and linking them into clusters.", "Identifying the article usage and understanding the context.", "None of the above"], "complexity": 1}, {"id": 45, "context": "A number of corpora mark richer discourse phenomena. The ISNotes corpus annotates a portion of OntoNotes for information status, include bridging examples (Hou et al., 2018). The LitBank coreference corpus (Bamman et al., 2020) contains coreference annotations for 210,532 tokens from 100 different literary novels, including singletons and quantified and negated noun phrases. The AnCora-CO coreference corpus (Recasens and Mart, 2010) contains 400,000 words each of Spanish (AnCora-CO-Es) and Catalan (AnCora-CO-Ca) news data, and includes labels for complex phenomena like discourse deixis in both languages. The ARRAU corpus (Uryupina et al., 2020) contains 350,000 words of English marking all NPs, which means singleton clusters are available. ARRAU includes diverse genres like dialog (the TRAINS data) and fiction (the Pear Stories), and has labels for bridging references, discourse deixis, generics, and ambiguous anaphoric relations. ", "Bloom_type": "comprehension", "question": "Which corpus provides annotated coreference annotations for a large dataset of literary novels?", "options": ["LitBank coreference corpus", "ISNotes corpus", "AnCora-CO coreference corpus", "ARRAU corpus"], "complexity": 1}, {"id": 46, "context": "The modern approach is instead to perform mention detection, anaphoricity, and coreference jointly in a single end-to-end model (Ng 2005b, Denis and Baldridge 2007, Rahman and Ng 2009). For example mention detection in the Lee et al. (2017b),2018 system is based on a single end-to-end neural network that computes a score for each mention being referential, a score for two mentions being coreference, and combines them to make a decision, training all these scores with a single end-to-end loss. We`ll describe this method in detail in Section 23.6. 7 ", "Bloom_type": "comprehension", "question": "What does the modern approach involve when it comes to detecting mentions and their relationships?", "options": ["mention detection, anaphoricity, and coreference together", "mention detection only", "anaphoricity only", "coreference only"], "complexity": 1}, {"id": 47, "context": "We begin with the mention-pair architecture, the simplest and most influential coreference architecture, which introduces many of the features of more complex algorithms, even though other architectures perform better. The mention-pair architecture is based around a classifier that as its name suggestsis given a pair of mentions, a candidate anaphor and a candidate antecedent, and makes a binary classification decision: coreferring or not. ", "Bloom_type": "comprehension", "question": "Explain how the mention-pair architecture differs from other coreference architectures?", "options": ["The mention-pair architecture uses a different algorithm for making decisions.", "Other coreference architectures are simpler than the mention-pair architecture.", "The mention-pair architecture focuses solely on pairs of mentions.", "Coreference architectures do not involve classifiers."], "complexity": 1}, {"id": 48, "context": "In early formulations, for mention i, the classifier decides which of the prior mentions is the antecedent (Denis and Baldridge, 2008). But suppose i is 1 in fact not anaphoric, and none of the antecedents should be chosen? Such a model would need to run a separate anaphoricity classifier on i. Instead, it turns out to be better to jointly learn anaphoricity detection and coreference together with a single loss (Rahman and Ng, 2009). ", "Bloom_type": "comprehension", "question": "What does the response suggest about how models handle non-anaphoric mentions?", "options": ["Models can use coreference resolution alongside anaphoricity detection.", "Models can only detect anaphoricity.", "Models cannot handle non-anaphoric mentions at all.", "The response provides no insight into handling non-anaphoric mentions."], "complexity": 1}, {"id": 49, "context": "However, there are a wide variety of methods for doing this comparison. In fact, there are 5 common metrics used to evaluate coreference algorithms: the link based MUC (Vilain et al., 1995) and BLANC (Recasens and Hovy 2011, Luo et al. 2014) metrics, the mention based B3 metric (Bagga and Baldwin, 1998), the entity based CEAF metric (Luo, 2005), and the link based entity aware LEA metric (Moosavi and Strube, 2016). ", "Bloom_type": "comprehension", "question": "Which method is NOT mentioned as one of the five common metrics used to evaluate coreference algorithms?", "options": ["The mention based B3 metric (Bagga and Baldwin, 1998)", "The link based MUC (Vilain et al., 1995)", "The entity based CEAF metric (Luo, 2005)", "The link based entity aware LEA metric (Moosavi and Strube, 2016)"], "complexity": 1}, {"id": 50, "context": "Alternative metrics have been proposed that deal with particular coreference domains or tasks. For example, consider the task of resolving mentions to named entities (persons, organizations, geopolitical entities), which might be useful for information extraction or knowledge base completion. A hypothesis chain that correctly contains all the pronouns referring to an entity, but has no version of the name itself, or is linked with a wrong name, is not useful for this task. We might instead want a metric that weights each mention by how informative it is (with names being most informative) (Chen and Ng, 2013) or a metric that considers a hypothesis to match a gold chain only if it contains at least one variant of a name (the NEC F1 metric of Agarwal et al. (2019)). ", "Bloom_type": "comprehension", "question": "What type of metric could be more suitable for resolving mentions to named entities?", "options": ["Metrics considering the informativeness of mentions", "Metrics based on weight of mentions", "Metrics focusing on linking names correctly", "Metrics that require matching at least one name variant"], "complexity": 1}, {"id": 51, "context": "In an attempt to get the field of NLP to focus more on methods involving world knowledge and common-sense reasoning, Levesque (2011) proposed a challenge task called the Winograd Schema Challenge.8 The problems in the challenge task are coreference problems designed to be easily disambiguated by the human reader, but hopefully not solvable by simple techniques such as selectional restrictions, or other basic word association methods. ", "Bloom_type": "comprehension", "question": "What type of problems does the Winograd Schema Challenge aim to address?", "options": ["Coreference problems", "Syntax errors", "Semantic ambiguities", "Phonological inconsistencies"], "complexity": 1}, {"id": 52, "context": "One possible source of this bias is that female entities are significantly underrepresented in the OntoNotes dataset, used to train most coreference systems. Zhao et al. (2018a) propose a way to overcome this bias: they generate a second gender-swapped dataset in which all male entities in OntoNotes are replaced with female ones and vice versa, and retrain coreference systems on the combined original and swapped OntoNotes data, also using debiased GloVE embeddings (Bolukbasi et al., 2016). The resulting coreference systems no longer exhibit bias on the WinoBias dataset, without significantly impacting OntoNotes coreference accuracy. In a follow-up paper, Zhao et al. (2019) show that the same biases exist in ELMo contextualized word vector representations and coref systems that use them. They showed that retraining ELMo with data augmentation again reduces or removes bias in coreference systems on WinoBias. ", "Bloom_type": "comprehension", "question": "What did Zhao et al. propose to address the bias issue in coreference systems?", "options": ["Generating a new dataset by swapping genders in OntoNotes", "Using debiased GloVe embeddings for training", "Training coreference systems only on OntoNotes", "Removing bias from ELMo contextualized word vectors"], "complexity": 1}, {"id": 53, "context": "Webster et al. (2018) introduces another dataset, GAP, and the task of Gendered Pronoun Resolution as a tool for developing improved coreference algorithms for gendered pronouns. GAP is a gender-balanced labeled corpus of 4,454 sentences with gendered ambiguous pronouns (by contrast, only 20% of the gendered pronouns in the English OntoNotes training data are feminine). The examples were created by drawing on naturally occurring sentences from Wikipedia pages to create hard to resolve cases with two named entities of the same gender and an ambiguous pronoun that may refer to either person (or neither), like the following: ", "Bloom_type": "comprehension", "question": "What does the introduction of GAP indicate about the development of coreference algorithms?", "options": ["It underscores the necessity of improving coreference algorithms.", "It suggests the need for more balanced datasets.", "It implies the importance of gendered pronoun resolution.", "It highlights the difficulty in resolving gendered pronouns."], "complexity": 1}, {"id": 54, "context": " This is the task of linking together mentions in text which corefer, i.e. refer to the same discourse entity in the discourse model, resulting in a set of coreference chains (also called clusters or entities). ", "Bloom_type": "comprehension", "question": "What does the term 'coreference' describe in the context provided?", "options": ["The process of referring back to the same entity across different parts of a text", "The act of linking mentions within sentences", "The identification of entities in a discourse model", "The creation of clusters based on synonyms"], "complexity": 1}, {"id": 55, "context": " Some NPs are not referring expressions, such as pleonastic it in It is raining.  Many corpora have human-labeled coreference annotations that can be used for supervised learning, including OntoNotes for English, Chinese, and Arabic, ARRAU for English, and AnCora for Spanish and Catalan. ", "Bloom_type": "comprehension", "question": "What do some NP refer to?", "options": ["Coreferential entities", "Pleonastic it", "Non-referential expressions", "Referential expressions"], "complexity": 1}, {"id": 56, "context": " Mention detection can start with all nouns and named entities and then use anaphoricity classifiers or referentiality classifiers to filter out non-mentions.  Three common architectures for coreference are mention-pair, mention-rank, and entity-based, each of which can make use of feature-based or neural classifiers. ", "Bloom_type": "comprehension", "question": "Which architecture does NOT utilize neural classifiers for coreference resolution?", "options": ["neural classifier-free", "mention-pair", "mention-rank", "entity-based"], "complexity": 1}, {"id": 57, "context": "tection and coreference in a single end-to-end architecture. ", "Bloom_type": "comprehension", "question": "Explain how coreference works within an end-to-end architecture?", "options": ["Coreference involves linking together entities based on their shared reference points within a sentence.", "Coreference refers to the repetition of phrases across different parts of speech.", "Coreference means referring back to a previously mentioned entity using pronouns.", "Coreference is unrelated to the functioning of end-to-end architectures."], "complexity": 1}, {"id": 58, "context": " The Winograd Schema Challenge problems are difficult coreference problems that seem to require world knowledge or sophisticated reasoning to solve.  Coreference systems exhibit gender bias which can be evaluated using datasets ", "Bloom_type": "comprehension", "question": "Explain why Winograd Schema Challenge problems are considered difficult coreference problems?", "options": ["They necessitate extensive world knowledge.", "They involve complex logical reasoning.", "They require understanding of cultural norms.", "They demand advanced computational skills."], "complexity": 1}, {"id": 59, "context": "Coreference competitions as part of the US DARPA-sponsored MUC conferences provided early labeled coreference datasets (the 1995 MUC-6 and 1998 MUC7 corpora), and set the tone for much later work, choosing to focus exclusively on the simplest cases of identity coreference (ignoring difficult cases like bridging, metonymy, and part-whole) and drawing the community toward supervised machine learning and metrics like the MUC metric (Vilain et al., 1995). The later ACE evaluations produced labeled coreference corpora in English, Chinese, and Arabic that were widely used for model training and evaluation. ", "Bloom_type": "comprehension", "question": "What was one of the primary focuses of coreference research during the period when it was first established?", "options": ["Focusing solely on simple cases of identity coreference", "Handling complex types of coreference relations", "Using unsupervised methods for coreference resolution", "Developing new languages for coreference analysis"], "complexity": 1}, {"id": 60, "context": "The move from mention-pair to mention-ranking approaches was pioneered by Yang et al. (2003) and Iida et al. (2003) who proposed pairwise ranking methods, then extended by Denis and Baldridge (2008) who proposed to do ranking via a softmax over all prior mentions. The idea of doing mention detection, anaphoricity, and coreference jointly in a single end-to-end model grew out of the early proposal of Ng (2005b) to use a dummy antecedent for mention-ranking, allowing non-referential` to be a choice for coreference classifiers, Denis and Baldridge`s 2007 joint system combining anaphoricity classifier probabilities with coreference probabilities, the Denis and Baldridge (2008) ranking model, and the Rahman and Ng (2009) proposal to train the two models jointly with a single objective. ", "Bloom_type": "comprehension", "question": "What did Denis and Baldridge propose to do ranking via a softmax over all prior mentions?", "options": ["all of the above", "mention detection", "anaphoricity", "coreference"], "complexity": 1}, {"id": 61, "context": "Coreference is also related to the task of entity linking discussed in Chapter 14. Coreference can help entity linking by giving more possible surface forms to help link to the right Wikipedia page, and conversely entity linking can help improve coreference resolution. Consider this example from Hajishirzi et al. (2013): ", "Bloom_type": "comprehension", "question": "How does coreference relate to entity linking?", "options": ["Coreference helps entity linking by providing additional surface forms for linking.", "Entity linking improves coreference resolution by reducing the number of possible surface forms.", "Coreference and entity linking are unrelated concepts.", "None of the above"], "complexity": 1}, {"id": 62, "context": "Integrating entity linking into coreference can help draw encyclopedic knowledge (like the fact that Donald Tsang is a president) to help disambiguate the mention the President. Ponzetto and Strube (2006) 2007 and Ratinov and Roth (2012) showed that such attributes extracted from Wikipedia pages could be used to build richer models of entity mentions in coreference. More recent research shows how to do linking and coreference jointly (Hajishirzi et al. 2013, Zheng et al. 2013) or even jointly with named entity tagging as well (Durrett and Klein 2014). ", "Bloom_type": "comprehension", "question": "What does integrating entity linking into coreference allow for?", "options": ["Improves the accuracy of disambiguating mentions", "Draws encyclopedic knowledge based on Wikipedia pages", "Enhances the complexity of entity mentions", "Increases the number of entities linked"], "complexity": 1}, {"id": 63, "context": "Fig. 24.8 from Barzilay and Lapata (2008) shows a grid for the text shown in Fig. 24.9. There is one row for each of the six sentences. The second column, for the entity trial`, is O    X, showing that the trial appears in the first sentence as direct object, in the last sentence as an oblique, and does not appear in the middle sentences. The third column, for the entity Microsoft, shows that it appears as subject in sentence 1 (it also appears as the object of the preposition against, but entities that appear multiple times are recorded with their highest-ranked grammatical function). Computing the entity grids requires extracting entities and doing coreference ", "Bloom_type": "comprehension", "question": "What computational task involves identifying and resolving references between mentions within a text?", "options": ["Coreference resolution", "Entity extraction", "Dependency parsing", "Part-of-speech tagging"], "complexity": 1}, {"id": 64, "context": "Finally, we`ll need to compare words and other strings. We`ll introduce a metric called edit distance that measures how similar two strings are based on the number of edits (insertions, deletions, substitutions) it takes to change one string into the other. Edit distance is an algorithm with applications throughout language processing, from spelling correction to speech recognition to coreference resolution. ", "Bloom_type": "application", "question": "What is the purpose of introducing the edit distance metric?", "options": ["To resolve word references in texts", "To measure the similarity between sentences", "To identify synonyms within a sentence", "To detect plagiarism in documents"], "complexity": 2}, {"id": 65, "context": "Much of natural language processing is concerned with measuring how similar two strings are. For example in spelling correction, the user typed some erroneous stringlet`s say graffeand we want to know what the user meant. The user probably intended a word that is similar to graffe. Among candidate similar words, the word giraffe, which differs by only one letter from graffe, seems intuitively to be more similar than, say grail or graf, which differ in more letters. Another example comes from coreference, the task of deciding whether two strings such as the following refer to the same entity: ", "Bloom_type": "application", "question": "Which method should be used to determine if two strings referring to the same entity?", "options": ["Identify common substrings.", "Measure the similarity between all possible pairs of strings.", "Compare each string individually for differences.", "Use a dictionary lookup to find synonyms."], "complexity": 2}, {"id": 66, "context": "At the same time in the early 1990s logistic regression was developed and applied to NLP at IBM Research under the name maximum entropy modeling or maxent (Berger et al., 1996), seemingly independent of the statistical literature. Under that name it was applied to language modeling (Rosenfeld, 1996), part-of-speech tagging (Ratnaparkhi, 1996), parsing (Ratnaparkhi, 1997), coreference resolution (Kehler, 1997b), and text classification (Nigam et al., 1999). ", "Bloom_type": "application", "question": "What is an example of how logistic regression was used independently of the statistical literature?", "options": ["Logistic regression was used for named entity recognition.", "Logistic regression was used for sentiment analysis.", "Logistic regression was used for topic modeling.", "Logistic regression was used for entity recognition."], "complexity": 2}, {"id": 67, "context": "conclude with linguistic structures that tend to be related to discourse and meaning over larger texts, including coreference and discourse coherence. In each case we`ll give algorithms for automatically annotating the relevant structure. ", "Bloom_type": "application", "question": "Which of the following is an example of automatic annotation for coreference?", "options": ["Automatically tagging phrases referring back to previously mentioned entities.", "Identifying synonyms within a sentence.", "Detecting parallelism between clauses.", "Classifying sentences based on their syntactic complexity."], "complexity": 2}, {"id": 68, "context": "One common approach is to make repeated rule-based passes over a text, starting with rules with very high precision but low recall, and, in subsequent stages, using machine learning methods that take the output of the first pass into account (an approach first worked out for coreference (Lee et al., 2017a)): ", "Bloom_type": "application", "question": "What method does the paper suggest as an initial step before applying machine learning techniques?", "options": ["Coreference resolution", "Rule-based passes", "Machine learning methods", "Text analysis"], "complexity": 2}, {"id": 69, "context": "Highly inflectional languages also have much more information than English coded in word morphology, like case (nominative, accusative, genitive) or gender (masculine, feminine). Because this information is important for tasks like parsing and coreference resolution, part-of-speech taggers for morphologically rich lan", "Bloom_type": "application", "question": "What task does the additional information in highly inflectional languages help with?", "options": ["Coreference Resolution", "Parsing", "Part-of-Speech Tagging", "Dependency Parsing"], "complexity": 2}, {"id": 70, "context": "The result of processing these two sentences is the five draft templates (Fig. 20.21) that must then be merged into the single hierarchical structure shown in Fig. 20.19. The merging algorithm, after performing coreference resolution, merges two activities that are likely to be describing the same events. ", "Bloom_type": "application", "question": "What step should come before the merging of two activities?", "options": ["Perform coreference resolution", "Merge two activities", "Process the sentences", "Generate the hierarchical structure"], "complexity": 2}, {"id": 71, "context": "This results in a (S,A,P) tuple for a given entity mention; To get scores for the representation of an entity in a complete document, we can run coreference resolution and average the (S,A,P) scores for all the mentions. Fig. 22.13 shows the scores from their algorithm for characters from the movie The Dark Knight when run on Wikipedia plot summary texts with gold coreference. ", "Bloom_type": "application", "question": "What is the first step in scoring the representation of an entity in a complete document?", "options": ["Run coreference resolution", "Calculate the average score", "Identify all entity mentions", "Combine S, A, P tuples"], "complexity": 2}, {"id": 72, "context": "Coreference is an important component of natural language processing. A dialogue system that has just told the user There is a 2pm flight on United and a 4pm one on Cathay Pacific must know which flight the user means by I`ll take the second one. A question answering system that uses Wikipedia to answer a question about Marie Curie must know who she was in the sentence She was born in Warsaw. And a machine translation system translating from a language like Spanish, in which pronouns can be dropped, must use coreference from the previous sentence to decide whether the Spanish sentence Me encanta el conocimiento, dice.` should be translated as I love knowledge, he says`, or I love knowledge, she says`. Indeed, this example comes from an actual news article in El Pas about a female professor and was mistranslated as he in machine translation because of inaccurate coreference resolution (Schiebinger, 2013). ", "Bloom_type": "application", "question": "In a dialogue system, how does it ensure the user understands which flight they mean?", "options": ["By using coreference to link the flights mentioned earlier.", "By asking the user for clarification after each choice.", "By providing additional options for the user to choose from.", "By repeating the flights' times multiple times."], "complexity": 2}, {"id": 73, "context": "In this chapter we focus on the task of coreference resolution. Coreference resolution is the task of determining whether two mentions corefer, by which we mean they refer to the same entity in the discourse model (the same discourse entity). The set of coreferring expressions is often called a coreference chain or a cluster. For example, in processing (23.1), a coreference resolution algorithm would need to find at least four coreference chains, corresponding to the four entities in the discourse model in Fig. 23.1. ", "Bloom_type": "application", "question": "What does the set of corefering expressions represent?", "options": ["A collection of all possible pairs of entities that could potentially refer to each other", "The number of entities in the discourse model", "The total length of the discourse model", "The frequency of occurrence of each entity in the discourse model"], "complexity": 2}, {"id": 74, "context": "Note that mentions can be nested; for example the mention her is syntactically part of another mention, her pay, referring to a completely different discourse entity. Coreference resolution thus comprises two tasks (although they are often performed jointly): (1) identifying the mentions, and (2) clustering them into coreference chains/discourse entities. ", "Bloom_type": "application", "question": "What does coreference resolution involve?", "options": ["Both identifying mentions and clustering them.", "Identifying the mentions only.", "Clustering mentions into coreference chains.", "Only performing joint tasks."], "complexity": 2}, {"id": 75, "context": "Before turning to algorithms, however, we mention some important tasks we will only touch on briefly at the end of this chapter. First are the famous Winograd Schema problems (so-called because they were first pointed out by Terry Winograd in his dissertation). These entity coreference resolution problems are designed to be too difficult to be solved by the resolution methods we describe in this chapter, and the kind of real-world knowledge they require has made them a kind of challenge task for natural language processing. For example, consider the task of determining the correct antecedent of the pronoun they in the following example: (23.2) The city council denied the demonstrators a permit because ", "Bloom_type": "application", "question": "What is an example of a coreference resolution problem?", "options": ["Winograd Schema problems", "Entity recognition", "Part-of-speech tagging", "Named entity classification"], "complexity": 2}, {"id": 76, "context": "We can formulate the task of coreference resolution as follows: Given a text T , find all entities and the coreference links between them. We evaluate our task by comparing the links our system creates with those in human-created gold coreference annotations on T . ", "Bloom_type": "application", "question": "What is the first step in solving the problem of coreference resolution?", "options": ["Identify all entities in the text", "Create coreference links manually", "Compare system-generated links with gold annotations", "Evaluate the quality of the generated links"], "complexity": 2}, {"id": 77, "context": "For most coreference evaluation campaigns, the input to the system is the raw text of articles, and systems must detect mentions and then link them into clusters. Solving this task requires dealing with pronominal anaphora (figuring out that her refers to Victoria Chen), filtering out non-referential pronouns like the pleonastic It in It has been ten years), dealing with definite noun phrases to figure out that the 38-year-old is coreferent with Victoria Chen, and that the company is the same as Megabucks. And we need to deal with names, to realize that Megabucks is the same as Megabucks Banking. ", "Bloom_type": "application", "question": "What is the first step in solving the coreference evaluation task?", "options": ["Identify all the pronouns in the text", "Filter out non-coreferential pronouns", "Decide on the referents for each pronoun", "Combine all the mentioned entities"], "complexity": 2}, {"id": 78, "context": "A number of corpora mark richer discourse phenomena. The ISNotes corpus annotates a portion of OntoNotes for information status, include bridging examples (Hou et al., 2018). The LitBank coreference corpus (Bamman et al., 2020) contains coreference annotations for 210,532 tokens from 100 different literary novels, including singletons and quantified and negated noun phrases. The AnCora-CO coreference corpus (Recasens and Mart, 2010) contains 400,000 words each of Spanish (AnCora-CO-Es) and Catalan (AnCora-CO-Ca) news data, and includes labels for complex phenomena like discourse deixis in both languages. The ARRAU corpus (Uryupina et al., 2020) contains 350,000 words of English marking all NPs, which means singleton clusters are available. ARRAU includes diverse genres like dialog (the TRAINS data) and fiction (the Pear Stories), and has labels for bridging references, discourse deixis, generics, and ambiguous anaphoric relations. ", "Bloom_type": "application", "question": "Which corpus provides annotated coreference annotations for a large dataset?", "options": ["ARRAU", "ISNotes", "LitBank", "AnCora-CO"], "complexity": 2}, {"id": 79, "context": "The modern approach is instead to perform mention detection, anaphoricity, and coreference jointly in a single end-to-end model (Ng 2005b, Denis and Baldridge 2007, Rahman and Ng 2009). For example mention detection in the Lee et al. (2017b),2018 system is based on a single end-to-end neural network that computes a score for each mention being referential, a score for two mentions being coreference, and combines them to make a decision, training all these scores with a single end-to-end loss. We`ll describe this method in detail in Section 23.6. 7 ", "Bloom_type": "application", "question": "What does the modern approach involve when detecting mentions?", "options": ["Mention detection, anaphoricity, and coreference", "Only mention detection", "Anaphoricity only", "Coreference only"], "complexity": 2}, {"id": 80, "context": "We begin with the mention-pair architecture, the simplest and most influential coreference architecture, which introduces many of the features of more complex algorithms, even though other architectures perform better. The mention-pair architecture is based around a classifier that as its name suggestsis given a pair of mentions, a candidate anaphor and a candidate antecedent, and makes a binary classification decision: coreferring or not. ", "Bloom_type": "application", "question": "What does the mention-pair architecture primarily focus on?", "options": ["Binary classification decisions", "The complexity of the algorithm", "Candidate antecedents", "Mention pairs"], "complexity": 2}, {"id": 81, "context": "In early formulations, for mention i, the classifier decides which of the prior mentions is the antecedent (Denis and Baldridge, 2008). But suppose i is 1 in fact not anaphoric, and none of the antecedents should be chosen? Such a model would need to run a separate anaphoricity classifier on i. Instead, it turns out to be better to jointly learn anaphoricity detection and coreference together with a single loss (Rahman and Ng, 2009). ", "Bloom_type": "application", "question": "What approach does Rahman and Ng suggest for handling non-anaphoric mentions?", "options": ["Use a single loss function to detect both anaphoricity and coreference.", "Run a separate anaphoricity classifier on each mention.", "Identify the antecedent based solely on syntactic dependencies.", "Ignore all mentions that are not anaphoric."], "complexity": 2}, {"id": 82, "context": "However, there are a wide variety of methods for doing this comparison. In fact, there are 5 common metrics used to evaluate coreference algorithms: the link based MUC (Vilain et al., 1995) and BLANC (Recasens and Hovy 2011, Luo et al. 2014) metrics, the mention based B3 metric (Bagga and Baldwin, 1998), the entity based CEAF metric (Luo, 2005), and the link based entity aware LEA metric (Moosavi and Strube, 2016). ", "Bloom_type": "application", "question": "Which method is not commonly used to evaluate coreference algorithms?", "options": ["The link based MUC (Vilain et al., 1995)", "The mention based B3 metric (Bagga and Baldwin, 1998)", "The entity based CEAF metric (Luo, 2005)", "The link based entity aware LEA metric (Moosavi and Strube, 2016)"], "complexity": 2}, {"id": 83, "context": "Alternative metrics have been proposed that deal with particular coreference domains or tasks. For example, consider the task of resolving mentions to named entities (persons, organizations, geopolitical entities), which might be useful for information extraction or knowledge base completion. A hypothesis chain that correctly contains all the pronouns referring to an entity, but has no version of the name itself, or is linked with a wrong name, is not useful for this task. We might instead want a metric that weights each mention by how informative it is (with names being most informative) (Chen and Ng, 2013) or a metric that considers a hypothesis to match a gold chain only if it contains at least one variant of a name (the NEC F1 metric of Agarwal et al. (2019)). ", "Bloom_type": "application", "question": "Which approach could be used to improve the accuracy of resolving mentions to named entities?", "options": ["Develop a method that combines both approaches mentioned above.", "Use a metric that assigns higher weight to names over other mentions.", "Implement a metric that requires at least one variant of a name in the hypothesis.", "Create a new metric that focuses solely on the informativeness of mentions."], "complexity": 2}, {"id": 84, "context": "In an attempt to get the field of NLP to focus more on methods involving world knowledge and common-sense reasoning, Levesque (2011) proposed a challenge task called the Winograd Schema Challenge.8 The problems in the challenge task are coreference problems designed to be easily disambiguated by the human reader, but hopefully not solvable by simple techniques such as selectional restrictions, or other basic word association methods. ", "Bloom_type": "application", "question": "What is the main goal of the Winograd Schema Challenge?", "options": ["To identify coreference errors in natural language", "To test the effectiveness of machine learning models", "To evaluate the performance of natural language processing algorithms", "To assess the ability of humans to understand complex sentences"], "complexity": 2}, {"id": 85, "context": "One possible source of this bias is that female entities are significantly underrepresented in the OntoNotes dataset, used to train most coreference systems. Zhao et al. (2018a) propose a way to overcome this bias: they generate a second gender-swapped dataset in which all male entities in OntoNotes are replaced with female ones and vice versa, and retrain coreference systems on the combined original and swapped OntoNotes data, also using debiased GloVE embeddings (Bolukbasi et al., 2016). The resulting coreference systems no longer exhibit bias on the WinoBias dataset, without significantly impacting OntoNotes coreference accuracy. In a follow-up paper, Zhao et al. (2019) show that the same biases exist in ELMo contextualized word vector representations and coref systems that use them. They showed that retraining ELMo with data augmentation again reduces or removes bias in coreference systems on WinoBias. ", "Bloom_type": "application", "question": "What method did Zhao et al. use to address the gender bias in coreference systems?", "options": ["Generate a new dataset with swapped genders for both male and female entities.", "Use only the original OntoNotes dataset for training.", "Train coreference systems directly on the WinoBias dataset.", "Augment ELMo embeddings with additional data."], "complexity": 2}, {"id": 86, "context": "Webster et al. (2018) introduces another dataset, GAP, and the task of Gendered Pronoun Resolution as a tool for developing improved coreference algorithms for gendered pronouns. GAP is a gender-balanced labeled corpus of 4,454 sentences with gendered ambiguous pronouns (by contrast, only 20% of the gendered pronouns in the English OntoNotes training data are feminine). The examples were created by drawing on naturally occurring sentences from Wikipedia pages to create hard to resolve cases with two named entities of the same gender and an ambiguous pronoun that may refer to either person (or neither), like the following: ", "Bloom_type": "application", "question": "What was the main goal of introducing the GAP dataset?", "options": ["To develop improved coreference algorithms for gendered pronouns", "To improve the labeling of gendered pronouns in the English OntoNotes training data", "To provide a balanced dataset for gendered pronouns across different genders", "To enhance the resolution of gendered pronouns in natural language processing tasks"], "complexity": 2}, {"id": 87, "context": " This is the task of linking together mentions in text which corefer, i.e. refer to the same discourse entity in the discourse model, resulting in a set of coreference chains (also called clusters or entities). ", "Bloom_type": "application", "question": "What is the main goal when dealing with coreference resolution?", "options": ["To link mentions in text that refer to the same entity.", "To identify synonyms within sentences.", "To separate different clauses in a sentence.", "To determine antonyms between words."], "complexity": 2}, {"id": 88, "context": " Some NPs are not referring expressions, such as pleonastic it in It is raining.  Many corpora have human-labeled coreference annotations that can be used for supervised learning, including OntoNotes for English, Chinese, and Arabic, ARRAU for English, and AnCora for Spanish and Catalan. ", "Bloom_type": "application", "question": "What type of annotation involves labeling coreference relationships?", "options": ["Coreference resolution", "Part-of-speech tagging", "Dependency parsing", "Named entity recognition"], "complexity": 2}, {"id": 89, "context": " Mention detection can start with all nouns and named entities and then use anaphoricity classifiers or referentiality classifiers to filter out non-mentions.  Three common architectures for coreference are mention-pair, mention-rank, and entity-based, each of which can make use of feature-based or neural classifiers. ", "Bloom_type": "application", "question": "Which architecture is not commonly used for detecting mentions?", "options": ["Entity-based", "Mention-pair", "Mention-rank", "Feature-based"], "complexity": 2}, {"id": 90, "context": "tection and coreference in a single end-to-end architecture. ", "Bloom_type": "application", "question": "What is the first step in developing an end-to-end architecture that incorporates both detection and coreference?", "options": ["Implement the detection algorithm within the architecture.", "Identify the entities mentioned in the text.", "Define the coreference rules for the system.", "Ensure proper handling of overlapping mentions."], "complexity": 2}, {"id": 91, "context": " The Winograd Schema Challenge problems are difficult coreference problems that seem to require world knowledge or sophisticated reasoning to solve.  Coreference systems exhibit gender bias which can be evaluated using datasets ", "Bloom_type": "application", "question": "What is an example of a coreference problem?", "options": ["John went to the store, bought some milk, and returned home.", "The dog chased the cat.", "She likes apples and bananas.", "They both love ice cream."], "complexity": 2}, {"id": 92, "context": "Coreference competitions as part of the US DARPA-sponsored MUC conferences provided early labeled coreference datasets (the 1995 MUC-6 and 1998 MUC7 corpora), and set the tone for much later work, choosing to focus exclusively on the simplest cases of identity coreference (ignoring difficult cases like bridging, metonymy, and part-whole) and drawing the community toward supervised machine learning and metrics like the MUC metric (Vilain et al., 1995). The later ACE evaluations produced labeled coreference corpora in English, Chinese, and Arabic that were widely used for model training and evaluation. ", "Bloom_type": "application", "question": "What was the primary focus of coreference competitions during their inception?", "options": ["Focusing solely on simple cases of identity coreference", "Handling complex types of coreference such as bridging, metonymy, and part-whole", "Developing new methods for identifying coreferences", "Creating large-scale unlabeled data sets"], "complexity": 2}, {"id": 93, "context": "The move from mention-pair to mention-ranking approaches was pioneered by Yang et al. (2003) and Iida et al. (2003) who proposed pairwise ranking methods, then extended by Denis and Baldridge (2008) who proposed to do ranking via a softmax over all prior mentions. The idea of doing mention detection, anaphoricity, and coreference jointly in a single end-to-end model grew out of the early proposal of Ng (2005b) to use a dummy antecedent for mention-ranking, allowing non-referential` to be a choice for coreference classifiers, Denis and Baldridge`s 2007 joint system combining anaphoricity classifier probabilities with coreference probabilities, the Denis and Baldridge (2008) ranking model, and the Rahman and Ng (2009) proposal to train the two models jointly with a single objective. ", "Bloom_type": "application", "question": "What is the next step after proposing pairwise ranking methods?", "options": ["Extend the pairwise ranking approach using a softmax function", "Develop a new method for mention-detection", "Combine anaphoricity and coreference classifications in a single model", "Train both anaphoricity and coreference classifiers together"], "complexity": 2}, {"id": 94, "context": "Coreference is also related to the task of entity linking discussed in Chapter 14. Coreference can help entity linking by giving more possible surface forms to help link to the right Wikipedia page, and conversely entity linking can help improve coreference resolution. Consider this example from Hajishirzi et al. (2013): ", "Bloom_type": "application", "question": "In what way does coreference contribute to entity linking?", "options": ["Coreference improves the accuracy of entity linking by providing additional surface forms for linking.", "Coreference helps identify entities directly.", "Entity linking enhances coreference resolution through synonymy.", "Coreference eliminates the need for entity linking."], "complexity": 2}, {"id": 95, "context": "Integrating entity linking into coreference can help draw encyclopedic knowledge (like the fact that Donald Tsang is a president) to help disambiguate the mention the President. Ponzetto and Strube (2006) 2007 and Ratinov and Roth (2012) showed that such attributes extracted from Wikipedia pages could be used to build richer models of entity mentions in coreference. More recent research shows how to do linking and coreference jointly (Hajishirzi et al. 2013, Zheng et al. 2013) or even jointly with named entity tagging as well (Durrett and Klein 2014). ", "Bloom_type": "application", "question": "What approach does the response suggest for integrating entity linking into coreference?", "options": ["Combine entity linking and coreference simultaneously", "Integrate entity linking after coreference resolution", "Use only coreference resolution methods", "Ignore both entity linking and coreference"], "complexity": 2}, {"id": 96, "context": "Fig. 24.8 from Barzilay and Lapata (2008) shows a grid for the text shown in Fig. 24.9. There is one row for each of the six sentences. The second column, for the entity trial`, is O    X, showing that the trial appears in the first sentence as direct object, in the last sentence as an oblique, and does not appear in the middle sentences. The third column, for the entity Microsoft, shows that it appears as subject in sentence 1 (it also appears as the object of the preposition against, but entities that appear multiple times are recorded with their highest-ranked grammatical function). Computing the entity grids requires extracting entities and doing coreference ", "Bloom_type": "application", "question": "What computational task involves identifying and resolving references between mentions within a text?", "options": ["Coreference resolution", "Entity extraction", "Dependency parsing", "Part-of-speech tagging"], "complexity": 2}]}, "lexicon": {"max_id": 50, "Questions": [{"id": 0, "context": "Finally, in some situations we might have insufficient labeled training data to train accurate naive Bayes classifiers using all words in the training set to estimate positive and negative sentiment. In such cases we can instead derive the positive and negative word features from sentiment lexicons, lists of words that are preannotated with positive or negative sentiment. Four popular lexicons are the General Inquirer (Stone et al., 1966), LIWC (Pennebaker et al., 2007), the opinion lexicon of Hu and Liu (2004a) and the MPQA Subjectivity Lexicon (Wilson et al., 2005). ", "Bloom_type": "remember", "question": "Which type of list is used for deriving positive and negative word features in sentiment analysis?", "options": ["Sentiment lexicons", "Word frequency lists", "Stopword lists", "Part-of-speech lists"], "complexity": 0}, {"id": 1, "context": "A common way to use lexicons in a naive Bayes classifier is to add a feature that is counted whenever a word from that lexicon occurs. Thus we might add a feature called this word occurs in the positive lexicon`, and treat all instances of words in the lexicon as counts for that one feature, instead of counting each word separately. Similarly, we might add as a second feature this word occurs in the negative lexicon` of words in the negative lexicon. If we have lots of training data, and if the test data matches the training data, using just two features won`t work as well as using all the words. But when training data is sparse or not representative of the test set, using dense lexicon features instead of sparse individual-word features may generalize better. ", "Bloom_type": "remember", "question": "In a naive Bayes classifier, why is it beneficial to use dense lexicon features instead of sparse individual-word features?", "options": ["To enhance generalization performance on both sparse and dense datasets", "To reduce computational complexity", "To increase model accuracy on sparse datasets", "None of the above"], "complexity": 0}, {"id": 2, "context": "1. Treat the target word and a neighboring context word as positive examples. 2. Randomly sample other words in the lexicon to get negative samples. 3. Use logistic regression to train a classifier to distinguish those two cases. 4. Use the learned weights as the embeddings. ", "Bloom_type": "remember", "question": "In the process of creating word embeddings, what is used to classify whether a word is part of the target or negative example?", "options": ["Logistic regression", "Neural networks", "Random forest", "Support vector machine"], "complexity": 0}, {"id": 3, "context": "The Computational Grammar Coder (CGC) of Klein and Simmons (1963) had three components: a lexicon, a morphological analyzer, and a context disambiguator. The small 1500-word lexicon listed only function words and other irregular words. The morphological analyzer used inflectional and derivational suffixes to assign part-of-speech classes. These were run over words to produce candidate parts of speech which were then disambiguated by a set of 500 context rules by relying on surrounding islands of unambiguous words. For example, one rule said that between an ARTICLE and a VERB, the only allowable sequences were ADJ-NOUN, NOUNADVERB, or NOUN-NOUN. The TAGGIT tagger (Greene and Rubin, 1971) used the same architecture as Klein and Simmons (1963), with a bigger dictionary and more tags (87). TAGGIT was applied to the Brown corpus and, according to Francis and Kucera (1982, p. 9), accurately tagged 77% of the corpus; the remainder of the Brown corpus was then tagged by hand. All these early algorithms were based on a two-stage architecture in which a dictionary was first used to assign each word a set of potential parts of speech, and then lists of handwritten disambiguation rules winnowed the set down to a single part of speech per word. ", "Bloom_type": "remember", "question": "What did the CGC use for its lexical data?", "options": ["A smaller but more detailed lexicon", "The Brown Corpus", "A large database of internet texts", "None of the above"], "complexity": 0}, {"id": 4, "context": " The symbols that are used in a CFG are divided into two classes. The symbols that correspond to words in the language (the, nightclub) are called terminal symbols; the lexicon is the set of rules that introduce these terminal symbols. The symbols that express abstractions over these terminals are called non-terminals. In each context-free rule, the item to the right of the arrow ( ) is an ordered list of one or more terminals and non-terminals; to the left of the arrow is a single non-terminal symbol expressing some cluster or generalization. The non-terminal associated with each word in the lexicon is its lexical category, or part of speech. ", "Bloom_type": "remember", "question": "What do we call the set of rules that introduce terminal symbols in a CFG?", "options": ["Lexical rules", "Syntactic rules", "Semantic rules", "Phonological rules"], "complexity": 0}, {"id": 5, "context": "Although the idea of semantic roles dates back to Pan. ini, they were re-introduced into modern linguistics by Gruber (1965), Fillmore (1966) and Fillmore (1968). Fillmore had become interested in argument structure by studying Lucien Tesni`ere`s groundbreaking Elements de Syntaxe Structurale (Tesni`ere, 1959) in which the term dependency` was introduced and the foundations were laid for dependency grammar. Following Tesni`ere`s terminology, Fillmore first referred to argument roles as actants (Fillmore, 1966) but quickly switched to the term case, (see Fillmore (2003)) and proposed a universal list of semantic roles or cases (Agent, Patient, Instrument, etc.), that could be taken on by the arguments of predicates. Verbs would be listed in the lexicon with their case frame, the list of obligatory (or optional) case arguments. The idea that semantic roles could provide an intermediate level of semantic representation that could help map from syntactic parse structures to deeper, more fully-specified representations of meaning was quickly adopted in natural language processing, and systems for extracting case frames were created for machine translation (Wilks, 1973), question-answering (Hendrix et al., 1973), spoken-language processing (Nash-Webber, 1975), and dialogue systems (Bobrow et al., 1977). Generalpurpose semantic role labelers were developed. The earliest ones (Simmons, 1973) first parsed a sentence by means of an ATN (Augmented Transition Network) parser. Each verb then had a set of rules specifying how the parse should be mapped to semantic roles. These rules mainly made reference to grammatical functions (subject, object, complement of specific prepositions) but also checked constituent internal features such as the animacy of head nouns. Later systems assigned roles from prebuilt parse trees, again by using dictionaries with verb-specific case frames (Levin 1977, Marcus 1980). ", "Bloom_type": "remember", "question": "According to the context, what did Fillmore propose regarding semantic roles?", "options": ["Fillmore suggested a universal list of semantic roles or cases.", "Semantic roles are not important in modern linguistics.", "Semantic roles were initially rejected by Panini.", "Fillmore focused solely on syntactic parse structures."], "complexity": 0}, {"id": 6, "context": "In the next sections we introduce basic theories of emotion, show how sentiment lexicons are a special case of emotion lexicons, and mention some useful lexicons. We then survey three ways for building lexicons: human labeling, semi-supervised, and supervised. Finally, we talk about how to detect affect toward a particular entity, and introduce connotation frames. ", "Bloom_type": "remember", "question": "What is discussed first in the passage?", "options": ["ways for building lexicons", "basic theories of emotion", "sentiment lexicons", "how to detect affect towards an entity"], "complexity": 0}, {"id": 7, "context": "In the simplest lexicons this dimension is represented in a binary fashion, with a wordlist for positive words and a wordlist for negative words. The oldest is the General Inquirer (Stone et al., 1966), which drew on content analysis and on early work in the cognitive psychology of word meaning (Osgood et al., 1957). The General Inquirer has a lexicon of 1915 positive words and a lexicon of 2291 negative words (as well as other lexicons discussed below). The MPQA Subjectivity lexicon (Wilson et al., 2005) has 2718 positive and 4912 negative words drawn from prior lexicons plus a bootstrapped list of subjective words and phrases (Riloff and Wiebe, 2003). Each entry in the lexicon is hand-labeled for sentiment and also labeled for reliability (strongly subjective or weakly subjective). The polarity lexicon of Hu and Liu (2004b) gives 2006 positive and 4783 negative words, drawn from product reviews, labeled using a bootstrapping method from WordNet. ", "Bloom_type": "remember", "question": "Which lexicon was used by Stone et al. in their study?", "options": ["General Inquirer", "MPQA Subjectivity lexicon", "Polarity lexicon of Hu and Liu", "WordNet"], "complexity": 0}, {"id": 8, "context": "LIWC, Linguistic Inquiry and Word Count, is a widely used set of 73 lexicons containing over 2300 words (Pennebaker et al., 2007), designed to capture aspects of lexical meaning relevant for social psychological tasks. In addition to sentiment-related lexicons like ones for negative emotion (bad, weird, hate, problem, tough) and positive emotion (love, nice, sweet), LIWC includes lexicons for categories like anger, sadness, cognitive mechanisms, perception, tentative, and inhibition, shown in Fig. 22.6. ", "Bloom_type": "remember", "question": "What does LIWC stand for?", "options": ["Linguistic Inquiry and Word Count", "Lexical Information and Word Classification", "Language Identification and Word Catalogue", "Learning Intelligence and Word Collection"], "complexity": 0}, {"id": 9, "context": "There are various other hand-built affective lexicons. The General Inquirer includes additional lexicons for dimensions like strong vs. weak, active vs. passive, overstated vs. understated, as well as lexicons for categories like pleasure, pain, virtue, vice, motivation, and cognitive orientation. ", "Bloom_type": "remember", "question": "What does the General Inquirer include besides other hand-built affective lexicons?", "options": ["Both A) and B)", "Additional lexicons for dimensions like strong vs. weak, active vs. passive, overstated vs. understated", "Categories like pleasure, pain, virtue, vice, motivation, and cognitive orientation", "None of the above"], "complexity": 0}, {"id": 10, "context": "Another common way to learn sentiment lexicons is to start from a set of seed words that define two poles of a semantic axis (words like good or bad), and then find ways to label each word w by its similarity to the two seed sets. Here we summarize two families of seed-based semi-supervised lexicon induction algorithms, axis-based and graph-based. ", "Bloom_type": "remember", "question": "In the process of learning sentiment lexicons, what method involves starting with a set of seed words that represent two poles on a semantic axis?", "options": ["Seed-based semi-supervised lexicon induction", "Word frequency analysis", "Text summarization", "Machine translation"], "complexity": 0}, {"id": 11, "context": "If a dictionary of words with sentiment scores is sufficient, we`re done! Or if we need to group words into a positive and a negative lexicon, we can use a threshold or other method to give us discrete lexicons. ", "Bloom_type": "remember", "question": "In the process of creating a sentiment analysis tool, what two main steps are typically involved?", "options": ["Analyzing texts and categorizing sentiments", "Creating a database and training a machine learning model", "Collecting data and developing algorithms", "Designing user interfaces and testing models"], "complexity": 0}, {"id": 12, "context": "In Chapter 4 we introduced the naive Bayes algorithm for sentiment analysis. The lexicons we have focused on throughout the chapter so far can be used in a number of ways to improve sentiment detection. ", "Bloom_type": "remember", "question": "In sentiment analysis, what is the role of lexicons?", "options": ["To identify positive and negative sentiments within sentences", "To enhance the accuracy of word embeddings", "To provide synonyms for each word in the corpus", "To reduce computational complexity by removing stop words"], "complexity": 0}, {"id": 13, "context": "connotational aspect of word meaning can be represented in lexicons. ", "Bloom_type": "remember", "question": "In linguistics, what is referred to as a collection of words with similar meanings?", "options": ["Semantics", "Phonetics", "Syntax", "Morphology"], "complexity": 0}, {"id": 14, "context": " Lexicons can be learned in a fully supervised manner, when a convenient training signal can be found in the world, such as ratings assigned by users on a review site. ", "Bloom_type": "remember", "question": "In what way can lexicons be learned?", "options": ["Both A) and B)", "By observing natural language patterns", "Through user feedback analysis", "None of the above"], "complexity": 0}, {"id": 15, "context": " Affect can be detected, just like sentiment, by using standard supervised text classification techniques, using all the words or bigrams in a text as features. Additional features can be drawn from counts of words in lexicons. ", "Bloom_type": "remember", "question": "Which method is used for detecting affective states in texts?", "options": ["Drawing on word counts from lexical databases", "Using only the words in the text", "Combining both methods equally", "Neither method is effective"], "complexity": 0}, {"id": 16, "context": "Most of the semi-supervised methods we describe for extending sentiment dictionaries drew on the early idea that synonyms and antonyms tend to co-occur in the same sentence (Miller and Charles 1991, Justeson and Katz 1991, Riloff and Shepherd 1997). Other semi-supervised methods for learning cues to affective meaning rely on information extraction techniques, like the AutoSlog pattern extractors (Riloff and Wiebe, 2003). Graph based algorithms for sentiment were first suggested by Hatzivassiloglou and McKeown (1997), and graph propagation became a standard method (Zhu and Ghahramani 2002, Zhu et al. 2003, Zhou et al. 2004a, Velikovich et al. 2010). Crowdsourcing can also be used to improve precision by filtering the result of semi-supervised lexicon learning (Riloff and Shepherd 1997, Fast et al. 2016). ", "Bloom_type": "remember", "question": "What did Miller and Charles draw upon in their work?", "options": ["The idea that synonyms and antonyms tend to co-occur in the same sentence", "Information extraction techniques such as AutoSlog pattern extractors", "Graph-based algorithms for sentiment analysis", "Crowdsourced data for improving sentiment dictionaries"], "complexity": 0}, {"id": 17, "context": "Finally, in some situations we might have insufficient labeled training data to train accurate naive Bayes classifiers using all words in the training set to estimate positive and negative sentiment. In such cases we can instead derive the positive and negative word features from sentiment lexicons, lists of words that are preannotated with positive or negative sentiment. Four popular lexicons are the General Inquirer (Stone et al., 1966), LIWC (Pennebaker et al., 2007), the opinion lexicon of Hu and Liu (2004a) and the MPQA Subjectivity Lexicon (Wilson et al., 2005). ", "Bloom_type": "comprehension", "question": "Which of the following best describes the role of sentiment lexicons in machine learning models?", "options": ["Sentiment lexicons offer pre-annotated lists of words to estimate sentiment.", "Sentiment lexicons provide additional training data for Naive Bayes classifiers.", "Sentiment lexicons help in identifying specific keywords for sentiment analysis.", "Sentiment lexicons replace the need for labeled training data entirely."], "complexity": 1}, {"id": 18, "context": "A common way to use lexicons in a naive Bayes classifier is to add a feature that is counted whenever a word from that lexicon occurs. Thus we might add a feature called this word occurs in the positive lexicon`, and treat all instances of words in the lexicon as counts for that one feature, instead of counting each word separately. Similarly, we might add as a second feature this word occurs in the negative lexicon` of words in the negative lexicon. If we have lots of training data, and if the test data matches the training data, using just two features won`t work as well as using all the words. But when training data is sparse or not representative of the test set, using dense lexicon features instead of sparse individual-word features may generalize better. ", "Bloom_type": "comprehension", "question": "Explain how adding features based on lexicons can improve Naive Bayes classifiers?", "options": ["It enhances classification performance by leveraging semantic relationships between words.", "It increases the model's accuracy by focusing only on frequent words.", "It reduces computational complexity by combining similar words into a single feature.", "It simplifies the model by ignoring rare words."], "complexity": 1}, {"id": 19, "context": "1. Treat the target word and a neighboring context word as positive examples. 2. Randomly sample other words in the lexicon to get negative samples. 3. Use logistic regression to train a classifier to distinguish those two cases. 4. Use the learned weights as the embeddings. ", "Bloom_type": "comprehension", "question": "How does one create an embedding for a new word based on its usage within a lexicon?", "options": ["By treating the target word and its immediate neighbors as positive examples and using logistic regression to learn the weights.", "By randomly sampling words from the lexicon, ignoring their context.", "By focusing solely on the frequency of the word in the lexicon.", "By manually inputting the word into a neural network."], "complexity": 1}, {"id": 20, "context": "The Computational Grammar Coder (CGC) of Klein and Simmons (1963) had three components: a lexicon, a morphological analyzer, and a context disambiguator. The small 1500-word lexicon listed only function words and other irregular words. The morphological analyzer used inflectional and derivational suffixes to assign part-of-speech classes. These were run over words to produce candidate parts of speech which were then disambiguated by a set of 500 context rules by relying on surrounding islands of unambiguous words. For example, one rule said that between an ARTICLE and a VERB, the only allowable sequences were ADJ-NOUN, NOUNADVERB, or NOUN-NOUN. The TAGGIT tagger (Greene and Rubin, 1971) used the same architecture as Klein and Simmons (1963), with a bigger dictionary and more tags (87). TAGGIT was applied to the Brown corpus and, according to Francis and Kucera (1982, p. 9), accurately tagged 77% of the corpus; the remainder of the Brown corpus was then tagged by hand. All these early algorithms were based on a two-stage architecture in which a dictionary was first used to assign each word a set of potential parts of speech, and then lists of handwritten disambiguation rules winnowed the set down to a single part of speech per word. ", "Bloom_type": "comprehension", "question": "What was the primary difference between the CGC and TAGGIT systems?", "options": ["TAGGIT relied solely on context rules for disambiguation, while the CGC did not.", "The CGC used a smaller lexicon compared to TAGGIT.", "The CGC employed fewer context rules than TAGGIT.", "Both systems used identical dictionaries."], "complexity": 1}, {"id": 21, "context": " The symbols that are used in a CFG are divided into two classes. The symbols that correspond to words in the language (the, nightclub) are called terminal symbols; the lexicon is the set of rules that introduce these terminal symbols. The symbols that express abstractions over these terminals are called non-terminals. In each context-free rule, the item to the right of the arrow ( ) is an ordered list of one or more terminals and non-terminals; to the left of the arrow is a single non-terminal symbol expressing some cluster or generalization. The non-terminal associated with each word in the lexicon is its lexical category, or part of speech. ", "Bloom_type": "comprehension", "question": "What do we call the set of rules that introduces terminal symbols in a CFG?", "options": ["The lexicon", "The grammar", "The syntax", "The semantics"], "complexity": 1}, {"id": 22, "context": "Although the idea of semantic roles dates back to Pan. ini, they were re-introduced into modern linguistics by Gruber (1965), Fillmore (1966) and Fillmore (1968). Fillmore had become interested in argument structure by studying Lucien Tesni`ere`s groundbreaking Elements de Syntaxe Structurale (Tesni`ere, 1959) in which the term dependency` was introduced and the foundations were laid for dependency grammar. Following Tesni`ere`s terminology, Fillmore first referred to argument roles as actants (Fillmore, 1966) but quickly switched to the term case, (see Fillmore (2003)) and proposed a universal list of semantic roles or cases (Agent, Patient, Instrument, etc.), that could be taken on by the arguments of predicates. Verbs would be listed in the lexicon with their case frame, the list of obligatory (or optional) case arguments. The idea that semantic roles could provide an intermediate level of semantic representation that could help map from syntactic parse structures to deeper, more fully-specified representations of meaning was quickly adopted in natural language processing, and systems for extracting case frames were created for machine translation (Wilks, 1973), question-answering (Hendrix et al., 1973), spoken-language processing (Nash-Webber, 1975), and dialogue systems (Bobrow et al., 1977). Generalpurpose semantic role labelers were developed. The earliest ones (Simmons, 1973) first parsed a sentence by means of an ATN (Augmented Transition Network) parser. Each verb then had a set of rules specifying how the parse should be mapped to semantic roles. These rules mainly made reference to grammatical functions (subject, object, complement of specific prepositions) but also checked constituent internal features such as the animacy of head nouns. Later systems assigned roles from prebuilt parse trees, again by using dictionaries with verb-specific case frames (Levin 1977, Marcus 1980). ", "Bloom_type": "comprehension", "question": "What did Fillmore initially refer to as argument roles before switching to the term 'case'? ", "options": ["Actants", "Subjects", "Objects", "Predicates"], "complexity": 1}, {"id": 23, "context": "In the next sections we introduce basic theories of emotion, show how sentiment lexicons are a special case of emotion lexicons, and mention some useful lexicons. We then survey three ways for building lexicons: human labeling, semi-supervised, and supervised. Finally, we talk about how to detect affect toward a particular entity, and introduce connotation frames. ", "Bloom_type": "comprehension", "question": "What type of lexicons do we discuss in the first part of the passage?", "options": ["Sentiment lexicons", "Emotion lexicons", "Affect lexicons", "Connotation lexicons"], "complexity": 1}, {"id": 24, "context": "In the simplest lexicons this dimension is represented in a binary fashion, with a wordlist for positive words and a wordlist for negative words. The oldest is the General Inquirer (Stone et al., 1966), which drew on content analysis and on early work in the cognitive psychology of word meaning (Osgood et al., 1957). The General Inquirer has a lexicon of 1915 positive words and a lexicon of 2291 negative words (as well as other lexicons discussed below). The MPQA Subjectivity lexicon (Wilson et al., 2005) has 2718 positive and 4912 negative words drawn from prior lexicons plus a bootstrapped list of subjective words and phrases (Riloff and Wiebe, 2003). Each entry in the lexicon is hand-labeled for sentiment and also labeled for reliability (strongly subjective or weakly subjective). The polarity lexicon of Hu and Liu (2004b) gives 2006 positive and 4783 negative words, drawn from product reviews, labeled using a bootstrapping method from WordNet. ", "Bloom_type": "comprehension", "question": "What are some examples of lexicons mentioned in the context regarding their size and source?", "options": ["The General Inquirer has a lexicon of 1915 positive words and 2291 negative words.", "The MPQA Subjectivity lexicon has 2718 positive and 4912 negative words.", "Each entry in the lexicon is hand-labeled for sentiment and also labeled for reliability.", "The polarity lexicon of Hu and Liu has 2006 positive and 4783 negative words."], "complexity": 1}, {"id": 25, "context": "LIWC, Linguistic Inquiry and Word Count, is a widely used set of 73 lexicons containing over 2300 words (Pennebaker et al., 2007), designed to capture aspects of lexical meaning relevant for social psychological tasks. In addition to sentiment-related lexicons like ones for negative emotion (bad, weird, hate, problem, tough) and positive emotion (love, nice, sweet), LIWC includes lexicons for categories like anger, sadness, cognitive mechanisms, perception, tentative, and inhibition, shown in Fig. 22.6. ", "Bloom_type": "comprehension", "question": "What does LIWC include beyond sentiment-related lexicons?", "options": ["Both A and B are included", "Lexicons for categories like anger, sadness, cognitive mechanisms, perception, tentative, and inhibition", "Sentiment-related lexicons like those for negative emotion (bad, weird, hate, problem, tough) and positive emotion (love, nice, sweet)", "None of the above"], "complexity": 1}, {"id": 26, "context": "There are various other hand-built affective lexicons. The General Inquirer includes additional lexicons for dimensions like strong vs. weak, active vs. passive, overstated vs. understated, as well as lexicons for categories like pleasure, pain, virtue, vice, motivation, and cognitive orientation. ", "Bloom_type": "comprehension", "question": "What type of lexicons does The General Inquirer include besides those for specific dimensions and categories?", "options": ["Lexicons for emotions", "Lexicons for physical sensations", "Lexicons for abstract concepts", "Lexicons for personality traits"], "complexity": 1}, {"id": 27, "context": "Another common way to learn sentiment lexicons is to start from a set of seed words that define two poles of a semantic axis (words like good or bad), and then find ways to label each word w by its similarity to the two seed sets. Here we summarize two families of seed-based semi-supervised lexicon induction algorithms, axis-based and graph-based. ", "Bloom_type": "comprehension", "question": "What are two families of seed-based semi-supervised lexicon induction algorithms mentioned in the response?", "options": ["Tree-based and graph-based", "Axis-based and tree-based", "Graph-based and vector-based", "Vector-based and axis-based"], "complexity": 1}, {"id": 28, "context": "If a dictionary of words with sentiment scores is sufficient, we`re done! Or if we need to group words into a positive and a negative lexicon, we can use a threshold or other method to give us discrete lexicons. ", "Bloom_type": "comprehension", "question": "What are two common methods for creating a lexicon based on sentiment scores?", "options": ["Thresholds and clustering algorithms", "Clustering algorithms and word embeddings", "Word embeddings and natural language processing techniques", "Natural language processing techniques and thresholds"], "complexity": 1}, {"id": 29, "context": "In Chapter 4 we introduced the naive Bayes algorithm for sentiment analysis. The lexicons we have focused on throughout the chapter so far can be used in a number of ways to improve sentiment detection. ", "Bloom_type": "comprehension", "question": "What are some ways the lexicons discussed in this chapter can be utilized for sentiment analysis?", "options": ["To enhance the accuracy of sentiment classification by providing additional context.", "To increase the speed of data processing during sentiment analysis.", "To reduce the computational complexity of training models.", "To eliminate the need for human intervention in sentiment analysis."], "complexity": 1}, {"id": 30, "context": "connotational aspect of word meaning can be represented in lexicons. ", "Bloom_type": "comprehension", "question": "What does the connotational aspect of word meaning refer to?", "options": ["The emotional and cultural associations linked to a word", "The literal definition of a word", "The historical origin of a word", "The grammatical structure of a word"], "complexity": 1}, {"id": 31, "context": " Lexicons can be learned in a fully supervised manner, when a convenient training signal can be found in the world, such as ratings assigned by users on a review site. ", "Bloom_type": "comprehension", "question": "What type of learning does lexicon acquisition involve?", "options": ["Supervised learning", "Unsupervised learning", "Reinforcement learning", "Self-organizing map learning"], "complexity": 1}, {"id": 32, "context": " Affect can be detected, just like sentiment, by using standard supervised text classification techniques, using all the words or bigrams in a text as features. Additional features can be drawn from counts of words in lexicons. ", "Bloom_type": "comprehension", "question": "What are some additional features for detecting affective states based on word usage?", "options": ["Combining both A) and C)", "Counting specific emotion-related words", "Analyzing sentence structure patterns", "Using bigram frequency analysis"], "complexity": 1}, {"id": 33, "context": "Most of the semi-supervised methods we describe for extending sentiment dictionaries drew on the early idea that synonyms and antonyms tend to co-occur in the same sentence (Miller and Charles 1991, Justeson and Katz 1991, Riloff and Shepherd 1997). Other semi-supervised methods for learning cues to affective meaning rely on information extraction techniques, like the AutoSlog pattern extractors (Riloff and Wiebe, 2003). Graph based algorithms for sentiment were first suggested by Hatzivassiloglou and McKeown (1997), and graph propagation became a standard method (Zhu and Ghahramani 2002, Zhu et al. 2003, Zhou et al. 2004a, Velikovich et al. 2010). Crowdsourcing can also be used to improve precision by filtering the result of semi-supervised lexicon learning (Riloff and Shepherd 1997, Fast et al. 2016). ", "Bloom_type": "comprehension", "question": "What are some common approaches to extend sentiment dictionaries using semi-supervised methods?", "options": ["Synonym and antonym co-occurrence patterns and auto-slog pattern extractors", "Graph-based algorithms and crowdsourced data improvement", "Sentence-level analysis and word embeddings", "Word sense disambiguation and dependency parsing"], "complexity": 1}, {"id": 34, "context": "Finally, in some situations we might have insufficient labeled training data to train accurate naive Bayes classifiers using all words in the training set to estimate positive and negative sentiment. In such cases we can instead derive the positive and negative word features from sentiment lexicons, lists of words that are preannotated with positive or negative sentiment. Four popular lexicons are the General Inquirer (Stone et al., 1966), LIWC (Pennebaker et al., 2007), the opinion lexicon of Hu and Liu (2004a) and the MPQA Subjectivity Lexicon (Wilson et al., 2005). ", "Bloom_type": "application", "question": "Which method is used to derive positive and negative word features for sentiment analysis?", "options": ["Use the four popular lexicons directly as features", "Train naive Bayes classifiers on all words in the training set", "Derive features from the training data itself", "Combine both methods"], "complexity": 2}, {"id": 35, "context": "A common way to use lexicons in a naive Bayes classifier is to add a feature that is counted whenever a word from that lexicon occurs. Thus we might add a feature called this word occurs in the positive lexicon`, and treat all instances of words in the lexicon as counts for that one feature, instead of counting each word separately. Similarly, we might add as a second feature this word occurs in the negative lexicon` of words in the negative lexicon. If we have lots of training data, and if the test data matches the training data, using just two features won`t work as well as using all the words. But when training data is sparse or not representative of the test set, using dense lexicon features instead of sparse individual-word features may generalize better. ", "Bloom_type": "application", "question": "What is the main difference between using dense lexicon features and sparse individual-word features?", "options": ["Dense lexicon features count each word individually, while sparse individual-word features sum up the occurrences.", "Sparse individual-word features are more accurate than dense lexicon features.", "Dense lexicon features can handle larger datasets, but they require more computational resources.", "Sparse individual-word features are easier to compute because they don't need to store large dictionaries."], "complexity": 2}, {"id": 36, "context": "1. Treat the target word and a neighboring context word as positive examples. 2. Randomly sample other words in the lexicon to get negative samples. 3. Use logistic regression to train a classifier to distinguish those two cases. 4. Use the learned weights as the embeddings. ", "Bloom_type": "application", "question": "What is the first step in creating an embedding using the method described?", "options": ["Choose positive and negative examples", "Select random words for training", "Train a neural network model", "Initialize the weight matrix"], "complexity": 2}, {"id": 37, "context": "The Computational Grammar Coder (CGC) of Klein and Simmons (1963) had three components: a lexicon, a morphological analyzer, and a context disambiguator. The small 1500-word lexicon listed only function words and other irregular words. The morphological analyzer used inflectional and derivational suffixes to assign part-of-speech classes. These were run over words to produce candidate parts of speech which were then disambiguated by a set of 500 context rules by relying on surrounding islands of unambiguous words. For example, one rule said that between an ARTICLE and a VERB, the only allowable sequences were ADJ-NOUN, NOUNADVERB, or NOUN-NOUN. The TAGGIT tagger (Greene and Rubin, 1971) used the same architecture as Klein and Simmons (1963), with a bigger dictionary and more tags (87). TAGGIT was applied to the Brown corpus and, according to Francis and Kucera (1982, p. 9), accurately tagged 77% of the corpus; the remainder of the Brown corpus was then tagged by hand. All these early algorithms were based on a two-stage architecture in which a dictionary was first used to assign each word a set of potential parts of speech, and then lists of handwritten disambiguation rules winnowed the set down to a single part of speech per word. ", "Bloom_type": "application", "question": "What is the main difference between the CGC and TAGGIT?", "options": ["The CGC relies solely on dictionaries for part-of-speech tagging, while TAGGIT also uses context-based rules.", "The CGC uses a smaller lexicon than TAGGIT.", "TAGGIT uses fewer context rules compared to the CGC.", "The CGC employs a larger dictionary than TAGGIT."], "complexity": 2}, {"id": 38, "context": " The symbols that are used in a CFG are divided into two classes. The symbols that correspond to words in the language (the, nightclub) are called terminal symbols; the lexicon is the set of rules that introduce these terminal symbols. The symbols that express abstractions over these terminals are called non-terminals. In each context-free rule, the item to the right of the arrow ( ) is an ordered list of one or more terminals and non-terminals; to the left of the arrow is a single non-terminal symbol expressing some cluster or generalization. The non-terminal associated with each word in the lexicon is its lexical category, or part of speech. ", "Bloom_type": "application", "question": "What is the term for the set of rules that define how terminal symbols can be combined to form non-terminal symbols?", "options": ["Production rules", "Grammar structure", "Lexical analysis", "Syntax tree"], "complexity": 2}, {"id": 39, "context": "Although the idea of semantic roles dates back to Pan. ini, they were re-introduced into modern linguistics by Gruber (1965), Fillmore (1966) and Fillmore (1968). Fillmore had become interested in argument structure by studying Lucien Tesni`ere`s groundbreaking Elements de Syntaxe Structurale (Tesni`ere, 1959) in which the term dependency` was introduced and the foundations were laid for dependency grammar. Following Tesni`ere`s terminology, Fillmore first referred to argument roles as actants (Fillmore, 1966) but quickly switched to the term case, (see Fillmore (2003)) and proposed a universal list of semantic roles or cases (Agent, Patient, Instrument, etc.), that could be taken on by the arguments of predicates. Verbs would be listed in the lexicon with their case frame, the list of obligatory (or optional) case arguments. The idea that semantic roles could provide an intermediate level of semantic representation that could help map from syntactic parse structures to deeper, more fully-specified representations of meaning was quickly adopted in natural language processing, and systems for extracting case frames were created for machine translation (Wilks, 1973), question-answering (Hendrix et al., 1973), spoken-language processing (Nash-Webber, 1975), and dialogue systems (Bobrow et al., 1977). Generalpurpose semantic role labelers were developed. The earliest ones (Simmons, 1973) first parsed a sentence by means of an ATN (Augmented Transition Network) parser. Each verb then had a set of rules specifying how the parse should be mapped to semantic roles. These rules mainly made reference to grammatical functions (subject, object, complement of specific prepositions) but also checked constituent internal features such as the animacy of head nouns. Later systems assigned roles from prebuilt parse trees, again by using dictionaries with verb-specific case frames (Levin 1977, Marcus 1980). ", "Bloom_type": "application", "question": "What did Fillmore propose regarding semantic roles?", "options": ["He defined a universal list of semantic roles.", "He suggested creating a new vocabulary.", "He introduced the term 'dependency'.", "He focused solely on syntactic parsing."], "complexity": 2}, {"id": 40, "context": "In the next sections we introduce basic theories of emotion, show how sentiment lexicons are a special case of emotion lexicons, and mention some useful lexicons. We then survey three ways for building lexicons: human labeling, semi-supervised, and supervised. Finally, we talk about how to detect affect toward a particular entity, and introduce connotation frames. ", "Bloom_type": "application", "question": "What is the first step in creating an emotion lexicon?", "options": ["Collect data on various emotions", "Identify entities with positive emotions", "Label sentences with emotional content", "Define terms and concepts"], "complexity": 2}, {"id": 41, "context": "In the simplest lexicons this dimension is represented in a binary fashion, with a wordlist for positive words and a wordlist for negative words. The oldest is the General Inquirer (Stone et al., 1966), which drew on content analysis and on early work in the cognitive psychology of word meaning (Osgood et al., 1957). The General Inquirer has a lexicon of 1915 positive words and a lexicon of 2291 negative words (as well as other lexicons discussed below). The MPQA Subjectivity lexicon (Wilson et al., 2005) has 2718 positive and 4912 negative words drawn from prior lexicons plus a bootstrapped list of subjective words and phrases (Riloff and Wiebe, 2003). Each entry in the lexicon is hand-labeled for sentiment and also labeled for reliability (strongly subjective or weakly subjective). The polarity lexicon of Hu and Liu (2004b) gives 2006 positive and 4783 negative words, drawn from product reviews, labeled using a bootstrapping method from WordNet. ", "Bloom_type": "application", "question": "Which lexicon provides a more comprehensive coverage of both positive and negative words?", "options": ["MPQA Subjectivity Lexicon", "The General Inquirer", "Polarity Lexicon of Hu and Liu", "None of the above"], "complexity": 2}, {"id": 42, "context": "LIWC, Linguistic Inquiry and Word Count, is a widely used set of 73 lexicons containing over 2300 words (Pennebaker et al., 2007), designed to capture aspects of lexical meaning relevant for social psychological tasks. In addition to sentiment-related lexicons like ones for negative emotion (bad, weird, hate, problem, tough) and positive emotion (love, nice, sweet), LIWC includes lexicons for categories like anger, sadness, cognitive mechanisms, perception, tentative, and inhibition, shown in Fig. 22.6. ", "Bloom_type": "application", "question": "What aspect of lexical meaning does LIWC primarily focus on?", "options": ["All of the above", "Sentiment-related lexicons such as those for negative and positive emotions", "Categories like anger, sadness, cognitive mechanisms, perception, tentative, and inhibition", "Words with high frequency in natural language"], "complexity": 2}, {"id": 43, "context": "There are various other hand-built affective lexicons. The General Inquirer includes additional lexicons for dimensions like strong vs. weak, active vs. passive, overstated vs. understated, as well as lexicons for categories like pleasure, pain, virtue, vice, motivation, and cognitive orientation. ", "Bloom_type": "application", "question": "Which of the following is an example of a lexicon included in the General Inquirer?", "options": ["affective lexicon for categories like pleasure, pain, virtue, vice", "affective lexicon for dimensions like strong vs. weak", "affective lexicon for cognitive orientation", "affective lexicon for motivations"], "complexity": 2}, {"id": 44, "context": "Another common way to learn sentiment lexicons is to start from a set of seed words that define two poles of a semantic axis (words like good or bad), and then find ways to label each word w by its similarity to the two seed sets. Here we summarize two families of seed-based semi-supervised lexicon induction algorithms, axis-based and graph-based. ", "Bloom_type": "application", "question": "Which type of seed-based semi-supervised lexicon induction algorithm involves defining two poles on a semantic axis?", "options": ["Axis-based", "Graph-based", "Both Axis-based and Graph-based", "None of the above"], "complexity": 2}, {"id": 45, "context": "If a dictionary of words with sentiment scores is sufficient, we`re done! Or if we need to group words into a positive and a negative lexicon, we can use a threshold or other method to give us discrete lexicons. ", "Bloom_type": "application", "question": "What should we do after obtaining a dictionary of words with sentiment scores?", "options": ["Classify them into positive and negative sentiments", "Use it directly for analysis", "Sort the words based on their frequency", "Combine all words into one large lexicon"], "complexity": 2}, {"id": 46, "context": "In Chapter 4 we introduced the naive Bayes algorithm for sentiment analysis. The lexicons we have focused on throughout the chapter so far can be used in a number of ways to improve sentiment detection. ", "Bloom_type": "application", "question": "What is an example of how lexicons are utilized in sentiment analysis?", "options": ["Lexicons are used to identify specific keywords associated with positive or negative sentiments.", "Lexicons are used to directly classify sentences as positive, negative, or neutral.", "Lexicons are used to calculate the probability distribution of word frequencies in training data.", "Lexicons are used to train machine learning models using labeled datasets."], "complexity": 2}, {"id": 47, "context": "connotational aspect of word meaning can be represented in lexicons. ", "Bloom_type": "application", "question": "In which way can the connotational aspect of word meaning be depicted using a lexicon?", "options": ["By organizing words according to their emotional associations", "By listing synonyms for each word", "By categorizing words based on their usage frequency", "By providing examples of how words are used in sentences"], "complexity": 2}, {"id": 48, "context": " Lexicons can be learned in a fully supervised manner, when a convenient training signal can be found in the world, such as ratings assigned by users on a review site. ", "Bloom_type": "application", "question": "What is a common method for learning lexicons?", "options": ["Through supervised learning with user ratings", "Using unsupervised machine learning techniques", "By manually creating rules based on user feedback", "Combining both manual creation and supervised learning"], "complexity": 2}, {"id": 49, "context": " Affect can be detected, just like sentiment, by using standard supervised text classification techniques, using all the words or bigrams in a text as features. Additional features can be drawn from counts of words in lexicons. ", "Bloom_type": "application", "question": "What is an example of additional feature used for detecting affect?", "options": ["The frequency of specific emotion-related words in the text", "The number of vowels in each word", "The length of sentences containing positive words only", "The total count of characters in the entire document"], "complexity": 2}, {"id": 50, "context": "Most of the semi-supervised methods we describe for extending sentiment dictionaries drew on the early idea that synonyms and antonyms tend to co-occur in the same sentence (Miller and Charles 1991, Justeson and Katz 1991, Riloff and Shepherd 1997). Other semi-supervised methods for learning cues to affective meaning rely on information extraction techniques, like the AutoSlog pattern extractors (Riloff and Wiebe, 2003). Graph based algorithms for sentiment were first suggested by Hatzivassiloglou and McKeown (1997), and graph propagation became a standard method (Zhu and Ghahramani 2002, Zhu et al. 2003, Zhou et al. 2004a, Velikovich et al. 2010). Crowdsourcing can also be used to improve precision by filtering the result of semi-supervised lexicon learning (Riloff and Shepherd 1997, Fast et al. 2016). ", "Bloom_type": "application", "question": "Which technique was not mentioned as being used for improving the precision of semi-supervised lexicon learning?", "options": ["Lexical analysis", "Crowdsourcing", "Graph-based algorithms", "AutoSlog pattern extractors"], "complexity": 2}]}, "logistic regression": {"max_id": 107, "Questions": [{"id": 0, "context": "In the first part of the book we introduce the fundamental suite of algorithmic tools that make up the modern neural language model that is the heart of end-to-end NLP systems. We begin with tokenization and preprocessing, as well as useful algorithms like computing edit distance, and then proceed to the tasks of classification, logistic regression, neural networks, proceeding through feedforward networks, recurrent networks, and then transformers. We`ll also see the role of embeddings as a model of word meaning. ", "Bloom_type": "remember", "question": "In what type of models do we primarily use logistic regression for classification tasks?", "options": ["Neural networks", "Feedforward networks", "Recurrent networks", "Transformers"], "complexity": 0}, {"id": 1, "context": "Many kinds of machine learning algorithms are used to build classifiers. This chapter introduces naive Bayes; the following one introduces logistic regression. These exemplify two ways of doing classification. Generative classifiers like naive Bayes build a model of how a class could generate some input data. Given an observation, they return the class most likely to have generated the observation. Discriminative classifiers like logistic regression instead learn what features from the input are most useful to discriminate between the different possible classes. While discriminative systems are often more accurate and hence more commonly used, generative classifiers still have a role. ", "Bloom_type": "remember", "question": "Which type of classifier is described as learning what features from the input are most useful to discriminate between the different possible classes?", "options": ["Logistic Regression", "Naive Bayes", "Support Vector Machines", "K-Nearest Neighbors"], "complexity": 0}, {"id": 2, "context": "In this chapter we introduce an algorithm that is admirably suited for discovering the link between features or clues and some particular outcome: logistic regression. Indeed, logistic regression is one of the most important analytic tools in the social and natural sciences. In natural language processing, logistic regression is the baseline supervised machine learning algorithm for classification, and also has a very close relationship with neural networks. As we will see in Chapter 7, a neural network can be viewed as a series of logistic regression classifiers stacked on top of each other. Thus the classification and machine learning techniques introduced here will play an important role throughout the book. ", "Bloom_type": "remember", "question": "In what field is logistic regression considered one of the most important analytic tools?", "options": ["Both Social and Natural Sciences", "Social Sciences", "Natural Sciences", "None of the above"], "complexity": 0}, {"id": 3, "context": "Logistic regression can be used to classify an observation into one of two classes (like positive sentiment` and negative sentiment`), or into one of many classes. Because the mathematics for the two-class case is simpler, we`ll describe this special case of logistic regression first in the next few sections, and then briefly summarize the use of multinomial logistic regression for more than two classes in Section 5.3. We`ll introduce the mathematics of logistic regression in the next few sections. ", "Bloom_type": "remember", "question": "In logistic regression, what does the classification typically involve?", "options": ["Classifying observations into one of two categories", "Determining the probability of a single event occurring", "Calculating the mean value of a continuous variable", "Predicting future stock prices"], "complexity": 0}, {"id": 4, "context": "The goal of binary logistic regression is to train a classifier that can make a binary decision about the class of a new input observation. Here we introduce the sigmoid classifier that will help us make this decision. ", "Bloom_type": "remember", "question": "In logistic regression, what does the sigmoid function do?", "options": ["It calculates the probability of an event occurring.", "It determines the accuracy of the model.", "It predicts the outcome based on features.", "It normalizes the data before training."], "complexity": 0}, {"id": 5, "context": "Logistic regression solves this task by learning, from a training set, a vector of weights and a bias term. Each weight wi is a real number, and is associated with one of the input features xi. The weight wi represents how important that input feature is to the classification decision, and can be positive (providing evidence that the instance being classified belongs in the positive class) or negative (providing evidence that the instance being classified belongs in the negative class). Thus we might expect in a sentiment task the word awesome to have a high positive weight, and abysmal to have a very negative weight. The bias term, also called the intercept, is another real number that`s added to the weighted inputs. ", "Bloom_type": "remember", "question": "In logistic regression, what does the bias term represent?", "options": ["It adjusts the overall strength of the model's predictions.", "It represents the importance of each input feature.", "It determines the threshold for classification decisions.", "It calculates the final output value before applying the sigmoid function."], "complexity": 0}, {"id": 6, "context": "Let`s have some examples of applying logistic regression as a classifier for language tasks. ", "Bloom_type": "remember", "question": "In what type of classification problems is logistic regression commonly used?", "options": ["Text classification", "Image recognition", "Sentiment analysis", "Audio processing"], "complexity": 0}, {"id": 7, "context": "Logistic regression is applied to all sorts of NLP tasks, and any property of the input can be a feature. Consider the task of period disambiguation: deciding if a period is the end of a sentence or part of a word, by classifying each period into one of two classes, EOS (end-of-sentence) and not-EOS. We might use features like x1 below expressing that the current word is lower case, perhaps with a positive weight. Or a feature expressing that the current word is in our abbreviations dictionary (Prof.), perhaps with a negative weight. A feature can also express a combination of properties. For example a period following an upper case word is likely to be an EOS, but if the word itself is St. and the previous word is capitalized then the period is likely part of a shortening of the word street following a street name. ", "Bloom_type": "remember", "question": "In the context of period disambiguation using logistic regression, what type of feature expresses that the current word is in the abbreviations dictionary?", "options": ["x3", "x2", "x4", "x5"], "complexity": 0}, {"id": 8, "context": "Logistic regression has a number of advantages over naive Bayes. Naive Bayes has overly strong conditional independence assumptions. Consider two features which are strongly correlated; in fact, imagine that we just add the same feature f1 twice. Naive Bayes will treat both copies of f1 as if they were separate, multiplying them both in, overestimating the evidence. By contrast, logistic regression is much more robust to correlated features; if two features f1 and f2 are perfectly correlated, regression will simply assign part of the weight to w1 and part to w2. Thus when there are many correlated features, logistic regression will assign a more accurate probability than naive Bayes. So logistic regression generally works better on larger documents or datasets and is a common default. ", "Bloom_type": "remember", "question": "In what way does logistic regression outperform naive Bayes?", "options": ["Logistic regression assigns weights based on correlation.", "Logistic regression assumes conditional independence between features.", "Naive Bayes can handle large datasets better.", "Naive Bayes uses fewer parameters for model fitting."], "complexity": 0}, {"id": 9, "context": "If you work out the matrix arithmetic, you can see that the estimated score of the first output class y1 (before we take the softmax) will correctly turn out to be w1  x + b1. One helpful interpretation of the weight matrix W is to see each row wk as a prototype of class k. The weight vector wk that is learned represents the class as a kind of template. Since two vectors that are more similar to each other have a higher dot product with each other, the dot product acts as a similarity function. Logistic regression is thus learning an exemplar representation for each class, such that incoming vectors are assigned the class k they are most similar to from the K classes. ", "Bloom_type": "remember", "question": "In logistic regression, how does the weight vector wk represent each class?", "options": ["As a template for classification", "As a prototype of class k", "As a similarity function", "As a set of weights"], "complexity": 0}, {"id": 10, "context": "How are the parameters of the model, the weights w and bias b, learned? Logistic regression is an instance of supervised classification in which we know the correct label y (either 0 or 1) for each observation x. What the system produces via Eq. 5.5 is y, the system`s estimate of the true y. We want to learn parameters (meaning w and b) that make y for each training observation as close as possible to the true y. ", "Bloom_type": "remember", "question": "In logistic regression, how do the parameters \\( w \\) and \\( b \\) get adjusted?", "options": ["Through maximizing the likelihood function using gradient descent.", "By minimizing the sum of squared errors between predicted and actual values.", "Using a fixed set of weights determined by prior knowledge.", "By setting all weights to zero and adjusting them manually."], "complexity": 0}, {"id": 11, "context": "For logistic regression, this loss function is conveniently convex. A convex function has at most one minimum; there are no local minima to get stuck in, so gradient descent starting from any point is guaranteed to find the minimum. (By contrast, the loss for multi-layer neural networks is non-convex, and gradient descent may get stuck in local minima for neural network training and never find the global optimum.) ", "Bloom_type": "remember", "question": "Which of the following statements about logistic regression is true?", "options": ["Gradient descent can guarantee finding the global minimum for logistic regression.", "Logistic regression does not use a loss function.", "The loss function for logistic regression is always non-convex.", "Multi-layer neural networks cannot be trained using gradient descent."], "complexity": 0}, {"id": 12, "context": "In an actual logistic regression, the parameter vector w is much longer than 1 or 2, since the input feature vector x can be quite long, and we need a weight wi for each xi. For each dimension/variable wi in w (plus the bias b), the gradient will have a component that tells us the slope with respect to that variable. In each dimension wi, we express the slope as a partial derivative  of the loss function. Essentially  wi we`re asking: How much would a small change in that variable wi influence the total loss function L? ", "Bloom_type": "remember", "question": "In logistic regression, why does the parameter vector \\( \\mathbf{w} \\) often contain many more elements than just one or two?", "options": ["To capture the impact of changes in all features on the model output.", "To account for fewer variables in the dataset.", "Because the input feature vectors are typically very short.", "To simplify the computation process by reducing the number of weights."], "complexity": 0}, {"id": 13, "context": "Often we want to know more than just the correct classification of an observation. We want to know why the classifier made the decision it did. That is, we want our decision to be interpretable. Interpretability can be hard to define strictly, but the core idea is that as humans we should know why our algorithms reach the conclusions they do. Because the features to logistic regression are often human-designed, one way to understand a classifier`s decision is to understand the role each feature plays in the decision. Logistic regression can be combined with statistical tests (the likelihood ratio test, or the Wald test); investigating whether a particular feature is significant by one of these tests, or inspecting its magnitude (how large is the weight w associated with the feature?) can help us interpret why the classifier made the decision it makes. This is enormously important for building transparent models. ", "Bloom_type": "remember", "question": "In logistic regression, what aspect helps in understanding why the classifier reached its conclusion?", "options": ["The significance of individual features", "The number of observations", "The complexity of the model", "The overall accuracy of the model"], "complexity": 0}, {"id": 14, "context": "Furthermore, in addition to its use as a classifier, logistic regression in NLP and many other fields is widely used as an analytic tool for testing hypotheses about the effect of various explanatory variables (features). In text classification, perhaps we want to know if logically negative words (no, not, never) are more likely to be associated with negative sentiment, or if negative reviews of movies are more likely to discuss the cinematography. However, in doing so it`s necessary to control for potential confounds: other factors that might influence sentiment (the movie genre, the year it was made, perhaps the length of the review in words). Or we might be studying the relationship between NLP-extracted linguistic features and non-linguistic outcomes (hospital readmissions, political outcomes, or product sales), but need to control for confounds (the age of the patient, the county of voting, the brand of the product). In such cases, logistic regression allows us to test whether some feature is associated with some outcome above and beyond the effect of other features. ", "Bloom_type": "remember", "question": "In what way does logistic regression help in controlling for potential confounds when analyzing data?", "options": ["It provides a statistical method to isolate the impact of one variable while holding others constant.", "It eliminates all irrelevant variables from the analysis.", "It automatically adjusts for any existing correlations within the dataset.", "It predicts future trends based on historical data."], "complexity": 0}, {"id": 15, "context": " Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision. ", "Bloom_type": "remember", "question": "In logistic regression, what does the sigmoid function do?", "options": ["It transforms the weighted sum into a probability between 0 and 1.", "It calculates the absolute value of the weights.", "It determines the threshold for making decisions.", "It normalizes the feature values before classification."], "complexity": 0}, {"id": 16, "context": " Logistic regression can be used with two classes (e.g., positive and negative sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.). ", "Bloom_type": "remember", "question": "In what way is logistic regression utilized?", "options": ["It can handle both binary and multi-class classifications.", "It is primarily used for predicting binary outcomes.", "It is only applicable for multi-class classifications.", "It cannot be applied to any type of data."], "complexity": 0}, {"id": 17, "context": "Logistic regression was developed in the field of statistics, where it was used for the analysis of binary data by the 1960s, and was particularly common in medicine (Cox, 1969). Starting in the late 1970s it became widely used in linguistics as one of the formal foundations of the study of linguistic variation (Sankoff and Labov, 1979). ", "Bloom_type": "remember", "question": "In which field did logistic regression originate?", "options": ["Statistics", "Medicine", "Linguistics", "Psychology"], "complexity": 0}, {"id": 18, "context": "Nonetheless, logistic regression didn`t become common in natural language processing until the 1990s, when it seems to have appeared simultaneously from two directions. The first source was the neighboring fields of information retrieval and speech processing, both of which had made use of regression, and both of which lent many other statistical techniques to NLP. Indeed a very early use of logistic regression for document routing was one of the first NLP applications to use (LSI) embeddings as word representations (Schutze et al., 1995). ", "Bloom_type": "remember", "question": "When did logistic regression start becoming popular in natural language processing?", "options": ["The 1990s", "The 1980s", "The 1970s", "The 1960s"], "complexity": 0}, {"id": 19, "context": "At the same time in the early 1990s logistic regression was developed and applied to NLP at IBM Research under the name maximum entropy modeling or maxent (Berger et al., 1996), seemingly independent of the statistical literature. Under that name it was applied to language modeling (Rosenfeld, 1996), part-of-speech tagging (Ratnaparkhi, 1996), parsing (Ratnaparkhi, 1997), coreference resolution (Kehler, 1997b), and text classification (Nigam et al., 1999). ", "Bloom_type": "remember", "question": "What method was used for applying logistic regression in natural language processing before its development?", "options": ["Maximum Entropy Modeling", "Logistic Regression", "Bayesian Networks", "Decision Trees"], "complexity": 0}, {"id": 20, "context": "1. Treat the target word and a neighboring context word as positive examples. 2. Randomly sample other words in the lexicon to get negative samples. 3. Use logistic regression to train a classifier to distinguish those two cases. 4. Use the learned weights as the embeddings. ", "Bloom_type": "remember", "question": "In the process of training a classifier using logistic regression, what is the primary goal?", "options": ["To classify positive examples from negative ones", "To find the most frequent words in the lexicon", "To predict the next word in a sentence", "To determine the sentiment of a sentence"], "complexity": 0}, {"id": 21, "context": "The dot product c  to  (since the elements in word2vec embeddings can be negative, the dot product can be negative). To turn the dot product into a probability, we`ll use the logistic or sigmoid function  (x), the fundamental core of logistic regression: ", "Bloom_type": "remember", "question": "In logistic regression, what is used to convert the dot product into a probability?", "options": ["Sigmoid function", "Linear function", "Cosine similarity", "Euclidean distance"], "complexity": 0}, {"id": 22, "context": " Dense vector models have dimensionality 501000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are likely to occur nearby in text`. This probability is computed from the dot product between the embeddings for the two words. ", "Bloom_type": "remember", "question": "In word2vec, what algorithm is used to train the logistic regression classifier?", "options": ["Skip-gram", "K-means clustering", "SVM (Support Vector Machines)", "DBSCAN (Density-Based Spatial Clustering of Applications with Noise)"], "complexity": 0}, {"id": 23, "context": "The use of dense vectors to model word meaning, and indeed the term embedding, grew out of the latent semantic indexing (LSI) model (Deerwester et al., 1988) recast as LSA (latent semantic analysis) (Deerwester et al., 1990). In LSA singular value decompositionSVD is applied to a term-document matrix (each cell weighted by log frequency and normalized by entropy), and then the first 300 dimensions are used as the LSA embedding. Singular Value Decomposition (SVD) is a method for finding the most important dimensions of a data set, those dimensions along which the data varies the most. LSA was then quickly widely applied: as a cognitive model Landauer and Dumais (1997), and for tasks like spell checking (Jones and Martin, 1997), language modeling (Bellegarda 1997, Coccaro and Jurafsky 1998, Bellegarda 2000), morphology induction (Schone and Jurafsky 2000, Schone and Jurafsky 2001b), multiword expressions (MWEs) (Schone and Jurafsky, 2001a), and essay grading (Rehder et al., 1998). Related models were simultaneously developed and applied to word sense disambiguation by Schutze (1992b). LSA also led to the earliest use of embeddings to represent words in a probabilistic classifier, in the logistic regression document router of Schutze et al. (1995). The idea of SVD on the term-term matrix (rather than the term-document matrix) as a model of meaning for NLP was proposed soon after LSA by Schutze (1992b). Schutze applied the low-rank (97-dimensional) embeddings produced by SVD to the task of word sense disambiguation, analyzed the resulting semantic space, and also suggested possible techniques like dropping high-order dimensions. See Schutze (1997). ", "Bloom_type": "remember", "question": "In what way did logistic regression contribute to the development of word embeddings?", "options": ["It introduced the concept of using dense vectors to model word meaning.", "It directly created the first 300 dimensions for LSA embedding.", "It utilized SVD on the term-term matrix instead of the term-document matrix.", "It was the primary application of LSA in cognitive models."], "complexity": 0}, {"id": 24, "context": "Instead, a modern neural network is a network of small computing units, each of which takes a vector of input values and produces a single output value. In this chapter we introduce the neural net applied to classification. The architecture we introduce is called a feedforward network because the computation proceeds iteratively from one layer of units to the next. The use of modern neural nets is often called deep learning, because modern networks are often deep (have many layers). Neural networks share much of the same mathematics as logistic regression. But neural networks are a more powerful classifier than logistic regression, and indeed a minimal neural network (technically one with a single hidden layer`) can be shown to learn any function. ", "Bloom_type": "remember", "question": "What does the term \"deep learning\" refer to when referring to neural networks?", "options": ["The application of neural networks to solve classification problems", "The ability of neural networks to perform complex tasks", "The use of large datasets for training neural networks", "The mathematical similarity between neural networks and logistic regression"], "complexity": 0}, {"id": 25, "context": "Neural net classifiers are different from logistic regression in another way. With logistic regression, we applied the regression classifier to many different tasks by developing many rich kinds of feature templates based on domain knowledge. When working with neural networks, it is more common to avoid most uses of rich handderived features, instead building neural networks that take raw words as inputs and learn to induce features as part of the process of learning to classify. We saw examples of this kind of representation learning for embeddings in Chapter 6. Nets that are very deep are particularly good at representation learning. For that reason deep neural nets are the right tool for tasks that offer sufficient data to learn features automatically. ", "Bloom_type": "remember", "question": "In what type of machine learning task do neural networks typically use raw word inputs rather than rich hand-derived features?", "options": ["Classification", "Clustering", "Regression", "Association rule mining"], "complexity": 0}, {"id": 26, "context": "That means we can think of a neural network classifier with one hidden layer as building a vector h which is a hidden layer representation of the input, and then running standard multinomial logistic regression on the features that the network develops in h. By contrast, in Chapter 5 the features were mainly designed by hand via feature templates. So a neural network is like multinomial logistic regression, but (a) with many layers, since a deep neural network is like layer after layer of logistic regression classifiers; (b) with those intermediate layers having many possible activation functions (tanh, ReLU, sigmoid) instead of just sigmoid (although we`ll continue to use  for convenience to mean any activation function); (c) rather than forming the features by feature templates, the prior layers of the network induce the feature representations themselves. ", "Bloom_type": "remember", "question": "In what way does a neural network differ from standard multinomial logistic regression?", "options": ["It uses more layers.", "It has fewer layers.", "It uses different activation functions.", "It forms features manually."], "complexity": 0}, {"id": 27, "context": "Fig. 7.10 shows a sketch of this architecture. As we mentioned earlier, adding this hidden layer to our logistic regression classifier allows the network to represent the non-linear interactions between features. This alone might give us a better sentiment classifier. ", "Bloom_type": "remember", "question": "In logistic regression, what does adding a hidden layer allow?", "options": ["It enables the representation of non-linear relationships between features.", "It increases the computational speed of the model.", "It simplifies the decision boundary for classification.", "It reduces the need for feature scaling."], "complexity": 0}, {"id": 28, "context": "First, we`ll need a loss function that models the distance between the system output and the gold output, and it`s common to use the loss function used for logistic regression, the cross-entropy loss. ", "Bloom_type": "remember", "question": "In machine learning, what is commonly used as a loss function when modeling the distance between the system output and the gold output?", "options": ["Cross-Entropy Loss", "Mean Squared Error", "Log Loss", "RMSprop"], "complexity": 0}, {"id": 29, "context": "Third, gradient descent requires knowing the gradient of the loss function, the vector that contains the partial derivative of the loss function with respect to each of the parameters. In logistic regression, for each observation we could directly compute the derivative of the loss function with respect to an individual w or b. But for neural networks, with millions of parameters in many layers, it`s much harder to see how to compute the partial derivative of some weight in layer 1 when the loss is attached to some much later layer. How do we partial out the loss over all those intermediate layers? The answer is the algorithm called error backpropagation or backward differentiation. ", "Bloom_type": "remember", "question": "In logistic regression, why can't we directly compute the derivative of the loss function with respect to an individual \\( w \\) or \\( b \\)?", "options": ["Because computing derivatives is computationally expensive.", "Because there are too many observations.", "Because the loss function is not differentiable at certain points.", "Because the data is missing."], "complexity": 0}, {"id": 30, "context": "The cross-entropy loss that is used in neural networks is the same one we saw for logistic regression. If the neural network is being used as a binary classifier, with the sigmoid at the final layer, the loss function is the same logistic regression loss we saw in Eq. 5.23: ", "Bloom_type": "remember", "question": "In which scenario would you use logistic regression?", "options": ["To determine if a patient has diabetes based on blood test results", "To classify images of handwritten digits", "To predict the probability of rain tomorrow", "To calculate the distance between two points"], "complexity": 0}, {"id": 31, "context": "For logistic regression we can initialize gradient descent with all the weights and biases having the value 0. In neural networks, by contrast, we need to initialize the weights with small random numbers. It`s also helpful to normalize the input values to have 0 mean and unit variance. ", "Bloom_type": "remember", "question": "In logistic regression, why is it important to initialize the weights and biases with zero values?", "options": ["To align the initial parameters with the data distribution", "To ensure the model converges faster", "To prevent overfitting of the model", "To make the computation more efficient"], "complexity": 0}, {"id": 32, "context": "The roots of the neural language model lie in multiple places. One was the application in the 1990s, again in Jelinek`s group at IBM Research, of discriminative classifiers to language models. Roni Rosenfeld in his dissertation (Rosenfeld, 1992) first applied logistic regression (under the name maximum entropy or maxent models) to language modeling in that IBM lab, and published a more fully formed version in Rosenfeld (1996). His model integrated various sorts of information in a logistic regression predictor, including n-gram information along with ", "Bloom_type": "remember", "question": "In which year did Roni Rosenfeld publish a more fully developed version of his logistic regression model for language modeling?", "options": ["1997", "1985", "1993", "1994"], "complexity": 0}, {"id": 33, "context": "For sequence classification we represent the entire input to be classified by a single vector. We can represent a sequence in various ways. One way is to take the sum or the mean of the last output vector from each token in the sequence. For BERT, we instead add a new unique token to the vocabulary called [CLS], and prepended it to the start of all input sequences, both during pretraining and encoding. The output vector in the final layer of the model for the [CLS] input represents the entire input sequence and serves as the input to a classifier head, a logistic regression or neural network classifier that makes the relevant decision. ", "Bloom_type": "remember", "question": "In sequence classification using logistic regression, what does the output vector in the final layer of the model for the [CLS] input represent?", "options": ["The entire input sequence", "The average of all tokens", "The total number of tokens", "The sum of all tokens"], "complexity": 0}, {"id": 34, "context": "Learning in CRFs relies on the same supervised learning algorithms we presented for logistic regression. Given a sequence of observations, feature functions, and corresponding outputs, we use stochastic gradient descent to train the weights to maximize the log-likelihood of the training corpus. The local nature of linear-chain CRFs means that the forward-backward algorithm introduced for HMMs in Appendix A can be extended to a CRF version that will efficiently compute the necessary derivatives. As with logistic regression, L1 or L2 regularization is important. ", "Bloom_type": "remember", "question": "In what way does logistic regression differ from linear-chain CRFs?", "options": ["Logistic regression cannot handle sequences of observations directly.", "Logistic regression uses stochastic gradient descent for weight optimization.", "Linear-chain CRFs require more computational resources than logistic regression.", "L1 or L2 regularization is not applicable to logistic regression."], "complexity": 0}, {"id": 35, "context": "The most common algorithms involve supervised classification: a training set is labeled for the affective meaning to be detected, and a classifier is built using features extracted from the training set. As with sentiment analysis, if the training set is large enough, and the test set is sufficiently similar to the training set, simply using all the words or all the bigrams as features in a powerful classifier like SVM or logistic regression, as described in Fig. 4.2 in Chapter 4, is an excellent algorithm whose performance is hard to beat. Thus we can treat affective meaning classification of a text sample as simple document classification. ", "Bloom_type": "remember", "question": "In which type of machine learning does logistic regression play a significant role?", "options": ["Supervised learning", "Unsupervised learning", "Reinforcement learning", "Deep learning"], "complexity": 0}, {"id": 36, "context": "In the first part of the book we introduce the fundamental suite of algorithmic tools that make up the modern neural language model that is the heart of end-to-end NLP systems. We begin with tokenization and preprocessing, as well as useful algorithms like computing edit distance, and then proceed to the tasks of classification, logistic regression, neural networks, proceeding through feedforward networks, recurrent networks, and then transformers. We`ll also see the role of embeddings as a model of word meaning. ", "Bloom_type": "comprehension", "question": "What algorithm do we start with when introducing the fundamental suite of algorithmic tools for neural language models?", "options": ["Tokenization and preprocessing", "Logistic Regression", "Neural Networks", "Feedforward Networks"], "complexity": 1}, {"id": 37, "context": "Many kinds of machine learning algorithms are used to build classifiers. This chapter introduces naive Bayes; the following one introduces logistic regression. These exemplify two ways of doing classification. Generative classifiers like naive Bayes build a model of how a class could generate some input data. Given an observation, they return the class most likely to have generated the observation. Discriminative classifiers like logistic regression instead learn what features from the input are most useful to discriminate between the different possible classes. While discriminative systems are often more accurate and hence more commonly used, generative classifiers still have a role. ", "Bloom_type": "comprehension", "question": "What distinguishes discriminative classifiers from generative classifiers?", "options": ["Discriminative classifiers aim to maximize the probability of observing the input data given the class label, unlike generative classifiers which try to estimate the joint distribution of inputs and outputs.", "Discriminative classifiers use all available data for training whereas generative classifiers only consider the target variable.", "Generative classifiers focus on predicting continuous values while discriminative classifiers predict discrete categories.", "Logistic regression is less accurate than naive Bayes."], "complexity": 1}, {"id": 38, "context": "In this chapter we introduce an algorithm that is admirably suited for discovering the link between features or clues and some particular outcome: logistic regression. Indeed, logistic regression is one of the most important analytic tools in the social and natural sciences. In natural language processing, logistic regression is the baseline supervised machine learning algorithm for classification, and also has a very close relationship with neural networks. As we will see in Chapter 7, a neural network can be viewed as a series of logistic regression classifiers stacked on top of each other. Thus the classification and machine learning techniques introduced here will play an important role throughout the book. ", "Bloom_type": "comprehension", "question": "What is the significance of logistic regression in various fields?", "options": ["It is crucial for analyzing outcomes in social and natural sciences.", "It serves as the foundation for all types of machine learning algorithms.", "Logistic regression is only applicable in natural language processing.", "Logistic regression is less significant than other statistical methods."], "complexity": 1}, {"id": 39, "context": "Logistic regression can be used to classify an observation into one of two classes (like positive sentiment` and negative sentiment`), or into one of many classes. Because the mathematics for the two-class case is simpler, we`ll describe this special case of logistic regression first in the next few sections, and then briefly summarize the use of multinomial logistic regression for more than two classes in Section 5.3. We`ll introduce the mathematics of logistic regression in the next few sections. ", "Bloom_type": "comprehension", "question": "What is the primary focus when discussing logistic regression?", "options": ["Two-class classification", "Multinomial logistic regression", "Classification into multiple classes", "Simpler mathematical calculations"], "complexity": 1}, {"id": 40, "context": "The goal of binary logistic regression is to train a classifier that can make a binary decision about the class of a new input observation. Here we introduce the sigmoid classifier that will help us make this decision. ", "Bloom_type": "comprehension", "question": "What is the primary objective of binary logistic regression?", "options": ["To classify inputs into two categories based on their features", "To predict continuous outcomes for each input", "To find the best fitting line for linear data", "To calculate probabilities for each input independently"], "complexity": 1}, {"id": 41, "context": "Logistic regression solves this task by learning, from a training set, a vector of weights and a bias term. Each weight wi is a real number, and is associated with one of the input features xi. The weight wi represents how important that input feature is to the classification decision, and can be positive (providing evidence that the instance being classified belongs in the positive class) or negative (providing evidence that the instance being classified belongs in the negative class). Thus we might expect in a sentiment task the word awesome to have a high positive weight, and abysmal to have a very negative weight. The bias term, also called the intercept, is another real number that`s added to the weighted inputs. ", "Bloom_type": "comprehension", "question": "What does logistic regression do when it comes to solving tasks involving classification?", "options": ["It learns a function that maps input data points to probabilities.", "It predicts numerical values based on input features.", "It calculates the exact probability of each class.", "It ignores the importance of input features."], "complexity": 1}, {"id": 42, "context": "Let`s have some examples of applying logistic regression as a classifier for language tasks. ", "Bloom_type": "comprehension", "question": "What is an example of using logistic regression as a classifier for language tasks?", "options": ["Classifying emails into spam and non-spam", "Predicting stock market trends", "Determining weather conditions", "Analyzing heart disease risk factors"], "complexity": 1}, {"id": 43, "context": "Logistic regression is applied to all sorts of NLP tasks, and any property of the input can be a feature. Consider the task of period disambiguation: deciding if a period is the end of a sentence or part of a word, by classifying each period into one of two classes, EOS (end-of-sentence) and not-EOS. We might use features like x1 below expressing that the current word is lower case, perhaps with a positive weight. Or a feature expressing that the current word is in our abbreviations dictionary (Prof.), perhaps with a negative weight. A feature can also express a combination of properties. For example a period following an upper case word is likely to be an EOS, but if the word itself is St. and the previous word is capitalized then the period is likely part of a shortening of the word street following a street name. ", "Bloom_type": "comprehension", "question": "How does logistic regression classify periods in sentences based on their properties?", "options": ["By assigning weights to different features and using a threshold to determine if a period is an EOS or not-EOS.", "By randomly selecting features for classification without considering their properties.", "By ignoring the properties of the input and focusing solely on the length of the sentence.", "By always predicting the EOS regardless of the input properties."], "complexity": 1}, {"id": 44, "context": "Logistic regression has a number of advantages over naive Bayes. Naive Bayes has overly strong conditional independence assumptions. Consider two features which are strongly correlated; in fact, imagine that we just add the same feature f1 twice. Naive Bayes will treat both copies of f1 as if they were separate, multiplying them both in, overestimating the evidence. By contrast, logistic regression is much more robust to correlated features; if two features f1 and f2 are perfectly correlated, regression will simply assign part of the weight to w1 and part to w2. Thus when there are many correlated features, logistic regression will assign a more accurate probability than naive Bayes. So logistic regression generally works better on larger documents or datasets and is a common default. ", "Bloom_type": "comprehension", "question": "Explain how logistic regression differs from naive Bayes in handling correlated features?", "options": ["Naive Bayes treats each copy of the same feature as independent.", "Logistic regression assigns all weights equally regardless of correlation.", "Logistic regression ignores correlated features entirely.", "Naive Bayes accurately estimates probabilities for correlated features."], "complexity": 1}, {"id": 45, "context": "If you work out the matrix arithmetic, you can see that the estimated score of the first output class y1 (before we take the softmax) will correctly turn out to be w1  x + b1. One helpful interpretation of the weight matrix W is to see each row wk as a prototype of class k. The weight vector wk that is learned represents the class as a kind of template. Since two vectors that are more similar to each other have a higher dot product with each other, the dot product acts as a similarity function. Logistic regression is thus learning an exemplar representation for each class, such that incoming vectors are assigned the class k they are most similar to from the K classes. ", "Bloom_type": "comprehension", "question": "What does logistic regression do by learning an exemplar representation for each class based on their similarities?", "options": ["It learns prototypes of classes.", "It calculates the exact values of weights.", "It assigns vectors to the nearest class.", "It measures the distance between vectors."], "complexity": 1}, {"id": 46, "context": "How are the parameters of the model, the weights w and bias b, learned? Logistic regression is an instance of supervised classification in which we know the correct label y (either 0 or 1) for each observation x. What the system produces via Eq. 5.5 is y, the system`s estimate of the true y. We want to learn parameters (meaning w and b) that make y for each training observation as close as possible to the true y. ", "Bloom_type": "comprehension", "question": "How do logistic regression models determine their parameters \\( w \\) and \\( b \\)?", "options": ["Using gradient descent to adjust weights iteratively until convergence.", "By minimizing the sum of squared errors between predicted and actual values.", "Through maximizing the likelihood function based on observed data.", "By setting all weights to zero and adjusting them incrementally."], "complexity": 1}, {"id": 47, "context": "For logistic regression, this loss function is conveniently convex. A convex function has at most one minimum; there are no local minima to get stuck in, so gradient descent starting from any point is guaranteed to find the minimum. (By contrast, the loss for multi-layer neural networks is non-convex, and gradient descent may get stuck in local minima for neural network training and never find the global optimum.) ", "Bloom_type": "comprehension", "question": "Explain how logistic regression differs from other machine learning models in terms of its loss function?", "options": ["Logistic regression\u2019s loss function is always convex, ensuring convergence to the global minimum.", "Logistic regression uses a non-convex loss function which can lead to multiple local minima.", "Logistic regression does not have a loss function because it is a deterministic model.", "Logistic regression requires more computational resources than other models due to its complex loss function."], "complexity": 1}, {"id": 48, "context": "In an actual logistic regression, the parameter vector w is much longer than 1 or 2, since the input feature vector x can be quite long, and we need a weight wi for each xi. For each dimension/variable wi in w (plus the bias b), the gradient will have a component that tells us the slope with respect to that variable. In each dimension wi, we express the slope as a partial derivative  of the loss function. Essentially  wi we`re asking: How much would a small change in that variable wi influence the total loss function L? ", "Bloom_type": "comprehension", "question": "Explain how logistic regression handles a large number of features compared to its parameters?", "options": ["Logistic regression increases the complexity of the model to accommodate more features.", "Logistic regression uses fewer weights per feature.", "Logistic regression simplifies the model by reducing the number of features.", "Logistic regression does not handle features; it only deals with outcomes."], "complexity": 1}, {"id": 49, "context": "Often we want to know more than just the correct classification of an observation. We want to know why the classifier made the decision it did. That is, we want our decision to be interpretable. Interpretability can be hard to define strictly, but the core idea is that as humans we should know why our algorithms reach the conclusions they do. Because the features to logistic regression are often human-designed, one way to understand a classifier`s decision is to understand the role each feature plays in the decision. Logistic regression can be combined with statistical tests (the likelihood ratio test, or the Wald test); investigating whether a particular feature is significant by one of these tests, or inspecting its magnitude (how large is the weight w associated with the feature?) can help us interpret why the classifier made the decision it makes. This is enormously important for building transparent models. ", "Bloom_type": "comprehension", "question": "How does interpreting the role of features in logistic regression contribute to model transparency?", "options": ["It helps identify which features have the most impact on the classifier\u2019s decisions.", "It allows users to directly manipulate the weights of the features.", "It enables the removal of all irrelevant features from the dataset.", "It guarantees that the model will always make accurate predictions."], "complexity": 1}, {"id": 50, "context": "Furthermore, in addition to its use as a classifier, logistic regression in NLP and many other fields is widely used as an analytic tool for testing hypotheses about the effect of various explanatory variables (features). In text classification, perhaps we want to know if logically negative words (no, not, never) are more likely to be associated with negative sentiment, or if negative reviews of movies are more likely to discuss the cinematography. However, in doing so it`s necessary to control for potential confounds: other factors that might influence sentiment (the movie genre, the year it was made, perhaps the length of the review in words). Or we might be studying the relationship between NLP-extracted linguistic features and non-linguistic outcomes (hospital readmissions, political outcomes, or product sales), but need to control for confounds (the age of the patient, the county of voting, the brand of the product). In such cases, logistic regression allows us to test whether some feature is associated with some outcome above and beyond the effect of other features. ", "Bloom_type": "comprehension", "question": "In which field is logistic regression most commonly applied besides being a classifier?", "options": ["Medical diagnosis", "Text classification", "Financial forecasting", "Educational assessment"], "complexity": 1}, {"id": 51, "context": " Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision. ", "Bloom_type": "comprehension", "question": "What does logistic regression do when it comes to extracting features from an input?", "options": ["It applies mathematical operations directly to raw data.", "It transforms inputs into binary values.", "It converts inputs into numerical values.", "It identifies patterns within categorical variables."], "complexity": 1}, {"id": 52, "context": " Logistic regression can be used with two classes (e.g., positive and negative sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.). ", "Bloom_type": "comprehension", "question": "What are some applications of logistic regression?", "options": ["For both binary and multi-class classification problems", "Only for binary classification problems", "Logistic regression cannot handle more than two classes", "It is only used for predicting numerical values"], "complexity": 1}, {"id": 53, "context": "Logistic regression was developed in the field of statistics, where it was used for the analysis of binary data by the 1960s, and was particularly common in medicine (Cox, 1969). Starting in the late 1970s it became widely used in linguistics as one of the formal foundations of the study of linguistic variation (Sankoff and Labov, 1979). ", "Bloom_type": "comprehension", "question": "In which fields did logistic regression become increasingly popular starting from the late 1970s?", "options": ["Linguistics and Statistics", "Statistics and Linguistics", "Medicine and Linguistics", "Medicine and Statistics"], "complexity": 1}, {"id": 54, "context": "Nonetheless, logistic regression didn`t become common in natural language processing until the 1990s, when it seems to have appeared simultaneously from two directions. The first source was the neighboring fields of information retrieval and speech processing, both of which had made use of regression, and both of which lent many other statistical techniques to NLP. Indeed a very early use of logistic regression for document routing was one of the first NLP applications to use (LSI) embeddings as word representations (Schutze et al., 1995). ", "Bloom_type": "comprehension", "question": "When did logistic regression start becoming more commonly used in natural language processing?", "options": ["The 1990s", "In the late 1800s", "During World War II", "The 1970s"], "complexity": 1}, {"id": 55, "context": "At the same time in the early 1990s logistic regression was developed and applied to NLP at IBM Research under the name maximum entropy modeling or maxent (Berger et al., 1996), seemingly independent of the statistical literature. Under that name it was applied to language modeling (Rosenfeld, 1996), part-of-speech tagging (Ratnaparkhi, 1996), parsing (Ratnaparkhi, 1997), coreference resolution (Kehler, 1997b), and text classification (Nigam et al., 1999). ", "Bloom_type": "comprehension", "question": "What were some applications of logistic regression before its development into maximum entropy modeling?", "options": ["Part-of-speech tagging, parsing, coreference resolution, and text classification", "Text classification, parsing, coreference resolution, and part-of-speech tagging", "Language modeling, parsing, coreference resolution, and text classification", "Text classification, language modeling, coreference resolution, and parsing"], "complexity": 1}, {"id": 56, "context": "1. Treat the target word and a neighboring context word as positive examples. 2. Randomly sample other words in the lexicon to get negative samples. 3. Use logistic regression to train a classifier to distinguish those two cases. 4. Use the learned weights as the embeddings. ", "Bloom_type": "comprehension", "question": "What technique is used to train a classifier for distinguishing between positive and negative examples in logistic regression?", "options": ["Logistic regression", "Random sampling", "Neural networks", "Bayesian methods"], "complexity": 1}, {"id": 57, "context": "The dot product c  to  (since the elements in word2vec embeddings can be negative, the dot product can be negative). To turn the dot product into a probability, we`ll use the logistic or sigmoid function  (x), the fundamental core of logistic regression: ", "Bloom_type": "comprehension", "question": "What mathematical operation is typically applied to the dot product obtained from Word2Vec embeddings before using it in Logistic Regression?", "options": ["Squaring", "Absolute value", "Logarithm", "Taking the square root"], "complexity": 1}, {"id": 58, "context": " Dense vector models have dimensionality 501000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are likely to occur nearby in text`. This probability is computed from the dot product between the embeddings for the two words. ", "Bloom_type": "comprehension", "question": "What does the logistic regression classifier do when computing the probability that two words are likely to occur near each other in text?", "options": ["It uses the dot product between the word vectors.", "It calculates the cosine similarity between the word vectors.", "It computes the Euclidean distance between the word vectors.", "It applies a sigmoid function to the dot product."], "complexity": 1}, {"id": 59, "context": "The use of dense vectors to model word meaning, and indeed the term embedding, grew out of the latent semantic indexing (LSI) model (Deerwester et al., 1988) recast as LSA (latent semantic analysis) (Deerwester et al., 1990). In LSA singular value decompositionSVD is applied to a term-document matrix (each cell weighted by log frequency and normalized by entropy), and then the first 300 dimensions are used as the LSA embedding. Singular Value Decomposition (SVD) is a method for finding the most important dimensions of a data set, those dimensions along which the data varies the most. LSA was then quickly widely applied: as a cognitive model Landauer and Dumais (1997), and for tasks like spell checking (Jones and Martin, 1997), language modeling (Bellegarda 1997, Coccaro and Jurafsky 1998, Bellegarda 2000), morphology induction (Schone and Jurafsky 2000, Schone and Jurafsky 2001b), multiword expressions (MWEs) (Schone and Jurafsky, 2001a), and essay grading (Rehder et al., 1998). Related models were simultaneously developed and applied to word sense disambiguation by Schutze (1992b). LSA also led to the earliest use of embeddings to represent words in a probabilistic classifier, in the logistic regression document router of Schutze et al. (1995). The idea of SVD on the term-term matrix (rather than the term-document matrix) as a model of meaning for NLP was proposed soon after LSA by Schutze (1992b). Schutze applied the low-rank (97-dimensional) embeddings produced by SVD to the task of word sense disambiguation, analyzed the resulting semantic space, and also suggested possible techniques like dropping high-order dimensions. See Schutze (1997). ", "Bloom_type": "comprehension", "question": "What did the logistic regression document router contribute to the field of natural language processing?", "options": ["Both A and B", "Introduced the concept of using SVD on the term-document matrix instead of the term-term matrix.", "Proposed the use of embeddings to represent words in a probabilistic classifier.", "Demonstrated the effectiveness of LSA in various applications."], "complexity": 1}, {"id": 60, "context": "Instead, a modern neural network is a network of small computing units, each of which takes a vector of input values and produces a single output value. In this chapter we introduce the neural net applied to classification. The architecture we introduce is called a feedforward network because the computation proceeds iteratively from one layer of units to the next. The use of modern neural nets is often called deep learning, because modern networks are often deep (have many layers). Neural networks share much of the same mathematics as logistic regression. But neural networks are a more powerful classifier than logistic regression, and indeed a minimal neural network (technically one with a single hidden layer`) can be shown to learn any function. ", "Bloom_type": "comprehension", "question": "What distinguishes neural networks from logistic regression?", "options": ["Neural networks can handle non-linear relationships better.", "Neural networks have fewer layers.", "Logistic regression uses more complex algorithms.", "Neural networks require less data for training."], "complexity": 1}, {"id": 61, "context": "Neural net classifiers are different from logistic regression in another way. With logistic regression, we applied the regression classifier to many different tasks by developing many rich kinds of feature templates based on domain knowledge. When working with neural networks, it is more common to avoid most uses of rich handderived features, instead building neural networks that take raw words as inputs and learn to induce features as part of the process of learning to classify. We saw examples of this kind of representation learning for embeddings in Chapter 6. Nets that are very deep are particularly good at representation learning. For that reason deep neural nets are the right tool for tasks that offer sufficient data to learn features automatically. ", "Bloom_type": "comprehension", "question": "What distinguishes logistic regression from other types of neural network classifiers?", "options": ["Both A and B are true distinctions between logistic regression and neural network classifiers.", "Logistic regression applies regression techniques to various tasks using pre-defined feature sets.", "Neural networks use richly derived features exclusively, avoiding any manual feature engineering.", "Deep neural networks excel at feature extraction through extensive training."], "complexity": 1}, {"id": 62, "context": "That means we can think of a neural network classifier with one hidden layer as building a vector h which is a hidden layer representation of the input, and then running standard multinomial logistic regression on the features that the network develops in h. By contrast, in Chapter 5 the features were mainly designed by hand via feature templates. So a neural network is like multinomial logistic regression, but (a) with many layers, since a deep neural network is like layer after layer of logistic regression classifiers; (b) with those intermediate layers having many possible activation functions (tanh, ReLU, sigmoid) instead of just sigmoid (although we`ll continue to use  for convenience to mean any activation function); (c) rather than forming the features by feature templates, the prior layers of the network induce the feature representations themselves. ", "Bloom_type": "comprehension", "question": "What distinguishes a neural network from traditional logistic regression?", "options": ["Both A and C are true.", "Neural networks have more layers and use different activation functions.", "Logistic regression uses only sigmoid activations.", "Traditional logistic regression requires hand-designed features."], "complexity": 1}, {"id": 63, "context": "Fig. 7.10 shows a sketch of this architecture. As we mentioned earlier, adding this hidden layer to our logistic regression classifier allows the network to represent the non-linear interactions between features. This alone might give us a better sentiment classifier. ", "Bloom_type": "comprehension", "question": "What does Fig. 7.10 illustrate about logistic regression?", "options": ["The addition of a hidden layer improves classification accuracy", "The structure of a neural network", "Logistic regression can only classify linearly separable data", "The importance of feature interactions for machine learning models"], "complexity": 1}, {"id": 64, "context": "First, we`ll need a loss function that models the distance between the system output and the gold output, and it`s common to use the loss function used for logistic regression, the cross-entropy loss. ", "Bloom_type": "comprehension", "question": "What type of loss function is commonly used when applying logistic regression?", "options": ["Cross-Entropy Loss", "Mean Squared Error (MSE)", "Huber Loss", "RMSprop Loss"], "complexity": 1}, {"id": 65, "context": "Third, gradient descent requires knowing the gradient of the loss function, the vector that contains the partial derivative of the loss function with respect to each of the parameters. In logistic regression, for each observation we could directly compute the derivative of the loss function with respect to an individual w or b. But for neural networks, with millions of parameters in many layers, it`s much harder to see how to compute the partial derivative of some weight in layer 1 when the loss is attached to some much later layer. How do we partial out the loss over all those intermediate layers? The answer is the algorithm called error backpropagation or backward differentiation. ", "Bloom_type": "comprehension", "question": "How does error backpropagation help in computing the partial derivatives in deep learning models?", "options": ["It computes the derivatives by summing up the errors from all previous layers.", "It simplifies the computation by reducing the number of parameters.", "It allows direct calculation of derivatives at every step using gradients.", "It uses a different method to calculate derivatives based on the network architecture."], "complexity": 1}, {"id": 66, "context": "The cross-entropy loss that is used in neural networks is the same one we saw for logistic regression. If the neural network is being used as a binary classifier, with the sigmoid at the final layer, the loss function is the same logistic regression loss we saw in Eq. 5.23: ", "Bloom_type": "comprehension", "question": "What is the relationship between the cross-entropy loss in neural networks and logistic regression?", "options": ["The cross-entropy loss in neural networks is identical to logistic regression.", "The cross-entropy loss in neural networks is different from logistic regression.", "Logistic regression is only applicable when using softmax activation functions.", "Neural networks cannot use logistic regression."], "complexity": 1}, {"id": 67, "context": "For logistic regression we can initialize gradient descent with all the weights and biases having the value 0. In neural networks, by contrast, we need to initialize the weights with small random numbers. It`s also helpful to normalize the input values to have 0 mean and unit variance. ", "Bloom_type": "comprehension", "question": "What is a key difference between initializing weights for logistic regression and neural networks?", "options": ["Both A and B are correct.", "Logistic regression initializes weights with all zeros while neural networks use small random numbers.", "Neural networks require normalization of inputs unlike logistic regression.", "Logistic regression does not need initialization of weights and biases."], "complexity": 1}, {"id": 68, "context": "The roots of the neural language model lie in multiple places. One was the application in the 1990s, again in Jelinek`s group at IBM Research, of discriminative classifiers to language models. Roni Rosenfeld in his dissertation (Rosenfeld, 1992) first applied logistic regression (under the name maximum entropy or maxent models) to language modeling in that IBM lab, and published a more fully formed version in Rosenfeld (1996). His model integrated various sorts of information in a logistic regression predictor, including n-gram information along with ", "Bloom_type": "comprehension", "question": "In which year did Roni Rosenfeld publish a more fully developed version of his logistic regression model for language modeling?", "options": ["1998", "1990", "1995", "2000"], "complexity": 1}, {"id": 69, "context": "For sequence classification we represent the entire input to be classified by a single vector. We can represent a sequence in various ways. One way is to take the sum or the mean of the last output vector from each token in the sequence. For BERT, we instead add a new unique token to the vocabulary called [CLS], and prepended it to the start of all input sequences, both during pretraining and encoding. The output vector in the final layer of the model for the [CLS] input represents the entire input sequence and serves as the input to a classifier head, a logistic regression or neural network classifier that makes the relevant decision. ", "Bloom_type": "comprehension", "question": "What does the [CLS] token do in BERT?", "options": ["It represents the entire input sequence and serves as the input to a classifier head.", "It adds noise to the input sequence.", "It changes the order of tokens in the sequence.", "It removes the need for a classifier head."], "complexity": 1}, {"id": 70, "context": "Learning in CRFs relies on the same supervised learning algorithms we presented for logistic regression. Given a sequence of observations, feature functions, and corresponding outputs, we use stochastic gradient descent to train the weights to maximize the log-likelihood of the training corpus. The local nature of linear-chain CRFs means that the forward-backward algorithm introduced for HMMs in Appendix A can be extended to a CRF version that will efficiently compute the necessary derivatives. As with logistic regression, L1 or L2 regularization is important. ", "Bloom_type": "comprehension", "question": "What is an important consideration when using logistic regression compared to other models?", "options": ["The need for L1 or L2 regularization", "The efficiency of computing derivatives", "The importance of feature functions", "The requirement for stochastic gradient descent"], "complexity": 1}, {"id": 71, "context": "The most common algorithms involve supervised classification: a training set is labeled for the affective meaning to be detected, and a classifier is built using features extracted from the training set. As with sentiment analysis, if the training set is large enough, and the test set is sufficiently similar to the training set, simply using all the words or all the bigrams as features in a powerful classifier like SVM or logistic regression, as described in Fig. 4.2 in Chapter 4, is an excellent algorithm whose performance is hard to beat. Thus we can treat affective meaning classification of a text sample as simple document classification. ", "Bloom_type": "comprehension", "question": "Explain how logistic regression fits into the classification methods discussed in the context?", "options": ["Logistic regression is one of the simplest classifiers among those mentioned.", "Logistic regression is used exclusively for sentiment analysis.", "Logistic regression is only applicable when dealing with binary data.", "Logistic regression cannot be applied to affective meaning classification."], "complexity": 1}, {"id": 72, "context": "In the first part of the book we introduce the fundamental suite of algorithmic tools that make up the modern neural language model that is the heart of end-to-end NLP systems. We begin with tokenization and preprocessing, as well as useful algorithms like computing edit distance, and then proceed to the tasks of classification, logistic regression, neural networks, proceeding through feedforward networks, recurrent networks, and then transformers. We`ll also see the role of embeddings as a model of word meaning. ", "Bloom_type": "application", "question": "What algorithm do we start with when learning about neural language models?", "options": ["Logistic Regression", "Feedforward Networks", "Recurrent Networks", "Transformers"], "complexity": 2}, {"id": 73, "context": "Many kinds of machine learning algorithms are used to build classifiers. This chapter introduces naive Bayes; the following one introduces logistic regression. These exemplify two ways of doing classification. Generative classifiers like naive Bayes build a model of how a class could generate some input data. Given an observation, they return the class most likely to have generated the observation. Discriminative classifiers like logistic regression instead learn what features from the input are most useful to discriminate between the different possible classes. While discriminative systems are often more accurate and hence more commonly used, generative classifiers still have a role. ", "Bloom_type": "application", "question": "Which type of classifier is primarily concerned with predicting the probability of an event occurring based on input variables?", "options": ["Logistic Regression", "Naive Bayes Classifier", "Support Vector Machine", "K-Nearest Neighbors"], "complexity": 2}, {"id": 74, "context": "In this chapter we introduce an algorithm that is admirably suited for discovering the link between features or clues and some particular outcome: logistic regression. Indeed, logistic regression is one of the most important analytic tools in the social and natural sciences. In natural language processing, logistic regression is the baseline supervised machine learning algorithm for classification, and also has a very close relationship with neural networks. As we will see in Chapter 7, a neural network can be viewed as a series of logistic regression classifiers stacked on top of each other. Thus the classification and machine learning techniques introduced here will play an important role throughout the book. ", "Bloom_type": "application", "question": "What is the primary purpose of logistic regression?", "options": ["To classify data points into categories", "To discover the correlation between two variables", "To predict continuous outcomes", "To find patterns in large datasets"], "complexity": 2}, {"id": 75, "context": "Logistic regression can be used to classify an observation into one of two classes (like positive sentiment` and negative sentiment`), or into one of many classes. Because the mathematics for the two-class case is simpler, we`ll describe this special case of logistic regression first in the next few sections, and then briefly summarize the use of multinomial logistic regression for more than two classes in Section 5.3. We`ll introduce the mathematics of logistic regression in the next few sections. ", "Bloom_type": "application", "question": "What is the primary focus when applying logistic regression?", "options": ["Classifying observations into two categories", "Classifying observations into three categories", "Classifying observations into four categories", "Classifying observations into any number of categories"], "complexity": 2}, {"id": 76, "context": "The goal of binary logistic regression is to train a classifier that can make a binary decision about the class of a new input observation. Here we introduce the sigmoid classifier that will help us make this decision. ", "Bloom_type": "application", "question": "What is the primary objective of training a logistic regression model?", "options": ["To classify data points as belonging to two distinct classes", "To predict continuous values for each feature", "To find the best fit line for linear regression", "To calculate the mean squared error"], "complexity": 2}, {"id": 77, "context": "Logistic regression solves this task by learning, from a training set, a vector of weights and a bias term. Each weight wi is a real number, and is associated with one of the input features xi. The weight wi represents how important that input feature is to the classification decision, and can be positive (providing evidence that the instance being classified belongs in the positive class) or negative (providing evidence that the instance being classified belongs in the negative class). Thus we might expect in a sentiment task the word awesome to have a high positive weight, and abysmal to have a very negative weight. The bias term, also called the intercept, is another real number that`s added to the weighted inputs. ", "Bloom_type": "application", "question": "What does logistic regression learn from the training data?", "options": ["The optimal values for the weights and bias terms", "The probability distribution of the output classes", "The correlation between the input features and the target variable", "The accuracy of the model on unseen data"], "complexity": 2}, {"id": 78, "context": "Let`s have some examples of applying logistic regression as a classifier for language tasks. ", "Bloom_type": "application", "question": "What is the first step in using logistic regression as a classifier?", "options": ["Selecting features", "Collecting data", "Training the model on labeled data", "Calculating probabilities"], "complexity": 2}, {"id": 79, "context": "Logistic regression is applied to all sorts of NLP tasks, and any property of the input can be a feature. Consider the task of period disambiguation: deciding if a period is the end of a sentence or part of a word, by classifying each period into one of two classes, EOS (end-of-sentence) and not-EOS. We might use features like x1 below expressing that the current word is lower case, perhaps with a positive weight. Or a feature expressing that the current word is in our abbreviations dictionary (Prof.), perhaps with a negative weight. A feature can also express a combination of properties. For example a period following an upper case word is likely to be an EOS, but if the word itself is St. and the previous word is capitalized then the period is likely part of a shortening of the word street following a street name. ", "Bloom_type": "application", "question": "What type of feature could be used to determine whether a period is at the end of a sentence?", "options": ["A feature representing the capitalization of the preceding word.", "A feature indicating the presence of punctuation marks after the period.", "A feature showing the frequency of periods in sentences.", "A feature denoting the length of the sentence."], "complexity": 2}, {"id": 80, "context": "Logistic regression has a number of advantages over naive Bayes. Naive Bayes has overly strong conditional independence assumptions. Consider two features which are strongly correlated; in fact, imagine that we just add the same feature f1 twice. Naive Bayes will treat both copies of f1 as if they were separate, multiplying them both in, overestimating the evidence. By contrast, logistic regression is much more robust to correlated features; if two features f1 and f2 are perfectly correlated, regression will simply assign part of the weight to w1 and part to w2. Thus when there are many correlated features, logistic regression will assign a more accurate probability than naive Bayes. So logistic regression generally works better on larger documents or datasets and is a common default. ", "Bloom_type": "application", "question": "Which method is less prone to overestimating the evidence due to correlated features?", "options": ["Logistic Regression", "Naive Bayes", "Both methods equally", "Neither method"], "complexity": 2}, {"id": 81, "context": "If you work out the matrix arithmetic, you can see that the estimated score of the first output class y1 (before we take the softmax) will correctly turn out to be w1  x + b1. One helpful interpretation of the weight matrix W is to see each row wk as a prototype of class k. The weight vector wk that is learned represents the class as a kind of template. Since two vectors that are more similar to each other have a higher dot product with each other, the dot product acts as a similarity function. Logistic regression is thus learning an exemplar representation for each class, such that incoming vectors are assigned the class k they are most similar to from the K classes. ", "Bloom_type": "application", "question": "What does logistic regression learn about each class?", "options": ["Logistic regression learns a linear transformation of input features to represent each class.", "Logistic regression learns a probability distribution over all possible classes.", "Logistic regression learns a distance metric between input vectors and class prototypes.", "Logistic regression learns a feature map that maps inputs directly to their class labels."], "complexity": 2}, {"id": 82, "context": "How are the parameters of the model, the weights w and bias b, learned? Logistic regression is an instance of supervised classification in which we know the correct label y (either 0 or 1) for each observation x. What the system produces via Eq. 5.5 is y, the system`s estimate of the true y. We want to learn parameters (meaning w and b) that make y for each training observation as close as possible to the true y. ", "Bloom_type": "application", "question": "What method is used to update the parameters w and b in logistic regression?", "options": ["Gradient descent", "Backpropagation", "Newton-Raphson method", "Expectation-Maximization algorithm"], "complexity": 2}, {"id": 83, "context": "For logistic regression, this loss function is conveniently convex. A convex function has at most one minimum; there are no local minima to get stuck in, so gradient descent starting from any point is guaranteed to find the minimum. (By contrast, the loss for multi-layer neural networks is non-convex, and gradient descent may get stuck in local minima for neural network training and never find the global optimum.) ", "Bloom_type": "application", "question": "Which of the following statements about logistic regression is true?", "options": ["Gradient descent will always converge to the global minimum for logistic regression.", "Logistic regression can only handle linearly separable data.", "The loss function for logistic regression is always convex.", "Multi-layer neural networks cannot be trained using gradient descent."], "complexity": 2}, {"id": 84, "context": "In an actual logistic regression, the parameter vector w is much longer than 1 or 2, since the input feature vector x can be quite long, and we need a weight wi for each xi. For each dimension/variable wi in w (plus the bias b), the gradient will have a component that tells us the slope with respect to that variable. In each dimension wi, we express the slope as a partial derivative  of the loss function. Essentially  wi we`re asking: How much would a small change in that variable wi influence the total loss function L? ", "Bloom_type": "application", "question": "What does the gradient in logistic regression represent?", "options": ["The rate at which the loss function changes with respect to a single weight", "The sum of all weights in the model", "The average value of the loss function across all data points", "The number of features in the dataset"], "complexity": 2}, {"id": 85, "context": "Often we want to know more than just the correct classification of an observation. We want to know why the classifier made the decision it did. That is, we want our decision to be interpretable. Interpretability can be hard to define strictly, but the core idea is that as humans we should know why our algorithms reach the conclusions they do. Because the features to logistic regression are often human-designed, one way to understand a classifier`s decision is to understand the role each feature plays in the decision. Logistic regression can be combined with statistical tests (the likelihood ratio test, or the Wald test); investigating whether a particular feature is significant by one of these tests, or inspecting its magnitude (how large is the weight w associated with the feature?) can help us interpret why the classifier made the decision it makes. This is enormously important for building transparent models. ", "Bloom_type": "application", "question": "What method can be used to investigate if a specific feature is significant in a logistic regression model?", "options": ["Perform a likelihood ratio test on the coefficients.", "Use the accuracy score directly on the dataset.", "Calculate the mean squared error between predicted values and actual outcomes.", "Run a chi-squared test on the data."], "complexity": 2}, {"id": 86, "context": "Furthermore, in addition to its use as a classifier, logistic regression in NLP and many other fields is widely used as an analytic tool for testing hypotheses about the effect of various explanatory variables (features). In text classification, perhaps we want to know if logically negative words (no, not, never) are more likely to be associated with negative sentiment, or if negative reviews of movies are more likely to discuss the cinematography. However, in doing so it`s necessary to control for potential confounds: other factors that might influence sentiment (the movie genre, the year it was made, perhaps the length of the review in words). Or we might be studying the relationship between NLP-extracted linguistic features and non-linguistic outcomes (hospital readmissions, political outcomes, or product sales), but need to control for confounds (the age of the patient, the county of voting, the brand of the product). In such cases, logistic regression allows us to test whether some feature is associated with some outcome above and beyond the effect of other features. ", "Bloom_type": "application", "question": "What is the primary purpose of using logistic regression in NLP?", "options": ["To analyze the impact of different features on sentiment analysis", "To classify documents based on their content", "To predict numerical values from textual data", "To perform sentiment analysis directly"], "complexity": 2}, {"id": 87, "context": " Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision. ", "Bloom_type": "application", "question": "What is the first step in applying logistic regression?", "options": ["Define the weights of the features", "Choose a dataset for training", "Select an appropriate threshold value", "Calculate the sum of weighted features"], "complexity": 2}, {"id": 88, "context": " Logistic regression can be used with two classes (e.g., positive and negative sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.). ", "Bloom_type": "application", "question": "In which scenario is logistic regression most commonly applied?", "options": ["To determine if an email is spam or not", "To classify images of cats and dogs", "For predicting stock market trends", "To analyze customer reviews for product ratings"], "complexity": 2}, {"id": 89, "context": "Logistic regression was developed in the field of statistics, where it was used for the analysis of binary data by the 1960s, and was particularly common in medicine (Cox, 1969). Starting in the late 1970s it became widely used in linguistics as one of the formal foundations of the study of linguistic variation (Sankoff and Labov, 1979). ", "Bloom_type": "application", "question": "What is the primary application of logistic regression before the 1960s?", "options": ["Classifying binary events", "Predicting continuous outcomes", "Analyzing multivariate data", "Forecasting economic trends"], "complexity": 2}, {"id": 90, "context": "Nonetheless, logistic regression didn`t become common in natural language processing until the 1990s, when it seems to have appeared simultaneously from two directions. The first source was the neighboring fields of information retrieval and speech processing, both of which had made use of regression, and both of which lent many other statistical techniques to NLP. Indeed a very early use of logistic regression for document routing was one of the first NLP applications to use (LSI) embeddings as word representations (Schutze et al., 1995). ", "Bloom_type": "application", "question": "What technique did logistic regression initially borrow from the fields of Information Retrieval and Speech Processing?", "options": ["LSI Embeddings", "Logistic Regression itself", "Regression Techniques", "Statistical Methods"], "complexity": 2}, {"id": 91, "context": "At the same time in the early 1990s logistic regression was developed and applied to NLP at IBM Research under the name maximum entropy modeling or maxent (Berger et al., 1996), seemingly independent of the statistical literature. Under that name it was applied to language modeling (Rosenfeld, 1996), part-of-speech tagging (Ratnaparkhi, 1996), parsing (Ratnaparkhi, 1997), coreference resolution (Kehler, 1997b), and text classification (Nigam et al., 1999). ", "Bloom_type": "application", "question": "What is the main difference between logistic regression and maximum entropy modeling?", "options": ["Logistic regression is based on probability theory, while maximum entropy modeling relies on frequency counts.", "Logistic regression focuses on binary classification, while maximum entropy modeling can handle multi-class problems.", "Maximum entropy modeling uses linear models, whereas logistic regression employs non-linear models.", "Logistic regression requires more computational resources than maximum entropy modeling."], "complexity": 2}, {"id": 92, "context": "1. Treat the target word and a neighboring context word as positive examples. 2. Randomly sample other words in the lexicon to get negative samples. 3. Use logistic regression to train a classifier to distinguish those two cases. 4. Use the learned weights as the embeddings. ", "Bloom_type": "application", "question": "What is the first step in training a logistic regression model for language modeling?", "options": ["Choose positive examples based on context similarity", "Select random words from the lexicon", "Train the model using all available data", "Initialize the weights randomly"], "complexity": 2}, {"id": 93, "context": "The dot product c  to  (since the elements in word2vec embeddings can be negative, the dot product can be negative). To turn the dot product into a probability, we`ll use the logistic or sigmoid function  (x), the fundamental core of logistic regression: ", "Bloom_type": "application", "question": "In logistic regression, what is used to convert the dot product into a probability?", "options": ["Sigmoid function", "Logarithmic function", "Exponential function", "Hyperbolic tangent"], "complexity": 2}, {"id": 94, "context": " Dense vector models have dimensionality 501000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are likely to occur nearby in text`. This probability is computed from the dot product between the embeddings for the two words. ", "Bloom_type": "application", "question": "What does logistic regression classify based on?", "options": ["The dot product of word vectors", "The cosine similarity of word vectors", "The Euclidean distance between word vectors", "The angle between word vectors"], "complexity": 2}, {"id": 95, "context": "The use of dense vectors to model word meaning, and indeed the term embedding, grew out of the latent semantic indexing (LSI) model (Deerwester et al., 1988) recast as LSA (latent semantic analysis) (Deerwester et al., 1990). In LSA singular value decompositionSVD is applied to a term-document matrix (each cell weighted by log frequency and normalized by entropy), and then the first 300 dimensions are used as the LSA embedding. Singular Value Decomposition (SVD) is a method for finding the most important dimensions of a data set, those dimensions along which the data varies the most. LSA was then quickly widely applied: as a cognitive model Landauer and Dumais (1997), and for tasks like spell checking (Jones and Martin, 1997), language modeling (Bellegarda 1997, Coccaro and Jurafsky 1998, Bellegarda 2000), morphology induction (Schone and Jurafsky 2000, Schone and Jurafsky 2001b), multiword expressions (MWEs) (Schone and Jurafsky, 2001a), and essay grading (Rehder et al., 1998). Related models were simultaneously developed and applied to word sense disambiguation by Schutze (1992b). LSA also led to the earliest use of embeddings to represent words in a probabilistic classifier, in the logistic regression document router of Schutze et al. (1995). The idea of SVD on the term-term matrix (rather than the term-document matrix) as a model of meaning for NLP was proposed soon after LSA by Schutze (1992b). Schutze applied the low-rank (97-dimensional) embeddings produced by SVD to the task of word sense disambiguation, analyzed the resulting semantic space, and also suggested possible techniques like dropping high-order dimensions. See Schutze (1997). ", "Bloom_type": "application", "question": "What technique did Schutze initially use to represent words using embeddings?", "options": ["Logistic Regression", "Latent Semantic Analysis", "Linear Discriminant Analysis", "Principal Component Analysis"], "complexity": 2}, {"id": 96, "context": "Instead, a modern neural network is a network of small computing units, each of which takes a vector of input values and produces a single output value. In this chapter we introduce the neural net applied to classification. The architecture we introduce is called a feedforward network because the computation proceeds iteratively from one layer of units to the next. The use of modern neural nets is often called deep learning, because modern networks are often deep (have many layers). Neural networks share much of the same mathematics as logistic regression. But neural networks are a more powerful classifier than logistic regression, and indeed a minimal neural network (technically one with a single hidden layer`) can be shown to learn any function. ", "Bloom_type": "application", "question": "What does the term 'deep learning' refer to?", "options": ["The application of neural networks to solve complex problems", "The study of ancient languages", "The practice of using traditional methods for data analysis", "The development of new algorithms for machine translation"], "complexity": 2}, {"id": 97, "context": "Neural net classifiers are different from logistic regression in another way. With logistic regression, we applied the regression classifier to many different tasks by developing many rich kinds of feature templates based on domain knowledge. When working with neural networks, it is more common to avoid most uses of rich handderived features, instead building neural networks that take raw words as inputs and learn to induce features as part of the process of learning to classify. We saw examples of this kind of representation learning for embeddings in Chapter 6. Nets that are very deep are particularly good at representation learning. For that reason deep neural nets are the right tool for tasks that offer sufficient data to learn features automatically. ", "Bloom_type": "application", "question": "What distinguishes logistic regression from other machine learning models?", "options": ["It does not require any domain-specific knowledge when creating features.", "It requires less computational power than other models.", "It can handle both numerical and categorical data equally well.", "It focuses on predicting continuous outcomes rather than discrete ones."], "complexity": 2}, {"id": 98, "context": "That means we can think of a neural network classifier with one hidden layer as building a vector h which is a hidden layer representation of the input, and then running standard multinomial logistic regression on the features that the network develops in h. By contrast, in Chapter 5 the features were mainly designed by hand via feature templates. So a neural network is like multinomial logistic regression, but (a) with many layers, since a deep neural network is like layer after layer of logistic regression classifiers; (b) with those intermediate layers having many possible activation functions (tanh, ReLU, sigmoid) instead of just sigmoid (although we`ll continue to use  for convenience to mean any activation function); (c) rather than forming the features by feature templates, the prior layers of the network induce the feature representations themselves. ", "Bloom_type": "application", "question": "What does the development of features in a neural network differ from in traditional machine learning?", "options": ["It doesn't involve feature engineering.", "It uses fewer layers.", "It relies solely on hand-designed features.", "It requires less computational power."], "complexity": 2}, {"id": 99, "context": "Fig. 7.10 shows a sketch of this architecture. As we mentioned earlier, adding this hidden layer to our logistic regression classifier allows the network to represent the non-linear interactions between features. This alone might give us a better sentiment classifier. ", "Bloom_type": "application", "question": "What is added to the logistic regression classifier to improve its performance?", "options": ["A hidden layer with nonlinear activation functions", "An additional input layer", "A new output layer", "More training data"], "complexity": 2}, {"id": 100, "context": "First, we`ll need a loss function that models the distance between the system output and the gold output, and it`s common to use the loss function used for logistic regression, the cross-entropy loss. ", "Bloom_type": "application", "question": "What is commonly used as a loss function for logistic regression?", "options": ["Cross-Entropy Loss", "Mean Squared Error (MSE)", "Log Loss", "Huber Loss"], "complexity": 2}, {"id": 101, "context": "Third, gradient descent requires knowing the gradient of the loss function, the vector that contains the partial derivative of the loss function with respect to each of the parameters. In logistic regression, for each observation we could directly compute the derivative of the loss function with respect to an individual w or b. But for neural networks, with millions of parameters in many layers, it`s much harder to see how to compute the partial derivative of some weight in layer 1 when the loss is attached to some much later layer. How do we partial out the loss over all those intermediate layers? The answer is the algorithm called error backpropagation or backward differentiation. ", "Bloom_type": "application", "question": "In the context of neural networks, what technique is used to calculate the gradients of weights across multiple layers?", "options": ["Backward differentiation", "Forward propagation", "Gradient descent", "Error minimization"], "complexity": 2}, {"id": 102, "context": "The cross-entropy loss that is used in neural networks is the same one we saw for logistic regression. If the neural network is being used as a binary classifier, with the sigmoid at the final layer, the loss function is the same logistic regression loss we saw in Eq. 5.23: ", "Bloom_type": "application", "question": "What is the response when using logistic regression as a binary classifier?", "options": ["The same logistic regression loss as in Eq. 5.23.", "The cross-entropy loss of neural networks.", "Both A) and B)", "None of the above"], "complexity": 2}, {"id": 103, "context": "For logistic regression we can initialize gradient descent with all the weights and biases having the value 0. In neural networks, by contrast, we need to initialize the weights with small random numbers. It`s also helpful to normalize the input values to have 0 mean and unit variance. ", "Bloom_type": "application", "question": "What is an important step when initializing weights for logistic regression?", "options": ["Initialize all weights and biases randomly.", "Initialize all weights and biases with the same value.", "Normalize the input values before feeding them into the network.", "Use large initial values for weights and biases."], "complexity": 2}, {"id": 104, "context": "The roots of the neural language model lie in multiple places. One was the application in the 1990s, again in Jelinek`s group at IBM Research, of discriminative classifiers to language models. Roni Rosenfeld in his dissertation (Rosenfeld, 1992) first applied logistic regression (under the name maximum entropy or maxent models) to language modeling in that IBM lab, and published a more fully formed version in Rosenfeld (1996). His model integrated various sorts of information in a logistic regression predictor, including n-gram information along with ", "Bloom_type": "application", "question": "What did Roni Rosenfeld contribute to the field of natural language processing through his work on logistic regression?", "options": ["Proposed a way to combine discriminative classifiers with language models.", "Developed a new algorithm for training neural networks.", "Introduced the concept of maximum likelihood estimation.", "Created a method for integrating n-grams with other linguistic features."], "complexity": 2}, {"id": 105, "context": "For sequence classification we represent the entire input to be classified by a single vector. We can represent a sequence in various ways. One way is to take the sum or the mean of the last output vector from each token in the sequence. For BERT, we instead add a new unique token to the vocabulary called [CLS], and prepended it to the start of all input sequences, both during pretraining and encoding. The output vector in the final layer of the model for the [CLS] input represents the entire input sequence and serves as the input to a classifier head, a logistic regression or neural network classifier that makes the relevant decision. ", "Bloom_type": "application", "question": "What does the output vector from the [CLS] token in BERT serve as?", "options": ["The representation of the entire input sequence", "The average of all tokens in the sequence", "The sum of all tokens in the sequence", "None of the above"], "complexity": 2}, {"id": 106, "context": "Learning in CRFs relies on the same supervised learning algorithms we presented for logistic regression. Given a sequence of observations, feature functions, and corresponding outputs, we use stochastic gradient descent to train the weights to maximize the log-likelihood of the training corpus. The local nature of linear-chain CRFs means that the forward-backward algorithm introduced for HMMs in Appendix A can be extended to a CRF version that will efficiently compute the necessary derivatives. As with logistic regression, L1 or L2 regularization is important. ", "Bloom_type": "application", "question": "What technique is used to optimize the parameters of a Linear Chain Conditional Random Field (CRF)?", "options": ["Stochastic Gradient Descent", "Forward-Backward Algorithm", "Log-Likelihood Maximization", "L1 or L2 Regularization"], "complexity": 2}, {"id": 107, "context": "The most common algorithms involve supervised classification: a training set is labeled for the affective meaning to be detected, and a classifier is built using features extracted from the training set. As with sentiment analysis, if the training set is large enough, and the test set is sufficiently similar to the training set, simply using all the words or all the bigrams as features in a powerful classifier like SVM or logistic regression, as described in Fig. 4.2 in Chapter 4, is an excellent algorithm whose performance is hard to beat. Thus we can treat affective meaning classification of a text sample as simple document classification. ", "Bloom_type": "application", "question": "What type of algorithm is commonly used for affective meaning classification?", "options": ["Logistic Regression", "Decision tree", "Support Vector Machine (SVM)", "K-Nearest Neighbors"], "complexity": 2}]}, "grammar": {"max_id": 74, "Questions": [{"id": 0, "context": "Why would we want to predict upcoming words, or know the probability of a sentence? One reason is for generation: choosing contextually better words. For example we can correct grammar or spelling errors like Their are two midterms, in which There was mistyped as Their, or Everything has improve, in which improve should have been improved. The phrase There are is more probable than Their are, and has improved than has improve, so a language model can help users select the more grammatical variant. Or for a speech system to recognize that you said I will be back soonish and not I will be bassoon dish, it helps to know that back soonish is a more probable sequence. Language models can also help in augmentative and alternative communication (Trnka et al. 2007, Kane et al. 2017). People can use AAC systems if they are physically unable to speak or sign but can instead use eye gaze or other movements to select words from a menu. Word prediction can be used to suggest likely words for the menu. ", "Bloom_type": "remember", "question": "In what way does predicting upcoming words benefit language models?", "options": ["All of the above", "It improves the accuracy of spelling corrections.", "It enhances the ability to understand complex sentences.", "It aids in recognizing natural speech patterns."], "complexity": 0}, {"id": 1, "context": "We mentioned above that perplexity is a function of both the text and the language model: given a text W , different language models will have different perplexities. Because of this, perplexity can be used to compare different language models. For example, here we trained unigram, bigram, and trigram grammars on 38 million words from the Wall Street Journal newspaper. We then computed the perplexity of each of these models on a WSJ test set using Eq. 3.16 for unigrams, Eq. 3.17 for bigrams, and the corresponding equation for trigrams. The table below shows the perplexity of the 1.5 million word test set according to each of the language models. ", "Bloom_type": "remember", "question": "Which type of grammar was trained on 38 million words from the Wall Street Journal newspaper?", "options": ["Trigram Grammar", "Unigram Grammar", "Bigram Grammar", "N-gram Grammar"], "complexity": 0}, {"id": 2, "context": "The resurgence of n-gram language models came from Fred Jelinek and colleagues at the IBM Thomas J. Watson Research Center, who were influenced by Shannon, and James Baker at CMU, who was influenced by the prior, classified work of Leonard Baum and colleagues on these topics at labs like the US Institute for Defense Analyses (IDA) after they were declassified. Independently these two labs successfully used n-grams in their speech recognition systems at the same time (Baker 1975b, Jelinek et al. 1975, Baker 1975a, Bahl et al. 1983, Jelinek 1990). The terms language model and perplexity were first used for this technology by the IBM group. Jelinek and his colleagues used the term language model in a pretty modern way, to mean the entire set of linguistic influences on word sequence probabilities, including grammar, semantics, discourse, and even speaker characteristics, rather than just the particular n-gram model itself. ", "Bloom_type": "remember", "question": "Who introduced the term \"language model\"?", "options": ["The IBM group", "Fred Jelinek", "James Baker", "Leonard Baum"], "complexity": 0}, {"id": 3, "context": "RNN-based language models can also be used to generate text. Text generation is of enormous practical importance, part of tasks like question answering, machine translation, text summarization, grammar correction, story generation, and conversational dialogue; any task where a system needs to produce text, conditioned on some other text. This use of a language model to generate text is one of the areas in which the impact of neural language models on NLP has been the largest. Text generation, along with image generation and code generation, constitute a new area of AI that is often called generative AI. ", "Bloom_type": "remember", "question": "In what way does the use of RNN-based language models for generating text contribute significantly to Natural Language Processing (NLP)?", "options": ["It enables the creation of entirely new types of AI applications beyond traditional NLP.", "It enhances the accuracy of existing NLP techniques.", "It increases the speed at which NLP algorithms process data.", "It allows for more complex reasoning processes within NLP systems."], "complexity": 0}, {"id": 4, "context": "There were also approaches around the turn of the century that were based on syntactic structure (Chapter 18). Models based on transduction grammars (also called synchronous grammars) assign a parallel syntactic tree structure to a pair of sentences in different languages, with the goal of translating the sentences by applying reordering operations on the trees. From a generative perspective, we can view a transduction grammar as generating pairs of aligned sentences in two languages. Some of the most widely used models included the inversion transduction grammar (Wu, 1996) and synchronous context-free grammars (Chiang, 2005), ", "Bloom_type": "remember", "question": "What is one example of a model based on transduction grammars?", "options": ["Inversion transduction grammar", "Synchronous context-free grammars", "Syntactic structure", "Parallel syntactic tree structures"], "complexity": 0}, {"id": 5, "context": "Rule-based methods were also the earliest methods for part-of-speech tagging. Rule-based taggers like the English Constraint Grammar system (Karlsson et al. 1995, Voutilainen 1999) use a two-stage formalism invented in the 1950s and 1960s: (1) a morphological analyzer with tens of thousands of word stem entries returns all parts of speech for a word, then (2) a large set of thousands of constraints are applied to the input sentence to rule out parts of speech inconsistent with the context. ", "Bloom_type": "remember", "question": "Which method was used first for part-of-speech tagging?", "options": ["Rule-based", "Machine learning", "Hybrid", "Statistical"], "complexity": 0}, {"id": 6, "context": "The study of grammar has an ancient pedigree. The grammar of Sanskrit was described by the Indian grammarian Pan. ini sometime between the 7th and 4th centuries BCE, in his famous treatise the As.t.adhyay (8 books`). And our word syntax comes from the Greek syntaxis, meaning setting out together or arrangement, and refers to the way words are arranged together. We have seen syntactic notions in previous chapters like the use of part-of-speech categories (Chapter 17). In this chapter and the next one we introduce formal models for capturing more sophisticated notions of grammatical structure and algorithms for parsing these structures. ", "Bloom_type": "remember", "question": "Who is credited with describing the grammar of Sanskrit?", "options": ["P\u0101\u1e47ini", "Panini", "Sanskrit", "Grammar"], "complexity": 0}, {"id": 7, "context": "Our focus in this chapter is context-free grammars and the CKY algorithm for parsing them. Context-free grammars are the backbone of many formal models of the syntax of natural language (and, for that matter, of computer languages). Syntactic parsing is the task of assigning a syntactic structure to a sentence. Parse trees (whether for context-free grammars or for the dependency or CCG formalisms we introduce in following chapters) can be used in applications such as grammar checking: sentence that cannot be parsed may have grammatical errors (or at least be hard to read). Parse trees can be an intermediate stage of representation for formal semantic analysis. And parsers and the grammatical structure they assign a sentence are a useful text analysis tool for text data science applications that require modeling the relationship of elements in sentences. ", "Bloom_type": "remember", "question": "What is the primary focus of this chapter regarding grammars?", "options": ["Parsing algorithms", "Lexical analysis", "Syntax generation", "Dependency structures"], "complexity": 0}, {"id": 8, "context": "A widely used formal system for modeling constituent structure in natural language is the context-free grammar, or CFG. Context-free grammars are also called phrase-structure grammars, and the formalism is equivalent to Backus-Naur form, or BNF. The idea of basing a grammar on constituent structure dates back to the psychologist Wilhelm Wundt (1900) but was not formalized until Chomsky (1956) and, independently, Backus (1959). ", "Bloom_type": "remember", "question": "Which of the following best describes the origin of the concept of using a formal system based on constituent structure in natural language?", "options": ["It originated from the work of Wilhelm Wundt in the early 20th century.", "The concept emerged as a result of independent developments by both Chomsky and Backus in the mid-20th century.", "The formalization of this concept can be traced back to the work of Wilhelm Wundt in the late 19th century.", "The development of this concept was influenced by the works of both Chomsky and Backus throughout the entire 20th century."], "complexity": 0}, {"id": 9, "context": "m. We can then formally define the language LG generated by a grammar G as the set of strings composed of terminal symbols that can be derived from the designated start symbol S. ", "Bloom_type": "remember", "question": "What is defined as the set of strings composed of terminal symbols that can be derived from the designated start symbol S?", "options": ["The grammar", "The syntax", "The semantics", "The vocabulary"], "complexity": 0}, {"id": 10, "context": "A formal language is defined as a (possibly infinite) set of strings of words. This suggests that we could ask if two grammars are equivalent by asking if they generate the same set of strings. In fact, it is possible to have two distinct context-free grammars generate the same language. We say that two grammars are strongly equivalent if they generate the same set of strings and if they assign the same phrase structure to each sentence (allowing merely for renaming of the non-terminal symbols). Two grammars are weakly equivalent if they generate the same set of strings but do not assign the same phrase structure to each sentence. ", "Bloom_type": "remember", "question": "In formal language theory, what does it mean when two grammars are said to be equivalent?", "options": ["They both generate the same set of strings.", "They both generate different sets of strings.", "One grammar generates all strings while the other generates none.", "The phrases structures assigned to sentences differ."], "complexity": 0}, {"id": 11, "context": "It is sometimes useful to have a normal form for grammars, in which each of the productions takes a particular form. For example, a context-free grammar is in Chomsky normal form (CNF) (Chomsky, 1963) if it is (cid:15)-free and if in addition each production is either of the form A a. That is, the right-hand side of each rule either has two non-terminal symbols or one terminal symbol. Chomsky normal form grammars are binary branching, that is they have binary trees (down to the prelexical nodes). We make use of this binary branching property in the CKY parsing algorithm in Section 18.6. ", "Bloom_type": "remember", "question": "In what way does having a normal form for grammars enhance their usefulness?", "options": ["It increases the efficiency of parsing algorithms.", "It simplifies the process of generating sentences.", "It allows for easier translation into other languages.", "It enables more complex sentence structures."], "complexity": 0}, {"id": 12, "context": "The dynamic programming advantage arises from the context-free nature of our grammar rulesonce a constituent has been discovered in a segment of the input we can record its presence and make it available for use in any subsequent derivation that might require it. This provides both time and storage efficiencies since subtrees can be looked up in a table, not reanalyzed. This section presents the Cocke-KasamiYounger (CKY) algorithm, the most widely used dynamic-programming based approach to parsing. Chart parsing (Kaplan 1973, Kay 1982) is a related approach, and dynamic programming methods are often referred to as chart parsing methods. ", "Bloom_type": "remember", "question": "What does the dynamic programming advantage provide in terms of parsing efficiency?", "options": ["It speeds up the process by avoiding redundant calculations.", "It reduces the need for extensive memory usage.", "It decreases the complexity of syntax analysis significantly.", "It simplifies the grammatical structure of sentences."], "complexity": 0}, {"id": 13, "context": "Figure 18.10 shows the results of applying this entire conversion procedure to the L1 grammar introduced earlier on page 395. Note that this figure doesn`t show the original lexical rules; since these original lexical rules are already in CNF, they all carry over unchanged to the new grammar. Figure 18.10 does, however, show the various places where the process of eliminating unit productions has, in effect, created new lexical rules. For example, all the original verbs have been promoted to both VPs and to Ss in the converted grammar. ", "Bloom_type": "remember", "question": "What is shown in Figure 18.10 regarding the conversion procedure?", "options": ["The newly created lexical rules after elimination of unit productions", "The original lexical rules before conversion", "The overall structure of the converted grammar", "The final lexical rules after complete transformation"], "complexity": 0}, {"id": 14, "context": " In many languages, groups of consecutive words act as a group or a constituent, which can be modeled by context-free grammars (which are also known as phrase-structure grammars). ", "Bloom_type": "remember", "question": "In what type of grammar do groups of consecutive words form constituents?", "options": ["Context-Free Grammar", "Procedural Grammar", "Dependency Grammar", "Syntactic Grammar"], "complexity": 0}, {"id": 15, "context": " A context-free grammar consists of a set of rules or productions, expressed over a set of non-terminal symbols and a set of terminal symbols. Formally, a particular context-free language is the set of strings that can be derived from a particular context-free grammar. ", "Bloom_type": "remember", "question": "In formal linguistics, what does a context-free grammar consist of?", "options": ["A set of rules or productions, expressed over a set of non-terminal symbols", "A set of rules or productions, expressed over a set of terminal symbols", "A set of rules or productions, expressed over both sets of non-terminal and terminal symbols", "A set of rules or productions, expressed over a set of terminal symbols only"], "complexity": 0}, {"id": 16, "context": " CKY restricts the form of the grammar to Chomsky normal form (CNF).  The basic CKY algorithm compactly represents all possible parses of the sen", "Bloom_type": "remember", "question": "What does CKY restrict the form of the grammar to?", "options": ["Chomsky normal form", "Context-free grammars", "Regular grammars", "PDA grammars"], "complexity": 0}, {"id": 17, "context": "The context-free grammar was a formalization of this idea of hierarchical constituency defined in Chomsky (1956) and further expanded upon (and argued against) in Chomsky (1957) and Chomsky (1956/1975). Shortly after Chomsky`s initial work, the context-free grammar was reinvented by Backus (1959) and independently by Naur et al. (1960) in their descriptions of the ALGOL programming language; Backus (1996) noted that he was influenced by the productions of Emil Post and that Naur`s work was independent of his (Backus`) own. After this early work, a great number of computational models of natural language processing were based on context-free grammars because of the early development of efficient parsing algorithms. ", "Bloom_type": "remember", "question": "Who first reinvented the context-free grammar?", "options": ["Backus", "Chomsky", "Naur et al.", "Post"], "complexity": 0}, {"id": 18, "context": "The earliest disambiguation algorithms for parsing were based on probabilistic context-free grammars, first worked out by Booth (1969) and Salomaa (1969); see Appendix C for more history. Neural methods were first applied to parsing at around the same time as statistical parsing methods were developed (Henderson, 1994). In the earliest work neural networks were used to estimate some of the probabilities for statistical constituency parsers (Henderson, 2003, 2004; Emami and Jelinek, 2005) . The next decades saw a wide variety of neural parsing algorithms, including recursive neural architectures (Socher et al., 2011, 2013), encoder-decoder models (Vinyals et al., 2015; Choe and Charniak, 2016), and the idea of focusing on spans (Cross and Huang, 2016). For more on the span-based self-attention approach we describe in this chapter see Stern et al. (2017), Gaddy et al. (2018), Kitaev and Klein (2018), and Kitaev et al. (2019). See Chapter 20 for the parallel history of neural dependency parsing. ", "Bloom_type": "remember", "question": "Which method was not mentioned in the response?", "options": ["Recursive neural architectures", "Encoder-decoder models", "Span-based self-attention approach", "Neural dependency parsing"], "complexity": 0}, {"id": 19, "context": "Figure 19.1 on the next page shows the dependency analysis from Eq. 19.1 but visualized as a tree, alongside its corresponding phrase-structure analysis of the kind given in the prior chapter. Note the absence of nodes corresponding to phrasal constituents or lexical categories in the dependency parse; the internal structure of the dependency parse consists solely of directed relations between words. These headdependent relationships directly encode important information that is often buried in the more complex phrase-structure parses. For example, the arguments to the verb prefer are directly linked to it in the dependency structure, while their connection to the main verb is more distant in the phrase-structure tree. Similarly, morning and Denver, modifiers of flight, are linked to it directly in the dependency structure. This fact that the head-dependent relations are a good proxy for the semantic relationship between predicates and their arguments is an important reason why dependency grammars are currently more common than constituency grammars in natural language processing. ", "Bloom_type": "remember", "question": "In what way does the dependency parse differ from the phrase-structure parse?", "options": ["The dependency parse has no nodes for phrasal constituents.", "The dependency parse uses fewer words.", "The dependency parse represents the internal structure of phrases.", "The dependency parse is less complex than the phrase-structure parse."], "complexity": 0}, {"id": 20, "context": "In addition to specifying the head-dependent pairs, dependency grammars allow us to classify the kinds of grammatical relations, or grammatical function that the dependent plays with respect to its head. These include familiar notions such as subject, direct object and indirect object. In English these notions strongly correlate with, but by no means determine, both position in a sentence and constituent type and are therefore somewhat redundant with the kind of information found in phrase-structure trees. However, in languages with more flexible word order, the information encoded directly in these grammatical relations is critical since phrasebased constituent syntax provides little help. ", "Bloom_type": "remember", "question": "In what way do grammatical functions differ from constituents in determining sentence structure?", "options": ["Grammatical functions provide less information than constituents.", "Grammatical functions are determined solely by position in a sentence.", "Grammatical functions are not influenced by word order flexibility.", "Grammatical functions can only be determined through phrase-based constituent syntax."], "complexity": 0}, {"id": 21, "context": "The dependency-based approach to grammar is much older than the relatively recent phrase-structure or constituency grammars, which date only to the 20th century. Dependency grammar dates back to the Indian grammarian Pan. ini sometime between the 7th and 4th centuries BCE, as well as the ancient Greek linguistic traditions. Contemporary theories of dependency grammar all draw heavily on the 20th century work of Tesni`ere (1959). ", "Bloom_type": "remember", "question": "Which type of grammar theory is considered the oldest?", "options": ["Dependency grammar", "Phrase-structure grammar", "Constituency grammar", "None of the above"], "complexity": 0}, {"id": 22, "context": "Automatic parsing using dependency grammars was first introduced into computational linguistics by early work on machine translation at the RAND Corporation led by David Hays. This work on dependency parsing closely paralleled work on constituent parsing and made explicit use of grammars to guide the parsing process. After this early period, computational work on dependency parsing remained intermittent over the following decades. Notable implementations of dependency parsers for English during this period include Link Grammar (Sleator and Temperley, 1993), Constraint Grammar (Karlsson et al., 1995), and MINIPAR (Lin, 2003). ", "Bloom_type": "remember", "question": "What is an example of a notable implementation of a dependency parser for English mentioned in the context?", "options": ["Link Grammar", "Constraint Grammar", "MINIPAR", "Dependency Parsing"], "complexity": 0}, {"id": 23, "context": "Dependency parsing saw a major resurgence in the late 1990`s with the appearance of large dependency-based treebanks and the associated advent of data driven approaches described in this chapter. Eisner (1996) developed an efficient dynamic programming approach to dependency parsing based on bilexical grammars derived from the Penn Treebank. Covington (2001) introduced the deterministic word by word approach underlying current transition-based approaches. Yamada and Matsumoto (2003) and Kudo and Matsumoto (2002) introduced both the shift-reduce paradigm and the use of supervised machine learning in the form of support vector machines to dependency parsing. ", "Bloom_type": "remember", "question": "In which year did Eisner develop an efficient dynamic programming approach to dependency parsing?", "options": ["1996", "1985", "1990", "2001"], "complexity": 0}, {"id": 24, "context": "The Hobbs algorithm9 is a tree-search algorithm that was the first in a long series of syntax-based methods for identifying reference robustly in naturally occurring text. The input to the Hobbs algorithm is a pronoun to be resolved, together with a syntactic (constituency) parse of the sentences up to and including the current sentence. The details of the algorithm depend on the grammar used, but can be understood from a simplified version due to Kehler et al. (2004) that just searches through the list of NPs in the current and prior sentences. This simplified Hobbs algorithm searches NPs in the following order: (i) in the current sentence from right-to-left, starting with the first NP to the left of the pronoun, (ii) in the previous sentence from left-to-right, (iii) in two sentences prior from left-to-right, and (iv) in ", "Bloom_type": "remember", "question": "In which order does the simplified Hobbs algorithm search for NPs?", "options": ["Right-to-left, Left-to-right, Two sentences prior, Current sentence", "Left-to-right, Right-to-left, Two sentences prior, Current sentence", "Current sentence, Two sentences prior, Left-to-right, Right-to-left", "Two sentences prior, Current sentence, Left-to-right, Right-to-left"], "complexity": 0}, {"id": 25, "context": "Why would we want to predict upcoming words, or know the probability of a sentence? One reason is for generation: choosing contextually better words. For example we can correct grammar or spelling errors like Their are two midterms, in which There was mistyped as Their, or Everything has improve, in which improve should have been improved. The phrase There are is more probable than Their are, and has improved than has improve, so a language model can help users select the more grammatical variant. Or for a speech system to recognize that you said I will be back soonish and not I will be bassoon dish, it helps to know that back soonish is a more probable sequence. Language models can also help in augmentative and alternative communication (Trnka et al. 2007, Kane et al. 2017). People can use AAC systems if they are physically unable to speak or sign but can instead use eye gaze or other movements to select words from a menu. Word prediction can be used to suggest likely words for the menu. ", "Bloom_type": "comprehension", "question": "How does predicting upcoming words benefit language models in generating natural-sounding sentences?", "options": ["It allows the model to choose contextually better words.", "It ensures all words are spelled correctly.", "It prevents the model from making grammatical mistakes.", "It increases the speed of word selection."], "complexity": 1}, {"id": 26, "context": "We mentioned above that perplexity is a function of both the text and the language model: given a text W , different language models will have different perplexities. Because of this, perplexity can be used to compare different language models. For example, here we trained unigram, bigram, and trigram grammars on 38 million words from the Wall Street Journal newspaper. We then computed the perplexity of each of these models on a WSJ test set using Eq. 3.16 for unigrams, Eq. 3.17 for bigrams, and the corresponding equation for trigrams. The table below shows the perplexity of the 1.5 million word test set according to each of the language models. ", "Bloom_type": "comprehension", "question": "Explain how perplexity can be used to compare different language models?", "options": ["Perplexity compares the complexity of a language model\u2019s grammar against a fixed standard.", "Perplexity measures the accuracy of predictions made by a language model.", "Perplexity calculates the number of errors made by a language model when predicting words.", "Perplexity assesses the efficiency of a language model in generating coherent sentences."], "complexity": 1}, {"id": 27, "context": "The resurgence of n-gram language models came from Fred Jelinek and colleagues at the IBM Thomas J. Watson Research Center, who were influenced by Shannon, and James Baker at CMU, who was influenced by the prior, classified work of Leonard Baum and colleagues on these topics at labs like the US Institute for Defense Analyses (IDA) after they were declassified. Independently these two labs successfully used n-grams in their speech recognition systems at the same time (Baker 1975b, Jelinek et al. 1975, Baker 1975a, Bahl et al. 1983, Jelinek 1990). The terms language model and perplexity were first used for this technology by the IBM group. Jelinek and his colleagues used the term language model in a pretty modern way, to mean the entire set of linguistic influences on word sequence probabilities, including grammar, semantics, discourse, and even speaker characteristics, rather than just the particular n-gram model itself. ", "Bloom_type": "comprehension", "question": "What did Jelinek and his colleagues use the term 'language model' to refer to?", "options": ["The entire set of linguistic influences on word sequence probabilities", "Just the specific n-gram model", "Only the grammatical aspects of language", "The technical implementation of n-grams"], "complexity": 1}, {"id": 28, "context": "RNN-based language models can also be used to generate text. Text generation is of enormous practical importance, part of tasks like question answering, machine translation, text summarization, grammar correction, story generation, and conversational dialogue; any task where a system needs to produce text, conditioned on some other text. This use of a language model to generate text is one of the areas in which the impact of neural language models on NLP has been the largest. Text generation, along with image generation and code generation, constitute a new area of AI that is often called generative AI. ", "Bloom_type": "comprehension", "question": "What are some practical applications of using an RNN-based language model for generating text?", "options": ["Story generation and all of the above", "Text summarization and conversational dialogue", "Grammar correction and code generation", "Machine translation and image generation"], "complexity": 1}, {"id": 29, "context": "There were also approaches around the turn of the century that were based on syntactic structure (Chapter 18). Models based on transduction grammars (also called synchronous grammars) assign a parallel syntactic tree structure to a pair of sentences in different languages, with the goal of translating the sentences by applying reordering operations on the trees. From a generative perspective, we can view a transduction grammar as generating pairs of aligned sentences in two languages. Some of the most widely used models included the inversion transduction grammar (Wu, 1996) and synchronous context-free grammars (Chiang, 2005), ", "Bloom_type": "comprehension", "question": "What are some examples of transduction grammars mentioned in the context?", "options": ["Inversion transduction grammar and synchronous context-free grammars", "Synchronous context-free grammars only", "Only inversion transduction grammar was discussed", "No transduction grammars were mentioned"], "complexity": 1}, {"id": 30, "context": "Rule-based methods were also the earliest methods for part-of-speech tagging. Rule-based taggers like the English Constraint Grammar system (Karlsson et al. 1995, Voutilainen 1999) use a two-stage formalism invented in the 1950s and 1960s: (1) a morphological analyzer with tens of thousands of word stem entries returns all parts of speech for a word, then (2) a large set of thousands of constraints are applied to the input sentence to rule out parts of speech inconsistent with the context. ", "Bloom_type": "comprehension", "question": "What was one of the earliest methods for part-of-speech tagging?", "options": ["Rule-based methods", "Machine learning algorithms", "Statistical models", "Hybrid approaches"], "complexity": 1}, {"id": 31, "context": "The study of grammar has an ancient pedigree. The grammar of Sanskrit was described by the Indian grammarian Pan. ini sometime between the 7th and 4th centuries BCE, in his famous treatise the As.t.adhyay (8 books`). And our word syntax comes from the Greek syntaxis, meaning setting out together or arrangement, and refers to the way words are arranged together. We have seen syntactic notions in previous chapters like the use of part-of-speech categories (Chapter 17). In this chapter and the next one we introduce formal models for capturing more sophisticated notions of grammatical structure and algorithms for parsing these structures. ", "Bloom_type": "comprehension", "question": "What does the term'syntax' refer to in linguistics?", "options": ["The set of rules governing sentence structure", "The historical development of languages", "The classification of parts of speech", "The study of phonetics and pronunciation"], "complexity": 1}, {"id": 32, "context": "Our focus in this chapter is context-free grammars and the CKY algorithm for parsing them. Context-free grammars are the backbone of many formal models of the syntax of natural language (and, for that matter, of computer languages). Syntactic parsing is the task of assigning a syntactic structure to a sentence. Parse trees (whether for context-free grammars or for the dependency or CCG formalisms we introduce in following chapters) can be used in applications such as grammar checking: sentence that cannot be parsed may have grammatical errors (or at least be hard to read). Parse trees can be an intermediate stage of representation for formal semantic analysis. And parsers and the grammatical structure they assign a sentence are a useful text analysis tool for text data science applications that require modeling the relationship of elements in sentences. ", "Bloom_type": "comprehension", "question": "What is the primary purpose of using context-free grammars in the context of natural language processing?", "options": ["To analyze the grammatical correctness of sentences", "To create complex sentence structures", "To develop new programming languages", "To improve text classification algorithms"], "complexity": 1}, {"id": 33, "context": "A widely used formal system for modeling constituent structure in natural language is the context-free grammar, or CFG. Context-free grammars are also called phrase-structure grammars, and the formalism is equivalent to Backus-Naur form, or BNF. The idea of basing a grammar on constituent structure dates back to the psychologist Wilhelm Wundt (1900) but was not formalized until Chomsky (1956) and, independently, Backus (1959). ", "Bloom_type": "comprehension", "question": "What did Wilhelm Wundt contribute to the concept of formalizing a grammar based on constituent structure?", "options": ["He formalized the idea of using constituent structure in grammatical analysis.", "He introduced the first formal grammar.", "He developed the first computer program for parsing sentences.", "He proposed the use of Backus-Naur Form (BNF)"], "complexity": 1}, {"id": 34, "context": "m. We can then formally define the language LG generated by a grammar G as the set of strings composed of terminal symbols that can be derived from the designated start symbol S. ", "Bloom_type": "comprehension", "question": "What does the formal definition of a language LG generated by a grammar G describe?", "options": ["The set of all possible strings that can be formed using the grammar\u2019s production rules", "The rules for constructing sentences within the language", "The sequence of steps needed to derive a sentence from the grammar", "The meaning behind each string in the language"], "complexity": 1}, {"id": 35, "context": "A formal language is defined as a (possibly infinite) set of strings of words. This suggests that we could ask if two grammars are equivalent by asking if they generate the same set of strings. In fact, it is possible to have two distinct context-free grammars generate the same language. We say that two grammars are strongly equivalent if they generate the same set of strings and if they assign the same phrase structure to each sentence (allowing merely for renaming of the non-terminal symbols). Two grammars are weakly equivalent if they generate the same set of strings but do not assign the same phrase structure to each sentence. ", "Bloom_type": "comprehension", "question": "Explain how two grammars can be considered equivalent even though they may generate different languages?", "options": ["By generating the same set of strings and assigning identical phrase structures to all sentences.", "By having the same number of rules in their grammar.", "By using the exact same vocabulary in their definitions.", "Both B and C are correct."], "complexity": 1}, {"id": 36, "context": "It is sometimes useful to have a normal form for grammars, in which each of the productions takes a particular form. For example, a context-free grammar is in Chomsky normal form (CNF) (Chomsky, 1963) if it is (cid:15)-free and if in addition each production is either of the form A a. That is, the right-hand side of each rule either has two non-terminal symbols or one terminal symbol. Chomsky normal form grammars are binary branching, that is they have binary trees (down to the prelexical nodes). We make use of this binary branching property in the CKY parsing algorithm in Section 18.6. ", "Bloom_type": "comprehension", "question": "What does having a normal form for grammars mean?", "options": ["It ensures all productions follow specific patterns for easier analysis.", "It simplifies the grammar by removing unnecessary rules.", "It guarantees every production can only involve one type of element.", "It makes grammars more complex but easier to understand."], "complexity": 1}, {"id": 37, "context": "The dynamic programming advantage arises from the context-free nature of our grammar rulesonce a constituent has been discovered in a segment of the input we can record its presence and make it available for use in any subsequent derivation that might require it. This provides both time and storage efficiencies since subtrees can be looked up in a table, not reanalyzed. This section presents the Cocke-KasamiYounger (CKY) algorithm, the most widely used dynamic-programming based approach to parsing. Chart parsing (Kaplan 1973, Kay 1982) is a related approach, and dynamic programming methods are often referred to as chart parsing methods. ", "Bloom_type": "comprehension", "question": "What does the dynamic programming advantage provide in parsing? ", "options": ["Both A and B", "Time efficiency by analyzing subtrees directly", "Storage efficiency through lookup tables", "Neither A nor B"], "complexity": 1}, {"id": 38, "context": "Figure 18.10 shows the results of applying this entire conversion procedure to the L1 grammar introduced earlier on page 395. Note that this figure doesn`t show the original lexical rules; since these original lexical rules are already in CNF, they all carry over unchanged to the new grammar. Figure 18.10 does, however, show the various places where the process of eliminating unit productions has, in effect, created new lexical rules. For example, all the original verbs have been promoted to both VPs and to Ss in the converted grammar. ", "Bloom_type": "comprehension", "question": "What aspect of the grammar was modified by the elimination of unit productions?", "options": ["New lexical rules were added.", "The original lexical rules were retained.", "The grammatical structure remained unchanged.", "No changes were made."], "complexity": 1}, {"id": 39, "context": " In many languages, groups of consecutive words act as a group or a constituent, which can be modeled by context-free grammars (which are also known as phrase-structure grammars). ", "Bloom_type": "comprehension", "question": "What do we call a set of words that work together as a unit in language structure?", "options": ["Constituents", "Phrases", "Sentences", "Words"], "complexity": 1}, {"id": 40, "context": " A context-free grammar consists of a set of rules or productions, expressed over a set of non-terminal symbols and a set of terminal symbols. Formally, a particular context-free language is the set of strings that can be derived from a particular context-free grammar. ", "Bloom_type": "comprehension", "question": "What does a context-free grammar consist of?", "options": ["A set of rules or productions, expressed over a set of non-terminal symbols and a set of terminal symbols.", "A set of instructions for solving problems.", "A collection of all possible sentences in a language.", "A list of examples of grammatical structures."], "complexity": 1}, {"id": 41, "context": " CKY restricts the form of the grammar to Chomsky normal form (CNF).  The basic CKY algorithm compactly represents all possible parses of the sen", "Bloom_type": "comprehension", "question": "What does CKY restrict the grammar to?", "options": ["Chomsky normal form (CNF)", "Context-free grammars only", "Regular expressions", "Syntactic structures"], "complexity": 1}, {"id": 42, "context": "The context-free grammar was a formalization of this idea of hierarchical constituency defined in Chomsky (1956) and further expanded upon (and argued against) in Chomsky (1957) and Chomsky (1956/1975). Shortly after Chomsky`s initial work, the context-free grammar was reinvented by Backus (1959) and independently by Naur et al. (1960) in their descriptions of the ALGOL programming language; Backus (1996) noted that he was influenced by the productions of Emil Post and that Naur`s work was independent of his (Backus`) own. After this early work, a great number of computational models of natural language processing were based on context-free grammars because of the early development of efficient parsing algorithms. ", "Bloom_type": "comprehension", "question": "What did Chomsky argue about the context-free grammar?", "options": ["The context-free grammar is too rigid and does not allow for flexibility.", "The context-free grammar should only be used for simple sentences.", "The context-free grammar cannot handle complex sentence structures.", "The context-free grammar is superior to other grammatical systems."], "complexity": 1}, {"id": 43, "context": "The earliest disambiguation algorithms for parsing were based on probabilistic context-free grammars, first worked out by Booth (1969) and Salomaa (1969); see Appendix C for more history. Neural methods were first applied to parsing at around the same time as statistical parsing methods were developed (Henderson, 1994). In the earliest work neural networks were used to estimate some of the probabilities for statistical constituency parsers (Henderson, 2003, 2004; Emami and Jelinek, 2005) . The next decades saw a wide variety of neural parsing algorithms, including recursive neural architectures (Socher et al., 2011, 2013), encoder-decoder models (Vinyals et al., 2015; Choe and Charniak, 2016), and the idea of focusing on spans (Cross and Huang, 2016). For more on the span-based self-attention approach we describe in this chapter see Stern et al. (2017), Gaddy et al. (2018), Kitaev and Klein (2018), and Kitaev et al. (2019). See Chapter 20 for the parallel history of neural dependency parsing. ", "Bloom_type": "comprehension", "question": "What was one of the early applications of neural methods in parsing?", "options": ["Estimating probabilities for statistical constituency parsers", "Parsing sentences into constituent structures", "Generating syntactic trees from input texts", "Classifying parts-of-speech tags automatically"], "complexity": 1}, {"id": 44, "context": "Figure 19.1 on the next page shows the dependency analysis from Eq. 19.1 but visualized as a tree, alongside its corresponding phrase-structure analysis of the kind given in the prior chapter. Note the absence of nodes corresponding to phrasal constituents or lexical categories in the dependency parse; the internal structure of the dependency parse consists solely of directed relations between words. These headdependent relationships directly encode important information that is often buried in the more complex phrase-structure parses. For example, the arguments to the verb prefer are directly linked to it in the dependency structure, while their connection to the main verb is more distant in the phrase-structure tree. Similarly, morning and Denver, modifiers of flight, are linked to it directly in the dependency structure. This fact that the head-dependent relations are a good proxy for the semantic relationship between predicates and their arguments is an important reason why dependency grammars are currently more common than constituency grammars in natural language processing. ", "Bloom_type": "comprehension", "question": "Explain how the dependency grammar differs from the phrase-structure grammar in terms of encoding semantic relationships?", "options": ["Dependency grammar uses direct links between words to represent semantic relationships, whereas phrase-structure grammar relies on indirect connections.", "Phrase-structure grammar focuses on encoding semantic relationships through head-dependent relations, unlike dependency grammar which does not.", "Dependency grammar provides a better representation of semantic relationships because it avoids the complexity of phrase-structure trees.", "Both types of grammars equally well represent semantic relationships."], "complexity": 1}, {"id": 45, "context": "In addition to specifying the head-dependent pairs, dependency grammars allow us to classify the kinds of grammatical relations, or grammatical function that the dependent plays with respect to its head. These include familiar notions such as subject, direct object and indirect object. In English these notions strongly correlate with, but by no means determine, both position in a sentence and constituent type and are therefore somewhat redundant with the kind of information found in phrase-structure trees. However, in languages with more flexible word order, the information encoded directly in these grammatical relations is critical since phrasebased constituent syntax provides little help. ", "Bloom_type": "comprehension", "question": "What does grammar specifically refer to in relation to dependency grammars?", "options": ["The classification of grammatical functions played by dependents", "The rules governing the structure of sentences", "The correlation between positions and constituent types", "The encoding of word order flexibility"], "complexity": 1}, {"id": 46, "context": "The dependency-based approach to grammar is much older than the relatively recent phrase-structure or constituency grammars, which date only to the 20th century. Dependency grammar dates back to the Indian grammarian Pan. ini sometime between the 7th and 4th centuries BCE, as well as the ancient Greek linguistic traditions. Contemporary theories of dependency grammar all draw heavily on the 20th century work of Tesni`ere (1959). ", "Bloom_type": "comprehension", "question": "Which theory of dependency grammar has its roots in the 20th century?", "options": ["Tesni\u00e8re's work from the 20th century", "Panini's work from the 7th and 4th centuries BCE", "Ancient Greek linguistic traditions", "Dependency grammar dating back to the Indian grammarian Panini"], "complexity": 1}, {"id": 47, "context": "Automatic parsing using dependency grammars was first introduced into computational linguistics by early work on machine translation at the RAND Corporation led by David Hays. This work on dependency parsing closely paralleled work on constituent parsing and made explicit use of grammars to guide the parsing process. After this early period, computational work on dependency parsing remained intermittent over the following decades. Notable implementations of dependency parsers for English during this period include Link Grammar (Sleator and Temperley, 1993), Constraint Grammar (Karlsson et al., 1995), and MINIPAR (Lin, 2003). ", "Bloom_type": "comprehension", "question": "What were some notable implementations of dependency parsers for English mentioned in the context?", "options": ["Link Grammar, Constraint Grammar, and MINIPAR", "Constraint Grammar, MINIPAR, and Dependency Parsing", "Dependency Parsing, MINIPAR, and Link Grammar", "MINIPAR, Constraint Grammar, and Dependency Parsing"], "complexity": 1}, {"id": 48, "context": "Dependency parsing saw a major resurgence in the late 1990`s with the appearance of large dependency-based treebanks and the associated advent of data driven approaches described in this chapter. Eisner (1996) developed an efficient dynamic programming approach to dependency parsing based on bilexical grammars derived from the Penn Treebank. Covington (2001) introduced the deterministic word by word approach underlying current transition-based approaches. Yamada and Matsumoto (2003) and Kudo and Matsumoto (2002) introduced both the shift-reduce paradigm and the use of supervised machine learning in the form of support vector machines to dependency parsing. ", "Bloom_type": "comprehension", "question": "What are some key methods for dependency parsing mentioned in the context?", "options": ["Dynamic programming, bilexical grammars, and support vector machines", "Efficient algorithms, lexical analysis, and syntactic rules", "Dependency trees, phrase structure grammar, and constituency parsing", "Word segmentation, part-of-speech tagging, and named entity recognition"], "complexity": 1}, {"id": 49, "context": "The Hobbs algorithm9 is a tree-search algorithm that was the first in a long series of syntax-based methods for identifying reference robustly in naturally occurring text. The input to the Hobbs algorithm is a pronoun to be resolved, together with a syntactic (constituency) parse of the sentences up to and including the current sentence. The details of the algorithm depend on the grammar used, but can be understood from a simplified version due to Kehler et al. (2004) that just searches through the list of NPs in the current and prior sentences. This simplified Hobbs algorithm searches NPs in the following order: (i) in the current sentence from right-to-left, starting with the first NP to the left of the pronoun, (ii) in the previous sentence from left-to-right, (iii) in two sentences prior from left-to-right, and (iv) in ", "Bloom_type": "comprehension", "question": "What does the simplified Hobbs algorithm do when searching through the list of NPs?", "options": ["It begins with the first NP to the left of the pronoun in the current sentence.", "It starts with the last NP in the current sentence and moves towards the pronoun.", "It skips over all NPs in the current and previous sentences.", "It only looks at NPs in the next sentence."], "complexity": 1}, {"id": 50, "context": "Why would we want to predict upcoming words, or know the probability of a sentence? One reason is for generation: choosing contextually better words. For example we can correct grammar or spelling errors like Their are two midterms, in which There was mistyped as Their, or Everything has improve, in which improve should have been improved. The phrase There are is more probable than Their are, and has improved than has improve, so a language model can help users select the more grammatical variant. Or for a speech system to recognize that you said I will be back soonish and not I will be bassoon dish, it helps to know that back soonish is a more probable sequence. Language models can also help in augmentative and alternative communication (Trnka et al. 2007, Kane et al. 2017). People can use AAC systems if they are physically unable to speak or sign but can instead use eye gaze or other movements to select words from a menu. Word prediction can be used to suggest likely words for the menu. ", "Bloom_type": "application", "question": "What is an application of predicting upcoming words in natural language processing?", "options": ["To assist individuals with disabilities in accessing spoken language.", "To enhance the accuracy of machine translation.", "To improve the efficiency of spell checkers.", "To increase the speed of internet browsing."], "complexity": 2}, {"id": 51, "context": "We mentioned above that perplexity is a function of both the text and the language model: given a text W , different language models will have different perplexities. Because of this, perplexity can be used to compare different language models. For example, here we trained unigram, bigram, and trigram grammars on 38 million words from the Wall Street Journal newspaper. We then computed the perplexity of each of these models on a WSJ test set using Eq. 3.16 for unigrams, Eq. 3.17 for bigrams, and the corresponding equation for trigrams. The table below shows the perplexity of the 1.5 million word test set according to each of the language models. ", "Bloom_type": "application", "question": "Which method should be applied first when comparing different language models based on their perplexity?", "options": ["Compute the perplexity using Eq. 3.16 for unigrams", "Compute the perplexity using Eq. 3.17 for bigrams", "Use the corresponding equation for trigrams", "Compute the perplexity using Eq. 3.16 for unigrams and then compute it again using Eq. 3.17 for bigrams"], "complexity": 2}, {"id": 52, "context": "The resurgence of n-gram language models came from Fred Jelinek and colleagues at the IBM Thomas J. Watson Research Center, who were influenced by Shannon, and James Baker at CMU, who was influenced by the prior, classified work of Leonard Baum and colleagues on these topics at labs like the US Institute for Defense Analyses (IDA) after they were declassified. Independently these two labs successfully used n-grams in their speech recognition systems at the same time (Baker 1975b, Jelinek et al. 1975, Baker 1975a, Bahl et al. 1983, Jelinek 1990). The terms language model and perplexity were first used for this technology by the IBM group. Jelinek and his colleagues used the term language model in a pretty modern way, to mean the entire set of linguistic influences on word sequence probabilities, including grammar, semantics, discourse, and even speaker characteristics, rather than just the particular n-gram model itself. ", "Bloom_type": "application", "question": "What did Jelinek and his colleagues originally refer to as the language model?", "options": ["The entire set of linguistic influences on word sequence probabilities", "The specific n-gram model", "The speech recognition system", "The IBM group"], "complexity": 2}, {"id": 53, "context": "RNN-based language models can also be used to generate text. Text generation is of enormous practical importance, part of tasks like question answering, machine translation, text summarization, grammar correction, story generation, and conversational dialogue; any task where a system needs to produce text, conditioned on some other text. This use of a language model to generate text is one of the areas in which the impact of neural language models on NLP has been the largest. Text generation, along with image generation and code generation, constitute a new area of AI that is often called generative AI. ", "Bloom_type": "application", "question": "What is an example of how RNN-based language models are applied for generating text?", "options": ["Generating responses to questions based on input text", "Using pre-trained models to translate languages", "Creating detailed stories using existing texts as templates", "Automatically writing computer programs"], "complexity": 2}, {"id": 54, "context": "There were also approaches around the turn of the century that were based on syntactic structure (Chapter 18). Models based on transduction grammars (also called synchronous grammars) assign a parallel syntactic tree structure to a pair of sentences in different languages, with the goal of translating the sentences by applying reordering operations on the trees. From a generative perspective, we can view a transduction grammar as generating pairs of aligned sentences in two languages. Some of the most widely used models included the inversion transduction grammar (Wu, 1996) and synchronous context-free grammars (Chiang, 2005), ", "Bloom_type": "application", "question": "Which model is considered a type of synchronous context-free grammar?", "options": ["Synchronous context-free grammar", "Inversion transduction grammar", "Both A and B are types of synchronous context-free grammars", "Neither A nor B are types of synchronous context-free grammars"], "complexity": 2}, {"id": 55, "context": "Rule-based methods were also the earliest methods for part-of-speech tagging. Rule-based taggers like the English Constraint Grammar system (Karlsson et al. 1995, Voutilainen 1999) use a two-stage formalism invented in the 1950s and 1960s: (1) a morphological analyzer with tens of thousands of word stem entries returns all parts of speech for a word, then (2) a large set of thousands of constraints are applied to the input sentence to rule out parts of speech inconsistent with the context. ", "Bloom_type": "application", "question": "What is the first step in applying rule-based methods for part-of-speech tagging?", "options": ["Analyze the morphology of each word", "Identify the parts of speech using constraints", "Apply constraints to eliminate incorrect parts of speech", "Combine both steps"], "complexity": 2}, {"id": 56, "context": "The study of grammar has an ancient pedigree. The grammar of Sanskrit was described by the Indian grammarian Pan. ini sometime between the 7th and 4th centuries BCE, in his famous treatise the As.t.adhyay (8 books`). And our word syntax comes from the Greek syntaxis, meaning setting out together or arrangement, and refers to the way words are arranged together. We have seen syntactic notions in previous chapters like the use of part-of-speech categories (Chapter 17). In this chapter and the next one we introduce formal models for capturing more sophisticated notions of grammatical structure and algorithms for parsing these structures. ", "Bloom_type": "application", "question": "What is the origin of the word'syntax'?", "options": ["It originates from the Greek word'syntaxis'.", "It comes from the Latin word'synthesis'.", "It is derived from the Sanskrit language.", "It is based on the English word'structure'."], "complexity": 2}, {"id": 57, "context": "Our focus in this chapter is context-free grammars and the CKY algorithm for parsing them. Context-free grammars are the backbone of many formal models of the syntax of natural language (and, for that matter, of computer languages). Syntactic parsing is the task of assigning a syntactic structure to a sentence. Parse trees (whether for context-free grammars or for the dependency or CCG formalisms we introduce in following chapters) can be used in applications such as grammar checking: sentence that cannot be parsed may have grammatical errors (or at least be hard to read). Parse trees can be an intermediate stage of representation for formal semantic analysis. And parsers and the grammatical structure they assign a sentence are a useful text analysis tool for text data science applications that require modeling the relationship of elements in sentences. ", "Bloom_type": "application", "question": "What is the primary goal of syntactic parsing?", "options": ["To determine the grammatical structure of the sentence", "To understand the meaning of the sentence", "To identify the parts of speech in the sentence", "To translate the sentence into another language"], "complexity": 2}, {"id": 58, "context": "A widely used formal system for modeling constituent structure in natural language is the context-free grammar, or CFG. Context-free grammars are also called phrase-structure grammars, and the formalism is equivalent to Backus-Naur form, or BNF. The idea of basing a grammar on constituent structure dates back to the psychologist Wilhelm Wundt (1900) but was not formalized until Chomsky (1956) and, independently, Backus (1959). ", "Bloom_type": "application", "question": "What is the origin of the idea behind context-free grammars?", "options": ["Both Chomsky and Backus", "Wilhelm Wundt", "Chomsky", "Backus"], "complexity": 2}, {"id": 59, "context": "m. We can then formally define the language LG generated by a grammar G as the set of strings composed of terminal symbols that can be derived from the designated start symbol S. ", "Bloom_type": "application", "question": "What is the formal definition of the language LG generated by a grammar G?", "options": ["The set of all strings that can be derived from the designated start symbol S using the production rules of G.", "The set of all possible strings formed by combining terminals using any combination of rules.", "The set of all strings that can be derived from the start symbol S using the production rules of G.", "The set of all strings that can be derived from the end symbol E using the production rules of G."], "complexity": 2}, {"id": 60, "context": "A formal language is defined as a (possibly infinite) set of strings of words. This suggests that we could ask if two grammars are equivalent by asking if they generate the same set of strings. In fact, it is possible to have two distinct context-free grammars generate the same language. We say that two grammars are strongly equivalent if they generate the same set of strings and if they assign the same phrase structure to each sentence (allowing merely for renaming of the non-terminal symbols). Two grammars are weakly equivalent if they generate the same set of strings but do not assign the same phrase structure to each sentence. ", "Bloom_type": "application", "question": "What does it mean when two grammars are said to be weakly equivalent?", "options": ["They generate the same set of strings but assign different phrase structures.", "They generate different sets of strings.", "They generate different sets of strings and assign different phrase structures.", "They generate the same set of strings and assign the same phrase structure."], "complexity": 2}, {"id": 61, "context": "It is sometimes useful to have a normal form for grammars, in which each of the productions takes a particular form. For example, a context-free grammar is in Chomsky normal form (CNF) (Chomsky, 1963) if it is (cid:15)-free and if in addition each production is either of the form A a. That is, the right-hand side of each rule either has two non-terminal symbols or one terminal symbol. Chomsky normal form grammars are binary branching, that is they have binary trees (down to the prelexical nodes). We make use of this binary branching property in the CKY parsing algorithm in Section 18.6. ", "Bloom_type": "application", "question": "What does it mean when a grammar is said to be in Chomsky normal form?", "options": ["Every production in the grammar has a single non-terminal on the left-hand side.", "The grammar can only contain terminal symbols.", "Each production in the grammar consists of exactly three non-terminals.", "The grammar allows only left-recursive rules."], "complexity": 2}, {"id": 62, "context": "The dynamic programming advantage arises from the context-free nature of our grammar rulesonce a constituent has been discovered in a segment of the input we can record its presence and make it available for use in any subsequent derivation that might require it. This provides both time and storage efficiencies since subtrees can be looked up in a table, not reanalyzed. This section presents the Cocke-KasamiYounger (CKY) algorithm, the most widely used dynamic-programming based approach to parsing. Chart parsing (Kaplan 1973, Kay 1982) is a related approach, and dynamic programming methods are often referred to as chart parsing methods. ", "Bloom_type": "application", "question": "What does the CKY algorithm primarily focus on when parsing?", "options": ["Finding all possible parse trees simultaneously", "Determining the longest possible parse tree", "Calculating the shortest path through a graph", "Identifying the most frequent parse tree"], "complexity": 2}, {"id": 63, "context": "Figure 18.10 shows the results of applying this entire conversion procedure to the L1 grammar introduced earlier on page 395. Note that this figure doesn`t show the original lexical rules; since these original lexical rules are already in CNF, they all carry over unchanged to the new grammar. Figure 18.10 does, however, show the various places where the process of eliminating unit productions has, in effect, created new lexical rules. For example, all the original verbs have been promoted to both VPs and to Ss in the converted grammar. ", "Bloom_type": "application", "question": "What is the main purpose of applying the conversion procedure shown in Figure 18.10?", "options": ["To create new lexical rules", "To eliminate unit productions entirely", "To promote verbs to VP and SS forms", "To maintain the original lexical rules"], "complexity": 2}, {"id": 64, "context": " In many languages, groups of consecutive words act as a group or a constituent, which can be modeled by context-free grammars (which are also known as phrase-structure grammars). ", "Bloom_type": "application", "question": "What is an example of how groups of words in language can be modeled using a grammar?", "options": ["The phrase 'the quick brown fox'", "The sentence 'I love eating pizza'", "The word 'cat'", "The clause 'if it rains tomorrow'"], "complexity": 2}, {"id": 65, "context": " A context-free grammar consists of a set of rules or productions, expressed over a set of non-terminal symbols and a set of terminal symbols. Formally, a particular context-free language is the set of strings that can be derived from a particular context-free grammar. ", "Bloom_type": "application", "question": "Which step should come first when constructing a context-free grammar?", "options": ["Choose the non-terminal symbols for the grammar", "Select the terminal symbols for the grammar", "Define the production rules based on the chosen symbols", "Combine all steps together"], "complexity": 2}, {"id": 66, "context": " CKY restricts the form of the grammar to Chomsky normal form (CNF).  The basic CKY algorithm compactly represents all possible parses of the sen", "Bloom_type": "application", "question": "What is the primary goal of applying the Chomsky Normal Form (CNF) to grammars?", "options": ["To simplify the parsing process", "To increase the complexity of the grammar", "To reduce the number of rules in the grammar", "To eliminate ambiguity in the language"], "complexity": 2}, {"id": 67, "context": "The context-free grammar was a formalization of this idea of hierarchical constituency defined in Chomsky (1956) and further expanded upon (and argued against) in Chomsky (1957) and Chomsky (1956/1975). Shortly after Chomsky`s initial work, the context-free grammar was reinvented by Backus (1959) and independently by Naur et al. (1960) in their descriptions of the ALGOL programming language; Backus (1996) noted that he was influenced by the productions of Emil Post and that Naur`s work was independent of his (Backus`) own. After this early work, a great number of computational models of natural language processing were based on context-free grammars because of the early development of efficient parsing algorithms. ", "Bloom_type": "application", "question": "Which of the following best describes the evolution of context-free grammars?", "options": ["Efficient parsing algorithms led to a significant increase in the popularity of context-free grammars.", "Context-free grammars were initially developed by Chomsky but later abandoned.", "The context-free grammar was first introduced by Backus and Naur in the 1950s.", "Chomsky's original work on context-free grammars was not well-received."], "complexity": 2}, {"id": 68, "context": "The earliest disambiguation algorithms for parsing were based on probabilistic context-free grammars, first worked out by Booth (1969) and Salomaa (1969); see Appendix C for more history. Neural methods were first applied to parsing at around the same time as statistical parsing methods were developed (Henderson, 1994). In the earliest work neural networks were used to estimate some of the probabilities for statistical constituency parsers (Henderson, 2003, 2004; Emami and Jelinek, 2005) . The next decades saw a wide variety of neural parsing algorithms, including recursive neural architectures (Socher et al., 2011, 2013), encoder-decoder models (Vinyals et al., 2015; Choe and Charniak, 2016), and the idea of focusing on spans (Cross and Huang, 2016). For more on the span-based self-attention approach we describe in this chapter see Stern et al. (2017), Gaddy et al. (2018), Kitaev and Klein (2018), and Kitaev et al. (2019). See Chapter 20 for the parallel history of neural dependency parsing. ", "Bloom_type": "application", "question": "What is the primary method used in early neural parsing algorithms?", "options": ["Statistical parsing methods", "Recursive neural architectures", "Encoder-decoder models", "Span-based self-attention approach"], "complexity": 2}, {"id": 69, "context": "Figure 19.1 on the next page shows the dependency analysis from Eq. 19.1 but visualized as a tree, alongside its corresponding phrase-structure analysis of the kind given in the prior chapter. Note the absence of nodes corresponding to phrasal constituents or lexical categories in the dependency parse; the internal structure of the dependency parse consists solely of directed relations between words. These headdependent relationships directly encode important information that is often buried in the more complex phrase-structure parses. For example, the arguments to the verb prefer are directly linked to it in the dependency structure, while their connection to the main verb is more distant in the phrase-structure tree. Similarly, morning and Denver, modifiers of flight, are linked to it directly in the dependency structure. This fact that the head-dependent relations are a good proxy for the semantic relationship between predicates and their arguments is an important reason why dependency grammars are currently more common than constituency grammars in natural language processing. ", "Bloom_type": "application", "question": "What does the dependency parse show about the internal structure compared to the phrase-structure tree?", "options": ["The dependency parse shows direct encoding of semantic relationships which are not present in the phrase-structure tree.", "The dependency parse shows no direct encoding of semantic relationships.", "The dependency parse shows only the syntactic structure with no semantic meaning.", "The dependency parse shows both syntactic and semantic structures equally well."], "complexity": 2}, {"id": 70, "context": "In addition to specifying the head-dependent pairs, dependency grammars allow us to classify the kinds of grammatical relations, or grammatical function that the dependent plays with respect to its head. These include familiar notions such as subject, direct object and indirect object. In English these notions strongly correlate with, but by no means determine, both position in a sentence and constituent type and are therefore somewhat redundant with the kind of information found in phrase-structure trees. However, in languages with more flexible word order, the information encoded directly in these grammatical relations is critical since phrasebased constituent syntax provides little help. ", "Bloom_type": "application", "question": "What does dependency grammar primarily focus on besides head-dependent pairs?", "options": ["Classifying grammatical functions of dependents with respect to their heads", "Determining the position and constituent type of phrases in sentences", "Encoding direct and indirect objects in sentences", "Providing additional information about constituents not covered by phrase-based syntax"], "complexity": 2}, {"id": 71, "context": "The dependency-based approach to grammar is much older than the relatively recent phrase-structure or constituency grammars, which date only to the 20th century. Dependency grammar dates back to the Indian grammarian Pan. ini sometime between the 7th and 4th centuries BCE, as well as the ancient Greek linguistic traditions. Contemporary theories of dependency grammar all draw heavily on the 20th century work of Tesni`ere (1959). ", "Bloom_type": "application", "question": "Which theory of dependency grammar is most closely associated with the works of Panini?", "options": ["Panini", "Tesni\u00e8re", "Noam Chomsky", "Saussure"], "complexity": 2}, {"id": 72, "context": "Automatic parsing using dependency grammars was first introduced into computational linguistics by early work on machine translation at the RAND Corporation led by David Hays. This work on dependency parsing closely paralleled work on constituent parsing and made explicit use of grammars to guide the parsing process. After this early period, computational work on dependency parsing remained intermittent over the following decades. Notable implementations of dependency parsers for English during this period include Link Grammar (Sleator and Temperley, 1993), Constraint Grammar (Karlsson et al., 1995), and MINIPAR (Lin, 2003). ", "Bloom_type": "application", "question": "What is an example of a dependency parser used for English?", "options": ["Link Grammar", "Constraint Grammar", "MINIPAR", "Dependency Parsing"], "complexity": 2}, {"id": 73, "context": "Dependency parsing saw a major resurgence in the late 1990`s with the appearance of large dependency-based treebanks and the associated advent of data driven approaches described in this chapter. Eisner (1996) developed an efficient dynamic programming approach to dependency parsing based on bilexical grammars derived from the Penn Treebank. Covington (2001) introduced the deterministic word by word approach underlying current transition-based approaches. Yamada and Matsumoto (2003) and Kudo and Matsumoto (2002) introduced both the shift-reduce paradigm and the use of supervised machine learning in the form of support vector machines to dependency parsing. ", "Bloom_type": "application", "question": "What is the main difference between the deterministic word-by-word approach and the shift-reduce paradigm?", "options": ["The deterministic word-by-word approach relies solely on lexical rules, whereas the shift-reduce paradigm incorporates syntactic dependencies.", "The deterministic word-by-word approach uses supervised machine learning, while the shift-reduce paradigm does not.", "The deterministic word-by-word approach requires more computational resources than the shift-reduce paradigm.", "The deterministic word-by-word approach focuses on syntax, while the shift-reduce paradigm emphasizes semantics."], "complexity": 2}, {"id": 74, "context": "The Hobbs algorithm9 is a tree-search algorithm that was the first in a long series of syntax-based methods for identifying reference robustly in naturally occurring text. The input to the Hobbs algorithm is a pronoun to be resolved, together with a syntactic (constituency) parse of the sentences up to and including the current sentence. The details of the algorithm depend on the grammar used, but can be understood from a simplified version due to Kehler et al. (2004) that just searches through the list of NPs in the current and prior sentences. This simplified Hobbs algorithm searches NPs in the following order: (i) in the current sentence from right-to-left, starting with the first NP to the left of the pronoun, (ii) in the previous sentence from left-to-right, (iii) in two sentences prior from left-to-right, and (iv) in ", "Bloom_type": "application", "question": "What is the next step after searching through the list of NPs in the current and prior sentences?", "options": ["Continue searching in the next two sentences", "Search for other pronouns in the same sentence", "Check if there are any other NP structures before the pronoun", "Identify the subject of the sentence"], "complexity": 2}]}, "dialogue": {"max_id": 109, "Questions": [{"id": 0, "context": "Surely we can do better than this! After all, we`ll want a model of word meaning to do all sorts of things for us. It should tell us that some words have similar meanings (cat is similar to dog), others are antonyms (cold is the opposite of hot), some have positive connotations (happy) while others have negative connotations (sad). It should represent the fact that the meanings of buy, sell, and pay offer differing perspectives on the same underlying purchasing event. (If I buy something from you, you`ve probably sold it to me, and I likely paid you.) More generally, a model of word meaning should allow us to draw inferences to address meaning-related tasks like question-answering or dialogue. ", "Bloom_type": "remember", "question": "Why is having a model of word meaning important for understanding language?", "options": ["To facilitate communication and reasoning about language", "To improve pronunciation", "To enhance vocabulary learning", "To increase reading speed"], "complexity": 0}, {"id": 1, "context": "RNN-based language models can also be used to generate text. Text generation is of enormous practical importance, part of tasks like question answering, machine translation, text summarization, grammar correction, story generation, and conversational dialogue; any task where a system needs to produce text, conditioned on some other text. This use of a language model to generate text is one of the areas in which the impact of neural language models on NLP has been the largest. Text generation, along with image generation and code generation, constitute a new area of AI that is often called generative AI. ", "Bloom_type": "remember", "question": "In what type of tasks are RNN-based language models primarily applied for generating text?", "options": ["For all mentioned tasks except for question answering", "Only for question answering", "Not applicable to any of these tasks", "Only for machine translation"], "complexity": 0}, {"id": 2, "context": "Encoder-decoder networks, sometimes called sequence-to-sequence networks, are models capable of generating contextually appropriate, arbitrary length, output sequences given an input sequence. Encoder-decoder networks have been applied to a very wide range of applications including summarization, question answering, and dialogue, but they are particularly popular for machine translation. ", "Bloom_type": "remember", "question": "In which application area are encoder-decoder networks most commonly used?", "options": ["Dialogue", "Summarization", "Question Answering", "Machine Translation"], "complexity": 0}, {"id": 3, "context": "In this second part of the book we introduce fundamental NLP applications: machine translation, information retrieval, question answering, dialogue systems, and speech recognition. ", "Bloom_type": "remember", "question": "Which topic is NOT discussed in the introduction?", "options": ["Machine Translation", "Information Retrieval", "Dialogue Systems", "Speech Recognition"], "complexity": 0}, {"id": 4, "context": "Minimum Bayes risk decoding can also be used for other NLP tasks; indeed it was widely applied to speech recognition (Stolcke et al., 1997; Goel and Byrne, 2000) before being applied to machine translation (Kumar and Byrne, 2004), and has been shown to work well across many other generation tasks as well (e.g., summarization, dialogue, and image captioning (Suzgun et al., 2023a)). ", "Bloom_type": "remember", "question": "In which application of minimum Bayes risk decoding did it first appear?", "options": ["Speech recognition", "Machine translation", "Summarization", "Image captioning"], "complexity": 0}, {"id": 5, "context": "The literature of the fantastic abounds in inanimate objects magically endowed with the gift of speech. From Ovid`s statue of Pygmalion to Mary Shelley`s story about Frankenstein, we continually reinvent stories about creating something and then having a chat with it. Legend has it that after finishing his sculpture Moses, Michelangelo thought it so lifelike that he tapped it on the knee and commanded it to speak. Perhaps this shouldn`t be surprising. Language is the mark of humanity and sentience, and conversation or dialogue is the most fundamental arena of language. It is the first kind of language we learn as children, and the kind we engage in constantly, whether we are ordering lunch, buying train tickets, or talking with our families, friends, or coworkers. ", "Bloom_type": "remember", "question": "What does the passage suggest about the relationship between language and creativity?", "options": ["Dialogue is essential for both communication and understanding.", "Language restricts creativity by limiting what can be said.", "Creativity requires only physical actions, not verbal ones.", "Inanimate objects cannot participate in creative processes."], "complexity": 0}, {"id": 6, "context": "This chapter introduces the fundamental algorithms of programs that use conversation to interact with users. We often distinguish between two kinds of architectures. Task-oriented dialogue systems converse with users to accomplish fixed tasks like controlling appliances or finding restaurants, relying on a data structure called the frame, which represents the knowledge a system needs to acquire from the user (like the time to set an alarm clock). Chatbots, by contrast, are designed to mimic the longer and more unstructured conversations or chats` characteristic of human-human interaction. Modern systems incorporate aspects of both; industrial chatbots like ChatGPT can carry on longer unstructured conversations; industrial digital assistants like Siri or Alexa are generally frame-based dialogue systems. ", "Bloom_type": "remember", "question": "What is the main difference between task-oriented dialogue systems and chatbots?", "options": ["Chatbots focus on accomplishing specific tasks like setting alarms, whereas task-oriented dialogue systems handle general inquiries.", "Task-oriented dialogue systems rely on structured data while chatbots use unstructured data.", "Task-oriented dialogue systems are designed for long-term interactions, unlike chatbots.", "Chatbots require a frame structure to represent user knowledge, unlike task-oriented dialogue systems."], "complexity": 0}, {"id": 7, "context": "The fact that chatbots and dialogue systems are designed for human-computer interaction has strong implications for their design and use. Many of these implications already became clear in one of the earliest chatbots, ELIZA (Weizenbaum, 1966). ELIZA was designed to simulate a Rogerian psychologist, based on a branch of clinical psychology whose methods involve drawing the patient out by reflecting patient`s statements back at them. Rogerian interactions are the rare type of conversation in which, as Weizenbaum points out, one can assume the pose of knowing almost nothing of the real world. If a patient says I went for a long boat ride and the psychiatrist says Tell me about boats, you don`t assume she didn`t know what ", "Bloom_type": "remember", "question": "In the context of chatbot design, why did ELIZA become an early example of how humans interact with computers?", "options": ["To explore the boundaries of human communication", "To test the limits of computer processing speed", "To demonstrate the effectiveness of artificial intelligence", "To showcase the potential of natural language understanding"], "complexity": 0}, {"id": 8, "context": "Both of these issues (emotional engagement and privacy) mean we need to think carefully about how we deploy chatbots and the people who are interacting with them. Dialogue research that uses human participants often requires getting permission from the Institutional Review Board (IRB) of your institution. ", "Bloom_type": "remember", "question": "In dialogue research, what is required before conducting experiments involving human participants?", "options": ["Permission from an IRB (Institutional Review Board)", "Approval from a legal authority", "Funding from a grant agency", "Interviews with potential subjects"], "complexity": 0}, {"id": 9, "context": "Turn structure has important implications for spoken dialogue. A human has to know when to stop talking; the client interrupts (in A16 and C17), so a system that was performing this role must know to stop talking (and that the user might be making a correction). A system also has to know when to start talking. For example, most of the time in conversation, speakers start their turns almost immediately after the other speaker finishes, without a long pause, because people are can usually predict when the other person is about to finish talking. Spoken dialogue systems must also detect whether a user is done speaking, so they can process the utterance and respond. This taskcalled endpointing or endpoint detection can be quite challenging because of noise and because people often pause in the middle of turns. ", "Bloom_type": "remember", "question": "In spoken dialogue systems, what does it mean if a user pauses mid-turn?", "options": ["The system needs to detect the end of the turn.", "The user is likely to continue speaking.", "The system should ignore the pause.", "The system should wait for another user input."], "complexity": 0}, {"id": 10, "context": "Full mixed initiative, while the norm for human-human conversations, can be difficult for dialogue systems. The most primitive dialogue systems tend to use system-initiative, where the system asks a question and the user can`t do anything until they answer it, or user-initiative like simple search engines, where the user specifies a query and the system passively responds. Even modern large language model-based dialogue systems, which come much closer to using full mixed initiative, often don`t have completely natural initiative switching. Getting this right is an important goal for modern systems. ", "Bloom_type": "remember", "question": "In what type of conversation does the system ask questions and the user has no control over when the conversation starts?", "options": ["System-initiative", "Full mixed initiative", "User-initiative", "Primitive dialogue systems"], "complexity": 0}, {"id": 11, "context": "These subtle characteristics of human conversations (turns, speech acts, grounding, dialogue structure, initiative, and implicature) are among the reasons it is difficult to build dialogue systems that can carry on natural conversations with humans. Many of these challenges are active areas of dialogue systems research. ", "Bloom_type": "remember", "question": "What aspect of human conversations makes it challenging for dialogue systems to create natural conversations?", "options": ["The lack of clear turns and speech acts", "The use of complex vocabulary", "The presence of background music", "The absence of grounding"], "complexity": 0}, {"id": 12, "context": "The frame and its slots in a task-based dialogue system specify what the system needs to know to perform its task. A hotel reservation system needs dates and locations. An alarm clock system needs a time. The system`s goal is to fill the slots in the frame with the fillers the user intends, and then perform the relevant action for the user (answering a question, or booking a flight). ", "Bloom_type": "remember", "question": "In a task-based dialogue system, what do the slots in the frame represent?", "options": ["The tasks the system needs to accomplish", "The specific actions the system should take", "The goals of the users", "The responses the system provides"], "complexity": 0}, {"id": 13, "context": "We can make a very simple frame-based dialogue system by wrapping a small amount of code around this slot extractor. Mainly we just need to ask the user questions until all the slots are full, do a database query, then report back to the user, using hand-built templates for generating sentences. ", "Bloom_type": "remember", "question": "How does one create a simple frame-based dialogue system?", "options": ["By asking users repetitive questions until all slots are filled", "By designing complex sentence structures", "By querying databases directly", "Using pre-built templates for responses"], "complexity": 0}, {"id": 14, "context": "While the naive slot-extractor system described above can handle simple dialogues, often we want more complex interactions. For example, we might want to confirm that we`ve understand the user, or ask them to repeat themselves. We can build a more sophisticated system using dialogue acts and dialogue state. ", "Bloom_type": "remember", "question": "In the context of handling more complex interactions in dialogues, what is an important aspect mentioned?", "options": ["Using dialogue acts and dialogue state", "Handling simple dialogues only", "Confirming understanding and repeating requests", "Extracting slots from input"], "complexity": 0}, {"id": 15, "context": "Figure 15.6 shows a tagset for a restaurant recommendation system, and Fig. 15.7 shows these tags labeling a sample dialogue from the HIS system (Young et al., 2010). This example also shows the content of each dialogue act, which are the slot fillers being communicated. So the user might INFORM the system that they want Italian food near a museum, or CONFIRM with the system that the price is reasonable. ", "Bloom_type": "remember", "question": "In the dialogue shown in Fig. 15.7, what does the user communicate about their preferences?", "options": ["They specify the type of cuisine as Italian.", "They confirm the location of the museum.", "They request confirmation on the price range.", "They inform the system about nearby attractions."], "complexity": 0}, {"id": 16, "context": "Dialogue act detection is done just like domain or intent classification, by passing the input sentence through an encoder and adding an act classifier. Often passing in the prior dialogue act as well can improve classification. And since dialogue acts ", "Bloom_type": "remember", "question": "In dialogue act detection, what is added along with the input sentence for better classification?", "options": ["The previous dialogue act", "The speaker's name", "The user's age", "The time of day"], "complexity": 0}, {"id": 17, "context": "In early commercial frame-based systems, the dialogue policy is simple: ask questions until all the slots are full, do a database query, then report back to the user. A more sophisticated dialogue policy can help a system decide when to answer the user`s questions, when to instead ask the user a clarification question, and so on. A dialogue policy thus decides what dialogue act to generate. Choosing a dialogue act to generate, along with its arguments, is sometimes called content planning. ", "Bloom_type": "remember", "question": "In early commercial frame-based systems, how was the dialogue policy determined?", "options": ["Through a series of predefined rules and decisions", "By randomly selecting from predefined options", "Based solely on the complexity of the system", "Using machine learning algorithms for prediction"], "complexity": 0}, {"id": 18, "context": "It is a common practice for dialogue systems to use further labeled data for finetuning. One function of this fine-tuning step is to improve the quality of the dialogue, training the system to produce responses that are sensible and interesting. Another function might be to improve safety, keeping a dialogue system from suggesting harmful actions (like financial fraud, medical harm, inciting hatred, or abusing the user or other people). ", "Bloom_type": "remember", "question": "What is one primary goal of using further labeled data during the fine-tuning process of dialogue systems?", "options": ["To improve the accuracy of the dialogue", "To enhance the efficiency of the dialogue", "To increase the complexity of the dialogue", "To reduce the cost of dialogue"], "complexity": 0}, {"id": 19, "context": "In the simplest method for improving quality and safety, speakers of the language are given an initial prompt and instructions to have high-quality, safe dialogues. They then interact with an initial dialogue system and their responses are used to finetune the model, usually as part of the instruct tuning step we introduced in Chapter 12. Thus a dialogue system learns to answer questions, follow other instructions, and also carry on high-quality, safe dialogues, in a single multi-task learning format. ", "Bloom_type": "remember", "question": "In what way do speakers improve quality and safety through dialogues?", "options": ["Both A) and C)", "By providing feedback", "By refining the instruction system", "By interacting with a dialogue system"], "complexity": 0}, {"id": 20, "context": "Alternatively, systems can be finetuned to to know when to use a search engine. For example, labelers can interact with a system, fact check each of the responses, and whenever the system emits an incorrect response, perform the web search queries that the system should have used to check its answer, and then the interation is recorded and used for fine-tuning. Or labelers can look at a transcript of a language model carrying on a dialogue, and similarly mark every place where a fact was wrong (or out-of-date) and write the set of search queries that would have been appropriate. A system is then fine-tuned to generate search query turns which are again passed to a search engine to generate the search responses. The set of pages or snippets returned by the search engine in the search response turn are then treated as the context for generation, similarly to the retrieval-based questionanswering methods of Chapter 14. ", "Bloom_type": "remember", "question": "In what way can systems be fine-tuned to improve their performance?", "options": ["Both B) and C)", "By interacting directly with humans", "By analyzing transcripts of conversations between models", "By searching for alternative answers online"], "complexity": 0}, {"id": 21, "context": "1. Study the user and task: Understand the users and the task by interviewing users, investigating similar systems, and studying related human-human dialogues. ", "Bloom_type": "remember", "question": "In what way is understanding users and tasks important before developing a system?", "options": ["To build a dialogue model between humans", "To ensure the system can perform well on its own", "To create an engaging interface for the users", "To align with industry standards"], "complexity": 0}, {"id": 22, "context": "These ethical issues are an important area of investigation, including finding ways to mitigate problems of abuse and toxicity, like detecting and responding appropriately to toxic contexts (Wolf et al. 2017, Dinan et al. 2020, Xu et al. 2020). Value sensitive design, carefully considering possible harms in advance (Friedman et al. 2017, Friedman and Hendry 2019) is also important; (Dinan et al., 2021) give a number of suggestions for best practices in dialogue system design. For example getting informed consent from participants, whether they are used for training, or whether they are interacting with a deployed system is important. Because dialogue systems by definition involve human participants, researchers also work on these issues with the Institutional Review Boards (IRB) at their institutions, who help protect the safety of experimental subjects. ", "Bloom_type": "remember", "question": "What does value sensitive design entail?", "options": ["It requires designers to consider all possible harms before proceeding with development.", "It involves designing systems that prioritize efficiency over ethical considerations.", "It focuses solely on maximizing user satisfaction without regard for potential harm.", "It mandates that all dialogue systems be tested only on non-human subjects."], "complexity": 0}, {"id": 23, "context": "Chatbots and dialogue systems are crucial speech and language processing applications that are already widely used commercially. ", "Bloom_type": "remember", "question": "What is a significant application of dialogue systems?", "options": ["Automated customer service", "Virtual reality games", "Social media platforms", "Weather forecasting"], "complexity": 0}, {"id": 24, "context": " In human dialogue, speaking is a kind of action; these acts are referred to as speech acts or dialogue acts. Speakers also attempt to achieve common ground by acknowledging that they have understand each other. Conversation also is characterized by turn structure and dialogue structure. ", "Bloom_type": "remember", "question": "In human dialogue, what do speakers use to acknowledge mutual understanding?", "options": ["Verbal affirmations", "Body language", "Facial expressions", "Non-verbal cues"], "complexity": 0}, {"id": 25, "context": " The dialogue-state architecture augments the GUS frame-and-slot architecture with richer representations and more sophisticated algorithms for keeping track of user`s dialogue acts, policies for generating its own dialogue acts, and a natural language component. ", "Bloom_type": "remember", "question": "In what way does the dialogue-state architecture enhance the GUS frame-and-slot architecture?", "options": ["It adds richer representations and more sophisticated algorithms.", "It simplifies the representation of user dialogues.", "It focuses on improving the generation of user dialogue acts.", "It enhances the natural language processing capabilities."], "complexity": 0}, {"id": 26, "context": " Dialogue systems are a kind of human-computer interaction, and general HCI principles apply in their design, including the role of the user, simulations such as Wizard-of-Oz systems, and the importance of iterative design and testing on real users. ", "Bloom_type": "remember", "question": "In dialogue systems, what is important for designing effective interactions with humans?", "options": ["Understanding the user's needs and behaviors", "The use of advanced AI algorithms", "Creating realistic simulations only", "Ignoring feedback from real users"], "complexity": 0}, {"id": 27, "context": "Computational-implemented theories of dialogue blossomed in the 1970. That period saw the very influential GUS system (Bobrow et al., 1977), which in the late 1970s established the frame-based paradigm that became the dominant industrial paradigm for dialogue systems for over 30 years. ", "Bloom_type": "remember", "question": "In what decade did computational-implemented theories of dialogue begin to flourish?", "options": ["1960", "1980", "1950", "1940"], "complexity": 0}, {"id": 28, "context": "In the 1990s, machine learning models that had first been applied to natural language processing began to be applied to dialogue tasks like slot filling (Miller et al. 1994, Pieraccini et al. 1991). This period also saw lots of analytic work on the linguistic properties of dialogue acts and on machine-learning-based methods for their detection. (Sag and Liberman 1975, Hinkelman and Allen 1989, Nagata and Morimoto 1994, Goodwin 1996, Chu-Carroll 1998, Shriberg et al. 1998, Stolcke et al. 2000, Gravano et al. 2012. This work strongly informed the development of the dialogue-state model (Larsson and Traum, 2000). Dialogue state tracking quickly became an important problem for task-oriented dialogue, and there has been an influential annual evaluation of state-tracking algorithms (Williams et al., 2016). The turn of the century saw a line of work on applying reinforcement learning to dialogue, which first came out of AT&T and Bell Laboratories with work on MDP dialogue systems (Walker 2000, Levin et al. 2000, Singh et al. 2002) along with work on cue phrases, prosody, and rejection and confirmation. Reinforcement learning research turned quickly to the more sophisticated POMDP models (Roy et al. 2000, Lemon et al. 2006, Williams and Young 2007) applied to small slotfilling dialogue tasks. Neural reinforcement learning models have been used both for chatbot systems, for example simulating dialogues between two dialogue systems, rewarding good conversational properties like coherence and ease of answering (Li et al., 2016a), and for task-oriented dialogue (Williams et al., 2017). ", "Bloom_type": "remember", "question": "In what decade did machine learning models begin to be applied to dialogue tasks?", "options": ["The 1990s", "The 1980s", "The 1970s", "The 1960s"], "complexity": 0}, {"id": 29, "context": "By around 2010 the GUS architecture finally began to be widely used commercially in dialogue systems on phones like Apple`s SIRI (Bellegarda, 2013) and other digital assistants. ", "Bloom_type": "remember", "question": "In what year did the GUS architecture start being widely used commercially?", "options": ["2010", "2005", "2015", "2020"], "complexity": 0}, {"id": 30, "context": "Automatic transcription of speech by any speaker in any environment is still far from solved, but ASR technology has matured to the point where it is now viable for many practical tasks. Speech is a natural interface for communicating with smart home appliances, personal assistants, or cellphones, where keyboards are less convenient, in telephony applications like call-routing (Accounting, please) or in sophisticated dialogue applications (I`d like to change the return date of my flight). ASR is also useful for general transcription, for example for automatically generating captions for audio or video text (transcribing movies or videos or live discussions). Transcription is important in fields like law where dictation plays an important role. Finally, ASR is important as part of augmentative communication (interaction between computers and humans with some disability resulting in difficulties or inabilities in typing or audition). The blind Milton famously dictated Paradise Lost to his daughters, and Henry James dictated his later novels after a repetitive stress injury. ", "Bloom_type": "remember", "question": "In what way can automatic speech recognition (ASR) benefit people with disabilities?", "options": ["It can assist them in interacting with devices through voice commands.", "It can replace traditional keyboard usage.", "It can improve their ability to type on a computer.", "It can enhance their auditory comprehension skills."], "complexity": 0}, {"id": 31, "context": "Modern speech synthesis has a wide variety of applications. TTS is used in conversational agents that conduct dialogues with people, plays a role in devices that read out loud for the blind or in games, and can be used to speak for sufferers of neurological disorders, such as the late astrophysicist Steven Hawking who, after he lost the use of his voice because of ALS, spoke by manipulating a TTS system. ", "Bloom_type": "remember", "question": "In what way does modern speech synthesis utilize dialogue?", "options": ["It uses dialogue to control game characters.", "It uses dialogue to create interactive stories.", "It uses dialogue to provide background music.", "It uses dialogue to enhance visual presentations."], "complexity": 0}, {"id": 32, "context": "Could we improve on word error rate as a metric? It would be nice, for example, to have something that didn`t give equal weight to every word, perhaps valuing content words like Tuesday more than function words like a or of. While researchers generally agree that this would be a good idea, it has proved difficult to agree on a metric that works in every application of ASR. For dialogue systems, however, where the desired semantic output is more clear, a metric called slot error rate or concept error rate has proved extremely useful; it is discussed in Chapter 15 on page 317. ", "Bloom_type": "remember", "question": "In the context provided, what does the term \"slot error rate\" refer to?", "options": ["The accuracy rate of identifying specific slots in speech recognition.", "The percentage of words that are spelled correctly.", "The number of times a word is repeated incorrectly.", "The total count of all errors made by an algorithm."], "complexity": 0}, {"id": 33, "context": "The goal of text-to-speech (TTS) systems is to map from strings of letters to waveforms, a technology that`s important for a variety of applications from dialogue systems to games to education. ", "Bloom_type": "remember", "question": "What are TTS systems primarily used for?", "options": ["Creating interactive conversations with users", "Mapping from numbers to symbols", "Converting images into sounds", "Translating languages between different alphabets"], "complexity": 0}, {"id": 34, "context": "Detecting emotion has the potential to improve a number of language processing tasks. Emotion recognition could help dialogue systems like tutoring systems detect that a student was unhappy, bored, hesitant, confident, and so on. Automatically detecting emotions in reviews or customer responses (anger, dissatisfaction, trust) could help businesses recognize specific problem areas or ones that are going well. Emotion can play a role in medical NLP tasks like helping diagnose depression or suicidal intent. Detecting emotions expressed toward characters in novels might play a role in understanding how different social groups were viewed by society at different times. ", "Bloom_type": "remember", "question": "In what way can emotion detection benefit dialogue systems?", "options": ["It enables better understanding of human behavior and interactions.", "It helps in improving the accuracy of speech recognition.", "It aids in enhancing the efficiency of data storage.", "It assists in personalizing user experiences through tailored content recommendations."], "complexity": 0}, {"id": 35, "context": "Coreference is an important component of natural language processing. A dialogue system that has just told the user There is a 2pm flight on United and a 4pm one on Cathay Pacific must know which flight the user means by I`ll take the second one. A question answering system that uses Wikipedia to answer a question about Marie Curie must know who she was in the sentence She was born in Warsaw. And a machine translation system translating from a language like Spanish, in which pronouns can be dropped, must use coreference from the previous sentence to decide whether the Spanish sentence Me encanta el conocimiento, dice.` should be translated as I love knowledge, he says`, or I love knowledge, she says`. Indeed, this example comes from an actual news article in El Pas about a female professor and was mistranslated as he in machine translation because of inaccurate coreference resolution (Schiebinger, 2013). ", "Bloom_type": "remember", "question": "In what way does coreference play a crucial role in natural language processing?", "options": ["It helps machines understand the meaning of pronouns.", "It enables machines to recognize synonyms in sentences.", "It allows machines to identify the gender of speakers in conversations.", "It assists machines in understanding the relationship between different parts of speech."], "complexity": 0}, {"id": 36, "context": "The dialogue above is from ELIZA, an early natural language processing system that could carry on a limited conversation with a user by imitating the responses of a Rogerian psychotherapist (Weizenbaum, 1966). ELIZA is a surprisingly simple program that uses pattern matching to recognize phrases like I need X and translate them into suitable outputs like What would it mean to you if you got X?. This simple technique succeeds in this domain because ELIZA doesn`t actually need to know anything to mimic a Rogerian psychotherapist. As Weizenbaum notes, this is one of the few dialogue genres where listeners can act as if they know nothing of the world. ELIZA`s mimicry of human conversation was remarkably successful: many people who interacted with ELIZA came to believe that it really understood them and their problems, many continued to believe in ELIZA`s abilities even after the program`s operation was explained to them (Weizenbaum, 1976), and even today such chatbots are a fun diversion. ", "Bloom_type": "comprehension", "question": "What aspect of ELIZA`s success does the author attribute to its ability to mimic human conversation?", "options": ["Pattern matching and phrase recognition", "The use of complex algorithms", "Its ability to understand real-world situations", "The listener's belief in the program's understanding"], "complexity": 1}, {"id": 37, "context": "Surely we can do better than this! After all, we`ll want a model of word meaning to do all sorts of things for us. It should tell us that some words have similar meanings (cat is similar to dog), others are antonyms (cold is the opposite of hot), some have positive connotations (happy) while others have negative connotations (sad). It should represent the fact that the meanings of buy, sell, and pay offer differing perspectives on the same underlying purchasing event. (If I buy something from you, you`ve probably sold it to me, and I likely paid you.) More generally, a model of word meaning should allow us to draw inferences to address meaning-related tasks like question-answering or dialogue. ", "Bloom_type": "comprehension", "question": "What does the response suggest about the importance of understanding word meanings?", "options": ["It suggests that understanding word meanings is crucial for various language processing tasks.", "It implies that understanding word meanings is only important for answering questions.", "It indicates that understanding word meanings is less important than learning grammar rules.", "It states that understanding word meanings is unnecessary for communication."], "complexity": 1}, {"id": 38, "context": "RNN-based language models can also be used to generate text. Text generation is of enormous practical importance, part of tasks like question answering, machine translation, text summarization, grammar correction, story generation, and conversational dialogue; any task where a system needs to produce text, conditioned on some other text. This use of a language model to generate text is one of the areas in which the impact of neural language models on NLP has been the largest. Text generation, along with image generation and code generation, constitute a new area of AI that is often called generative AI. ", "Bloom_type": "comprehension", "question": "What are some practical applications of generating text using RNN-based language models?", "options": ["Image generation and all of the above", "Grammar correction and story generation", "Machine translation and code generation", "Conversational dialogue and question answering"], "complexity": 1}, {"id": 39, "context": "Encoder-decoder networks, sometimes called sequence-to-sequence networks, are models capable of generating contextually appropriate, arbitrary length, output sequences given an input sequence. Encoder-decoder networks have been applied to a very wide range of applications including summarization, question answering, and dialogue, but they are particularly popular for machine translation. ", "Bloom_type": "comprehension", "question": "Explain how encoder-decoder networks are utilized in various applications mentioned in the context?", "options": ["Encoder-decoder networks are widely applied across diverse fields including summarization, Q&A, and dialogue generation.", "Encoder-decoder networks are exclusively used for summarizing large volumes of data.", "Encoder-decoder networks are only used for generating dialogue responses.", "Encoder-decoder networks are rarely used outside of machine translation."], "complexity": 1}, {"id": 40, "context": "In this second part of the book we introduce fundamental NLP applications: machine translation, information retrieval, question answering, dialogue systems, and speech recognition. ", "Bloom_type": "comprehension", "question": "Which of the following is NOT an example of a fundamental NLP application introduced in this book?", "options": ["Information Retrieval", "Machine Translation", "Dialogue Systems", "Speech Recognition"], "complexity": 1}, {"id": 41, "context": "Minimum Bayes risk decoding can also be used for other NLP tasks; indeed it was widely applied to speech recognition (Stolcke et al., 1997; Goel and Byrne, 2000) before being applied to machine translation (Kumar and Byrne, 2004), and has been shown to work well across many other generation tasks as well (e.g., summarization, dialogue, and image captioning (Suzgun et al., 2023a)). ", "Bloom_type": "comprehension", "question": "What task besides machine translation has minimum Bayes risk decoding been successfully applied to?", "options": ["Dialogue", "Speech recognition", "Image captioning", "Summarization"], "complexity": 1}, {"id": 42, "context": "The literature of the fantastic abounds in inanimate objects magically endowed with the gift of speech. From Ovid`s statue of Pygmalion to Mary Shelley`s story about Frankenstein, we continually reinvent stories about creating something and then having a chat with it. Legend has it that after finishing his sculpture Moses, Michelangelo thought it so lifelike that he tapped it on the knee and commanded it to speak. Perhaps this shouldn`t be surprising. Language is the mark of humanity and sentience, and conversation or dialogue is the most fundamental arena of language. It is the first kind of language we learn as children, and the kind we engage in constantly, whether we are ordering lunch, buying train tickets, or talking with our families, friends, or coworkers. ", "Bloom_type": "comprehension", "question": "What aspect of human interaction does the creation of dialogue between inanimate objects exemplify?", "options": ["The role of communication", "The power of imagination", "The importance of storytelling", "The impact of technology"], "complexity": 1}, {"id": 43, "context": "This chapter introduces the fundamental algorithms of programs that use conversation to interact with users. We often distinguish between two kinds of architectures. Task-oriented dialogue systems converse with users to accomplish fixed tasks like controlling appliances or finding restaurants, relying on a data structure called the frame, which represents the knowledge a system needs to acquire from the user (like the time to set an alarm clock). Chatbots, by contrast, are designed to mimic the longer and more unstructured conversations or chats` characteristic of human-human interaction. Modern systems incorporate aspects of both; industrial chatbots like ChatGPT can carry on longer unstructured conversations; industrial digital assistants like Siri or Alexa are generally frame-based dialogue systems. ", "Bloom_type": "comprehension", "question": "What distinguishes task-oriented dialogue systems from chatbots?", "options": ["Chatbots typically engage in longer, less structured conversations mimicking human-human interactions.", "Task-oriented dialogue systems rely on a data structure called the frame for acquiring user knowledge.", "Modern systems predominantly focus on incorporating elements of both types.", "None of the above"], "complexity": 1}, {"id": 44, "context": "The fact that chatbots and dialogue systems are designed for human-computer interaction has strong implications for their design and use. Many of these implications already became clear in one of the earliest chatbots, ELIZA (Weizenbaum, 1966). ELIZA was designed to simulate a Rogerian psychologist, based on a branch of clinical psychology whose methods involve drawing the patient out by reflecting patient`s statements back at them. Rogerian interactions are the rare type of conversation in which, as Weizenbaum points out, one can assume the pose of knowing almost nothing of the real world. If a patient says I went for a long boat ride and the psychiatrist says Tell me about boats, you don`t assume she didn`t know what ", "Bloom_type": "comprehension", "question": "What aspect of ELIZA's design reflects its ability to engage in a unique kind of conversation?", "options": ["The use of reflective questioning", "Its focus on psychological therapy techniques", "The incorporation of historical references", "The implementation of advanced natural language processing algorithms"], "complexity": 1}, {"id": 45, "context": "Both of these issues (emotional engagement and privacy) mean we need to think carefully about how we deploy chatbots and the people who are interacting with them. Dialogue research that uses human participants often requires getting permission from the Institutional Review Board (IRB) of your institution. ", "Bloom_type": "comprehension", "question": "What does dialogue research involving human participants typically require?", "options": ["Approval from an IRB", "Permission for data collection", "Access to all participant data", "No specific approval needed"], "complexity": 1}, {"id": 46, "context": "Turn structure has important implications for spoken dialogue. A human has to know when to stop talking; the client interrupts (in A16 and C17), so a system that was performing this role must know to stop talking (and that the user might be making a correction). A system also has to know when to start talking. For example, most of the time in conversation, speakers start their turns almost immediately after the other speaker finishes, without a long pause, because people are can usually predict when the other person is about to finish talking. Spoken dialogue systems must also detect whether a user is done speaking, so they can process the utterance and respond. This taskcalled endpointing or endpoint detection can be quite challenging because of noise and because people often pause in the middle of turns. ", "Bloom_type": "comprehension", "question": "Explain how spoken dialogue systems need to manage the timing between speakers?", "options": ["Systems must recognize when a speaker is finished talking and prepare to respond.", "Spoken dialogue systems should always wait for the next speaker before responding.", "Dialogue systems do not require any specific timing management.", "The timing between speakers does not affect the performance of spoken dialogue systems."], "complexity": 1}, {"id": 47, "context": "Full mixed initiative, while the norm for human-human conversations, can be difficult for dialogue systems. The most primitive dialogue systems tend to use system-initiative, where the system asks a question and the user can`t do anything until they answer it, or user-initiative like simple search engines, where the user specifies a query and the system passively responds. Even modern large language model-based dialogue systems, which come much closer to using full mixed initiative, often don`t have completely natural initiative switching. Getting this right is an important goal for modern systems. ", "Bloom_type": "comprehension", "question": "What type of dialogue systems are typically more common among humans compared to dialogue systems?", "options": ["System-initiative", "User-initiative", "Mixed initiative", "None of the above"], "complexity": 1}, {"id": 48, "context": "These subtle characteristics of human conversations (turns, speech acts, grounding, dialogue structure, initiative, and implicature) are among the reasons it is difficult to build dialogue systems that can carry on natural conversations with humans. Many of these challenges are active areas of dialogue systems research. ", "Bloom_type": "comprehension", "question": "What aspects of human conversations make it challenging for dialogue systems to mimic natural conversation?", "options": ["Subtle characteristics such as turns, speech acts, grounding, dialogue structure, initiative, and implicature", "The use of advanced AI algorithms", "Lack of data for training models", "Inability to understand sarcasm and humor"], "complexity": 1}, {"id": 49, "context": "The frame and its slots in a task-based dialogue system specify what the system needs to know to perform its task. A hotel reservation system needs dates and locations. An alarm clock system needs a time. The system`s goal is to fill the slots in the frame with the fillers the user intends, and then perform the relevant action for the user (answering a question, or booking a flight). ", "Bloom_type": "comprehension", "question": "What does the system need to know from the user to perform its task correctly?", "options": ["Both dates and times", "Dates and locations", "Time", "None of the above"], "complexity": 1}, {"id": 50, "context": "We can make a very simple frame-based dialogue system by wrapping a small amount of code around this slot extractor. Mainly we just need to ask the user questions until all the slots are full, do a database query, then report back to the user, using hand-built templates for generating sentences. ", "Bloom_type": "comprehension", "question": "What is the primary function of a dialog system when creating a simple one?", "options": ["To extract specific pieces of information from users", "To build complex sentence structures dynamically", "To perform calculations based on user inputs", "To store data in databases automatically"], "complexity": 1}, {"id": 51, "context": "While the naive slot-extractor system described above can handle simple dialogues, often we want more complex interactions. For example, we might want to confirm that we`ve understand the user, or ask them to repeat themselves. We can build a more sophisticated system using dialogue acts and dialogue state. ", "Bloom_type": "comprehension", "question": "Explain how a more sophisticated system for handling complex interactions would incorporate dialogue acts and dialogue state?", "options": ["The system employs natural language processing (NLP) techniques to analyze user responses and adjust its behavior accordingly.", "The system uses predefined scripts to respond to different types of questions.", "The system relies solely on machine learning algorithms to predict user actions.", "The system ignores dialogue acts and focuses only on dialogue state."], "complexity": 1}, {"id": 52, "context": "Figure 15.6 shows a tagset for a restaurant recommendation system, and Fig. 15.7 shows these tags labeling a sample dialogue from the HIS system (Young et al., 2010). This example also shows the content of each dialogue act, which are the slot fillers being communicated. So the user might INFORM the system that they want Italian food near a museum, or CONFIRM with the system that the price is reasonable. ", "Bloom_type": "comprehension", "question": "What does Fig. 15.7 show about the dialogue acts in the HIS system?", "options": ["The content of each dialogue act", "The slots filled by the user", "The price range of the menu items", "The location of the restaurant"], "complexity": 1}, {"id": 53, "context": "Dialogue act detection is done just like domain or intent classification, by passing the input sentence through an encoder and adding an act classifier. Often passing in the prior dialogue act as well can improve classification. And since dialogue acts ", "Bloom_type": "comprehension", "question": "How does dialogue act detection differ from domain or intent classification?", "options": ["It involves additional steps involving prior dialogue acts.", "It uses different classifiers for each task.", "It requires more data for training.", "It focuses solely on the content of the sentences."], "complexity": 1}, {"id": 54, "context": "In early commercial frame-based systems, the dialogue policy is simple: ask questions until all the slots are full, do a database query, then report back to the user. A more sophisticated dialogue policy can help a system decide when to answer the user`s questions, when to instead ask the user a clarification question, and so on. A dialogue policy thus decides what dialogue act to generate. Choosing a dialogue act to generate, along with its arguments, is sometimes called content planning. ", "Bloom_type": "comprehension", "question": "What does choosing a dialogue act to generate involve?", "options": ["Choosing between different types of dialogue acts", "Deciding on the type of question to ask", "Determining the next set of clarifying questions", "Selecting the appropriate response based on user input"], "complexity": 1}, {"id": 55, "context": "It is a common practice for dialogue systems to use further labeled data for finetuning. One function of this fine-tuning step is to improve the quality of the dialogue, training the system to produce responses that are sensible and interesting. Another function might be to improve safety, keeping a dialogue system from suggesting harmful actions (like financial fraud, medical harm, inciting hatred, or abusing the user or other people). ", "Bloom_type": "comprehension", "question": "What two functions does the fine-tuning step serve for dialogue systems?", "options": ["Improving response quality and enhancing safety", "Enhancing response quality and improving efficiency", "Increasing accuracy and reducing latency", "Stabilizing performance and optimizing speed"], "complexity": 1}, {"id": 56, "context": "In the simplest method for improving quality and safety, speakers of the language are given an initial prompt and instructions to have high-quality, safe dialogues. They then interact with an initial dialogue system and their responses are used to finetune the model, usually as part of the instruct tuning step we introduced in Chapter 12. Thus a dialogue system learns to answer questions, follow other instructions, and also carry on high-quality, safe dialogues, in a single multi-task learning format. ", "Bloom_type": "comprehension", "question": "What does the simple method for improving quality and safety involve?", "options": ["Providing prompts and instructions to improve quality and safety.", "Creating dialogue systems from scratch.", "Using only one task at a time.", "Learning to respond to questions and follow instructions."], "complexity": 1}, {"id": 57, "context": "Alternatively, systems can be finetuned to to know when to use a search engine. For example, labelers can interact with a system, fact check each of the responses, and whenever the system emits an incorrect response, perform the web search queries that the system should have used to check its answer, and then the interation is recorded and used for fine-tuning. Or labelers can look at a transcript of a language model carrying on a dialogue, and similarly mark every place where a fact was wrong (or out-of-date) and write the set of search queries that would have been appropriate. A system is then fine-tuned to generate search query turns which are again passed to a search engine to generate the search responses. The set of pages or snippets returned by the search engine in the search response turn are then treated as the context for generation, similarly to the retrieval-based questionanswering methods of Chapter 14. ", "Bloom_type": "comprehension", "question": "How do labelers interact with a system to improve it?", "options": ["By recording interactions and using them for fine-tuning", "By providing feedback on the accuracy of the generated search queries", "By manually writing the search queries themselves", "By focusing solely on the correctness of the search results"], "complexity": 1}, {"id": 58, "context": "1. Study the user and task: Understand the users and the task by interviewing users, investigating similar systems, and studying related human-human dialogues. ", "Bloom_type": "comprehension", "question": "What is the primary focus when studying dialogue?", "options": ["Analyzing existing human-human dialogues", "Creating new algorithms for natural language processing", "Designing interactive interfaces", "Training machine learning models"], "complexity": 1}, {"id": 59, "context": "These ethical issues are an important area of investigation, including finding ways to mitigate problems of abuse and toxicity, like detecting and responding appropriately to toxic contexts (Wolf et al. 2017, Dinan et al. 2020, Xu et al. 2020). Value sensitive design, carefully considering possible harms in advance (Friedman et al. 2017, Friedman and Hendry 2019) is also important; (Dinan et al., 2021) give a number of suggestions for best practices in dialogue system design. For example getting informed consent from participants, whether they are used for training, or whether they are interacting with a deployed system is important. Because dialogue systems by definition involve human participants, researchers also work on these issues with the Institutional Review Boards (IRB) at their institutions, who help protect the safety of experimental subjects. ", "Bloom_type": "comprehension", "question": "What aspect of dialogue systems does the response emphasize regarding participant protection?", "options": ["Ensuring informed consent", "Mitigating abusive behavior", "Detecting toxic contexts", "Responding to toxicity"], "complexity": 1}, {"id": 60, "context": "Chatbots and dialogue systems are crucial speech and language processing applications that are already widely used commercially. ", "Bloom_type": "comprehension", "question": "What aspect of speech and language processing do chatbots and dialogue systems primarily focus on?", "options": ["Natural language understanding", "Speech recognition", "Text generation", "Voice synthesis"], "complexity": 1}, {"id": 61, "context": " In human dialogue, speaking is a kind of action; these acts are referred to as speech acts or dialogue acts. Speakers also attempt to achieve common ground by acknowledging that they have understand each other. Conversation also is characterized by turn structure and dialogue structure. ", "Bloom_type": "comprehension", "question": "What aspect of human dialogue involves speakers trying to establish mutual understanding?", "options": ["Speaking", "Listening", "Non-verbal communication", "Silence"], "complexity": 1}, {"id": 62, "context": " The dialogue-state architecture augments the GUS frame-and-slot architecture with richer representations and more sophisticated algorithms for keeping track of user`s dialogue acts, policies for generating its own dialogue acts, and a natural language component. ", "Bloom_type": "comprehension", "question": "What does the dialogue-state architecture do differently from the GUS frame-and-slot architecture?", "options": ["It adds richer representations and more sophisticated algorithms.", "It focuses solely on user interaction.", "It simplifies the natural language component.", "It eliminates the need for policies."], "complexity": 1}, {"id": 63, "context": " Dialogue systems are a kind of human-computer interaction, and general HCI principles apply in their design, including the role of the user, simulations such as Wizard-of-Oz systems, and the importance of iterative design and testing on real users. ", "Bloom_type": "comprehension", "question": "What principle applies most directly when designing dialogue systems?", "options": ["General HCI principles", "The role of the user", "Simulations such as Wizard-of-Oz systems", "Iterative design and testing on real users"], "complexity": 1}, {"id": 64, "context": "Computational-implemented theories of dialogue blossomed in the 1970. That period saw the very influential GUS system (Bobrow et al., 1977), which in the late 1970s established the frame-based paradigm that became the dominant industrial paradigm for dialogue systems for over 30 years. ", "Bloom_type": "comprehension", "question": "What was one of the significant developments in computational dialogue theory during the 1970s?", "options": ["The establishment of the frame-based paradigm", "The introduction of natural language processing techniques", "The development of artificial intelligence models", "The creation of interactive user interfaces"], "complexity": 1}, {"id": 65, "context": "In the 1990s, machine learning models that had first been applied to natural language processing began to be applied to dialogue tasks like slot filling (Miller et al. 1994, Pieraccini et al. 1991). This period also saw lots of analytic work on the linguistic properties of dialogue acts and on machine-learning-based methods for their detection. (Sag and Liberman 1975, Hinkelman and Allen 1989, Nagata and Morimoto 1994, Goodwin 1996, Chu-Carroll 1998, Shriberg et al. 1998, Stolcke et al. 2000, Gravano et al. 2012. This work strongly informed the development of the dialogue-state model (Larsson and Traum, 2000). Dialogue state tracking quickly became an important problem for task-oriented dialogue, and there has been an influential annual evaluation of state-tracking algorithms (Williams et al., 2016). The turn of the century saw a line of work on applying reinforcement learning to dialogue, which first came out of AT&T and Bell Laboratories with work on MDP dialogue systems (Walker 2000, Levin et al. 2000, Singh et al. 2002) along with work on cue phrases, prosody, and rejection and confirmation. Reinforcement learning research turned quickly to the more sophisticated POMDP models (Roy et al. 2000, Lemon et al. 2006, Williams and Young 2007) applied to small slotfilling dialogue tasks. Neural reinforcement learning models have been used both for chatbot systems, for example simulating dialogues between two dialogue systems, rewarding good conversational properties like coherence and ease of answering (Li et al., 2016a), and for task-oriented dialogue (Williams et al., 2017). ", "Bloom_type": "comprehension", "question": "What was one significant area of focus during the early applications of machine learning to dialogue tasks?", "options": ["Dialogue act analysis", "Natural language generation", "Reinforcement learning", "Slot-filling techniques"], "complexity": 1}, {"id": 66, "context": "By around 2010 the GUS architecture finally began to be widely used commercially in dialogue systems on phones like Apple`s SIRI (Bellegarda, 2013) and other digital assistants. ", "Bloom_type": "comprehension", "question": "What was one of the key applications for GUS architecture by around 2010?", "options": ["Enhancing user interaction through natural language processing", "Improving phone call quality", "Increasing battery life of devices", "Reducing data usage on mobile networks"], "complexity": 1}, {"id": 67, "context": "Automatic transcription of speech by any speaker in any environment is still far from solved, but ASR technology has matured to the point where it is now viable for many practical tasks. Speech is a natural interface for communicating with smart home appliances, personal assistants, or cellphones, where keyboards are less convenient, in telephony applications like call-routing (Accounting, please) or in sophisticated dialogue applications (I`d like to change the return date of my flight). ASR is also useful for general transcription, for example for automatically generating captions for audio or video text (transcribing movies or videos or live discussions). Transcription is important in fields like law where dictation plays an important role. Finally, ASR is important as part of augmentative communication (interaction between computers and humans with some disability resulting in difficulties or inabilities in typing or audition). The blind Milton famously dictated Paradise Lost to his daughters, and Henry James dictated his later novels after a repetitive stress injury. ", "Bloom_type": "comprehension", "question": "What are some practical applications of automatic speech recognition (ASR)?", "options": ["Voice-controlled devices and virtual assistants.", "Transcription for legal documents only.", "Automated caption generation for films and TV shows.", "Augmentative communication for people with disabilities."], "complexity": 1}, {"id": 68, "context": "Modern speech synthesis has a wide variety of applications. TTS is used in conversational agents that conduct dialogues with people, plays a role in devices that read out loud for the blind or in games, and can be used to speak for sufferers of neurological disorders, such as the late astrophysicist Steven Hawking who, after he lost the use of his voice because of ALS, spoke by manipulating a TTS system. ", "Bloom_type": "comprehension", "question": "Explain how modern speech synthesis is utilized across different contexts?", "options": ["Speech synthesis is utilized in various applications including conversational agents, device usage, and speaking for those with neurological disorders.", "TTS is exclusively used in gaming.", "It is only applied in reading aloud for the visually impaired.", "Modern speech synthesis is not widely used."], "complexity": 1}, {"id": 69, "context": "Could we improve on word error rate as a metric? It would be nice, for example, to have something that didn`t give equal weight to every word, perhaps valuing content words like Tuesday more than function words like a or of. While researchers generally agree that this would be a good idea, it has proved difficult to agree on a metric that works in every application of ASR. For dialogue systems, however, where the desired semantic output is more clear, a metric called slot error rate or concept error rate has proved extremely useful; it is discussed in Chapter 15 on page 317. ", "Bloom_type": "comprehension", "question": "What type of error rate could potentially provide better accuracy for dialogue systems compared to traditional metrics?", "options": ["Dialogue error rate", "Word error rate", "Sentence error rate", "Phonetic error rate"], "complexity": 1}, {"id": 70, "context": "The goal of text-to-speech (TTS) systems is to map from strings of letters to waveforms, a technology that`s important for a variety of applications from dialogue systems to games to education. ", "Bloom_type": "comprehension", "question": "Explain how TTS systems relate to dialogue systems?", "options": ["TTS systems facilitate communication between humans and machines.", "TTS systems are unrelated to dialogue systems.", "TTS systems directly produce dialogue content.", "TTS systems enhance the quality of dialogue interactions."], "complexity": 1}, {"id": 71, "context": "Detecting emotion has the potential to improve a number of language processing tasks. Emotion recognition could help dialogue systems like tutoring systems detect that a student was unhappy, bored, hesitant, confident, and so on. Automatically detecting emotions in reviews or customer responses (anger, dissatisfaction, trust) could help businesses recognize specific problem areas or ones that are going well. Emotion can play a role in medical NLP tasks like helping diagnose depression or suicidal intent. Detecting emotions expressed toward characters in novels might play a role in understanding how different social groups were viewed by society at different times. ", "Bloom_type": "comprehension", "question": "What aspect of dialogue systems can benefit from automatically detecting emotions?", "options": ["Facilitating better user engagement and interaction", "Improving language translation accuracy", "Enhancing natural language generation capabilities", "Increasing the efficiency of speech-to-text conversion"], "complexity": 1}, {"id": 72, "context": "Coreference is an important component of natural language processing. A dialogue system that has just told the user There is a 2pm flight on United and a 4pm one on Cathay Pacific must know which flight the user means by I`ll take the second one. A question answering system that uses Wikipedia to answer a question about Marie Curie must know who she was in the sentence She was born in Warsaw. And a machine translation system translating from a language like Spanish, in which pronouns can be dropped, must use coreference from the previous sentence to decide whether the Spanish sentence Me encanta el conocimiento, dice.` should be translated as I love knowledge, he says`, or I love knowledge, she says`. Indeed, this example comes from an actual news article in El Pas about a female professor and was mistranslated as he in machine translation because of inaccurate coreference resolution (Schiebinger, 2013). ", "Bloom_type": "comprehension", "question": "Explain how coreference plays a role in different types of AI systems mentioned?", "options": ["Coreference helps machines understand the meaning of sentences better.", "Coreference allows machines to recognize synonyms more accurately.", "Coreference enables machines to predict future events based on past data.", "Coreference ensures that all parts of speech are correctly identified."], "complexity": 1}, {"id": 73, "context": "The dialogue above is from ELIZA, an early natural language processing system that could carry on a limited conversation with a user by imitating the responses of a Rogerian psychotherapist (Weizenbaum, 1966). ELIZA is a surprisingly simple program that uses pattern matching to recognize phrases like I need X and translate them into suitable outputs like What would it mean to you if you got X?. This simple technique succeeds in this domain because ELIZA doesn`t actually need to know anything to mimic a Rogerian psychotherapist. As Weizenbaum notes, this is one of the few dialogue genres where listeners can act as if they know nothing of the world. ELIZA`s mimicry of human conversation was remarkably successful: many people who interacted with ELIZA came to believe that it really understood them and their problems, many continued to believe in ELIZA`s abilities even after the program`s operation was explained to them (Weizenbaum, 1976), and even today such chatbots are a fun diversion. ", "Bloom_type": "application", "question": "What aspect of ELIZA`s success does the passage emphasize?", "options": ["The listener's belief in the program's understanding", "The complexity of the dialogues it generates", "The effectiveness of pattern matching for understanding", "The unpredictability of the conversation"], "complexity": 2}, {"id": 74, "context": "Surely we can do better than this! After all, we`ll want a model of word meaning to do all sorts of things for us. It should tell us that some words have similar meanings (cat is similar to dog), others are antonyms (cold is the opposite of hot), some have positive connotations (happy) while others have negative connotations (sad). It should represent the fact that the meanings of buy, sell, and pay offer differing perspectives on the same underlying purchasing event. (If I buy something from you, you`ve probably sold it to me, and I likely paid you.) More generally, a model of word meaning should allow us to draw inferences to address meaning-related tasks like question-answering or dialogue. ", "Bloom_type": "application", "question": "What is an essential component for developing a model of word meaning?", "options": ["Building models for dialogue understanding", "Creating a database of synonyms only", "Implementing algorithms for semantic analysis", "Designing interfaces for user interaction"], "complexity": 2}, {"id": 75, "context": "RNN-based language models can also be used to generate text. Text generation is of enormous practical importance, part of tasks like question answering, machine translation, text summarization, grammar correction, story generation, and conversational dialogue; any task where a system needs to produce text, conditioned on some other text. This use of a language model to generate text is one of the areas in which the impact of neural language models on NLP has been the largest. Text generation, along with image generation and code generation, constitute a new area of AI that is often called generative AI. ", "Bloom_type": "application", "question": "What type of AI involves generating text based on input conditions?", "options": ["Generative AI", "Reinforcement Learning", "Symbolic AI", "Rule-Based AI"], "complexity": 2}, {"id": 76, "context": "Encoder-decoder networks, sometimes called sequence-to-sequence networks, are models capable of generating contextually appropriate, arbitrary length, output sequences given an input sequence. Encoder-decoder networks have been applied to a very wide range of applications including summarization, question answering, and dialogue, but they are particularly popular for machine translation. ", "Bloom_type": "application", "question": "In which application area are encoder-decoder networks most commonly used?", "options": ["Dialogue generation and machine translation", "Summarization only", "Question Answering only", "None of the above"], "complexity": 2}, {"id": 77, "context": "In this second part of the book we introduce fundamental NLP applications: machine translation, information retrieval, question answering, dialogue systems, and speech recognition. ", "Bloom_type": "application", "question": "Which of these is NOT an application of natural language processing?", "options": ["Machine Translation", "Information Retrieval", "Dialogue Systems", "Speech Recognition"], "complexity": 2}, {"id": 78, "context": "Minimum Bayes risk decoding can also be used for other NLP tasks; indeed it was widely applied to speech recognition (Stolcke et al., 1997; Goel and Byrne, 2000) before being applied to machine translation (Kumar and Byrne, 2004), and has been shown to work well across many other generation tasks as well (e.g., summarization, dialogue, and image captioning (Suzgun et al., 2023a)). ", "Bloom_type": "application", "question": "What is an example of a task where minimum Bayes risk decoding has been successfully applied?", "options": ["Image captioning", "Speech recognition", "Machine translation", "Summarization"], "complexity": 2}, {"id": 79, "context": "The literature of the fantastic abounds in inanimate objects magically endowed with the gift of speech. From Ovid`s statue of Pygmalion to Mary Shelley`s story about Frankenstein, we continually reinvent stories about creating something and then having a chat with it. Legend has it that after finishing his sculpture Moses, Michelangelo thought it so lifelike that he tapped it on the knee and commanded it to speak. Perhaps this shouldn`t be surprising. Language is the mark of humanity and sentience, and conversation or dialogue is the most fundamental arena of language. It is the first kind of language we learn as children, and the kind we engage in constantly, whether we are ordering lunch, buying train tickets, or talking with our families, friends, or coworkers. ", "Bloom_type": "application", "question": "What aspect of human communication does the creation of sentient objects exemplify?", "options": ["The role of dialogue", "The power of imagination", "The importance of storytelling", "The significance of language"], "complexity": 2}, {"id": 80, "context": "This chapter introduces the fundamental algorithms of programs that use conversation to interact with users. We often distinguish between two kinds of architectures. Task-oriented dialogue systems converse with users to accomplish fixed tasks like controlling appliances or finding restaurants, relying on a data structure called the frame, which represents the knowledge a system needs to acquire from the user (like the time to set an alarm clock). Chatbots, by contrast, are designed to mimic the longer and more unstructured conversations or chats` characteristic of human-human interaction. Modern systems incorporate aspects of both; industrial chatbots like ChatGPT can carry on longer unstructured conversations; industrial digital assistants like Siri or Alexa are generally frame-based dialogue systems. ", "Bloom_type": "application", "question": "Which type of architecture is typically used for task-oriented dialogue systems?", "options": ["Chatbots", "Industrial chatbots", "Frame-based dialogue systems", "Human-human interaction"], "complexity": 2}, {"id": 81, "context": "The fact that chatbots and dialogue systems are designed for human-computer interaction has strong implications for their design and use. Many of these implications already became clear in one of the earliest chatbots, ELIZA (Weizenbaum, 1966). ELIZA was designed to simulate a Rogerian psychologist, based on a branch of clinical psychology whose methods involve drawing the patient out by reflecting patient`s statements back at them. Rogerian interactions are the rare type of conversation in which, as Weizenbaum points out, one can assume the pose of knowing almost nothing of the real world. If a patient says I went for a long boat ride and the psychiatrist says Tell me about boats, you don`t assume she didn`t know what ", "Bloom_type": "application", "question": "What is a key characteristic of Rogerian interactions?", "options": ["The therapist pretends not to know much about the real world.", "The therapist knows everything about the real world.", "The patient assumes the role of an expert in the subject matter.", "The patient reflects back on their experiences."], "complexity": 2}, {"id": 82, "context": "Both of these issues (emotional engagement and privacy) mean we need to think carefully about how we deploy chatbots and the people who are interacting with them. Dialogue research that uses human participants often requires getting permission from the Institutional Review Board (IRB) of your institution. ", "Bloom_type": "application", "question": "What is required before conducting dialogue research involving human participants?", "options": ["Both A and B", "Approval from an IRB", "Permission from all participants", "None of the above"], "complexity": 2}, {"id": 83, "context": "Turn structure has important implications for spoken dialogue. A human has to know when to stop talking; the client interrupts (in A16 and C17), so a system that was performing this role must know to stop talking (and that the user might be making a correction). A system also has to know when to start talking. For example, most of the time in conversation, speakers start their turns almost immediately after the other speaker finishes, without a long pause, because people are can usually predict when the other person is about to finish talking. Spoken dialogue systems must also detect whether a user is done speaking, so they can process the utterance and respond. This taskcalled endpointing or endpoint detection can be quite challenging because of noise and because people often pause in the middle of turns. ", "Bloom_type": "application", "question": "What is an essential skill for a spoken dialogue system to perform correctly?", "options": ["Recognizing the end of a speaker's turn", "Understanding the meaning of every word", "Predicting future events based on past actions", "Generating new ideas spontaneously"], "complexity": 2}, {"id": 84, "context": "Full mixed initiative, while the norm for human-human conversations, can be difficult for dialogue systems. The most primitive dialogue systems tend to use system-initiative, where the system asks a question and the user can`t do anything until they answer it, or user-initiative like simple search engines, where the user specifies a query and the system passively responds. Even modern large language model-based dialogue systems, which come much closer to using full mixed initiative, often don`t have completely natural initiative switching. Getting this right is an important goal for modern systems. ", "Bloom_type": "application", "question": "What is the main challenge when designing dialogue systems?", "options": ["Balancing between system-initiative and user-initiative effectively.", "Ensuring the system always initiates the conversation.", "Making sure users are always passive participants.", "Avoiding any kind of initiative-switching."], "complexity": 2}, {"id": 85, "context": "These subtle characteristics of human conversations (turns, speech acts, grounding, dialogue structure, initiative, and implicature) are among the reasons it is difficult to build dialogue systems that can carry on natural conversations with humans. Many of these challenges are active areas of dialogue systems research. ", "Bloom_type": "application", "question": "What aspect of human conversation contributes significantly to the difficulty in building dialogue systems?", "options": ["The subtlety of social cues", "The complexity of language structures", "The unpredictability of human emotions", "The variability of cultural backgrounds"], "complexity": 2}, {"id": 86, "context": "The frame and its slots in a task-based dialogue system specify what the system needs to know to perform its task. A hotel reservation system needs dates and locations. An alarm clock system needs a time. The system`s goal is to fill the slots in the frame with the fillers the user intends, and then perform the relevant action for the user (answering a question, or booking a flight). ", "Bloom_type": "application", "question": "What should the system focus on when filling the slots in the frame?", "options": ["Prioritizing slots based on the system's goals", "Filling all possible slots regardless of their relevance", "Only focusing on the most important slots needed for the task", "Choosing random slots to avoid biasing the response"], "complexity": 2}, {"id": 87, "context": "We can make a very simple frame-based dialogue system by wrapping a small amount of code around this slot extractor. Mainly we just need to ask the user questions until all the slots are full, do a database query, then report back to the user, using hand-built templates for generating sentences. ", "Bloom_type": "application", "question": "What is the first step in creating a simple frame-based dialogue system?", "options": ["Ask user questions", "Build a database query", "Generate sentence templates", "Extract slots from input"], "complexity": 2}, {"id": 88, "context": "While the naive slot-extractor system described above can handle simple dialogues, often we want more complex interactions. For example, we might want to confirm that we`ve understand the user, or ask them to repeat themselves. We can build a more sophisticated system using dialogue acts and dialogue state. ", "Bloom_type": "application", "question": "What is an important aspect of building a more sophisticated system for handling complex interactions?", "options": ["Considering dialogue acts and dialogue state", "Using only basic methods", "Ignoring dialogue acts and dialogue state", "Focusing solely on confirmation and repetition"], "complexity": 2}, {"id": 89, "context": "Figure 15.6 shows a tagset for a restaurant recommendation system, and Fig. 15.7 shows these tags labeling a sample dialogue from the HIS system (Young et al., 2010). This example also shows the content of each dialogue act, which are the slot fillers being communicated. So the user might INFORM the system that they want Italian food near a museum, or CONFIRM with the system that the price is reasonable. ", "Bloom_type": "application", "question": "What does the user communicate when they say 'INFORM'?", "options": ["They specify their preferences.", "They ask for more options.", "They request confirmation.", "They provide additional details."], "complexity": 2}, {"id": 90, "context": "Dialogue act detection is done just like domain or intent classification, by passing the input sentence through an encoder and adding an act classifier. Often passing in the prior dialogue act as well can improve classification. And since dialogue acts ", "Bloom_type": "application", "question": "What technique is used for dialogue act detection similar to domain or intent classification?", "options": ["Passing in both the input sentence and the prior dialogue act", "Passing the input sentence through an encoder alone", "Adding an act classifier only", "Using a pre-trained model directly"], "complexity": 2}, {"id": 91, "context": "In early commercial frame-based systems, the dialogue policy is simple: ask questions until all the slots are full, do a database query, then report back to the user. A more sophisticated dialogue policy can help a system decide when to answer the user`s questions, when to instead ask the user a clarification question, and so on. A dialogue policy thus decides what dialogue act to generate. Choosing a dialogue act to generate, along with its arguments, is sometimes called content planning. ", "Bloom_type": "application", "question": "What does choosing a dialogue act entail?", "options": ["It demands selecting the appropriate content for the conversation.", "It involves deciding which slot to fill next.", "It requires determining how to respond to the user's queries.", "It necessitates figuring out when to clarify the user's intent."], "complexity": 2}, {"id": 92, "context": "It is a common practice for dialogue systems to use further labeled data for finetuning. One function of this fine-tuning step is to improve the quality of the dialogue, training the system to produce responses that are sensible and interesting. Another function might be to improve safety, keeping a dialogue system from suggesting harmful actions (like financial fraud, medical harm, inciting hatred, or abusing the user or other people). ", "Bloom_type": "application", "question": "What is another function of fine-tuning in dialogue systems?", "options": ["To prevent the dialogue system from making mistakes.", "To reduce the size of the dataset used for training.", "To increase the complexity of the dialogue model.", "To enhance the accuracy of the dialogue responses."], "complexity": 2}, {"id": 93, "context": "In the simplest method for improving quality and safety, speakers of the language are given an initial prompt and instructions to have high-quality, safe dialogues. They then interact with an initial dialogue system and their responses are used to finetune the model, usually as part of the instruct tuning step we introduced in Chapter 12. Thus a dialogue system learns to answer questions, follow other instructions, and also carry on high-quality, safe dialogues, in a single multi-task learning format. ", "Bloom_type": "application", "question": "What is the first step in creating a dialogue system?", "options": ["Generating initial prompts and instructions", "Designing the architecture of the dialogue system", "Training the dialogue system using pre-existing data", "Collecting user feedback on existing systems"], "complexity": 2}, {"id": 94, "context": "Alternatively, systems can be finetuned to to know when to use a search engine. For example, labelers can interact with a system, fact check each of the responses, and whenever the system emits an incorrect response, perform the web search queries that the system should have used to check its answer, and then the interation is recorded and used for fine-tuning. Or labelers can look at a transcript of a language model carrying on a dialogue, and similarly mark every place where a fact was wrong (or out-of-date) and write the set of search queries that would have been appropriate. A system is then fine-tuned to generate search query turns which are again passed to a search engine to generate the search responses. The set of pages or snippets returned by the search engine in the search response turn are then treated as the context for generation, similarly to the retrieval-based questionanswering methods of Chapter 14. ", "Bloom_type": "application", "question": "What method could be used to improve the accuracy of a language model's dialogue interactions?", "options": ["Collect feedback on factual errors and create corresponding search queries.", "Only rely on the internal knowledge base of the model.", "Use only external search engines for verification.", "Record all interaction details including search queries."], "complexity": 2}, {"id": 95, "context": "1. Study the user and task: Understand the users and the task by interviewing users, investigating similar systems, and studying related human-human dialogues. ", "Bloom_type": "application", "question": "What is the first step in creating an effective dialogue system?", "options": ["Identify the users and their needs", "Develop a prototype of the dialogue system", "Choose programming languages for development", "Select appropriate algorithms for natural language processing"], "complexity": 2}, {"id": 96, "context": "These ethical issues are an important area of investigation, including finding ways to mitigate problems of abuse and toxicity, like detecting and responding appropriately to toxic contexts (Wolf et al. 2017, Dinan et al. 2020, Xu et al. 2020). Value sensitive design, carefully considering possible harms in advance (Friedman et al. 2017, Friedman and Hendry 2019) is also important; (Dinan et al., 2021) give a number of suggestions for best practices in dialogue system design. For example getting informed consent from participants, whether they are used for training, or whether they are interacting with a deployed system is important. Because dialogue systems by definition involve human participants, researchers also work on these issues with the Institutional Review Boards (IRB) at their institutions, who help protect the safety of experimental subjects. ", "Bloom_type": "application", "question": "What is an important step in designing dialogue systems?", "options": ["Ensure that all participants, especially those involved in training, are properly informed about their roles.", "Only focus on the technical aspects of the dialogue system.", "Ignore potential risks associated with using dialogue systems in experiments.", "Work directly with IRBs to ensure participant safety."], "complexity": 2}, {"id": 97, "context": "Chatbots and dialogue systems are crucial speech and language processing applications that are already widely used commercially. ", "Bloom_type": "application", "question": "What is the primary purpose of using chatbots and dialogue systems?", "options": ["To automate customer service interactions", "To improve website navigation", "To enhance educational content delivery", "To increase social media engagement"], "complexity": 2}, {"id": 98, "context": " In human dialogue, speaking is a kind of action; these acts are referred to as speech acts or dialogue acts. Speakers also attempt to achieve common ground by acknowledging that they have understand each other. Conversation also is characterized by turn structure and dialogue structure. ", "Bloom_type": "application", "question": "What is another way to describe the actions performed during a conversation?", "options": ["Dialogue acts", "Speech acts", "Turns", "Structure"], "complexity": 2}, {"id": 99, "context": " The dialogue-state architecture augments the GUS frame-and-slot architecture with richer representations and more sophisticated algorithms for keeping track of user`s dialogue acts, policies for generating its own dialogue acts, and a natural language component. ", "Bloom_type": "application", "question": "What does the dialogue-state architecture add to the GUS frame-and-slot architecture?", "options": ["It adds richer representations and more sophisticated algorithms.", "It simplifies the representation and reduces complexity.", "It removes the need for natural language processing.", "It eliminates the need for user interaction."], "complexity": 2}, {"id": 100, "context": " Dialogue systems are a kind of human-computer interaction, and general HCI principles apply in their design, including the role of the user, simulations such as Wizard-of-Oz systems, and the importance of iterative design and testing on real users. ", "Bloom_type": "application", "question": "What is an important aspect of designing dialogue systems?", "options": ["Incorporating user feedback throughout development", "Using only natural language processing techniques", "Focusing solely on visual interfaces", "Designing without considering usability"], "complexity": 2}, {"id": 101, "context": "Computational-implemented theories of dialogue blossomed in the 1970. That period saw the very influential GUS system (Bobrow et al., 1977), which in the late 1970s established the frame-based paradigm that became the dominant industrial paradigm for dialogue systems for over 30 years. ", "Bloom_type": "application", "question": "Which paradigm dominated dialogue systems until the early 2000s?", "options": ["Frame-based paradigm", "Rule-based paradigm", "Contextual paradigm", "Model-based paradigm"], "complexity": 2}, {"id": 102, "context": "In the 1990s, machine learning models that had first been applied to natural language processing began to be applied to dialogue tasks like slot filling (Miller et al. 1994, Pieraccini et al. 1991). This period also saw lots of analytic work on the linguistic properties of dialogue acts and on machine-learning-based methods for their detection. (Sag and Liberman 1975, Hinkelman and Allen 1989, Nagata and Morimoto 1994, Goodwin 1996, Chu-Carroll 1998, Shriberg et al. 1998, Stolcke et al. 2000, Gravano et al. 2012. This work strongly informed the development of the dialogue-state model (Larsson and Traum, 2000). Dialogue state tracking quickly became an important problem for task-oriented dialogue, and there has been an influential annual evaluation of state-tracking algorithms (Williams et al., 2016). The turn of the century saw a line of work on applying reinforcement learning to dialogue, which first came out of AT&T and Bell Laboratories with work on MDP dialogue systems (Walker 2000, Levin et al. 2000, Singh et al. 2002) along with work on cue phrases, prosody, and rejection and confirmation. Reinforcement learning research turned quickly to the more sophisticated POMDP models (Roy et al. 2000, Lemon et al. 2006, Williams and Young 2007) applied to small slotfilling dialogue tasks. Neural reinforcement learning models have been used both for chatbot systems, for example simulating dialogues between two dialogue systems, rewarding good conversational properties like coherence and ease of answering (Li et al., 2016a), and for task-oriented dialogue (Williams et al., 2017). ", "Bloom_type": "application", "question": "What was a significant application of machine learning in dialogue tasks during the 1990s?", "options": ["Applied reinforcement learning to dialogue", "Developed dialogue-state models", "Informed the creation of neural networks", "Enhanced the analysis of linguistic properties"], "complexity": 2}, {"id": 103, "context": "By around 2010 the GUS architecture finally began to be widely used commercially in dialogue systems on phones like Apple`s SIRI (Bellegarda, 2013) and other digital assistants. ", "Bloom_type": "application", "question": "What was the primary application of the GUS architecture before 2010?", "options": ["It had no commercial applications at all.", "It was primarily used for scientific research.", "It was mainly applied in desktop applications.", "It was utilized in virtual reality simulations."], "complexity": 2}, {"id": 104, "context": "Automatic transcription of speech by any speaker in any environment is still far from solved, but ASR technology has matured to the point where it is now viable for many practical tasks. Speech is a natural interface for communicating with smart home appliances, personal assistants, or cellphones, where keyboards are less convenient, in telephony applications like call-routing (Accounting, please) or in sophisticated dialogue applications (I`d like to change the return date of my flight). ASR is also useful for general transcription, for example for automatically generating captions for audio or video text (transcribing movies or videos or live discussions). Transcription is important in fields like law where dictation plays an important role. Finally, ASR is important as part of augmentative communication (interaction between computers and humans with some disability resulting in difficulties or inabilities in typing or audition). The blind Milton famously dictated Paradise Lost to his daughters, and Henry James dictated his later novels after a repetitive stress injury. ", "Bloom_type": "application", "question": "In which scenario is automatic speech recognition particularly beneficial?", "options": ["Automating customer service inquiries", "Transcribing speeches at conferences", "Recording lectures for offline viewing", "Generating subtitles for TV shows"], "complexity": 2}, {"id": 105, "context": "Modern speech synthesis has a wide variety of applications. TTS is used in conversational agents that conduct dialogues with people, plays a role in devices that read out loud for the blind or in games, and can be used to speak for sufferers of neurological disorders, such as the late astrophysicist Steven Hawking who, after he lost the use of his voice because of ALS, spoke by manipulating a TTS system. ", "Bloom_type": "application", "question": "In which application of modern speech synthesis does Steven Hawking primarily communicate?", "options": ["Speaking for sufferers of neurological disorders", "TTS in games", "Reading out loud for the blind", "Conversational agents"], "complexity": 2}, {"id": 106, "context": "Could we improve on word error rate as a metric? It would be nice, for example, to have something that didn`t give equal weight to every word, perhaps valuing content words like Tuesday more than function words like a or of. While researchers generally agree that this would be a good idea, it has proved difficult to agree on a metric that works in every application of ASR. For dialogue systems, however, where the desired semantic output is more clear, a metric called slot error rate or concept error rate has proved extremely useful; it is discussed in Chapter 15 on page 317. ", "Bloom_type": "application", "question": "What type of error rate could potentially provide better accuracy for dialogue systems?", "options": ["Concept error rate", "Word error rate", "Sentence error rate", "Dialogue error rate"], "complexity": 2}, {"id": 107, "context": "The goal of text-to-speech (TTS) systems is to map from strings of letters to waveforms, a technology that`s important for a variety of applications from dialogue systems to games to education. ", "Bloom_type": "application", "question": "What is the primary function of TTS systems?", "options": ["To create interactive voice responses", "To convert images into audio files", "To translate languages automatically", "To enhance visual content with sound"], "complexity": 2}, {"id": 108, "context": "Detecting emotion has the potential to improve a number of language processing tasks. Emotion recognition could help dialogue systems like tutoring systems detect that a student was unhappy, bored, hesitant, confident, and so on. Automatically detecting emotions in reviews or customer responses (anger, dissatisfaction, trust) could help businesses recognize specific problem areas or ones that are going well. Emotion can play a role in medical NLP tasks like helping diagnose depression or suicidal intent. Detecting emotions expressed toward characters in novels might play a role in understanding how different social groups were viewed by society at different times. ", "Bloom_type": "application", "question": "What is an example of how emotion detection can benefit dialogue systems?", "options": ["Detecting emotional states of users during conversations.", "Automatically translating sentences between languages.", "Improving the accuracy of speech-to-text conversion.", "Enhancing the speed of natural language generation."], "complexity": 2}, {"id": 109, "context": "Coreference is an important component of natural language processing. A dialogue system that has just told the user There is a 2pm flight on United and a 4pm one on Cathay Pacific must know which flight the user means by I`ll take the second one. A question answering system that uses Wikipedia to answer a question about Marie Curie must know who she was in the sentence She was born in Warsaw. And a machine translation system translating from a language like Spanish, in which pronouns can be dropped, must use coreference from the previous sentence to decide whether the Spanish sentence Me encanta el conocimiento, dice.` should be translated as I love knowledge, he says`, or I love knowledge, she says`. Indeed, this example comes from an actual news article in El Pas about a female professor and was mistranslated as he in machine translation because of inaccurate coreference resolution (Schiebinger, 2013). ", "Bloom_type": "application", "question": "In a dialogue system, how does it ensure that the user understands which flight they mean?", "options": ["By using coreference resolution techniques to match the user's query with the available flights.", "By asking the user for clarification immediately after providing the options.", "By prompting the user to choose between two options before proceeding.", "By repeating the same flight option multiple times until the user selects it."], "complexity": 2}]}, "hidden": {"max_id": 98, "Questions": [{"id": 0, "context": "By the next decade, Bengio et al. (2003) and Bengio et al. (2006) showed that neural language models could also be used to develop embeddings as part of the task of word prediction. Collobert and Weston (2007), Collobert and Weston (2008), and Collobert et al. (2011) then demonstrated that embeddings could be used to represent word meanings for a number of NLP tasks. Turian et al. (2010) compared the value of different kinds of embeddings for different NLP tasks. Mikolov et al. (2011) showed that recurrent neural nets could be used as language models. The idea of simplifying the hidden layer of these neural net language models to create the skipgram (and also CBOW) algorithms was proposed by Mikolov et al. (2013a). The negative sampling training algorithm was proposed in Mikolov et al. (2013b). There are numerous surveys of static embeddings and their parameterizations (Bullinaria and Levy 2007, Bullinaria and Levy 2012, Lapesa and Evert 2014, Kiela and Clark 2014, Levy et al. 2015). ", "Bloom_type": "remember", "question": "In what year did Bengio et al. first show that neural language models could be used for word prediction?", "options": ["2003", "2004", "2005", "2006"], "complexity": 0}, {"id": 1, "context": "Instead, a modern neural network is a network of small computing units, each of which takes a vector of input values and produces a single output value. In this chapter we introduce the neural net applied to classification. The architecture we introduce is called a feedforward network because the computation proceeds iteratively from one layer of units to the next. The use of modern neural nets is often called deep learning, because modern networks are often deep (have many layers). Neural networks share much of the same mathematics as logistic regression. But neural networks are a more powerful classifier than logistic regression, and indeed a minimal neural network (technically one with a single hidden layer`) can be shown to learn any function. ", "Bloom_type": "remember", "question": "What does the term \"hidden\" refer to in the context?", "options": ["The number of layers in a neural network", "The mathematical operations performed by neurons", "The type of data processed by the network", "The computational complexity of training the network"], "complexity": 0}, {"id": 2, "context": "It`s also instructive to look at the intermediate results, the outputs of the two hidden nodes h1 and h2. We showed in the previous paragraph that the h vector for the inputs x = [0, 0] was [0, 0]. Fig. 7.7b shows the values of the h layer for all 4 inputs. Notice that hidden representations of the two input points x = [0, 1] and x = [1, 0] (the two cases with XOR output = 1) are merged to the single point h = [1, 0]. The merger makes it easy to linearly separate the positive and negative cases of XOR. In other words, we can view the hidden layer of the network as forming a representation of the input. ", "Bloom_type": "remember", "question": "In the context provided, what does the merging of hidden representations for specific input points indicate?", "options": ["It suggests that these points represent the same class.", "It indicates that these points belong to different classes.", "It implies that these points do not need further processing.", "It means that these points cannot be separated by a simple line."], "complexity": 0}, {"id": 3, "context": "In this example we just stipulated the weights in Fig. 7.6. But for real examples the weights for neural networks are learned automatically using the error backpropagation algorithm to be introduced in Section 7.5. That means the hidden layers will learn to form useful representations. This intuition, that neural networks can automatically learn useful representations of the input, is one of their key advantages, and one that we will return to again and again in later chapters. ", "Bloom_type": "remember", "question": "What does the learning process involve when training a neural network with hidden layers?", "options": ["The weights are adjusted based on the error between predicted outputs and actual outcomes.", "The weights are manually set by the user.", "The hidden layers do not contribute to forming useful representations.", "The neural network learns to ignore irrelevant features."], "complexity": 0}, {"id": 4, "context": "That means we can think of a neural network classifier with one hidden layer as building a vector h which is a hidden layer representation of the input, and then running standard multinomial logistic regression on the features that the network develops in h. By contrast, in Chapter 5 the features were mainly designed by hand via feature templates. So a neural network is like multinomial logistic regression, but (a) with many layers, since a deep neural network is like layer after layer of logistic regression classifiers; (b) with those intermediate layers having many possible activation functions (tanh, ReLU, sigmoid) instead of just sigmoid (although we`ll continue to use  for convenience to mean any activation function); (c) rather than forming the features by feature templates, the prior layers of the network induce the feature representations themselves. ", "Bloom_type": "remember", "question": "In what way does a neural network differ from traditional multinomial logistic regression?", "options": ["It requires more manual feature design.", "It uses fewer layers.", "It has only one activation function per layer.", "It forms features using handcrafted templates."], "complexity": 0}, {"id": 5, "context": "Fig. 7.10 shows a sketch of this architecture. As we mentioned earlier, adding this hidden layer to our logistic regression classifier allows the network to represent the non-linear interactions between features. This alone might give us a better sentiment classifier. ", "Bloom_type": "remember", "question": "In the architecture described, what does the hidden layer add?", "options": ["It enhances the model\u2019s ability to handle non-linear relationships.", "It increases the speed of computation.", "It reduces the need for data preprocessing.", "It simplifies the training process."], "complexity": 0}, {"id": 6, "context": "Fig. 8.1 illustrates the structure of an RNN. As with ordinary feedforward networks, an input vector representing the current input, xt , is multiplied by a weight matrix and then passed through a non-linear activation function to compute the values for a layer of hidden units. This hidden layer is then used to calculate a corresponding output, yt . In a departure from our earlier window-based approach, sequences are processed by presenting one item at a time to the network. We`ll use subscripts to represent time, thus xt will mean the input vector x at time t. The key difference from a feedforward network lies in the recurrent link shown in the figure with the dashed line. This link augments the input to the computation at the hidden layer with the value of the hidden layer from the preceding point in time. ", "Bloom_type": "remember", "question": "In what way does the recurrent link in the RNN differ from a feedforward network?", "options": ["It processes inputs sequentially rather than simultaneously.", "It uses a different type of activation function.", "It requires more memory to store intermediate results.", "It increases the complexity of the model significantly."], "complexity": 0}, {"id": 7, "context": "The hidden layer from the previous time step provides a form of memory, or context, that encodes earlier processing and informs the decisions to be made at later points in time. Critically, this approach does not impose a fixed-length limit on this prior context; the context embodied in the previous hidden layer can include information extending back to the beginning of the sequence. ", "Bloom_type": "remember", "question": "In what way does the hidden layer provide context for decision-making?", "options": ["It allows unlimited length of previous context.", "It stores all past data indefinitely.", "It limits the amount of previous context based on sequence length.", "It only considers immediate past data."], "complexity": 0}, {"id": 8, "context": "To apply RNNs in this setting, we pass the text to be classified through the RNN a word at a time generating a new hidden layer representation at each time step. We can then take the hidden layer for the last token of the text, hn, to constitute a compressed representation of the entire sequence. We can pass this representation hn to a feedforward network that chooses a class via a softmax over the possible classes. Fig. 8.8 illustrates this approach. ", "Bloom_type": "remember", "question": "In the process of applying Recurrent Neural Networks (RNNs) to classify text, what is generated at each time step by passing the text through the RNN?", "options": ["Hidden layers", "Output tokens", "Word embeddings", "Context vectors"], "complexity": 0}, {"id": 9, "context": "Technically an autoregressive model is a model that predicts a value at time t based on a linear function of the previous values at times t 2, and so on. Although language models are not linear (since they have many layers of non-linearities), we loosely refer to this generation technique as autoregressive generation since the word generated at each time step is conditioned on the word selected by the network from the previous step. Fig. 8.9 illustrates this approach. In this figure, the details of the RNN`s hidden layers and recurrent connections are hidden within the blue block. This simple architecture underlies state-of-the-art approaches to applications such as machine translation, summarization, and question answering. The key to these approaches is to prime the generation component with an appropriate context. That is, instead of simply using <s> to get things started we can provide a richer task-appropriate context; for translation the context is the sentence in the source language; for summarization it`s the long text we want to summarize. ", "Bloom_type": "remember", "question": "In the context of language modeling, what does the term \"hidden\" specifically refer to?", "options": ["The weights of the neural network", "The actual content being generated", "The sequence of words in the input text", "The output probabilities after softmax"], "complexity": 0}, {"id": 10, "context": "To take advantage of context to the right of the current input, we can train an RNN on a reversed input sequence. With this approach, the hidden state at time t represents information about the sequence to the right of the current input: ", "Bloom_type": "remember", "question": "In the context of training an RNN with a reversed input sequence, what does the hidden state at time \\(t\\) represent?", "options": ["The entire sequence up to time \\(t\\)", "Information about the sequence to the left of the current input", "The next input value after time \\(t\\)", "None of the above"], "complexity": 0}, {"id": 11, "context": "Bidirectional RNNs have also proven to be quite effective for sequence classification. Recall from Fig. 8.8 that for sequence classification we used the final hidden state of the RNN as the input to a subsequent feedforward classifier. A difficulty with this approach is that the final state naturally reflects more information about the end of the sentence than its beginning. Bidirectional RNNs provide a simple solution to this problem; as shown in Fig. 8.12, we simply combine the final hidden states from the forward and backward passes (for example by concatenation) and use that as input for follow-on processing. ", "Bloom_type": "remember", "question": "In bidirectional RNNs, what is combined from the forward and backward passes?", "options": ["The final hidden state", "The initial hidden state", "The entire sequence data", "The intermediate hidden states"], "complexity": 0}, {"id": 12, "context": "It is the hidden state, ht , that provides the output for the LSTM at each time step. This output can be used as the input to subsequent layers in a stacked RNN, or at the final layer of a network ht can be used to provide the final output of the LSTM. ", "Bloom_type": "remember", "question": "In an LSTM, what does the hidden state represent?", "options": ["The memory cell holding previous states", "The initial input data fed into the network", "The current prediction made by the model", "The gradient of the weights"], "complexity": 0}, {"id": 13, "context": "In RNN language modeling, at a particular time t, we pass the prefix of t 1 tokens through the language model, using forward inference to produce a sequence of hidden states, ending with the hidden state corresponding to the last word of the prefix. We then use the final hidden state of the prefix as our starting point to generate the next token. ", "Bloom_type": "remember", "question": "In RNN language modeling, what is used to start generating the next token after passing the prefix through the language model?", "options": ["The last hidden state of the prefix", "The initial hidden state", "The first token of the prefix", "None of the above"], "complexity": 0}, {"id": 14, "context": "Fig. 8.17 shows an English source text (the green witch arrived), a sentence separator token (<s>, and a Spanish target text (llego la bruja verde). To translate a source text, we run it through the network performing forward inference to generate hidden states until we get to the end of the source. Then we begin autoregressive generation, asking for a word in the context of the hidden layer from the end of the source input as well as the end-of-sentence marker. Subsequent words are conditioned on the previous hidden state and the embedding for the last word generated. ", "Bloom_type": "remember", "question": "In translation using a neural network, what is the purpose of running the source text through the network?", "options": ["To infer hidden states", "To identify synonyms", "To detect grammatical errors", "To find collocations"], "complexity": 0}, {"id": 15, "context": "Let`s formalize and generalize this model a bit in Fig. 8.18. (To help keep things straight, we`ll use the superscripts e and d where needed to distinguish the hidden states of the encoder and the decoder.) The elements of the network on the left process the input sequence x and comprise the encoder. While our simplified figure shows only a single network layer for the encoder, stacked architectures are the norm, where the output states from the top layer of the stack are taken as the final representation, and the encoder consists of stacked biLSTMs where the hidden states from top layers from the forward and backward passes are concatenated to provide the contextualized representations for each time step. ", "Bloom_type": "remember", "question": "In the context provided, what does the term \"hidden\" refer to?", "options": ["The intermediate results before reaching the final output", "The actual data being processed", "The physical location within the system", "The initial state of the system"], "complexity": 0}, {"id": 16, "context": "The entire purpose of the encoder is to generate a contextualized representation of the input. This representation is embodied in the final hidden state of the encoder, he n. This representation, also called c for context, is then passed to the decoder. ", "Bloom_type": "remember", "question": "In the context of an encoder-decoder model, what does the hidden state represent?", "options": ["The final state of the encoder", "The output of the decoder", "The initial state of the encoder", "The intermediate states of the encoder"], "complexity": 0}, {"id": 17, "context": "As Fig. 8.18 shows, we do something more complex: we make the context vector c available to more than just the first decoder hidden state, to ensure that the influence of the context vector, c, doesn`t wane as the output sequence is generated. We do this by adding c as a parameter to the computation of the current hidden state. using the following equation: ", "Bloom_type": "remember", "question": "In the context of making the context vector \\(c\\) influential throughout the generation process, how are additional parameters added to the computation of the current hidden state?", "options": ["By incorporating the context vector into the computation through an additional layer", "By directly modifying the initial hidden state", "By multiplying the context vector with the previous hidden state", "By applying a sigmoid function to the context vector"], "complexity": 0}, {"id": 18, "context": "The attention mechanism is a solution to the bottleneck problem, a way of allowing the decoder to get information from all the hidden states of the encoder, not just the last hidden state. ", "Bloom_type": "remember", "question": "In the context of machine translation, what does the attention mechanism enable?", "options": ["Both A) and B)", "The encoder to focus on only the most recent input", "The decoder to access all previous inputs equally", "None of the above"], "complexity": 0}, {"id": 19, "context": "In the attention mechanism, as in the vanilla encoder-decoder model, the context vector c is a single vector that is a function of the hidden states of the encoder. But instead of being taken from the last hidden state, it`s a weighted average of all the hidden states of the decoder. And this weighted average is also informed by part of the decoder state as well, the state of the decoder right before the current token i. That is, c = f (he n, hd 1). The weights focus on (attend to`) a particular part of i the source text that is relevant for the token i that the decoder is currently producing. Attention thus replaces the static context vector with one that is dynamically derived from the encoder hidden states, but also informed by and hence different for each token in decoding. ", "Bloom_type": "remember", "question": "In the attention mechanism, what does the context vector c represent?", "options": ["A dynamic vector combining encoder hidden states and previous decoder state", "The sum of all hidden states of the encoder", "A fixed vector based on the last hidden state of the encoder", "The final output of the decoder"], "complexity": 0}, {"id": 20, "context": "The score that results from this dot product is a scalar that reflects the degree of similarity between the two vectors. The vector of these scores across all the encoder hidden states gives us the relevance of each encoder state to the current step of the decoder. ", "Bloom_type": "remember", "question": "In the context, what does the score result from the dot product represent?", "options": ["The degree of similarity between the two vectors", "The magnitude of the vectors", "The angle between the vectors", "The distance between the vectors"], "complexity": 0}, {"id": 21, "context": "It`s also possible to create more sophisticated scoring functions for attention models. Instead of simple dot product attention, we can get a more powerful function that computes the relevance of each encoder hidden state to the decoder hidden state by parameterizing the score with its own set of weights, Ws. 1, he score(hd i ", "Bloom_type": "remember", "question": "How do we improve the scoring function in attention models?", "options": ["By adding more parameters to the score computation", "By using only simple dot products", "By increasing the number of encoder hidden states", "By removing the decoder hidden state from consideration"], "complexity": 0}, {"id": 22, "context": " In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t ", "Bloom_type": "remember", "question": "In RNNs, what is the role of the hidden layer?", "options": ["It predicts the next element in the sequence.", "It processes all elements simultaneously.", "It stores the entire sequence for future use.", "It updates the weights of the connections."], "complexity": 0}, {"id": 23, "context": "Layer Norm At two stages in the transformer block we normalize the vector (Ba et al., 2016). This process, called layer norm (short for layer normalization), is one of many forms of normalization that can be used to improve training performance in deep neural networks by keeping the values of a hidden layer in a range that facilitates gradient-based training. ", "Bloom_type": "remember", "question": "In which part of the transformer block does layer normalization occur?", "options": ["Both at the input and output stages", "At the input stage", "At the output stage", "None of the above"], "complexity": 0}, {"id": 24, "context": "Note the key difference between this figure and the earlier RNN-based version shown in Fig. 8.6. There the calculation of the outputs and the losses at each step was inherently serial given the recurrence in the calculation of the hidden states. With transformers, each training item can be processed in parallel since the output for each element in the sequence is computed separately. ", "Bloom_type": "remember", "question": "In what way does the use of transformers differ from previous recurrent neural networks (RNNs) regarding processing?", "options": ["Transformers process one element at a time, unlike RNNs.", "Transformers process all elements sequentially, just like RNNs.", "Transformers require more memory than RNNs.", "Transformers cannot handle longer sequences compared to RNNs."], "complexity": 0}, {"id": 25, "context": "the Empress Maria Theresa the famous Mechanical Turk, a chess-playing automaton consisting of a wooden box filled with gears, behind which sat a robot mannequin who played chess by moving pieces with his mechanical arm. The Turk toured Europe and the Americas for decades, defeating Napoleon Bonaparte and even playing Charles Babbage. The Mechanical Turk might have been one of the early successes of artificial intelligence were it not for the fact that it was, alas, a hoax, powered by a human chess player hidden inside the box. ", "Bloom_type": "remember", "question": "What made the Mechanical Turk seem like an early success of artificial intelligence?", "options": ["The presence of a human chess player hidden inside the box", "The chess-playing ability of the robot mannequin", "The touristic appeal of the traveling exhibition", "The use of advanced technology in its construction"], "complexity": 0}, {"id": 26, "context": "a hidden state henc given the input x1...xt . The language model predictor takes as input the previous output token (not counting blanks), outputting a hidden state hpred . The two are passed through another network whose output is then passed through a softmax to predict the next character. ", "Bloom_type": "remember", "question": "In natural language processing, what does the term \"hidden\" refer to?", "options": ["Internal states of a neural network", "Visible data", "Unseen characters", "External inputs"], "complexity": 0}, {"id": 27, "context": "The third innovation of this period was the rise of the HMM. Hidden Markov models seem to have been applied to speech independently at two laboratories around 1972. One application arose from the work of statisticians, in particular Baum and colleagues at the Institute for Defense Analyses in Princeton who applied HMMs to various prediction problems (Baum and Petrie 1966, Baum and Eagon 1967). James Baker learned of this work and applied the algorithm to speech processing (Baker, 1975a) during his graduate work at CMU. Independently, Frederick Jelinek and collaborators (drawing from their research in information-theoretical models influenced by the work of Shannon (1948)) applied HMMs to speech at the IBM Thomas J. Watson Research Center (Jelinek et al., 1975). One early difference was the decoding algorithm; Baker`s DRAGON system used Viterbi (dynamic programming) decoding, while the IBM system applied Jelinek`s stack decoding algorithm (Jelinek, 1969). Baker then joined the IBM group for a brief time before founding the speech-recognition company Dragon Systems. ", "Bloom_type": "remember", "question": "In what year did James Baker learn about the work on hidden Markov models?", "options": ["1972", "1970", "1973", "1975"], "complexity": 0}, {"id": 28, "context": "In this section we introduce our first sequence labeling algorithm, the Hidden Markov Model, and show how to apply it to part-of-speech tagging. Recall that a sequence labeler is a model whose job is to assign a label to each unit in a sequence, thus mapping a sequence of observations to a sequence of labels of the same length. The HMM is a classic model that introduces many of the key concepts of sequence modeling that we will see again in more modern models. ", "Bloom_type": "remember", "question": "In what type of machine learning problem does the Hidden Markov Model (HMM) find its application?", "options": ["Sequence labeling", "Image recognition", "Clustering", "Classification"], "complexity": 0}, {"id": 29, "context": "A Markov chain is useful when we need to compute a probability for a sequence of observable events. In many cases, however, the events we are interested in are hidden: we don`t observe them directly. For example we don`t normally observe part-of-speech tags in a text. Rather, we see words, and must infer the tags from the word sequence. We call the tags hidden because they are not observed. ", "Bloom_type": "remember", "question": "In what way do we refer to the part-of-speech tags as hidden?", "options": ["They are not directly observable.", "They are always visible.", "We can easily observe them.", "They are only indirectly observable."], "complexity": 0}, {"id": 30, "context": "For any model, such as an HMM, that contains hidden variables, the task of determining the hidden variables sequence corresponding to the sequence of observations is called decoding. More formally, ", "Bloom_type": "remember", "question": "In a Hidden Markov Model (HMM), what does the process of finding the sequence of hidden states corresponding to a set of observed sequences refer to?", "options": ["Decoding", "Training", "Forward algorithm", "Backward algorithm"], "complexity": 0}, {"id": 31, "context": "Figure 17.14 shows a fleshed-out version of the sketch we saw in Fig. 17.11, the Viterbi lattice for computing the best hidden state sequence for the observation sequence Janet will back the bill. ", "Bloom_type": "remember", "question": "In the figure shown, what does \"hidden\" refer to?", "options": ["The path taken through the lattice by the algorithm", "The actual observations made by Janet", "The initial guess for the hidden states", "The final output of the algorithm"], "complexity": 0}, {"id": 32, "context": "A simple but very strong algorithm for Task 4 is to represent each of the two spans by BERT embeddings and take the last layer hidden state corresponding to the position of the [CLS] token, pass this through a single layer tanh feedforward network and then a softmax for sense classification (Nie et al., 2019). ", "Bloom_type": "remember", "question": "In the described algorithm, what does the term \"hidden\" refer to?", "options": ["The intermediate representation before feeding into the softmax layer", "The final output after passing through the tanh function", "The position of the [CLS] token within the sequence", "The entire process of embedding and classification"], "complexity": 0}, {"id": 33, "context": "By the next decade, Bengio et al. (2003) and Bengio et al. (2006) showed that neural language models could also be used to develop embeddings as part of the task of word prediction. Collobert and Weston (2007), Collobert and Weston (2008), and Collobert et al. (2011) then demonstrated that embeddings could be used to represent word meanings for a number of NLP tasks. Turian et al. (2010) compared the value of different kinds of embeddings for different NLP tasks. Mikolov et al. (2011) showed that recurrent neural nets could be used as language models. The idea of simplifying the hidden layer of these neural net language models to create the skipgram (and also CBOW) algorithms was proposed by Mikolov et al. (2013a). The negative sampling training algorithm was proposed in Mikolov et al. (2013b). There are numerous surveys of static embeddings and their parameterizations (Bullinaria and Levy 2007, Bullinaria and Levy 2012, Lapesa and Evert 2014, Kiela and Clark 2014, Levy et al. 2015). ", "Bloom_type": "comprehension", "question": "What were some key developments in the field of neural language models after Bengio et al.'s work in 2003 and 2006?", "options": ["All of the above", "The development of embeddings for word meaning representation", "The creation of skipgram and CBOW algorithms", "The introduction of negative sampling training algorithm"], "complexity": 1}, {"id": 34, "context": "Instead, a modern neural network is a network of small computing units, each of which takes a vector of input values and produces a single output value. In this chapter we introduce the neural net applied to classification. The architecture we introduce is called a feedforward network because the computation proceeds iteratively from one layer of units to the next. The use of modern neural nets is often called deep learning, because modern networks are often deep (have many layers). Neural networks share much of the same mathematics as logistic regression. But neural networks are a more powerful classifier than logistic regression, and indeed a minimal neural network (technically one with a single hidden layer`) can be shown to learn any function. ", "Bloom_type": "comprehension", "question": "What does the term 'hidden' refer to in the context of neural networks?", "options": ["The part of the network where computations occur but no outputs are produced.", "The initial stage of training before feeding data into the network.", "The final layer of the network responsible for making predictions.", "The computational steps performed by the network."], "complexity": 1}, {"id": 35, "context": "It`s also instructive to look at the intermediate results, the outputs of the two hidden nodes h1 and h2. We showed in the previous paragraph that the h vector for the inputs x = [0, 0] was [0, 0]. Fig. 7.7b shows the values of the h layer for all 4 inputs. Notice that hidden representations of the two input points x = [0, 1] and x = [1, 0] (the two cases with XOR output = 1) are merged to the single point h = [1, 0]. The merger makes it easy to linearly separate the positive and negative cases of XOR. In other words, we can view the hidden layer of the network as forming a representation of the input. ", "Bloom_type": "comprehension", "question": "What does the merging of hidden representations indicate about the network's ability to classify data?", "options": ["It implies the network can easily distinguish between different types of data.", "It indicates the network cannot classify any data.", "It suggests the network can only classify binary data.", "None of the above"], "complexity": 1}, {"id": 36, "context": "In this example we just stipulated the weights in Fig. 7.6. But for real examples the weights for neural networks are learned automatically using the error backpropagation algorithm to be introduced in Section 7.5. That means the hidden layers will learn to form useful representations. This intuition, that neural networks can automatically learn useful representations of the input, is one of their key advantages, and one that we will return to again and again in later chapters. ", "Bloom_type": "comprehension", "question": "What does the statement imply about the role of hidden layers in neural networks?", "options": ["Hidden layers are essential for automatic representation formation.", "Hidden layers do not contribute to forming useful representations.", "Hidden layers are only used for learning weights manually.", "The role of hidden layers is irrelevant to the overall function of neural networks."], "complexity": 1}, {"id": 37, "context": "That means we can think of a neural network classifier with one hidden layer as building a vector h which is a hidden layer representation of the input, and then running standard multinomial logistic regression on the features that the network develops in h. By contrast, in Chapter 5 the features were mainly designed by hand via feature templates. So a neural network is like multinomial logistic regression, but (a) with many layers, since a deep neural network is like layer after layer of logistic regression classifiers; (b) with those intermediate layers having many possible activation functions (tanh, ReLU, sigmoid) instead of just sigmoid (although we`ll continue to use  for convenience to mean any activation function); (c) rather than forming the features by feature templates, the prior layers of the network induce the feature representations themselves. ", "Bloom_type": "comprehension", "question": "How does a neural network differ from traditional multinomial logistic regression when it comes to feature development?", "options": ["The neural network creates its own feature representations through intermediate layers.", "The neural network uses hand-designed features.", "The neural network relies solely on sigmoid activation functions.", "The neural network forms features using feature templates."], "complexity": 1}, {"id": 38, "context": "Fig. 7.10 shows a sketch of this architecture. As we mentioned earlier, adding this hidden layer to our logistic regression classifier allows the network to represent the non-linear interactions between features. This alone might give us a better sentiment classifier. ", "Bloom_type": "comprehension", "question": "What does the addition of a hidden layer in a logistic regression classifier do?", "options": ["It enhances the ability to capture non-linear relationships.", "It increases the complexity of the model.", "It simplifies the decision boundaries.", "It reduces the number of parameters."], "complexity": 1}, {"id": 39, "context": "Fig. 8.1 illustrates the structure of an RNN. As with ordinary feedforward networks, an input vector representing the current input, xt , is multiplied by a weight matrix and then passed through a non-linear activation function to compute the values for a layer of hidden units. This hidden layer is then used to calculate a corresponding output, yt . In a departure from our earlier window-based approach, sequences are processed by presenting one item at a time to the network. We`ll use subscripts to represent time, thus xt will mean the input vector x at time t. The key difference from a feedforward network lies in the recurrent link shown in the figure with the dashed line. This link augments the input to the computation at the hidden layer with the value of the hidden layer from the preceding point in time. ", "Bloom_type": "comprehension", "question": "What distinguishes the hidden layer in an RNN from a regular feedforward network?", "options": ["The hidden layer processes inputs sequentially rather than simultaneously.", "The hidden layer uses a different type of activation function.", "The hidden layer incorporates feedback connections between layers.", "The hidden layer requires more computational resources."], "complexity": 1}, {"id": 40, "context": "The hidden layer from the previous time step provides a form of memory, or context, that encodes earlier processing and informs the decisions to be made at later points in time. Critically, this approach does not impose a fixed-length limit on this prior context; the context embodied in the previous hidden layer can include information extending back to the beginning of the sequence. ", "Bloom_type": "comprehension", "question": "How does the hidden layer function differently than traditional recurrent neural networks (RNNs)?", "options": ["It imposes a fixed-length limit on the previous context.", "It uses a different type of encoding for memory.", "It requires more computational resources.", "It cannot handle sequences longer than its length."], "complexity": 1}, {"id": 41, "context": "To apply RNNs in this setting, we pass the text to be classified through the RNN a word at a time generating a new hidden layer representation at each time step. We can then take the hidden layer for the last token of the text, hn, to constitute a compressed representation of the entire sequence. We can pass this representation hn to a feedforward network that chooses a class via a softmax over the possible classes. Fig. 8.8 illustrates this approach. ", "Bloom_type": "comprehension", "question": "What does the hidden layer represent after passing the text through an RNN?", "options": ["The final hidden layer output", "The current state of the RNN", "The next word in the sequence", "The initial hidden state"], "complexity": 1}, {"id": 42, "context": "Technically an autoregressive model is a model that predicts a value at time t based on a linear function of the previous values at times t 2, and so on. Although language models are not linear (since they have many layers of non-linearities), we loosely refer to this generation technique as autoregressive generation since the word generated at each time step is conditioned on the word selected by the network from the previous step. Fig. 8.9 illustrates this approach. In this figure, the details of the RNN`s hidden layers and recurrent connections are hidden within the blue block. This simple architecture underlies state-of-the-art approaches to applications such as machine translation, summarization, and question answering. The key to these approaches is to prime the generation component with an appropriate context. That is, instead of simply using <s> to get things started we can provide a richer task-appropriate context; for translation the context is the sentence in the source language; for summarization it`s the long text we want to summarize. ", "Bloom_type": "comprehension", "question": "What does the hidden layer in an autoregressive model represent?", "options": ["The internal representation of the data", "The output of the neural network", "The weights connecting the input to the hidden layer", "The entire structure of the neural network"], "complexity": 1}, {"id": 43, "context": "To take advantage of context to the right of the current input, we can train an RNN on a reversed input sequence. With this approach, the hidden state at time t represents information about the sequence to the right of the current input: ", "Bloom_type": "comprehension", "question": "What does the hidden state represent in the context of training an RNN?", "options": ["Information about the sequence to the left of the current input", "The next word in the sentence", "The entire input sequence", "The previous hidden state"], "complexity": 1}, {"id": 44, "context": "Bidirectional RNNs have also proven to be quite effective for sequence classification. Recall from Fig. 8.8 that for sequence classification we used the final hidden state of the RNN as the input to a subsequent feedforward classifier. A difficulty with this approach is that the final state naturally reflects more information about the end of the sentence than its beginning. Bidirectional RNNs provide a simple solution to this problem; as shown in Fig. 8.12, we simply combine the final hidden states from the forward and backward passes (for example by concatenation) and use that as input for follow-on processing. ", "Bloom_type": "comprehension", "question": "What is one way bidirectional RNNs address the issue of reflecting more information at the end of a sequence compared to its beginning?", "options": ["Combining the final hidden states from the forward and backward passes", "Using only the initial hidden state", "Removing the need for a feedforward classifier", "Increasing the complexity of the model"], "complexity": 1}, {"id": 45, "context": "It is the hidden state, ht , that provides the output for the LSTM at each time step. This output can be used as the input to subsequent layers in a stacked RNN, or at the final layer of a network ht can be used to provide the final output of the LSTM. ", "Bloom_type": "comprehension", "question": "What does the hidden state (ht) represent in an LSTM model?", "options": ["The current state of the network after processing all inputs up to that point", "The initial state before any input data is fed into the network", "The predicted output based solely on past states", "The memory cell storing intermediate results"], "complexity": 1}, {"id": 46, "context": "In RNN language modeling, at a particular time t, we pass the prefix of t 1 tokens through the language model, using forward inference to produce a sequence of hidden states, ending with the hidden state corresponding to the last word of the prefix. We then use the final hidden state of the prefix as our starting point to generate the next token. ", "Bloom_type": "comprehension", "question": "What does the term 'hidden' refer to in the context of RNN language modeling?", "options": ["The intermediate states passed through the network during training", "The actual output generated by the model", "The weights connecting different layers of the neural network", "The initial input before any processing occurs"], "complexity": 1}, {"id": 47, "context": "Fig. 8.17 shows an English source text (the green witch arrived), a sentence separator token (<s>, and a Spanish target text (llego la bruja verde). To translate a source text, we run it through the network performing forward inference to generate hidden states until we get to the end of the source. Then we begin autoregressive generation, asking for a word in the context of the hidden layer from the end of the source input as well as the end-of-sentence marker. Subsequent words are conditioned on the previous hidden state and the embedding for the last word generated. ", "Bloom_type": "comprehension", "question": "What does the phrase 'hidden states' refer to in the translation process?", "options": ["The intermediate representations during the translation process", "The actual meaning of the words in the source text", "The sequence of words in the target language after translation", "The final output after all translations have been completed"], "complexity": 1}, {"id": 48, "context": "Let`s formalize and generalize this model a bit in Fig. 8.18. (To help keep things straight, we`ll use the superscripts e and d where needed to distinguish the hidden states of the encoder and the decoder.) The elements of the network on the left process the input sequence x and comprise the encoder. While our simplified figure shows only a single network layer for the encoder, stacked architectures are the norm, where the output states from the top layer of the stack are taken as the final representation, and the encoder consists of stacked biLSTMs where the hidden states from top layers from the forward and backward passes are concatenated to provide the contextualized representations for each time step. ", "Bloom_type": "comprehension", "question": "What does the element on the left of the network do?", "options": ["It processes the input sequence x.", "It provides the initial state for the decoder.", "It generates the final representation of the input sequence.", "It concatenates the hidden states from different layers."], "complexity": 1}, {"id": 49, "context": "The entire purpose of the encoder is to generate a contextualized representation of the input. This representation is embodied in the final hidden state of the encoder, he n. This representation, also called c for context, is then passed to the decoder. ", "Bloom_type": "comprehension", "question": "What does the hidden state represent in the context of the encoder?", "options": ["The final state of the encoder", "The encoded features of the input", "The output of the decoder", "The initial state of the model"], "complexity": 1}, {"id": 50, "context": "As Fig. 8.18 shows, we do something more complex: we make the context vector c available to more than just the first decoder hidden state, to ensure that the influence of the context vector, c, doesn`t wane as the output sequence is generated. We do this by adding c as a parameter to the computation of the current hidden state. using the following equation: ", "Bloom_type": "comprehension", "question": "How does the model incorporate the context vector into its hidden states?", "options": ["By adding it as an additional input parameter during computation", "By setting it aside entirely", "By multiplying it with the previous hidden state", "By removing it after each generation step"], "complexity": 1}, {"id": 51, "context": "The attention mechanism is a solution to the bottleneck problem, a way of allowing the decoder to get information from all the hidden states of the encoder, not just the last hidden state. ", "Bloom_type": "comprehension", "question": "Explain how the attention mechanism addresses the issue of limited information access by the decoder?", "options": ["The attention mechanism ensures that the decoder can draw upon all hidden states but prioritizes the latest one.", "The attention mechanism allows the decoder to focus only on the most recent hidden state.", "The attention mechanism enables the decoder to use all hidden states equally, regardless of their relevance.", "The attention mechanism restricts the decoder to consider only the first hidden state for better efficiency."], "complexity": 1}, {"id": 52, "context": "In the attention mechanism, as in the vanilla encoder-decoder model, the context vector c is a single vector that is a function of the hidden states of the encoder. But instead of being taken from the last hidden state, it`s a weighted average of all the hidden states of the decoder. And this weighted average is also informed by part of the decoder state as well, the state of the decoder right before the current token i. That is, c = f (he n, hd 1). The weights focus on (attend to`) a particular part of i the source text that is relevant for the token i that the decoder is currently producing. Attention thus replaces the static context vector with one that is dynamically derived from the encoder hidden states, but also informed by and hence different for each token in decoding. ", "Bloom_type": "comprehension", "question": "How does the attention mechanism differ from the traditional context vector approach in an encoder-decoder setup?", "options": ["Both A and B are correct.", "The attention mechanism uses a fixed context vector throughout the entire decoding process.", "In the attention mechanism, the context vector is updated based on the decoder's previous state.", "The attention mechanism relies solely on the encoder's final hidden state."], "complexity": 1}, {"id": 53, "context": "The score that results from this dot product is a scalar that reflects the degree of similarity between the two vectors. The vector of these scores across all the encoder hidden states gives us the relevance of each encoder state to the current step of the decoder. ", "Bloom_type": "comprehension", "question": "What does the score resulting from the dot product reflect about the relationship between the two vectors?", "options": ["The similarity between the vectors", "The magnitude of the vectors", "The angle between the vectors", "The direction of the vectors"], "complexity": 1}, {"id": 54, "context": "It`s also possible to create more sophisticated scoring functions for attention models. Instead of simple dot product attention, we can get a more powerful function that computes the relevance of each encoder hidden state to the decoder hidden state by parameterizing the score with its own set of weights, Ws. 1, he score(hd i ", "Bloom_type": "comprehension", "question": "What does the phrase 'hidden states' refer to in this context?", "options": ["The internal representation of the model during training", "The output of the neural network at each time step", "The input data fed into the model", "The parameters of the model"], "complexity": 1}, {"id": 55, "context": " In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t ", "Bloom_type": "comprehension", "question": "What does the hidden layer represent in a Simple RNN?", "options": ["The previous outputs stored for future use", "The immediate inputs received by the network", "The weights connecting different layers of the network", "The final prediction made by the network"], "complexity": 1}, {"id": 56, "context": "Layer Norm At two stages in the transformer block we normalize the vector (Ba et al., 2016). This process, called layer norm (short for layer normalization), is one of many forms of normalization that can be used to improve training performance in deep neural networks by keeping the values of a hidden layer in a range that facilitates gradient-based training. ", "Bloom_type": "comprehension", "question": "What does layer norm do in the context of transformers?", "options": ["It prevents the exploding/vanishing gradient problem by scaling activations.", "It normalizes the input data before feeding it into the model.", "It adjusts the weights during backpropagation to prevent vanishing gradients.", "It ensures all neurons have an equal chance of being activated."], "complexity": 1}, {"id": 57, "context": "Note the key difference between this figure and the earlier RNN-based version shown in Fig. 8.6. There the calculation of the outputs and the losses at each step was inherently serial given the recurrence in the calculation of the hidden states. With transformers, each training item can be processed in parallel since the output for each element in the sequence is computed separately. ", "Bloom_type": "comprehension", "question": "How does the use of transformers differ from previous models in terms of processing sequences?", "options": ["Transformers allow parallel processing by calculating outputs independently.", "Previous models required sequential processing due to their recurrent nature.", "Transformers require serial processing because they are based on RNNs.", "Both A) and C)"], "complexity": 1}, {"id": 58, "context": "the Empress Maria Theresa the famous Mechanical Turk, a chess-playing automaton consisting of a wooden box filled with gears, behind which sat a robot mannequin who played chess by moving pieces with his mechanical arm. The Turk toured Europe and the Americas for decades, defeating Napoleon Bonaparte and even playing Charles Babbage. The Mechanical Turk might have been one of the early successes of artificial intelligence were it not for the fact that it was, alas, a hoax, powered by a human chess player hidden inside the box. ", "Bloom_type": "comprehension", "question": "Explain why the Mechanical Turk may have been considered an early success despite being a hoax?", "options": ["The audience believed they saw a real chess game rather than a performance.", "It was the first time a machine had won against a human.", "The human operator inside the box could play better than the machine.", "The design of the wooden box made it look more impressive."], "complexity": 1}, {"id": 59, "context": "a hidden state henc given the input x1...xt . The language model predictor takes as input the previous output token (not counting blanks), outputting a hidden state hpred . The two are passed through another network whose output is then passed through a softmax to predict the next character. ", "Bloom_type": "comprehension", "question": "What does the language model predictor do when it receives an input sequence?", "options": ["It combines the previous output token with the current input for prediction.", "It predicts the next hidden state based on the current input.", "It generates the next word directly from the input.", "It outputs the entire sequence at once."], "complexity": 1}, {"id": 60, "context": "The third innovation of this period was the rise of the HMM. Hidden Markov models seem to have been applied to speech independently at two laboratories around 1972. One application arose from the work of statisticians, in particular Baum and colleagues at the Institute for Defense Analyses in Princeton who applied HMMs to various prediction problems (Baum and Petrie 1966, Baum and Eagon 1967). James Baker learned of this work and applied the algorithm to speech processing (Baker, 1975a) during his graduate work at CMU. Independently, Frederick Jelinek and collaborators (drawing from their research in information-theoretical models influenced by the work of Shannon (1948)) applied HMMs to speech at the IBM Thomas J. Watson Research Center (Jelinek et al., 1975). One early difference was the decoding algorithm; Baker`s DRAGON system used Viterbi (dynamic programming) decoding, while the IBM system applied Jelinek`s stack decoding algorithm (Jelinek, 1969). Baker then joined the IBM group for a brief time before founding the speech-recognition company Dragon Systems. ", "Bloom_type": "comprehension", "question": "What were some differences between the approaches taken by the two groups when applying Hidden Markov Models to speech processing?", "options": ["The IBM group used dynamic programming decoding, while the CMU group used stack decoding.", "The IBM group used Viterbi decoding, while the CMU group used dynamic programming.", "Both groups used different algorithms for decoding but did not differ much in other aspects.", "None of the options are accurate."], "complexity": 1}, {"id": 61, "context": "In this section we introduce our first sequence labeling algorithm, the Hidden Markov Model, and show how to apply it to part-of-speech tagging. Recall that a sequence labeler is a model whose job is to assign a label to each unit in a sequence, thus mapping a sequence of observations to a sequence of labels of the same length. The HMM is a classic model that introduces many of the key concepts of sequence modeling that we will see again in more modern models. ", "Bloom_type": "comprehension", "question": "What does the Hidden Markov Model (HMM) do?", "options": ["It assigns labels to sequences.", "It predicts future events based on past data.", "It generates new sequences from existing ones.", "It combines different types of data into one."], "complexity": 1}, {"id": 62, "context": "A Markov chain is useful when we need to compute a probability for a sequence of observable events. In many cases, however, the events we are interested in are hidden: we don`t observe them directly. For example we don`t normally observe part-of-speech tags in a text. Rather, we see words, and must infer the tags from the word sequence. We call the tags hidden because they are not observed. ", "Bloom_type": "comprehension", "question": "Explain why it is important to consider hidden events in Markov chains?", "options": ["To increase the accuracy of predictions by inferring the underlying structure.", "To make the computation more complex and challenging.", "Hidden events are easier to observe than observable events.", "It simplifies the model by reducing the number of parameters."], "complexity": 1}, {"id": 63, "context": "For any model, such as an HMM, that contains hidden variables, the task of determining the hidden variables sequence corresponding to the sequence of observations is called decoding. More formally, ", "Bloom_type": "comprehension", "question": "What is the process of determining the hidden variable sequence corresponding to the observation sequence called?", "options": ["Decoding", "Encoding", "Training", "Prediction"], "complexity": 1}, {"id": 64, "context": "Figure 17.14 shows a fleshed-out version of the sketch we saw in Fig. 17.11, the Viterbi lattice for computing the best hidden state sequence for the observation sequence Janet will back the bill. ", "Bloom_type": "comprehension", "question": "What does the Viterbi lattice represent in the context of the observation sequence Janet will back the bill?", "options": ["The likelihood of each state being the true state at any point in time", "The probability distribution of all possible states", "The path taken by Janet through the lattice", "The number of observations made by Janet"], "complexity": 1}, {"id": 65, "context": "A simple but very strong algorithm for Task 4 is to represent each of the two spans by BERT embeddings and take the last layer hidden state corresponding to the position of the [CLS] token, pass this through a single layer tanh feedforward network and then a softmax for sense classification (Nie et al., 2019). ", "Bloom_type": "comprehension", "question": "What does the term 'hidden' refer to in the context?", "options": ["The internal representation of features within an artificial neural network.", "The output of the neural network layers after processing input data.", "The final result before applying any activation function.", "The initial weights of the model."], "complexity": 1}, {"id": 66, "context": "By the next decade, Bengio et al. (2003) and Bengio et al. (2006) showed that neural language models could also be used to develop embeddings as part of the task of word prediction. Collobert and Weston (2007), Collobert and Weston (2008), and Collobert et al. (2011) then demonstrated that embeddings could be used to represent word meanings for a number of NLP tasks. Turian et al. (2010) compared the value of different kinds of embeddings for different NLP tasks. Mikolov et al. (2011) showed that recurrent neural nets could be used as language models. The idea of simplifying the hidden layer of these neural net language models to create the skipgram (and also CBOW) algorithms was proposed by Mikolov et al. (2013a). The negative sampling training algorithm was proposed in Mikolov et al. (2013b). There are numerous surveys of static embeddings and their parameterizations (Bullinaria and Levy 2007, Bullinaria and Levy 2012, Lapesa and Evert 2014, Kiela and Clark 2014, Levy et al. 2015). ", "Bloom_type": "application", "question": "What is the first step in developing an embedding using neural language models?", "options": ["Implementing the skipgram algorithm", "Selecting the type of neural network architecture", "Choosing the dataset for training", "Training the model with negative sampling"], "complexity": 2}, {"id": 67, "context": "Instead, a modern neural network is a network of small computing units, each of which takes a vector of input values and produces a single output value. In this chapter we introduce the neural net applied to classification. The architecture we introduce is called a feedforward network because the computation proceeds iteratively from one layer of units to the next. The use of modern neural nets is often called deep learning, because modern networks are often deep (have many layers). Neural networks share much of the same mathematics as logistic regression. But neural networks are a more powerful classifier than logistic regression, and indeed a minimal neural network (technically one with a single hidden layer`) can be shown to learn any function. ", "Bloom_type": "application", "question": "What does it mean when a neural network is described as having a single hidden layer?", "options": ["It implies the network can approximate any function using fewer parameters.", "It means there is only one type of neuron in the network.", "It indicates the network will not perform well on complex tasks.", "It suggests the network cannot handle large amounts of data efficiently."], "complexity": 2}, {"id": 68, "context": "It`s also instructive to look at the intermediate results, the outputs of the two hidden nodes h1 and h2. We showed in the previous paragraph that the h vector for the inputs x = [0, 0] was [0, 0]. Fig. 7.7b shows the values of the h layer for all 4 inputs. Notice that hidden representations of the two input points x = [0, 1] and x = [1, 0] (the two cases with XOR output = 1) are merged to the single point h = [1, 0]. The merger makes it easy to linearly separate the positive and negative cases of XOR. In other words, we can view the hidden layer of the network as forming a representation of the input. ", "Bloom_type": "application", "question": "What is the purpose of examining the intermediate results of the hidden nodes?", "options": ["To visualize the merging of hidden nodes", "To understand the complexity of the model", "To improve the accuracy of the model", "To identify patterns in the data"], "complexity": 2}, {"id": 69, "context": "In this example we just stipulated the weights in Fig. 7.6. But for real examples the weights for neural networks are learned automatically using the error backpropagation algorithm to be introduced in Section 7.5. That means the hidden layers will learn to form useful representations. This intuition, that neural networks can automatically learn useful representations of the input, is one of their key advantages, and one that we will return to again and again in later chapters. ", "Bloom_type": "application", "question": "What does it mean when we say that neural networks can automatically learn useful representations?", "options": ["The network autonomously adjusts parameters during training to discover meaningful features.", "The network learns these representations through manual intervention.", "The network uses predefined rules to create these representations.", "The network relies on external data to determine these representations."], "complexity": 2}, {"id": 70, "context": "That means we can think of a neural network classifier with one hidden layer as building a vector h which is a hidden layer representation of the input, and then running standard multinomial logistic regression on the features that the network develops in h. By contrast, in Chapter 5 the features were mainly designed by hand via feature templates. So a neural network is like multinomial logistic regression, but (a) with many layers, since a deep neural network is like layer after layer of logistic regression classifiers; (b) with those intermediate layers having many possible activation functions (tanh, ReLU, sigmoid) instead of just sigmoid (although we`ll continue to use  for convenience to mean any activation function); (c) rather than forming the features by feature templates, the prior layers of the network induce the feature representations themselves. ", "Bloom_type": "application", "question": "What does the hidden layer represent in a neural network classifier?", "options": ["The intermediate feature representations", "The output layer", "The weights of the connections between neurons", "The biases of the neurons"], "complexity": 2}, {"id": 71, "context": "Fig. 7.10 shows a sketch of this architecture. As we mentioned earlier, adding this hidden layer to our logistic regression classifier allows the network to represent the non-linear interactions between features. This alone might give us a better sentiment classifier. ", "Bloom_type": "application", "question": "What is the purpose of adding a hidden layer to a logistic regression classifier?", "options": ["To improve the representation of non-linear interactions between features", "To increase the computational speed of the model", "To reduce the number of parameters needed for training", "To decrease the accuracy of the model"], "complexity": 2}, {"id": 72, "context": "Fig. 8.1 illustrates the structure of an RNN. As with ordinary feedforward networks, an input vector representing the current input, xt , is multiplied by a weight matrix and then passed through a non-linear activation function to compute the values for a layer of hidden units. This hidden layer is then used to calculate a corresponding output, yt . In a departure from our earlier window-based approach, sequences are processed by presenting one item at a time to the network. We`ll use subscripts to represent time, thus xt will mean the input vector x at time t. The key difference from a feedforward network lies in the recurrent link shown in the figure with the dashed line. This link augments the input to the computation at the hidden layer with the value of the hidden layer from the preceding point in time. ", "Bloom_type": "application", "question": "What is the main difference between processing sequences using an RNN compared to a feedforward network?", "options": ["RNNs incorporate feedback loops which allow them to maintain state across different time steps.", "RNNs require more memory than feedforward networks.", "Feedforward networks can only handle static inputs, while RNNs can process sequences.", "RNNs use a single input vector per time step, whereas feedforward networks use multiple vectors."], "complexity": 2}, {"id": 73, "context": "The hidden layer from the previous time step provides a form of memory, or context, that encodes earlier processing and informs the decisions to be made at later points in time. Critically, this approach does not impose a fixed-length limit on this prior context; the context embodied in the previous hidden layer can include information extending back to the beginning of the sequence. ", "Bloom_type": "application", "question": "What is the primary benefit of using a recurrent neural network with a hidden layer that allows for variable-length sequences?", "options": ["It enhances the model's capacity to handle long-term dependencies.", "It simplifies the training process.", "It increases computational efficiency.", "It reduces the need for data preprocessing."], "complexity": 2}, {"id": 74, "context": "To apply RNNs in this setting, we pass the text to be classified through the RNN a word at a time generating a new hidden layer representation at each time step. We can then take the hidden layer for the last token of the text, hn, to constitute a compressed representation of the entire sequence. We can pass this representation hn to a feedforward network that chooses a class via a softmax over the possible classes. Fig. 8.8 illustrates this approach. ", "Bloom_type": "application", "question": "What is the final step before passing the hidden layer representation to the feedforward network?", "options": ["Pass the hidden layer for the last token of the text", "Generate a new hidden layer representation", "Take the hidden layer for the first token of the text", "Combine all tokens into a single hidden layer"], "complexity": 2}, {"id": 75, "context": "Technically an autoregressive model is a model that predicts a value at time t based on a linear function of the previous values at times t 2, and so on. Although language models are not linear (since they have many layers of non-linearities), we loosely refer to this generation technique as autoregressive generation since the word generated at each time step is conditioned on the word selected by the network from the previous step. Fig. 8.9 illustrates this approach. In this figure, the details of the RNN`s hidden layers and recurrent connections are hidden within the blue block. This simple architecture underlies state-of-the-art approaches to applications such as machine translation, summarization, and question answering. The key to these approaches is to prime the generation component with an appropriate context. That is, instead of simply using <s> to get things started we can provide a richer task-appropriate context; for translation the context is the sentence in the source language; for summarization it`s the long text we want to summarize. ", "Bloom_type": "application", "question": "In the context of autoregressive generation, what does the hidden layer represent?", "options": ["The internal representation of the model that captures dependencies between inputs.", "The output of the neural network after processing all input data.", "The memory used to store intermediate results during computation.", "The sequence of characters being generated by the model."], "complexity": 2}, {"id": 76, "context": "To take advantage of context to the right of the current input, we can train an RNN on a reversed input sequence. With this approach, the hidden state at time t represents information about the sequence to the right of the current input: ", "Bloom_type": "application", "question": "What does the hidden state represent in the context of training an RNN?", "options": ["Information about the sequence to the left of the current input", "The next word in the sentence", "The entire previous sequence", "None of the above"], "complexity": 2}, {"id": 77, "context": "Bidirectional RNNs have also proven to be quite effective for sequence classification. Recall from Fig. 8.8 that for sequence classification we used the final hidden state of the RNN as the input to a subsequent feedforward classifier. A difficulty with this approach is that the final state naturally reflects more information about the end of the sentence than its beginning. Bidirectional RNNs provide a simple solution to this problem; as shown in Fig. 8.12, we simply combine the final hidden states from the forward and backward passes (for example by concatenation) and use that as input for follow-on processing. ", "Bloom_type": "application", "question": "What technique does bidirectional RNNs employ to address the issue of reflecting more information at the end of the sentence compared to the beginning?", "options": ["Combining the final hidden states from both directions", "Concatenating all hidden states", "Using only the initial hidden state", "Applying a different classifier"], "complexity": 2}, {"id": 78, "context": "It is the hidden state, ht , that provides the output for the LSTM at each time step. This output can be used as the input to subsequent layers in a stacked RNN, or at the final layer of a network ht can be used to provide the final output of the LSTM. ", "Bloom_type": "application", "question": "What does the hidden state (ht) represent in an LSTM?", "options": ["The intermediate state that flows through the LSTM during processing", "The initial input data fed into the LSTM", "The current output value produced by the LSTM", "The final output generated after all inputs are processed"], "complexity": 2}, {"id": 79, "context": "In RNN language modeling, at a particular time t, we pass the prefix of t 1 tokens through the language model, using forward inference to produce a sequence of hidden states, ending with the hidden state corresponding to the last word of the prefix. We then use the final hidden state of the prefix as our starting point to generate the next token. ", "Bloom_type": "application", "question": "What is the first step in generating the next token after passing the prefix through the language model?", "options": ["Initialize the model with the last hidden state of the prefix", "Pass the entire sentence through the model", "Use the initial hidden state for all subsequent steps", "Start with the first word of the prefix"], "complexity": 2}, {"id": 80, "context": "Fig. 8.17 shows an English source text (the green witch arrived), a sentence separator token (<s>, and a Spanish target text (llego la bruja verde). To translate a source text, we run it through the network performing forward inference to generate hidden states until we get to the end of the source. Then we begin autoregressive generation, asking for a word in the context of the hidden layer from the end of the source input as well as the end-of-sentence marker. Subsequent words are conditioned on the previous hidden state and the embedding for the last word generated. ", "Bloom_type": "application", "question": "What is the first step in translating an English source text?", "options": ["Run the source text through the network performing forward inference to generate hidden states.", "Begin autoregressive generation with a word in the context of the hidden layer.", "Ask for a word in the context of the hidden layer from the end of the source input.", "Use the embedding for the last word generated."], "complexity": 2}, {"id": 81, "context": "Let`s formalize and generalize this model a bit in Fig. 8.18. (To help keep things straight, we`ll use the superscripts e and d where needed to distinguish the hidden states of the encoder and the decoder.) The elements of the network on the left process the input sequence x and comprise the encoder. While our simplified figure shows only a single network layer for the encoder, stacked architectures are the norm, where the output states from the top layer of the stack are taken as the final representation, and the encoder consists of stacked biLSTMs where the hidden states from top layers from the forward and backward passes are concatenated to provide the contextualized representations for each time step. ", "Bloom_type": "application", "question": "What is the role of the hidden states in the encoder?", "options": ["To represent the current state of the system at each time step", "To store temporary data during processing", "To predict future inputs based on past outputs", "To filter out irrelevant features from the input"], "complexity": 2}, {"id": 82, "context": "The entire purpose of the encoder is to generate a contextualized representation of the input. This representation is embodied in the final hidden state of the encoder, he n. This representation, also called c for context, is then passed to the decoder. ", "Bloom_type": "application", "question": "What does the hidden state represent in the encoder?", "options": ["The final state of the encoder", "The output of the encoder", "The initial state of the encoder", "The intermediate states of the encoder"], "complexity": 2}, {"id": 83, "context": "As Fig. 8.18 shows, we do something more complex: we make the context vector c available to more than just the first decoder hidden state, to ensure that the influence of the context vector, c, doesn`t wane as the output sequence is generated. We do this by adding c as a parameter to the computation of the current hidden state. using the following equation: ", "Bloom_type": "application", "question": "What technique is used to maintain the influence of the context vector throughout the generation of an output sequence?", "options": ["Updating the hidden state with the context vector", "Adding c as a parameter to the previous hidden state", "Applying {method1} to {object1}", "Using {method2} on the context vector"], "complexity": 2}, {"id": 84, "context": "The attention mechanism is a solution to the bottleneck problem, a way of allowing the decoder to get information from all the hidden states of the encoder, not just the last hidden state. ", "Bloom_type": "application", "question": "What does the attention mechanism allow for in the decoder?", "options": ["To access any hidden state at any time", "To focus on only the most recent hidden state", "To ignore all previous hidden states", "To rely solely on the first hidden state"], "complexity": 2}, {"id": 85, "context": "In the attention mechanism, as in the vanilla encoder-decoder model, the context vector c is a single vector that is a function of the hidden states of the encoder. But instead of being taken from the last hidden state, it`s a weighted average of all the hidden states of the decoder. And this weighted average is also informed by part of the decoder state as well, the state of the decoder right before the current token i. That is, c = f (he n, hd 1). The weights focus on (attend to`) a particular part of i the source text that is relevant for the token i that the decoder is currently producing. Attention thus replaces the static context vector with one that is dynamically derived from the encoder hidden states, but also informed by and hence different for each token in decoding. ", "Bloom_type": "application", "question": "In an attention-based model, how does the context vector c differ from the traditional approach?", "options": ["It is updated based on the relevance of each token in the input sequence.", "It remains unchanged throughout the entire sequence.", "It is only influenced by the final hidden state of the encoder.", "It is calculated using a fixed set of weights applied uniformly across all tokens."], "complexity": 2}, {"id": 86, "context": "The score that results from this dot product is a scalar that reflects the degree of similarity between the two vectors. The vector of these scores across all the encoder hidden states gives us the relevance of each encoder state to the current step of the decoder. ", "Bloom_type": "application", "question": "What does the scalar value obtained from the dot product represent?", "options": ["The degree of similarity between the two vectors", "The magnitude of the vectors", "The angle between the vectors", "The sum of the components of the vectors"], "complexity": 2}, {"id": 87, "context": "It`s also possible to create more sophisticated scoring functions for attention models. Instead of simple dot product attention, we can get a more powerful function that computes the relevance of each encoder hidden state to the decoder hidden state by parameterizing the score with its own set of weights, Ws. 1, he score(hd i ", "Bloom_type": "application", "question": "What is the purpose of using a different set of weights (Ws) in computing the relevance of an encoder hidden state?", "options": ["To increase the accuracy of the model", "To reduce computational complexity", "To make the model more efficient", "To enhance the interpretability of the model"], "complexity": 2}, {"id": 88, "context": " In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t ", "Bloom_type": "application", "question": "What is the role of the hidden layer in a Simple RNN?", "options": ["It combines past states with current input.", "It stores all previous inputs.", "It processes the sequence elements sequentially.", "It generates new outputs for every input."], "complexity": 2}, {"id": 89, "context": "Layer Norm At two stages in the transformer block we normalize the vector (Ba et al., 2016). This process, called layer norm (short for layer normalization), is one of many forms of normalization that can be used to improve training performance in deep neural networks by keeping the values of a hidden layer in a range that facilitates gradient-based training. ", "Bloom_type": "application", "question": "What does layer norm specifically refer to in the context of transformers?", "options": ["It refers to standardizing the activations within each layer using a mean and variance.", "It refers to normalizing the input data before feeding it into the model.", "It refers to adjusting the weights during backpropagation to reduce errors.", "It refers to applying a linear transformation followed by an activation function."], "complexity": 2}, {"id": 90, "context": "Note the key difference between this figure and the earlier RNN-based version shown in Fig. 8.6. There the calculation of the outputs and the losses at each step was inherently serial given the recurrence in the calculation of the hidden states. With transformers, each training item can be processed in parallel since the output for each element in the sequence is computed separately. ", "Bloom_type": "application", "question": "What change does the introduction of transformers bring to the processing of input sequences?", "options": ["It allows processing items in parallel.", "It makes the computation of hidden states sequential.", "It increases the complexity of calculations.", "It requires more memory."], "complexity": 2}, {"id": 91, "context": "the Empress Maria Theresa the famous Mechanical Turk, a chess-playing automaton consisting of a wooden box filled with gears, behind which sat a robot mannequin who played chess by moving pieces with his mechanical arm. The Turk toured Europe and the Americas for decades, defeating Napoleon Bonaparte and even playing Charles Babbage. The Mechanical Turk might have been one of the early successes of artificial intelligence were it not for the fact that it was, alas, a hoax, powered by a human chess player hidden inside the box. ", "Bloom_type": "application", "question": "What was likely true about the Mechanical Turk?", "options": ["The human player was hidden inside the box.", "It was an actual machine capable of playing chess.", "The human player was visible outside the box.", "The Mechanical Turk was invented by Charles Babbage."], "complexity": 2}, {"id": 92, "context": "a hidden state henc given the input x1...xt . The language model predictor takes as input the previous output token (not counting blanks), outputting a hidden state hpred . The two are passed through another network whose output is then passed through a softmax to predict the next character. ", "Bloom_type": "application", "question": "What should be done with the hidden state after processing it?", "options": ["Use it immediately for prediction", "Store it for future reference", "Combine it with the current input for further processing", "Forget about it completely"], "complexity": 2}, {"id": 93, "context": "The third innovation of this period was the rise of the HMM. Hidden Markov models seem to have been applied to speech independently at two laboratories around 1972. One application arose from the work of statisticians, in particular Baum and colleagues at the Institute for Defense Analyses in Princeton who applied HMMs to various prediction problems (Baum and Petrie 1966, Baum and Eagon 1967). James Baker learned of this work and applied the algorithm to speech processing (Baker, 1975a) during his graduate work at CMU. Independently, Frederick Jelinek and collaborators (drawing from their research in information-theoretical models influenced by the work of Shannon (1948)) applied HMMs to speech at the IBM Thomas J. Watson Research Center (Jelinek et al., 1975). One early difference was the decoding algorithm; Baker`s DRAGON system used Viterbi (dynamic programming) decoding, while the IBM system applied Jelinek`s stack decoding algorithm (Jelinek, 1969). Baker then joined the IBM group for a brief time before founding the speech-recognition company Dragon Systems. ", "Bloom_type": "application", "question": "Which method did James Baker use for speech processing?", "options": ["Viterbi decoding", "Stack decoding", "Dynamic Programming", "None of the above"], "complexity": 2}, {"id": 94, "context": "In this section we introduce our first sequence labeling algorithm, the Hidden Markov Model, and show how to apply it to part-of-speech tagging. Recall that a sequence labeler is a model whose job is to assign a label to each unit in a sequence, thus mapping a sequence of observations to a sequence of labels of the same length. The HMM is a classic model that introduces many of the key concepts of sequence modeling that we will see again in more modern models. ", "Bloom_type": "application", "question": "What does the Hidden Markov Model (HMM) primarily focus on when applied to sequence labeling?", "options": ["Mapping sequences to labels", "Assigning probabilities to sequences", "Processing input data directly", "Identifying patterns within sequences"], "complexity": 2}, {"id": 95, "context": "A Markov chain is useful when we need to compute a probability for a sequence of observable events. In many cases, however, the events we are interested in are hidden: we don`t observe them directly. For example we don`t normally observe part-of-speech tags in a text. Rather, we see words, and must infer the tags from the word sequence. We call the tags hidden because they are not observed. ", "Bloom_type": "application", "question": "What is the main challenge in using a Markov chain when dealing with sequences of events that are not directly observable?", "options": ["Deciding which events should be considered as hidden.", "Computing the transition probabilities between observable events only.", "Determining the initial state distribution based on historical data.", "Identifying the most likely path through the sequence."], "complexity": 2}, {"id": 96, "context": "For any model, such as an HMM, that contains hidden variables, the task of determining the hidden variables sequence corresponding to the sequence of observations is called decoding. More formally, ", "Bloom_type": "application", "question": "What is the name for the process of finding the sequence of hidden variables based on observation sequences?", "options": ["Decoding", "Training", "Inference", "Optimization"], "complexity": 2}, {"id": 97, "context": "Figure 17.14 shows a fleshed-out version of the sketch we saw in Fig. 17.11, the Viterbi lattice for computing the best hidden state sequence for the observation sequence Janet will back the bill. ", "Bloom_type": "application", "question": "What is the purpose of adding more details to the Viterbi lattice shown in Fig. 17.14?", "options": ["To make it easier to understand the computation process", "To increase the complexity of the algorithm", "To add unnecessary elements for decoration", "To reduce the number of computations needed"], "complexity": 2}, {"id": 98, "context": "A simple but very strong algorithm for Task 4 is to represent each of the two spans by BERT embeddings and take the last layer hidden state corresponding to the position of the [CLS] token, pass this through a single layer tanh feedforward network and then a softmax for sense classification (Nie et al., 2019). ", "Bloom_type": "application", "question": "What is the final step before classifying the sense using softmax?", "options": ["Pass the input through a single layer tanh feedforward network", "Take the last layer hidden state corresponding to the position of the [CLS] token", "Represent each span by BERT embeddings", "Apply {method1} and then use its result in {method2}"], "complexity": 2}]}, "translation": {"max_id": 175, "Questions": [{"id": 0, "context": "Alignment Knowing the minimum edit distance is useful for algorithms like finding potential spelling error corrections. But the edit distance algorithm is important in another way; with a small change, it can also provide the minimum cost alignment between two strings. Aligning two strings is useful throughout speech and language processing. In speech recognition, minimum edit distance alignment is used to compute the word error rate (Chapter 16). Alignment plays a role in machine translation, in which sentences in a parallel corpus (a corpus with a text in two languages) need to be matched to each other. ", "Bloom_type": "remember", "question": "In what application of string alignment does the minimum edit distance play a crucial role?", "options": ["Speech recognition", "Machine learning", "Text classification", "Image segmentation"], "complexity": 0}, {"id": 1, "context": "The best way to evaluate the performance of a language model is to embed it in an application and measure how much the application improves. Such end-to-end evaluation is called extrinsic evaluation. Extrinsic evaluation is the only way to know if a particular improvement in the language model (or any component) is really going to help the task at hand. Thus for evaluating n-gram language models that are a component of some task like speech recognition or machine translation, we can compare the performance of two candidate language models by running the speech recognizer or machine translator twice, once with each language model, and seeing which gives the more accurate transcription. ", "Bloom_type": "remember", "question": "In what type of evaluation do you directly observe the impact on a specific task rather than just measuring improvements within the model itself?", "options": ["Extrinsic Evaluation", "Intrinsic Evaluation", "Internal Evaluation", "External Evaluation"], "complexity": 0}, {"id": 2, "context": "Language ID systems are trained on multilingual text, such as Wikipedia (Wikipedia text in 68 different languages was used in (Lui and Baldwin, 2011)), or newswire. To make sure that this multilingual text correctly reflects different regions, dialects, and socioeconomic classes, systems also add Twitter text in many languages geotagged to many regions (important for getting world English dialects from countries with large Anglophone populations like Nigeria or India), Bible and Quran translations, slang websites like Urban Dictionary, corpora of African American Vernacular English (Blodgett et al., 2016), and so on (Jurgens et al., 2017). ", "Bloom_type": "remember", "question": "In language identification systems, what is added to ensure that multilingual text correctly reflects various linguistic features?", "options": ["Geotagged tweets", "Multilingual text", "Slang websites", "Corpora of AAVE"], "complexity": 0}, {"id": 3, "context": "The bootstrap test (Efron and Tibshirani, 1993) can apply to any metric; from precision, recall, or F1 to the BLEU metric used in machine translation. The word bootstrapping refers to repeatedly drawing large numbers of samples with replacement (called bootstrap samples) from an original set. The intuition of the bootstrap test is that we can create many virtual test sets from an observed test set by repeatedly sampling from it. The method only makes the assumption that the sample is representative of the population. ", "Bloom_type": "remember", "question": "In the context of machine translation, what does the term \"bootstrapping\" refer to?", "options": ["Sampling data points randomly from the dataset", "Creating a new language model", "Using historical translations as reference", "Analyzing sentence structures manually"], "complexity": 0}, {"id": 4, "context": "Semantic Frames and Roles Closely related to semantic fields is the idea of a semantic frame. A semantic frame is a set of words that denote perspectives or participants in a particular type of event. A commercial transaction, for example, is a kind of event in which one entity trades money to another entity in return for some good or service, after which the good changes hands or perhaps the service is performed. This event can be encoded lexically by using verbs like buy (the event from the perspective of the buyer), sell (from the perspective of the seller), pay (focusing on the monetary aspect), or nouns like buyer. Frames have semantic roles (like buyer, seller, goods, money), and words in a sentence can take on these roles. Knowing that buy and sell have this relation makes it possible for a system to know that a sentence like Sam bought the book from Ling could be paraphrased as Ling sold the book to Sam, and that Sam has the role of the buyer in the frame and Ling the seller. Being able to recognize such paraphrases is important for question answering, and can help in shifting perspective for machine translation. ", "Bloom_type": "remember", "question": "In what way does understanding semantic frames aid in machine translation?", "options": ["It enables machines to shift perspective during translation.", "It helps machines understand the grammatical structure of sentences.", "It aids in recognizing synonyms within a sentence.", "It assists in translating between different languages directly."], "complexity": 0}, {"id": 5, "context": "RNN-based language models can also be used to generate text. Text generation is of enormous practical importance, part of tasks like question answering, machine translation, text summarization, grammar correction, story generation, and conversational dialogue; any task where a system needs to produce text, conditioned on some other text. This use of a language model to generate text is one of the areas in which the impact of neural language models on NLP has been the largest. Text generation, along with image generation and code generation, constitute a new area of AI that is often called generative AI. ", "Bloom_type": "remember", "question": "In what way does the use of RNN-based language models for text generation contribute significantly to Natural Language Processing (NLP)?", "options": ["It leads to significant improvements in various NLP applications such as question answering, machine translation, and text summarization.", "It enhances the accuracy of sentiment analysis.", "It improves the efficiency of data preprocessing.", "It increases the speed of training deep learning models."], "complexity": 0}, {"id": 6, "context": "Technically an autoregressive model is a model that predicts a value at time t based on a linear function of the previous values at times t 2, and so on. Although language models are not linear (since they have many layers of non-linearities), we loosely refer to this generation technique as autoregressive generation since the word generated at each time step is conditioned on the word selected by the network from the previous step. Fig. 8.9 illustrates this approach. In this figure, the details of the RNN`s hidden layers and recurrent connections are hidden within the blue block. This simple architecture underlies state-of-the-art approaches to applications such as machine translation, summarization, and question answering. The key to these approaches is to prime the generation component with an appropriate context. That is, instead of simply using <s> to get things started we can provide a richer task-appropriate context; for translation the context is the sentence in the source language; for summarization it`s the long text we want to summarize. ", "Bloom_type": "remember", "question": "In what way does the autoregressive model differ from traditional linear models?", "options": ["It uses fewer parameters.", "It requires more data input.", "It generates output faster.", "It has no feedback mechanism."], "complexity": 0}, {"id": 7, "context": "By contrast, encoder-decoder models are used especially for tasks like machine translation, where the input sequence and output sequence can have different lengths ", "Bloom_type": "remember", "question": "In what type of task are encoder-decoder models particularly useful?", "options": ["Machine translation", "Image recognition", "Speech synthesis", "Natural language generation"], "complexity": 0}, {"id": 8, "context": "Encoder-decoder networks, sometimes called sequence-to-sequence networks, are models capable of generating contextually appropriate, arbitrary length, output sequences given an input sequence. Encoder-decoder networks have been applied to a very wide range of applications including summarization, question answering, and dialogue, but they are particularly popular for machine translation. ", "Bloom_type": "remember", "question": "In which application has encoder-decoder networks shown particular popularity?", "options": ["Machine translation", "Summarization", "Image classification", "Speech recognition"], "complexity": 0}, {"id": 9, "context": "We only have to make one slight change to turn this language model with autoregressive generation into an encoder-decoder model that is a translation model that can translate from a source text in one language to a target text in a second: add a sentence separation marker at the end of the source text, and then simply concatenate the target text. ", "Bloom_type": "remember", "question": "In what way does adding a sentence separation marker affect the process of turning a language model?", "options": ["It simplifies the encoding step.", "It increases the complexity of the model.", "It enhances the accuracy of the translation.", "It decreases the efficiency of the model."], "complexity": 0}, {"id": 10, "context": "Transformer-based language models are complex, and so the details will unfold over the next 5 chapters. In the next sections we`ll introduce multi-head attention, the rest of the transformer block, and the input encoding and language modeling head components. Chapter 10 discusses how language models are pretrained, and how tokens are generated via sampling. Chapter 11 introduces masked language modeling and the BERT family of bidirectional transformer encoder models. Chapter 12 shows how to prompt LLMs to perform NLP tasks by giving instructions and demonstrations, and how to align the model with human preferences. Chapter 13 will introduce machine translation with the encoder-decoder architecture. ", "Bloom_type": "remember", "question": "In which chapter does the concept of multi-head attention first appear?", "options": ["Chapter 3", "Chapter 1", "Chapter 2", "Chapter 4"], "complexity": 0}, {"id": 11, "context": "able performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots. ", "Bloom_type": "remember", "question": "Why are translations particularly important for certain types of natural language processing tasks?", "options": ["For enhancing user interaction with AI systems", "To improve computational efficiency", "To increase the complexity of language models", "To reduce the reliance on human translators"], "complexity": 0}, {"id": 12, "context": "Large language models are mainly trained on text scraped from the web, augmented by more carefully curated data. Because these training corpora are so large, they are likely to contain many natural examples that can be helpful for NLP tasks, such as question and answer pairs (for example from FAQ lists), translations of sentences between various languages, documents together with their summaries, and so on. ", "Bloom_type": "remember", "question": "What type of data is primarily used to train large language models?", "options": ["Web content", "Audio recordings", "Visual images", "Text files"], "complexity": 0}, {"id": 13, "context": "Large pretrained neural language models exhibit many of the potential harms discussed in Chapter 4 and Chapter 6. Many of these harms become realized when pretrained language models are used for any downstream task, particularly those involving text generation, whether question answering, machine translation, or in assistive technologies like writing aids or web search query completion, or predictive typing for email (Olteanu et al., 2020). ", "Bloom_type": "remember", "question": "In what type of tasks do large pretrained neural language models often face significant risks due to their potential harms?", "options": ["Text generation", "Medical diagnosis", "Financial forecasting", "Quantum computing"], "complexity": 0}, {"id": 14, "context": " Many NLP taskssuch as question answering, summarization, sentiment, and machine translationcan be cast as tasks of word prediction and hence addressed with Large language models. ", "Bloom_type": "remember", "question": "In what field can many natural language processing (NLP) tasks such as question answering, summarization, sentiment analysis, and machine translation be considered?", "options": ["Computer science", "Mathematics", "Physics", "Chemistry"], "complexity": 0}, {"id": 15, "context": "N-gram language models were very widely used over the next 30 years and more, across a wide variety of NLP tasks like speech recognition and machine translations, often as one of multiple components of the model. The contexts for these n-gram models grew longer, with 5-gram models used quite commonly by very efficient LM toolkits (Stolcke, 2002; Heafield, 2011). ", "Bloom_type": "remember", "question": "What type of language models were widely used over the next 30 years?", "options": ["N-gram models", "Sequence-to-sequence models", "Transformer-based models", "RNN-based models"], "complexity": 0}, {"id": 16, "context": "If we want to solve general tasks like summarization or translation, we don`t want to have to create a new prompt each time we do the task. Instead the first step in prompting is to design one or more templates: task-specific prompting text along with slots for the particular input that is being processed. ", "Bloom_type": "remember", "question": "In the process of solving general tasks such as summarization or translation, why is it beneficial not to create a new prompt every time?", "options": ["To increase efficiency by reusing prompts", "To reduce computational resources", "To ensure privacy of user data", "To comply with legal regulations"], "complexity": 0}, {"id": 17, "context": "But more generally, the best way to select demonstrations from the training set is programmatically: choosing the set of demonstrations that most increases task performance of the prompt on a test set. Task performance for sentiment analysis or multiple-choice question answering can be measured in accuracy; for machine translation with chrF, and for summarization via Rouge. Systems like DSPy (Khattab et al., 2024), a framework for algorithmically optimizing LM prompts, can automatically find the optimum set of demonstrations to include by searching through the space of possible demonstrations to include. We`ll return to automatic prompt optimization in Section 12.5. ", "Bloom_type": "remember", "question": "In what type of tasks can systems like DSPy optimize the selection of demonstrations?", "options": ["Machine translation with chrF", "Sentiment analysis", "Multiple-choice question answering", "Summarization via Rouge"], "complexity": 0}, {"id": 18, "context": "Instruction tuning (short for instruction finetuning, and sometimes even shortened to instruct tuning) is a method for making an LLM better at following instructions. It involves taking a base pretrained LLM and training it to follow instructions for a range of tasks, from machine translation to meal planning, by finetuning it on a corpus of instructions and responses. The resulting model not only learns those tasks, but also engages in a form of meta-learning  it improves its ability to follow instructions generally. ", "Bloom_type": "remember", "question": "What does instruction tuning involve?", "options": ["It involves taking a base pretrained LLM and training it to follow instructions for a range of tasks.", "It involves taking a base pretrained LLM and training it to follow instructions for a range of languages.", "It involves taking a base pretrained LLM and training it to follow instructions for a range of subjects.", "It involves taking a base pretrained LLM and training it to follow instructions for a range of emotions."], "complexity": 0}, {"id": 19, "context": "Many huge instruction tuning datasets have been created, covering many tasks and languages. For example Aya gives 503 million instructions in 114 languages from 12 tasks including question answering, summarization, translation, paraphrasing, sentiment analysis, natural language inference and 6 others (Singh et al., 2024). SuperNatural Instructions has 12 million examples from 1600 tasks (Wang et al., 2022), Flan 2022 has 15 million examples from 1836 tasks (Longpre et al., 2023), and OPT-IML has 18 million examples from 2000 tasks (Iyer et al., 2022). ", "Bloom_type": "remember", "question": "What type of data is being discussed in this context?", "options": ["Text-based instructions", "Audio files", "Video clips", "Image databases"], "complexity": 0}, {"id": 20, "context": "Developing high quality supervised training data in this way is time consuming and costly. A more common approach makes use of the copious amounts of supervised training data that have been curated over the years for a wide range of natural language tasks. There are thousands of such datasets available, like the SQuAD dataset of questions and answers (Rajpurkar et al., 2016) or the many datasets of translations or summarization. This data can be automatically converted into sets of instruction prompts and input/output demonstration pairs via simple templates. ", "Bloom_type": "remember", "question": "What method does the response suggest for converting large amounts of existing training data into usable formats?", "options": ["Template-based conversion", "Manual transcription", "Machine learning algorithms", "Data anonymization"], "complexity": 0}, {"id": 21, "context": "Given access to labeled training data, candidate prompts can be scored based on execution accuracy (Honovich et al., 2023). In this approach, candidate prompts are combined with inputs sampled from the training data and passed to an LLM for decoding. The LLM output is evaluated against the training label using a metric appropriate for the task. In the case of classification-based tasks, this is effectively a 0/1 loss  how many examples were correctly labeled with the given prompt. Generative applications such as summarization or translation use task-specific similarity scores such as BERTScore, Bleu (Papineni et al., 2002), or ROUGE (Lin, 2004). ", "Bloom_type": "remember", "question": "In which type of application do generative models like summarization or translation use specific scoring metrics?", "options": ["Classification", "Regression", "Clustering", "Dimensionality reduction"], "complexity": 0}, {"id": 22, "context": "Language models are evaluated in many ways. we introduced some evaluations for in Section 10.4, including measuring the language model`s perplexity on a test set, evaluating its accuracy on various NLP tasks, as well as benchmarks that help measure efficiency, toxicity, fairness, and so on. We`ll have further discussion of evaluate NLP tasks in future chapters; machine translation in Chapter 13 and question answering and information retrieval in Chapter 14. ", "Bloom_type": "remember", "question": "In what chapter will they discuss how to evaluate NLP tasks?", "options": ["Chapter 13", "Chapter 12", "Chapter 14", "Chapter 15"], "complexity": 0}, {"id": 23, "context": "In this second part of the book we introduce fundamental NLP applications: machine translation, information retrieval, question answering, dialogue systems, and speech recognition. ", "Bloom_type": "remember", "question": "In what field are machine translation, information retrieval, question answering, dialogue systems, and speech recognition primarily applied?", "options": ["Artificial Intelligence", "Computer Science", "Mathematics", "Physics"], "complexity": 0}, {"id": 24, "context": "This chapter introduces machine translation (MT), the use of computers to translate from one language to another. ", "Bloom_type": "remember", "question": "What is the main focus of this chapter?", "options": ["The use of machines for translating languages", "The history of language", "The study of linguistics", "The development of artificial intelligence"], "complexity": 0}, {"id": 25, "context": "Machine translation in its present form therefore focuses on a number of very practical tasks. Perhaps the most common current use of machine translation is for information access. We might want to translate some instructions on the web, perhaps the recipe for a favorite dish, or the steps for putting together some furniture. Or we might want to read an article in a newspaper, or get information from an online resource like Wikipedia or a government webpage in some other language. MT for information access is probably one of the most common uses of NLP technology, and Google Translate alone (shown above) translates hundreds of billions of words a day between over 100 languages. Improvements in machine translation can thus help reduce what is often called the digital divide in information access: the fact that much more information is available in English and other languages spoken in wealthy countries. Web searches in English return much more information than searches in other languages, and online resources like Wikipedia are much larger in English and other higher-resourced languages. High-quality translation can help provide information to speakers of lower-resourced languages. ", "Bloom_type": "remember", "question": "In what way does high-quality machine translation contribute to reducing the digital divide?", "options": ["It allows people to search for information in their native language directly.", "It increases the speed of internet connections.", "It reduces the cost of accessing online resources.", "It enhances the quality of translations by improving accuracy."], "complexity": 0}, {"id": 26, "context": "Another common use of machine translation is to aid human translators. MT systems are routinely used to produce a draft translation that is fixed up in a post-editing phase by a human translator. This task is often called computer-aided translation or CAT. CAT is commonly used as part of localization: the task of adapting content or a product to a particular language community. ", "Bloom_type": "remember", "question": "What does CAT stand for?", "options": ["Computer-Assisted Translation", "Content-Aided Translation", "Creative-Assisted Translation", "Comprehensive-Assigned Translation"], "complexity": 0}, {"id": 27, "context": "Fig. 13.2 shows examples of other word order differences. All of these word order differences between languages can cause problems for translation, requiring the system to do huge structural reorderings as it generates the output. ", "Bloom_type": "remember", "question": "What causes problems for translation?", "options": ["The variation in sentence structure across languages", "The use of different alphabets", "The presence of idiomatic expressions", "The difference in vocabulary size"], "complexity": 0}, {"id": 28, "context": "Of course we also need to translate the individual words from one language to another. For any translation, the appropriate word can vary depending on the context. The English source-language word bass, for example, can appear in Spanish as the fish lubina or the musical instrument bajo. German uses two distinct words for what in English would be called a wall: Wand for walls inside a building, and Mauer for walls outside a building. Where English uses the word brother for any male sibling, Chinese and many other languages have distinct words for older brother and younger brother (Mandarin gege and didi, respectively). In all these cases, translating bass, wall, or brother from English would require a kind of specialization, disambiguating the different uses of a word. For this reason the fields of MT and Word Sense Disambiguation (Appendix G) are closely linked. ", "Bloom_type": "remember", "question": "In which field do MT and Word Sense Disambiguation (WSD) share close ties?", "options": ["Translation", "Machine Learning", "Natural Language Processing", "Computer Science"], "complexity": 0}, {"id": 29, "context": "Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. Large numbers of parallel corpora are available. Some are governmental; the Europarl corpus (Koehn, 2005), extracted from the proceedings of the European Parliament, contains between 400,000 and 2 million sentences each from 21 European languages. The United Nations Parallel Corpus contains on the order of 10 million sentences in the six official languages of the United Nations (Arabic, Chinese, English, French, Russian, Spanish) Ziemski et al. (2016). Other parallel corpora have been made from movie and TV subtitles, like the OpenSubtitles corpus (Lison and Tiedemann, 2016), or from general web text, like the ParaCrawl corpus of 223 million sentence pairs between 23 EU languages and English extracted from the CommonCrawl Banon et al. (2020). ", "Bloom_type": "remember", "question": "Which type of parallel corpus is often used for training machine translation models?", "options": ["Parallel corpora extracted from the European Parliament", "Governmental texts", "General web text", "Movie and TV subtitles"], "complexity": 0}, {"id": 30, "context": " a cost function that takes a span of source sentences and a span of target sentences and returns a score measuring how likely these spans are to be translations. ", "Bloom_type": "remember", "question": "In machine translation, what does a cost function evaluate?", "options": ["The similarity between the source and target sentences", "The length of the translated text", "The number of errors made during translation", "The time taken for translation"], "complexity": 0}, {"id": 31, "context": "Thus at the first step of decoding, we compute a softmax over the entire vocabulary, assigning a probability to each word. We then select the k-best options from this softmax output. These initial k outputs are the search frontier and these k initial words are called hypotheses. A hypothesis is an output sequence, a translation-sofar, together with its probability. ", "Bloom_type": "remember", "question": "In the process of decoding, what is selected based on the softmax output?", "options": ["The top k words", "The top k probabilities", "The top k sentences", "The top k translations"], "complexity": 0}, {"id": 32, "context": "For MT we generally use beam widths k between 5 and 10, giving us k hypotheses at the end. We can pass all k to the downstream application with their respective scores, or if we just need a single translation we can pass the most probable hypothesis. ", "Bloom_type": "remember", "question": "In machine translation (MT), what is typically used for beam width \\(k\\) when generating translations?", "options": ["Beam width \\(k\\) should be set high to cover more possible translations.", "Beam width \\(k\\) is irrelevant as it does not affect the quality of the translation.", "Beam width \\(k\\) should be set very low to ensure accuracy.", "Beam width \\(k\\) should be set based on the complexity of the language pair."], "complexity": 0}, {"id": 33, "context": "The intuition of minimum Bayes risk is that instead of trying to choose the translation which is most probable, we choose the one that is likely to have the least error. For example, we might want our decoding algorithm to find the translation which has the highest score on some evaluation metric. For example in Section 13.6 we will introduce metrics like chrF or BERTScore that measure the goodness-of-fit between a candidate translation and a set of reference human translations. A translation that maximizes this score, especially with a hypothetically huge set of perfect human translations is likely to be a good one (have minimum risk) even if it is not the most probable translation by our particular probability estimator. ", "Bloom_type": "remember", "question": "In the context of choosing a translation for optimal performance, what does the concept of minimum Bayes risk suggest?", "options": ["Selecting the translation based on its likelihood to have minimal error", "Choosing the translation with the highest probability", "Using the most probable translation as the best option", "Deciding on the translation with the lowest possible error"], "complexity": 0}, {"id": 34, "context": "Minimum Bayes risk decoding can also be used for other NLP tasks; indeed it was widely applied to speech recognition (Stolcke et al., 1997; Goel and Byrne, 2000) before being applied to machine translation (Kumar and Byrne, 2004), and has been shown to work well across many other generation tasks as well (e.g., summarization, dialogue, and image captioning (Suzgun et al., 2023a)). ", "Bloom_type": "remember", "question": "In which application of minimum Bayes risk decoding did it first appear?", "options": ["Speech recognition", "Machine translation", "Summarization", "Image captioning"], "complexity": 0}, {"id": 35, "context": "For some languages, and especially for English, online resources are widely available. There are many large parallel corpora that contain translations between English and many languages. But the vast majority of the world`s languages do not have large parallel training texts available. An important ongoing research question is how to get good translation with lesser resourced languages. The resource problem can even be true for high resource languages when we need to translate into low resource domains (for example in a particular genre that happens to have very little bitext). ", "Bloom_type": "remember", "question": "Which statement best describes the main challenge faced by researchers in improving translation quality?", "options": ["Translation quality cannot be improved due to insufficient parallel training texts.", "Researchers struggle with finding sufficient parallel texts for all language pairs.", "The primary issue is translating into low-resource domains rather than having enough parallel texts.", "Parallel corpora are only useful for high-resource languages."], "complexity": 0}, {"id": 36, "context": "The most common data augmentation technique for machine translation is called backtranslation. Backtranslation relies on the intuition that while parallel corpora may be limited for particular languages or domains, we can often find a large (or at least larger) monolingual corpus, to add to the smaller parallel corpora that are available. The algorithm makes use of monolingual corpora in the target language by creating synthetic bitexts. ", "Bloom_type": "remember", "question": "What does backtranslation rely on?", "options": ["Both A and B", "The availability of parallel corpora only", "Intuition about monolingual corpora being more extensive than parallel corpora", "Neither A nor B"], "complexity": 0}, {"id": 37, "context": "In backtranslation, our goal is to improve source-to-target MT, given a small parallel text (a bitext) in the source/target languages, and some monolingual data in the target language. We first use the bitext to train a MT system in the reverse direction: a target-to-source MT system . We then use it to translate the monolingual target data to the source language. Now we can add this synthetic bitext (natural target sentences, aligned with MT-produced source sentences) to our training data, and retrain our source-to-target MT model. For example suppose we want to translate from Navajo to English but only have a small Navajo-English bitext, although of course we can find lots of monolingual English data. We use the small bitext to build an MT engine going the other way (from English to Navajo). Once we translate the monolingual English text to Navajo, we can add this synthetic Navajo/English bitext to our training data. ", "Bloom_type": "remember", "question": "In the process of backtranslation, what is the initial step taken using the bitext?", "options": ["Training a translation model", "Aligning source and target sentences", "Generating synthetic bitexts", "Translating monolingual data"], "complexity": 0}, {"id": 38, "context": "Backtranslation has various parameters. One is how we generate the backtranslated data; we can run the decoder in greedy inference, or use beam search. Or we can do sampling, like the temperature sampling algorithm we saw in Chapter 9. Another parameter is the ratio of backtranslated data to natural bitext data; we can choose to upsample the bitext data (include multiple copies of each sentence). In general backtranslation works surprisingly well; one estimate suggests that a system trained on backtranslated text gets about 2/3 of the gain as would training on the same amount of natural bitext (Edunov et al., 2018). ", "Bloom_type": "remember", "question": "In backtranslation, what does the ratio of backtranslated data to natural bitext data determine?", "options": ["The quality of the generated backtranslated data", "The type of decoding method used", "The number of sentences included in the dataset", "The speed at which the translation process runs"], "complexity": 0}, {"id": 39, "context": "One advantage of a multilingual model is that they can improve the translation of lower-resourced languages by drawing on information from a similar language in the training data that happens to have more resources. Perhaps we don`t know the meaning of a word in Galician, but the word appears in the similar and higherresourced language Spanish. ", "Bloom_type": "remember", "question": "Why does a multilingual model benefit from using information from a similar language with more resources?", "options": ["To enhance the accuracy of translations for lower-resourced languages", "To increase the complexity of the model", "To reduce the computational cost of processing", "To decrease the diversity of the dataset"], "complexity": 0}, {"id": 40, "context": "The most accurate evaluations use human raters, such as online crowdworkers, to evaluate each translation along the two dimensions. For example, along the dimension of fluency, we can ask how intelligible, how clear, how readable, or how natural the MT output (the target text) is. We can give the raters a scale, for example, from 1 (totally unintelligible) to 5 (totally intelligible), or 1 to 100, and ask them to rate each sentence or paragraph of the MT output. ", "Bloom_type": "remember", "question": "What are the two main dimensions used for evaluating translations?", "options": ["Fluency and clarity", "Accuracy and consistency", "Naturalness and readability", "Intelligibility and comprehensibility"], "complexity": 0}, {"id": 41, "context": "We can do the same thing to judge the second dimension, adequacy, using raters to assign scores on a scale. If we have bilingual raters, we can give them the source sentence and a proposed target sentence, and rate, on a 5-point or 100-point scale, how much of the information in the source was preserved in the target. If we only have monolingual raters but we have a good human translation of the source text, we can give the monolingual raters the human reference translation and a target machine translation and again rate how much information is preserved. An alternative is to do ranking: give the raters a pair of candidate translations, and ask them which one they prefer. ", "Bloom_type": "remember", "question": "How can we measure the adequacy of a translation?", "options": ["Both A) and C)", "By assigning scores based on the similarity between the source and target sentences.", "By asking raters to compare the quality of different languages.", "By giving raters pairs of candidate translations and asking for preference."], "complexity": 0}, {"id": 42, "context": "As discussed above, an alternative way of using human raters is to have them post-edit translations, taking the MT output and changing it minimally until they feel it represents a correct translation. The difference between their post-edited translations and the original MT output can then be used as a measure of quality. ", "Bloom_type": "remember", "question": "In the context provided, what does the phrase \"post-editing translations\" refer to?", "options": ["Editing translations after the initial posting", "Referring back to the original text before editing", "Using machine learning models for translation improvement", "Comparing different types of translations side by side"], "complexity": 0}, {"id": 43, "context": "The simplest and most robust metric for MT evaluation is called chrF, which stands for character F-score (Popovic, 2015). chrF (along with many other earlier related metrics like BLEU, METEOR, TER, and others) is based on a simple intuition derived from the pioneering work of Miller and Beebe-Center (1956): a good machine translation will tend to contain characters and words that occur in a human translation of the same sentence. Consider a test set from a parallel corpus, in which each source sentence has both a gold human target translation and a candidate MT translation we`d like to evaluate. The chrF metric ranks each MT target sentence by a function of the number of character n-gram overlaps with the human translation. ", "Bloom_type": "remember", "question": "What does the chrF metric rank each MT target sentence by?", "options": ["The similarity between the MT target sentence and the human translation", "The length of the MT target sentence", "The number of word n-grams in the MT target sentence", "The number of character n-grams in the MT target sentence"], "complexity": 0}, {"id": 44, "context": "There are various alternative overlap metrics. For example, before the development of chrF, it was common to use a word-based overlap metric called BLEU (for BiLingual Evaluation Understudy), that is purely precision-based rather than combining precision and recall (Papineni et al., 2002). The BLEU score for a corpus of candidate translation sentences is a function of the n-gram word precision over all the sentences combined with a brevity penalty computed over the corpus as a whole. ", "Bloom_type": "remember", "question": "What does BLEU stand for?", "options": ["BiLingual Evaluation Understudy", "BiLinguistic Evaluation Study", "BiLinguistic Evaluation Understudy", "BiLinguistic Evaluation Study"], "complexity": 0}, {"id": 45, "context": "Because BLEU is a word-based metric, it is very sensitive to word tokenization, making it impossible to compare different systems if they rely on different tokenization standards, and doesn`t work as well in languages with complex morphology. Nonetheless, you will sometimes still see systems evaluated by BLEU, particularly for translation into English. In such cases it`s important to use packages that enforce standardization for tokenization like SACREBLEU (Post, 2018). ", "Bloom_type": "remember", "question": "Why does BLEU not work well in languages with complex morphology?", "options": ["Because BLEU cannot handle morphological complexity.", "Because BLEU is too simple.", "Because BLEU requires more computational resources.", "Because BLEU only considers sentence-level metrics."], "complexity": 0}, {"id": 46, "context": "For example, in some situations we might have datasets that have human assessments of translation quality. Such datasets consists of tuples (x, x, r), where x = (x1, . . . , xn) is a reference translation, x = ( x1, . . . , xm) is a candidate machine translation, and r R is a human rating that expresses the quality of x with respect to x. Given such data, algorithms like COMET (Rei et al., 2020) BLEURT (Sellam et al., 2020) train a predictor on the human-labeled datasets, for example by passing x and x through a version of BERT (trained with extra pretraining, and then finetuned on the human-labeled sentences), followed by a linear layer that is trained to predict r. The output of such models correlates highly with human labels. ", "Bloom_type": "remember", "question": "In the context provided, what does the term \"translation\" refer to?", "options": ["The act of converting written language from one language into another.", "The process of translating computer code between different programming languages.", "The skill of accurately interpreting spoken English.", "The method used to measure the similarity between two images."], "complexity": 0}, {"id": 47, "context": "Many ethical questions in MT require further research. One open problem is developing metrics for knowing what our systems don`t know. This is because MT systems can be used in urgent situations where human translators may be unavailable or delayed: in medical domains, to help translate when patients and doctors don`t speak the same language, or in legal domains, to help judges or lawyers communicate with witnesses or defendants. In order to do no harm`, systems need ways to assign confidence values to candidate translations, so they can abstain from giving incorrect translations that may cause harm. ", "Bloom_type": "remember", "question": "In which domain are MT systems particularly useful due to their ability to handle languages spoken by different individuals?", "options": ["Medical", "Finance", "Legal", "Education"], "complexity": 0}, {"id": 48, "context": "Machine translation is one of the most widely used applications of NLP, and the encoder-decoder model, first developed for MT is a key tool that has applications throughout NLP. ", "Bloom_type": "remember", "question": "What is a key tool in Natural Language Processing (NLP)?", "options": ["Encoder-decoder models", "Machine learning algorithms", "Sequence-to-sequence models", "Rule-based systems"], "complexity": 0}, {"id": 49, "context": " Machine translation models are trained on a parallel corpus, sometimes called ", "Bloom_type": "remember", "question": "What is often referred to as a parallel corpus when training machine translation models?", "options": ["Parallel data set", "Translation memory", "Interlingual model", "Synthetic language"], "complexity": 0}, {"id": 50, "context": " Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.  MT is evaluated by measuring a translation`s adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used. ", "Bloom_type": "remember", "question": "What is backtranslation?", "options": ["It is a technique for creating synthetic bitexts using monolingual corpora in the target language.", "It is a method for evaluating machine translation systems.", "It is a process where machine translations are run forward.", "It is an automated evaluation metric for machine translation."], "complexity": 0}, {"id": 51, "context": "In the early years, the space of MT architectures spanned three general models. In direct translation, the system proceeds word-by-word through the sourcelanguage text, translating each word incrementally. Direct translation uses a large bilingual dictionary, each of whose entries is a small program with the job of translating one word. In transfer approaches, we first parse the input text and then apply rules to transform the source-language parse into a target language parse. We then generate the target language sentence from the parse tree. In interlingua approaches, we analyze the source language text into some abstract meaning representation, called an interlingua. We then generate into the target language from this interlingual representation. A common way to visualize these three early approaches was the Vauquois triangle shown in Fig. 13.13. The triangle shows the increasing depth of analysis required (on both the analysis and generation end) as we move from the direct approach through transfer approaches to interlingual approaches. In addition, it shows the decreasing amount of transfer knowledge needed as we move up the triangle, from huge amounts of transfer at the direct level (almost all knowledge is transfer knowledge for each word) through transfer (transfer rules only for parse trees or thematic roles) through interlingua (no specific transfer knowledge). We can view the encoder-decoder network as an interlingual approach, with attention acting as an integration of direct and transfer, allowing words or their representations to be directly accessed by the decoder. ", "Bloom_type": "remember", "question": "In which type of translation does the system translate each word incrementally?", "options": ["Direct translation", "Transfer approaches", "Interlingua approaches", "All types"], "complexity": 0}, {"id": 52, "context": "By the turn of the century, most academic research on machine translation used statistical MT, either in the generative or discriminative mode. An extended version of the generative approach, called phrase-based translation was developed, based on inducing translations for phrase-pairs (Och 1998, Marcu and Wong 2002, Koehn et al. (2003), Och and Ney 2004, Deng and Byrne 2005, inter alia). ", "Bloom_type": "remember", "question": "Which method did not use a generative approach?", "options": ["Rule-based MT", "Phrase-based translation", "Statistical MT", "Discriminative MT"], "complexity": 0}, {"id": 53, "context": "Neural networks had been applied at various times to various aspects of machine translation; for example Schwenk et al. (2006) showed how to use neural language models to replace n-gram language models in a Spanish-English system based on IBM Model 4. The modern neural encoder-decoder approach was pioneered by Kalchbrenner and Blunsom (2013), who used a CNN encoder and an RNN decoder, and was first applied to MT by Bahdanau et al. (2015). The transformer encoderdecoder was proposed by Vaswani et al. (2017) (see the History section of Chapter 9). ", "Bloom_type": "remember", "question": "In what year did the transformer encoder-decoder architecture first appear?", "options": ["2017", "2006", "2013", "2015"], "complexity": 0}, {"id": 54, "context": "The encoder-decoder architecture was applied to speech at about the same time by two different groups, in the Listen Attend and Spell system of Chan et al. (2016) and the attention-based encoder decoder architecture of Chorowski et al. (2014) and Bahdanau et al. (2016). By 2018 Transformers were included in this encoderdecoder architecture. Karita et al. (2019) is a nice comparison of RNNs vs Transformers in encoder-architectures for ASR, TTS, and speech-to-speech translation. ", "Bloom_type": "remember", "question": "In what year did Karita et al. publish their work comparing RNNs versus Transformers in encoder architectures?", "options": ["2018", "2015", "2017", "2016"], "complexity": 0}, {"id": 55, "context": "Automatic parsing using dependency grammars was first introduced into computational linguistics by early work on machine translation at the RAND Corporation led by David Hays. This work on dependency parsing closely paralleled work on constituent parsing and made explicit use of grammars to guide the parsing process. After this early period, computational work on dependency parsing remained intermittent over the following decades. Notable implementations of dependency parsers for English during this period include Link Grammar (Sleator and Temperley, 1993), Constraint Grammar (Karlsson et al., 1995), and MINIPAR (Lin, 2003). ", "Bloom_type": "remember", "question": "What is an example of a notable implementation of a dependency parser for English mentioned in the context?", "options": ["Link Grammar", "Constraint Grammar", "MINIPAR", "Dependency Parsing"], "complexity": 0}, {"id": 56, "context": "The main reason computational systems use semantic roles is to act as a shallow meaning representation that can let us make simple inferences that aren`t possible from the pure surface string of words, or even from the parse tree. To extend the earlier examples, if a document says that Company A acquired Company B, we`d like to know that this answers the query Was Company B acquired? despite the fact that the two sentences have very different surface syntax. Similarly, this shallow semantics might act as a useful intermediate language in machine translation. ", "Bloom_type": "remember", "question": "In what way does shallow semantics help with making inferences about sentence meanings?", "options": ["It simplifies the process of natural language processing.", "It allows for complex syntactic analysis.", "It enhances the accuracy of word translations.", "It increases the complexity of sentence structures."], "complexity": 0}, {"id": 57, "context": "Coreference is an important component of natural language processing. A dialogue system that has just told the user There is a 2pm flight on United and a 4pm one on Cathay Pacific must know which flight the user means by I`ll take the second one. A question answering system that uses Wikipedia to answer a question about Marie Curie must know who she was in the sentence She was born in Warsaw. And a machine translation system translating from a language like Spanish, in which pronouns can be dropped, must use coreference from the previous sentence to decide whether the Spanish sentence Me encanta el conocimiento, dice.` should be translated as I love knowledge, he says`, or I love knowledge, she says`. Indeed, this example comes from an actual news article in El Pas about a female professor and was mistranslated as he in machine translation because of inaccurate coreference resolution (Schiebinger, 2013). ", "Bloom_type": "remember", "question": "In what way does coreference play a crucial role in natural language processing?", "options": ["It assists in recognizing the meaning of pronouns.", "It helps machines understand synonyms.", "It aids in identifying the speaker's gender.", "It enables machines to translate between languages."], "complexity": 0}, {"id": 58, "context": "Alignment Knowing the minimum edit distance is useful for algorithms like finding potential spelling error corrections. But the edit distance algorithm is important in another way; with a small change, it can also provide the minimum cost alignment between two strings. Aligning two strings is useful throughout speech and language processing. In speech recognition, minimum edit distance alignment is used to compute the word error rate (Chapter 16). Alignment plays a role in machine translation, in which sentences in a parallel corpus (a corpus with a text in two languages) need to be matched to each other. ", "Bloom_type": "comprehension", "question": "Explain how alignment plays a role in machine translation?", "options": ["Alignment enables matching of phrases across different languages to maintain coherence.", "Alignment helps in reducing the computational complexity of the translation model.", "Alignment ensures that every sentence in one language has an exact match in the other language.", "Alignment allows for dynamic reordering of words within a sentence during translation."], "complexity": 1}, {"id": 59, "context": "The best way to evaluate the performance of a language model is to embed it in an application and measure how much the application improves. Such end-to-end evaluation is called extrinsic evaluation. Extrinsic evaluation is the only way to know if a particular improvement in the language model (or any component) is really going to help the task at hand. Thus for evaluating n-gram language models that are a component of some task like speech recognition or machine translation, we can compare the performance of two candidate language models by running the speech recognizer or machine translator twice, once with each language model, and seeing which gives the more accurate transcription. ", "Bloom_type": "comprehension", "question": "Explain why extrinsic evaluation is crucial for assessing improvements in language models?", "options": ["It measures the accuracy of the model independently from its use.", "It allows direct comparison between different components.", "It provides feedback on the model's internal workings.", "It ensures the model performs well across various applications."], "complexity": 1}, {"id": 60, "context": "Language ID systems are trained on multilingual text, such as Wikipedia (Wikipedia text in 68 different languages was used in (Lui and Baldwin, 2011)), or newswire. To make sure that this multilingual text correctly reflects different regions, dialects, and socioeconomic classes, systems also add Twitter text in many languages geotagged to many regions (important for getting world English dialects from countries with large Anglophone populations like Nigeria or India), Bible and Quran translations, slang websites like Urban Dictionary, corpora of African American Vernacular English (Blodgett et al., 2016), and so on (Jurgens et al., 2017). ", "Bloom_type": "comprehension", "question": "What additional sources do language ID systems use besides multilingual text from Wikipedia?", "options": ["Twitter text in many languages geotagged to many regions", "Bible and Quran translations", "Slang websites like Urban Dictionary", "Corpora of African American Vernacular English"], "complexity": 1}, {"id": 61, "context": "The bootstrap test (Efron and Tibshirani, 1993) can apply to any metric; from precision, recall, or F1 to the BLEU metric used in machine translation. The word bootstrapping refers to repeatedly drawing large numbers of samples with replacement (called bootstrap samples) from an original set. The intuition of the bootstrap test is that we can create many virtual test sets from an observed test set by repeatedly sampling from it. The method only makes the assumption that the sample is representative of the population. ", "Bloom_type": "comprehension", "question": "What does the bootstrap test rely on for creating its virtual test sets?", "options": ["Randomly selected subsets of data", "The entire dataset at once", "Fixed-size samples drawn without replacement", "Samples based on specific criteria"], "complexity": 1}, {"id": 62, "context": "Semantic Frames and Roles Closely related to semantic fields is the idea of a semantic frame. A semantic frame is a set of words that denote perspectives or participants in a particular type of event. A commercial transaction, for example, is a kind of event in which one entity trades money to another entity in return for some good or service, after which the good changes hands or perhaps the service is performed. This event can be encoded lexically by using verbs like buy (the event from the perspective of the buyer), sell (from the perspective of the seller), pay (focusing on the monetary aspect), or nouns like buyer. Frames have semantic roles (like buyer, seller, goods, money), and words in a sentence can take on these roles. Knowing that buy and sell have this relation makes it possible for a system to know that a sentence like Sam bought the book from Ling could be paraphrased as Ling sold the book to Sam, and that Sam has the role of the buyer in the frame and Ling the seller. Being able to recognize such paraphrases is important for question answering, and can help in shifting perspective for machine translation. ", "Bloom_type": "comprehension", "question": "What does the concept of semantic frames relate to in the context of translating events?", "options": ["Understanding the roles and perspectives involved in an event", "Identifying the entities involved in a transaction", "Determining the monetary value of goods", "Recognizing synonyms in different languages"], "complexity": 1}, {"id": 63, "context": "RNN-based language models can also be used to generate text. Text generation is of enormous practical importance, part of tasks like question answering, machine translation, text summarization, grammar correction, story generation, and conversational dialogue; any task where a system needs to produce text, conditioned on some other text. This use of a language model to generate text is one of the areas in which the impact of neural language models on NLP has been the largest. Text generation, along with image generation and code generation, constitute a new area of AI that is often called generative AI. ", "Bloom_type": "comprehension", "question": "What are some practical applications of generating text using RNN-based language models?", "options": ["Translation, summarization, and coding", "Translation, summarization, and dialogues", "Grammar correction, summarization, and coding", "Story generation, coding, and summarization"], "complexity": 1}, {"id": 64, "context": "Technically an autoregressive model is a model that predicts a value at time t based on a linear function of the previous values at times t 2, and so on. Although language models are not linear (since they have many layers of non-linearities), we loosely refer to this generation technique as autoregressive generation since the word generated at each time step is conditioned on the word selected by the network from the previous step. Fig. 8.9 illustrates this approach. In this figure, the details of the RNN`s hidden layers and recurrent connections are hidden within the blue block. This simple architecture underlies state-of-the-art approaches to applications such as machine translation, summarization, and question answering. The key to these approaches is to prime the generation component with an appropriate context. That is, instead of simply using <s> to get things started we can provide a richer task-appropriate context; for translation the context is the sentence in the source language; for summarization it`s the long text we want to summarize. ", "Bloom_type": "comprehension", "question": "What does the autoregressive model predict at each time step?", "options": ["The next word in the sequence", "The entire sentence", "The previous word only", "None of the above"], "complexity": 1}, {"id": 65, "context": "By contrast, encoder-decoder models are used especially for tasks like machine translation, where the input sequence and output sequence can have different lengths ", "Bloom_type": "comprehension", "question": "Explain how encoder-decoder models differ from other types of models when it comes to handling sequences of varying lengths.", "options": ["Encoder-decoder models automatically adjust their length based on the input.", "Encoder-decoder models require identical input and output sequences.", "Encoder-decoder models cannot handle sequences of different lengths.", "Encoder-decoder models use fixed-length inputs and outputs."], "complexity": 1}, {"id": 66, "context": "Encoder-decoder networks, sometimes called sequence-to-sequence networks, are models capable of generating contextually appropriate, arbitrary length, output sequences given an input sequence. Encoder-decoder networks have been applied to a very wide range of applications including summarization, question answering, and dialogue, but they are particularly popular for machine translation. ", "Bloom_type": "comprehension", "question": "Explain how encoder-decoder networks can be applied to machine translation?", "options": ["Encoder-decoder networks translate sentences from one language into another by first encoding the source sentence and then decoding it back into the target language.", "Encoder-decoder networks only focus on translating short sentences within their own language.", "Encoder-decoder networks use pre-trained models to directly translate between any two languages without requiring training data.", "Encoder-decoder networks do not involve translation; instead, they are used for image recognition."], "complexity": 1}, {"id": 67, "context": "We only have to make one slight change to turn this language model with autoregressive generation into an encoder-decoder model that is a translation model that can translate from a source text in one language to a target text in a second: add a sentence separation marker at the end of the source text, and then simply concatenate the target text. ", "Bloom_type": "comprehension", "question": "What is the primary difference between an autoregressive model and a translation model?", "options": ["An autoregressive model generates output based on previous inputs, while a translation model does not.", "An autoregressive model requires no input data, whereas a translation model uses existing texts.", "Translation models use sentence separators, but autoregressive models do not.", "Autoregressive models are more complex than translation models."], "complexity": 1}, {"id": 68, "context": "Transformer-based language models are complex, and so the details will unfold over the next 5 chapters. In the next sections we`ll introduce multi-head attention, the rest of the transformer block, and the input encoding and language modeling head components. Chapter 10 discusses how language models are pretrained, and how tokens are generated via sampling. Chapter 11 introduces masked language modeling and the BERT family of bidirectional transformer encoder models. Chapter 12 shows how to prompt LLMs to perform NLP tasks by giving instructions and demonstrations, and how to align the model with human preferences. Chapter 13 will introduce machine translation with the encoder-decoder architecture. ", "Bloom_type": "comprehension", "question": "What topic does Chapter 13 focus on regarding language models?", "options": ["Machine translation using an encoder-decoder approach", "Pretraining and token generation", "Masked language modeling and BERT models", "Prompting for NLP tasks and alignment with human preferences"], "complexity": 1}, {"id": 69, "context": "able performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots. ", "Bloom_type": "comprehension", "question": "What aspect of AI capabilities does the passage emphasize due to their extensive training?", "options": ["Their capacity for translating languages", "Their ability to perform complex mathematical computations", "Their proficiency in understanding human emotions", "Their skill at recognizing visual patterns"], "complexity": 1}, {"id": 70, "context": "Large language models are mainly trained on text scraped from the web, augmented by more carefully curated data. Because these training corpora are so large, they are likely to contain many natural examples that can be helpful for NLP tasks, such as question and answer pairs (for example from FAQ lists), translations of sentences between various languages, documents together with their summaries, and so on. ", "Bloom_type": "comprehension", "question": "Explain how large language models utilize translation corpora for improving their performance?", "options": ["Translation corpora provide a vast amount of text data that helps improve model accuracy.", "Translation corpora are irrelevant to the training of LLMs.", "LLMs use only pre-existing datasets for translation improvements.", "Translation corpora hinder the development of better NLP techniques."], "complexity": 1}, {"id": 71, "context": "Large pretrained neural language models exhibit many of the potential harms discussed in Chapter 4 and Chapter 6. Many of these harms become realized when pretrained language models are used for any downstream task, particularly those involving text generation, whether question answering, machine translation, or in assistive technologies like writing aids or web search query completion, or predictive typing for email (Olteanu et al., 2020). ", "Bloom_type": "comprehension", "question": "Explain how large pretrained neural language models can lead to various harms described in the chapters.", "options": ["Models may produce harmful content unintentionally.", "Translation errors occur frequently.", "Text generation leads to ethical issues.", "Pretrained models enhance human creativity."], "complexity": 1}, {"id": 72, "context": " Many NLP taskssuch as question answering, summarization, sentiment, and machine translationcan be cast as tasks of word prediction and hence addressed with Large language models. ", "Bloom_type": "comprehension", "question": "Which type of NLP task can be most effectively addressed using large language models due to their ability to predict words?", "options": ["Machine translation", "Sentiment analysis", "Text summarization", "Question answering"], "complexity": 1}, {"id": 73, "context": "N-gram language models were very widely used over the next 30 years and more, across a wide variety of NLP tasks like speech recognition and machine translations, often as one of multiple components of the model. The contexts for these n-gram models grew longer, with 5-gram models used quite commonly by very efficient LM toolkits (Stolcke, 2002; Heafield, 2011). ", "Bloom_type": "comprehension", "question": "What was the primary use of N-gram language models before their widespread adoption?", "options": ["Speech recognition", "Translation", "Machine learning", "Natural language processing"], "complexity": 1}, {"id": 74, "context": "If we want to solve general tasks like summarization or translation, we don`t want to have to create a new prompt each time we do the task. Instead the first step in prompting is to design one or more templates: task-specific prompting text along with slots for the particular input that is being processed. ", "Bloom_type": "comprehension", "question": "What is the primary purpose of designing templates when translating tasks?", "options": ["To reduce the need for creating unique prompts every time", "To ensure consistency across different prompts", "To focus solely on summarizing content", "To eliminate the use of any templates altogether"], "complexity": 1}, {"id": 75, "context": "But more generally, the best way to select demonstrations from the training set is programmatically: choosing the set of demonstrations that most increases task performance of the prompt on a test set. Task performance for sentiment analysis or multiple-choice question answering can be measured in accuracy; for machine translation with chrF, and for summarization via Rouge. Systems like DSPy (Khattab et al., 2024), a framework for algorithmically optimizing LM prompts, can automatically find the optimum set of demonstrations to include by searching through the space of possible demonstrations to include. We`ll return to automatic prompt optimization in Section 12.5. ", "Bloom_type": "comprehension", "question": "What metric is commonly used to measure task performance in machine translation?", "options": ["BLEU Score", "Accuracy", "Precision", "Recall"], "complexity": 1}, {"id": 76, "context": "Instruction tuning (short for instruction finetuning, and sometimes even shortened to instruct tuning) is a method for making an LLM better at following instructions. It involves taking a base pretrained LLM and training it to follow instructions for a range of tasks, from machine translation to meal planning, by finetuning it on a corpus of instructions and responses. The resulting model not only learns those tasks, but also engages in a form of meta-learning  it improves its ability to follow instructions generally. ", "Bloom_type": "comprehension", "question": "What does instruction tuning involve?", "options": ["It involves taking a base pretrained LLM and training it to follow instructions for a range of tasks.", "It involves taking a base pretrained LLM and training it to follow instructions for a single task.", "It involves taking a base pretrained LLM and training it to follow instructions for a specific type of task.", "It involves taking a base pretrained LLM and training it to follow instructions for all possible tasks."], "complexity": 1}, {"id": 77, "context": "Many huge instruction tuning datasets have been created, covering many tasks and languages. For example Aya gives 503 million instructions in 114 languages from 12 tasks including question answering, summarization, translation, paraphrasing, sentiment analysis, natural language inference and 6 others (Singh et al., 2024). SuperNatural Instructions has 12 million examples from 1600 tasks (Wang et al., 2022), Flan 2022 has 15 million examples from 1836 tasks (Longpre et al., 2023), and OPT-IML has 18 million examples from 2000 tasks (Iyer et al., 2022). ", "Bloom_type": "comprehension", "question": "Which dataset contains the most number of translations?", "options": ["SuperNatural Instructions", "Flan 2022", "OPT-IML", "Not specified"], "complexity": 1}, {"id": 78, "context": "Developing high quality supervised training data in this way is time consuming and costly. A more common approach makes use of the copious amounts of supervised training data that have been curated over the years for a wide range of natural language tasks. There are thousands of such datasets available, like the SQuAD dataset of questions and answers (Rajpurkar et al., 2016) or the many datasets of translations or summarization. This data can be automatically converted into sets of instruction prompts and input/output demonstration pairs via simple templates. ", "Bloom_type": "comprehension", "question": "Explain how translation data can be efficiently utilized by converting it into instruction prompts and input/output demonstrations?", "options": ["Translation data is simplified using basic templates.", "Translation data is directly used as instruction prompts.", "Translation data is manually translated into English.", "Translation data is processed through complex algorithms to create prompts."], "complexity": 1}, {"id": 79, "context": "Given access to labeled training data, candidate prompts can be scored based on execution accuracy (Honovich et al., 2023). In this approach, candidate prompts are combined with inputs sampled from the training data and passed to an LLM for decoding. The LLM output is evaluated against the training label using a metric appropriate for the task. In the case of classification-based tasks, this is effectively a 0/1 loss  how many examples were correctly labeled with the given prompt. Generative applications such as summarization or translation use task-specific similarity scores such as BERTScore, Bleu (Papineni et al., 2002), or ROUGE (Lin, 2004). ", "Bloom_type": "comprehension", "question": "What type of evaluation method is commonly used for generative applications like summarization or translation?", "options": ["Similarity score evaluations", "Classification-based metrics", "Accuracy-based scoring", "Both A) and C)"], "complexity": 1}, {"id": 80, "context": "Language models are evaluated in many ways. we introduced some evaluations for in Section 10.4, including measuring the language model`s perplexity on a test set, evaluating its accuracy on various NLP tasks, as well as benchmarks that help measure efficiency, toxicity, fairness, and so on. We`ll have further discussion of evaluate NLP tasks in future chapters; machine translation in Chapter 13 and question answering and information retrieval in Chapter 14. ", "Bloom_type": "comprehension", "question": "What aspect of evaluation does this passage NOT mention?", "options": ["Accuracy on various NLP tasks", "Efficiency", "Toxicity", "Fairness"], "complexity": 1}, {"id": 81, "context": "In this second part of the book we introduce fundamental NLP applications: machine translation, information retrieval, question answering, dialogue systems, and speech recognition. ", "Bloom_type": "comprehension", "question": "Which of these NLP applications does NOT involve translation?", "options": ["Information Retrieval", "Machine Translation", "Dialogue Systems", "Speech Recognition"], "complexity": 1}, {"id": 82, "context": "This chapter introduces machine translation (MT), the use of computers to translate from one language to another. ", "Bloom_type": "comprehension", "question": "What does this chapter focus on introducing?", "options": ["Machine translation techniques and their applications", "The history of language translation", "The development of human translators", "The principles behind natural language processing"], "complexity": 1}, {"id": 83, "context": "Machine translation in its present form therefore focuses on a number of very practical tasks. Perhaps the most common current use of machine translation is for information access. We might want to translate some instructions on the web, perhaps the recipe for a favorite dish, or the steps for putting together some furniture. Or we might want to read an article in a newspaper, or get information from an online resource like Wikipedia or a government webpage in some other language. MT for information access is probably one of the most common uses of NLP technology, and Google Translate alone (shown above) translates hundreds of billions of words a day between over 100 languages. Improvements in machine translation can thus help reduce what is often called the digital divide in information access: the fact that much more information is available in English and other languages spoken in wealthy countries. Web searches in English return much more information than searches in other languages, and online resources like Wikipedia are much larger in English and other higher-resourced languages. High-quality translation can help provide information to speakers of lower-resourced languages. ", "Bloom_type": "comprehension", "question": "What is the primary focus of machine translation in its current form?", "options": ["Information access", "Speech recognition", "Text summarization", "Machine learning algorithms"], "complexity": 1}, {"id": 84, "context": "Another common use of machine translation is to aid human translators. MT systems are routinely used to produce a draft translation that is fixed up in a post-editing phase by a human translator. This task is often called computer-aided translation or CAT. CAT is commonly used as part of localization: the task of adapting content or a product to a particular language community. ", "Bloom_type": "comprehension", "question": "What does CAT stand for?", "options": ["Computer-Aided Translation", "Computer-Assisted Translation", "Computer-Assisted Technology", "Computer-Aided Technology"], "complexity": 1}, {"id": 85, "context": "Fig. 13.2 shows examples of other word order differences. All of these word order differences between languages can cause problems for translation, requiring the system to do huge structural reorderings as it generates the output. ", "Bloom_type": "comprehension", "question": "What problem does language structure difference cause during translation?", "options": ["It requires the system to perform extensive reordering of sentences.", "It necessitates the use of different sentence structures.", "It demands changes in vocabulary usage entirely.", "It calls for alterations in grammar rules."], "complexity": 1}, {"id": 86, "context": "Of course we also need to translate the individual words from one language to another. For any translation, the appropriate word can vary depending on the context. The English source-language word bass, for example, can appear in Spanish as the fish lubina or the musical instrument bajo. German uses two distinct words for what in English would be called a wall: Wand for walls inside a building, and Mauer for walls outside a building. Where English uses the word brother for any male sibling, Chinese and many other languages have distinct words for older brother and younger brother (Mandarin gege and didi, respectively). In all these cases, translating bass, wall, or brother from English would require a kind of specialization, disambiguating the different uses of a word. For this reason the fields of MT and Word Sense Disambiguation (Appendix G) are closely linked. ", "Bloom_type": "comprehension", "question": "Explain how translation differs based on the context?", "options": ["Translation necessitates specialized knowledge about the specific use of words within certain contexts.", "Translation involves changing the meaning of words across languages.", "Translation requires understanding the cultural nuances of each language.", "Translation does not change the meaning of words but rather their pronunciation."], "complexity": 1}, {"id": 87, "context": "Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. Large numbers of parallel corpora are available. Some are governmental; the Europarl corpus (Koehn, 2005), extracted from the proceedings of the European Parliament, contains between 400,000 and 2 million sentences each from 21 European languages. The United Nations Parallel Corpus contains on the order of 10 million sentences in the six official languages of the United Nations (Arabic, Chinese, English, French, Russian, Spanish) Ziemski et al. (2016). Other parallel corpora have been made from movie and TV subtitles, like the OpenSubtitles corpus (Lison and Tiedemann, 2016), or from general web text, like the ParaCrawl corpus of 223 million sentence pairs between 23 EU languages and English extracted from the CommonCrawl Banon et al. (2020). ", "Bloom_type": "comprehension", "question": "What type of parallel corpora do machine translation models typically use?", "options": ["Corpora created from movie and TV subtitles", "Governmental texts only", "Parallel corpora containing internet data", "Corpora derived from general web text"], "complexity": 1}, {"id": 88, "context": " a cost function that takes a span of source sentences and a span of target sentences and returns a score measuring how likely these spans are to be translations. ", "Bloom_type": "comprehension", "question": "What does a cost function measure when comparing source and target sentences for translation?", "options": ["The likelihood of the source sentence being a translation of the target sentence", "The length of the sentences", "The similarity between the sentence structures", "The number of words in each sentence"], "complexity": 1}, {"id": 89, "context": "Thus at the first step of decoding, we compute a softmax over the entire vocabulary, assigning a probability to each word. We then select the k-best options from this softmax output. These initial k outputs are the search frontier and these k initial words are called hypotheses. A hypothesis is an output sequence, a translation-sofar, together with its probability. ", "Bloom_type": "comprehension", "question": "What does the phrase 'k-best options' refer to in the context of translation?", "options": ["The k highest scoring hypotheses generated by the decoder", "The top k most probable translations based on their probabilities", "The k best possible translations available in the dictionary", "The k least likely translations rejected by the decoder"], "complexity": 1}, {"id": 90, "context": "For MT we generally use beam widths k between 5 and 10, giving us k hypotheses at the end. We can pass all k to the downstream application with their respective scores, or if we just need a single translation we can pass the most probable hypothesis. ", "Bloom_type": "comprehension", "question": "What are two common methods for passing translations through an MT system?", "options": ["Beam search and backoff", "Beam search and greedy decoding", "Greedy decoding and backoff", "Backoff and beam search"], "complexity": 1}, {"id": 91, "context": "The intuition of minimum Bayes risk is that instead of trying to choose the translation which is most probable, we choose the one that is likely to have the least error. For example, we might want our decoding algorithm to find the translation which has the highest score on some evaluation metric. For example in Section 13.6 we will introduce metrics like chrF or BERTScore that measure the goodness-of-fit between a candidate translation and a set of reference human translations. A translation that maximizes this score, especially with a hypothetically huge set of perfect human translations is likely to be a good one (have minimum risk) even if it is not the most probable translation by our particular probability estimator. ", "Bloom_type": "comprehension", "question": "Explain how choosing a translation based on its likelihood rather than being the most probable can lead to better results despite lower probability?", "options": ["Using the minimum Bayes risk approach allows for more accurate predictions.", "The translation with the lowest probability is always the best choice.", "Choosing the most probable translation ensures higher accuracy.", "Translating using the maximum likelihood method guarantees minimal error."], "complexity": 1}, {"id": 92, "context": "In practice, we don`t know the perfect set of translations for a given sentence. So the standard simplification used in MBR decoding algorithms is to instead choose the candidate translation which is most similar (by some measure of goodness-offit) with some set of candidate translations. We`re essentially approximating the enormous space of all possible translations U with a smaller set of possible candidate translations Y. ", "Bloom_type": "comprehension", "question": "What does the standard simplification in MBR decoding algorithms do?", "options": ["It simplifies the problem by choosing the best approximation from a limited set", "It chooses the most complex translation", "It selects the least similar translation", "It uses a larger set of candidate translations"], "complexity": 1}, {"id": 93, "context": "Minimum Bayes risk decoding can also be used for other NLP tasks; indeed it was widely applied to speech recognition (Stolcke et al., 1997; Goel and Byrne, 2000) before being applied to machine translation (Kumar and Byrne, 2004), and has been shown to work well across many other generation tasks as well (e.g., summarization, dialogue, and image captioning (Suzgun et al., 2023a)). ", "Bloom_type": "comprehension", "question": "What task besides machine translation has demonstrated effectiveness using minimum Bayes risk decoding?", "options": ["Summarization", "Speech recognition", "Image captioning", "Dialogue"], "complexity": 1}, {"id": 94, "context": "For some languages, and especially for English, online resources are widely available. There are many large parallel corpora that contain translations between English and many languages. But the vast majority of the world`s languages do not have large parallel training texts available. An important ongoing research question is how to get good translation with lesser resourced languages. The resource problem can even be true for high resource languages when we need to translate into low resource domains (for example in a particular genre that happens to have very little bitext). ", "Bloom_type": "comprehension", "question": "What is an important ongoing research question regarding translation?", "options": ["How to improve translation quality using only small amounts of parallel data.", "The best way to create new languages from scratch.", "Determining which languages should be prioritized for translation efforts.", "Finding ways to make translation more efficient."], "complexity": 1}, {"id": 95, "context": "The most common data augmentation technique for machine translation is called backtranslation. Backtranslation relies on the intuition that while parallel corpora may be limited for particular languages or domains, we can often find a large (or at least larger) monolingual corpus, to add to the smaller parallel corpora that are available. The algorithm makes use of monolingual corpora in the target language by creating synthetic bitexts. ", "Bloom_type": "comprehension", "question": "What does backtranslation rely on to improve machine translation quality?", "options": ["Both A and C", "The availability of parallel corpora only", "The existence of monolingual corpora in the source language", "The presence of large monolingual corpora in the target language"], "complexity": 1}, {"id": 96, "context": "In backtranslation, our goal is to improve source-to-target MT, given a small parallel text (a bitext) in the source/target languages, and some monolingual data in the target language. We first use the bitext to train a MT system in the reverse direction: a target-to-source MT system . We then use it to translate the monolingual target data to the source language. Now we can add this synthetic bitext (natural target sentences, aligned with MT-produced source sentences) to our training data, and retrain our source-to-target MT model. For example suppose we want to translate from Navajo to English but only have a small Navajo-English bitext, although of course we can find lots of monolingual English data. We use the small bitext to build an MT engine going the other way (from English to Navajo). Once we translate the monolingual English text to Navajo, we can add this synthetic Navajo/English bitext to our training data. ", "Bloom_type": "comprehension", "question": "What is the primary purpose of backtranslation in machine translation?", "options": ["To improve the quality of the source-to-target MT system by using a bitext for training.", "To create a new bitext by translating monolingual data into the target language.", "To directly translate monolingual data into the target language without any additional steps.", "To enhance the accuracy of the target-to-source MT system."], "complexity": 1}, {"id": 97, "context": "Backtranslation has various parameters. One is how we generate the backtranslated data; we can run the decoder in greedy inference, or use beam search. Or we can do sampling, like the temperature sampling algorithm we saw in Chapter 9. Another parameter is the ratio of backtranslated data to natural bitext data; we can choose to upsample the bitext data (include multiple copies of each sentence). In general backtranslation works surprisingly well; one estimate suggests that a system trained on backtranslated text gets about 2/3 of the gain as would training on the same amount of natural bitext (Edunov et al., 2018). ", "Bloom_type": "comprehension", "question": "What are two key parameters for backtranslation mentioned in the context?", "options": ["Greedy inference and beam search", "Temperature sampling and upsampling", "Natural bitext data and backtranslated data", "Decoder type and upsampling factor"], "complexity": 1}, {"id": 98, "context": "One advantage of a multilingual model is that they can improve the translation of lower-resourced languages by drawing on information from a similar language in the training data that happens to have more resources. Perhaps we don`t know the meaning of a word in Galician, but the word appears in the similar and higherresourced language Spanish. ", "Bloom_type": "comprehension", "question": "How do multilingual models benefit from using information from high-resource languages?", "options": ["They leverage information from languages with more resources.", "They translate words directly into English.", "They rely solely on their own knowledge base for translations.", "They use synonyms found in other languages."], "complexity": 1}, {"id": 99, "context": "The most accurate evaluations use human raters, such as online crowdworkers, to evaluate each translation along the two dimensions. For example, along the dimension of fluency, we can ask how intelligible, how clear, how readable, or how natural the MT output (the target text) is. We can give the raters a scale, for example, from 1 (totally unintelligible) to 5 (totally intelligible), or 1 to 100, and ask them to rate each sentence or paragraph of the MT output. ", "Bloom_type": "comprehension", "question": "What are the primary methods used by human raters when evaluating translations?", "options": ["Employing online crowdworkers", "Using machine learning algorithms", "Analyzing linguistic features only", "Both B) and C)"], "complexity": 1}, {"id": 100, "context": "We can do the same thing to judge the second dimension, adequacy, using raters to assign scores on a scale. If we have bilingual raters, we can give them the source sentence and a proposed target sentence, and rate, on a 5-point or 100-point scale, how much of the information in the source was preserved in the target. If we only have monolingual raters but we have a good human translation of the source text, we can give the monolingual raters the human reference translation and a target machine translation and again rate how much information is preserved. An alternative is to do ranking: give the raters a pair of candidate translations, and ask them which one they prefer. ", "Bloom_type": "comprehension", "question": "What are some methods for judging the adequacy of a translation?", "options": ["Both A) and B)", "Using raters to assign scores on a scale", "Giving the raters the source sentence and a proposed target sentence", "Providing a human reference translation and asking raters to choose between two candidates"], "complexity": 1}, {"id": 101, "context": "As discussed above, an alternative way of using human raters is to have them post-edit translations, taking the MT output and changing it minimally until they feel it represents a correct translation. The difference between their post-edited translations and the original MT output can then be used as a measure of quality. ", "Bloom_type": "comprehension", "question": "What is the primary purpose of having human raters post-edit translations after machine translation (MT) outputs?", "options": ["To improve the accuracy of the MT system by providing feedback on errors", "To increase the speed of translation processes", "To reduce the cost of translation services", "To enhance the creativity of the translated content"], "complexity": 1}, {"id": 102, "context": "The simplest and most robust metric for MT evaluation is called chrF, which stands for character F-score (Popovic, 2015). chrF (along with many other earlier related metrics like BLEU, METEOR, TER, and others) is based on a simple intuition derived from the pioneering work of Miller and Beebe-Center (1956): a good machine translation will tend to contain characters and words that occur in a human translation of the same sentence. Consider a test set from a parallel corpus, in which each source sentence has both a gold human target translation and a candidate MT translation we`d like to evaluate. The chrF metric ranks each MT target sentence by a function of the number of character n-gram overlaps with the human translation. ", "Bloom_type": "comprehension", "question": "What does the chrF metric measure when comparing MT translations to human translations?", "options": ["The frequency of specific character n-grams", "The accuracy rate of word-level matches", "The percentage of correctly translated sentences", "The overall length of the MT output"], "complexity": 1}, {"id": 103, "context": "There are various alternative overlap metrics. For example, before the development of chrF, it was common to use a word-based overlap metric called BLEU (for BiLingual Evaluation Understudy), that is purely precision-based rather than combining precision and recall (Papineni et al., 2002). The BLEU score for a corpus of candidate translation sentences is a function of the n-gram word precision over all the sentences combined with a brevity penalty computed over the corpus as a whole. ", "Bloom_type": "comprehension", "question": "What type of overlap metric did Papineni et al. mention being used before the development of chrF?", "options": ["Word-based BLEU", "Sentence-based BLEU", "Char-level BLEU", "Token-based BLEU"], "complexity": 1}, {"id": 104, "context": "Because BLEU is a word-based metric, it is very sensitive to word tokenization, making it impossible to compare different systems if they rely on different tokenization standards, and doesn`t work as well in languages with complex morphology. Nonetheless, you will sometimes still see systems evaluated by BLEU, particularly for translation into English. In such cases it`s important to use packages that enforce standardization for tokenization like SACREBLEU (Post, 2018). ", "Bloom_type": "comprehension", "question": "Explain why BLEU is sensitive to word tokenization when comparing different systems?", "options": ["Because BLEU evaluates based on sentence-level metrics rather than individual tokens.", "Because BLEU uses character-level comparison which can vary between systems.", "Because BLEU relies solely on morphological analysis for evaluation.", "Because BLEU does not consider punctuation marks."], "complexity": 1}, {"id": 105, "context": "For example, in some situations we might have datasets that have human assessments of translation quality. Such datasets consists of tuples (x, x, r), where x = (x1, . . . , xn) is a reference translation, x = ( x1, . . . , xm) is a candidate machine translation, and r R is a human rating that expresses the quality of x with respect to x. Given such data, algorithms like COMET (Rei et al., 2020) BLEURT (Sellam et al., 2020) train a predictor on the human-labeled datasets, for example by passing x and x through a version of BERT (trained with extra pretraining, and then finetuned on the human-labeled sentences), followed by a linear layer that is trained to predict r. The output of such models correlates highly with human labels. ", "Bloom_type": "comprehension", "question": "What are the components typically included in a dataset for human assessment of translation quality?", "options": ["x, x, r", "x, y, z", "a, b, c", "d, e, f"], "complexity": 1}, {"id": 106, "context": "Many ethical questions in MT require further research. One open problem is developing metrics for knowing what our systems don`t know. This is because MT systems can be used in urgent situations where human translators may be unavailable or delayed: in medical domains, to help translate when patients and doctors don`t speak the same language, or in legal domains, to help judges or lawyers communicate with witnesses or defendants. In order to do no harm`, systems need ways to assign confidence values to candidate translations, so they can abstain from giving incorrect translations that may cause harm. ", "Bloom_type": "comprehension", "question": "Explain why it is important for machine translation (MT) systems to have methods for assigning confidence values to candidate translations?", "options": ["To prevent the system from making decisions based solely on probability estimates", "To ensure that the system always translates accurately", "To make sure that the system does not produce any errors at all", "To allow the system to provide more detailed explanations about its decision-making process"], "complexity": 1}, {"id": 107, "context": "Machine translation is one of the most widely used applications of NLP, and the encoder-decoder model, first developed for MT is a key tool that has applications throughout NLP. ", "Bloom_type": "comprehension", "question": "What is the primary application of machine translation within Natural Language Processing (NLP)?", "options": ["Translation", "Encoder-Decoder Model", "Text Summarization", "Sentiment Analysis"], "complexity": 1}, {"id": 108, "context": " Machine translation models are trained on a parallel corpus, sometimes called ", "Bloom_type": "comprehension", "question": "What type of data do machine translation models typically use for training?", "options": ["Parallel corpora", "Untranslated texts", "Audio recordings", "Video clips"], "complexity": 1}, {"id": 109, "context": " Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.  MT is evaluated by measuring a translation`s adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used. ", "Bloom_type": "comprehension", "question": "What is backtranslation in the context of machine translation?", "options": ["It refers to the process of creating synthetic bitexts by reversing the direction of a machine translation engine.", "It involves translating sentences from one language into another using a trained machine translation model.", "Backtranslation is about improving the accuracy of machine translation models through manual corrections.", "It means using bilingual dictionaries to translate between languages."], "complexity": 1}, {"id": 110, "context": "In the early years, the space of MT architectures spanned three general models. In direct translation, the system proceeds word-by-word through the sourcelanguage text, translating each word incrementally. Direct translation uses a large bilingual dictionary, each of whose entries is a small program with the job of translating one word. In transfer approaches, we first parse the input text and then apply rules to transform the source-language parse into a target language parse. We then generate the target language sentence from the parse tree. In interlingua approaches, we analyze the source language text into some abstract meaning representation, called an interlingua. We then generate into the target language from this interlingual representation. A common way to visualize these three early approaches was the Vauquois triangle shown in Fig. 13.13. The triangle shows the increasing depth of analysis required (on both the analysis and generation end) as we move from the direct approach through transfer approaches to interlingual approaches. In addition, it shows the decreasing amount of transfer knowledge needed as we move up the triangle, from huge amounts of transfer at the direct level (almost all knowledge is transfer knowledge for each word) through transfer (transfer rules only for parse trees or thematic roles) through interlingua (no specific transfer knowledge). We can view the encoder-decoder network as an interlingual approach, with attention acting as an integration of direct and transfer, allowing words or their representations to be directly accessed by the decoder. ", "Bloom_type": "comprehension", "question": "What is the primary difference between direct translation and transfer approaches?", "options": ["Direct translation requires more transfer knowledge than transfer approaches.", "Transfer approaches use a larger bilingual dictionary than direct translation.", "Direct translation involves analyzing the source language text into an abstract meaning representation.", "Interlingual approaches do not require any transfer knowledge."], "complexity": 1}, {"id": 111, "context": "By the turn of the century, most academic research on machine translation used statistical MT, either in the generative or discriminative mode. An extended version of the generative approach, called phrase-based translation was developed, based on inducing translations for phrase-pairs (Och 1998, Marcu and Wong 2002, Koehn et al. (2003), Och and Ney 2004, Deng and Byrne 2005, inter alia). ", "Bloom_type": "comprehension", "question": "What type of machine translation model was introduced by Och and Ney in 2004?", "options": ["Phrase-Based Machine Translation", "Statistical Machine Translation", "Rule-Based Machine Translation", "Hybrid Machine Translation"], "complexity": 1}, {"id": 112, "context": "Neural networks had been applied at various times to various aspects of machine translation; for example Schwenk et al. (2006) showed how to use neural language models to replace n-gram language models in a Spanish-English system based on IBM Model 4. The modern neural encoder-decoder approach was pioneered by Kalchbrenner and Blunsom (2013), who used a CNN encoder and an RNN decoder, and was first applied to MT by Bahdanau et al. (2015). The transformer encoderdecoder was proposed by Vaswani et al. (2017) (see the History section of Chapter 9). ", "Bloom_type": "comprehension", "question": "What type of neural network architecture was initially introduced for machine translation purposes?", "options": ["Convolutional Neural Networks (CNN)", "Recurrent Neural Networks (RNN)", "Transformer Encoder-Decoder", "Both A and B"], "complexity": 1}, {"id": 113, "context": "The encoder-decoder architecture was applied to speech at about the same time by two different groups, in the Listen Attend and Spell system of Chan et al. (2016) and the attention-based encoder decoder architecture of Chorowski et al. (2014) and Bahdanau et al. (2016). By 2018 Transformers were included in this encoderdecoder architecture. Karita et al. (2019) is a nice comparison of RNNs vs Transformers in encoder-architectures for ASR, TTS, and speech-to-speech translation. ", "Bloom_type": "comprehension", "question": "What architectural change led to the inclusion of transformers in encoder-decoder architectures?", "options": ["The implementation of transformer models", "The introduction of recurrent neural networks (RNNs)", "The development of convolutional neural networks (CNNs)", "The use of long short-term memory (LSTM) units"], "complexity": 1}, {"id": 114, "context": "Automatic parsing using dependency grammars was first introduced into computational linguistics by early work on machine translation at the RAND Corporation led by David Hays. This work on dependency parsing closely paralleled work on constituent parsing and made explicit use of grammars to guide the parsing process. After this early period, computational work on dependency parsing remained intermittent over the following decades. Notable implementations of dependency parsers for English during this period include Link Grammar (Sleator and Temperley, 1993), Constraint Grammar (Karlsson et al., 1995), and MINIPAR (Lin, 2003). ", "Bloom_type": "comprehension", "question": "What were some notable implementations of dependency parsers for English mentioned in the context?", "options": ["Link Grammar, Constraint Grammar, and MINIPAR", "Constraint Grammar, MINIPAR, and Dependency Parsing", "Dependency Parsing, Link Grammar, and MINIPAR", "MINIPAR, Constraint Grammar, and Dependency Parsing"], "complexity": 1}, {"id": 115, "context": "The main reason computational systems use semantic roles is to act as a shallow meaning representation that can let us make simple inferences that aren`t possible from the pure surface string of words, or even from the parse tree. To extend the earlier examples, if a document says that Company A acquired Company B, we`d like to know that this answers the query Was Company B acquired? despite the fact that the two sentences have very different surface syntax. Similarly, this shallow semantics might act as a useful intermediate language in machine translation. ", "Bloom_type": "comprehension", "question": "Explain how shallow semantics are utilized in making inferences about sentence meanings?", "options": ["Shallow semantics allow for quick inference by focusing on core meaning rather than surface details.", "Shallow semantics help identify the grammatical structure of sentences.", "Shallow semantics provide a deeper understanding of the full meaning of sentences.", "Shallow semantics eliminate the need for any further analysis of sentence structures."], "complexity": 1}, {"id": 116, "context": "Coreference is an important component of natural language processing. A dialogue system that has just told the user There is a 2pm flight on United and a 4pm one on Cathay Pacific must know which flight the user means by I`ll take the second one. A question answering system that uses Wikipedia to answer a question about Marie Curie must know who she was in the sentence She was born in Warsaw. And a machine translation system translating from a language like Spanish, in which pronouns can be dropped, must use coreference from the previous sentence to decide whether the Spanish sentence Me encanta el conocimiento, dice.` should be translated as I love knowledge, he says`, or I love knowledge, she says`. Indeed, this example comes from an actual news article in El Pas about a female professor and was mistranslated as he in machine translation because of inaccurate coreference resolution (Schiebinger, 2013). ", "Bloom_type": "comprehension", "question": "Explain how coreference plays a role in machine translation systems when dealing with pronoun resolution?", "options": ["Coreference ensures that pronouns are correctly replaced with their antecedents based on context.", "Coreference helps machines understand the gender of speakers.", "Coreference allows machines to translate sentences into different languages.", "Coreference prevents machines from understanding the meaning of sentences."], "complexity": 1}, {"id": 117, "context": "Alignment Knowing the minimum edit distance is useful for algorithms like finding potential spelling error corrections. But the edit distance algorithm is important in another way; with a small change, it can also provide the minimum cost alignment between two strings. Aligning two strings is useful throughout speech and language processing. In speech recognition, minimum edit distance alignment is used to compute the word error rate (Chapter 16). Alignment plays a role in machine translation, in which sentences in a parallel corpus (a corpus with a text in two languages) need to be matched to each other. ", "Bloom_type": "application", "question": "What does aligning two strings help achieve?", "options": ["Computing the minimum cost alignment between two strings", "Finding the shortest path between two points on a map", "Determining the most efficient route for delivery trucks", "Calculating the shortest time required for an athlete to complete a race"], "complexity": 2}, {"id": 118, "context": "The best way to evaluate the performance of a language model is to embed it in an application and measure how much the application improves. Such end-to-end evaluation is called extrinsic evaluation. Extrinsic evaluation is the only way to know if a particular improvement in the language model (or any component) is really going to help the task at hand. Thus for evaluating n-gram language models that are a component of some task like speech recognition or machine translation, we can compare the performance of two candidate language models by running the speech recognizer or machine translator twice, once with each language model, and seeing which gives the more accurate transcription. ", "Bloom_type": "application", "question": "What is the primary method used to assess the effectiveness of a language model?", "options": ["Running the model through an application and measuring improvements", "Comparing the accuracy of different components separately", "Analyzing the internal structure of the model", "Testing the model on unrelated datasets"], "complexity": 2}, {"id": 119, "context": "Language ID systems are trained on multilingual text, such as Wikipedia (Wikipedia text in 68 different languages was used in (Lui and Baldwin, 2011)), or newswire. To make sure that this multilingual text correctly reflects different regions, dialects, and socioeconomic classes, systems also add Twitter text in many languages geotagged to many regions (important for getting world English dialects from countries with large Anglophone populations like Nigeria or India), Bible and Quran translations, slang websites like Urban Dictionary, corpora of African American Vernacular English (Blodgett et al., 2016), and so on (Jurgens et al., 2017). ", "Bloom_type": "application", "question": "What is an example of how language ID systems incorporate diverse sources of data?", "options": ["Combining various types of texts including Wikipedia, Twitter, Bible, Quran, slang sites, and African American Vernacular English corpora.", "Using only Wikipedia text for translation purposes.", "Only using Twitter text for linguistic analysis.", "Focusing solely on regional dialects and socioeconomic class differences."], "complexity": 2}, {"id": 120, "context": "The bootstrap test (Efron and Tibshirani, 1993) can apply to any metric; from precision, recall, or F1 to the BLEU metric used in machine translation. The word bootstrapping refers to repeatedly drawing large numbers of samples with replacement (called bootstrap samples) from an original set. The intuition of the bootstrap test is that we can create many virtual test sets from an observed test set by repeatedly sampling from it. The method only makes the assumption that the sample is representative of the population. ", "Bloom_type": "application", "question": "What does the bootstrap test rely on for creating virtual test sets?", "options": ["Samples drawn from the original set with replacement", "The original test set itself", "Randomly generated data points", "Using all available data"], "complexity": 2}, {"id": 121, "context": "Semantic Frames and Roles Closely related to semantic fields is the idea of a semantic frame. A semantic frame is a set of words that denote perspectives or participants in a particular type of event. A commercial transaction, for example, is a kind of event in which one entity trades money to another entity in return for some good or service, after which the good changes hands or perhaps the service is performed. This event can be encoded lexically by using verbs like buy (the event from the perspective of the buyer), sell (from the perspective of the seller), pay (focusing on the monetary aspect), or nouns like buyer. Frames have semantic roles (like buyer, seller, goods, money), and words in a sentence can take on these roles. Knowing that buy and sell have this relation makes it possible for a system to know that a sentence like Sam bought the book from Ling could be paraphrased as Ling sold the book to Sam, and that Sam has the role of the buyer in the frame and Ling the seller. Being able to recognize such paraphrases is important for question answering, and can help in shifting perspective for machine translation. ", "Bloom_type": "application", "question": "What is an essential step in developing a semantic framework?", "options": ["Identifying semantic roles and their corresponding verbs", "Creating a list of all possible sentences involving those roles", "Determining the monetary aspects involved in each transaction", "Translating the original language into a target language"], "complexity": 2}, {"id": 122, "context": "RNN-based language models can also be used to generate text. Text generation is of enormous practical importance, part of tasks like question answering, machine translation, text summarization, grammar correction, story generation, and conversational dialogue; any task where a system needs to produce text, conditioned on some other text. This use of a language model to generate text is one of the areas in which the impact of neural language models on NLP has been the largest. Text generation, along with image generation and code generation, constitute a new area of AI that is often called generative AI. ", "Bloom_type": "application", "question": "What is an example of using a language model for generating text?", "options": ["Using a language model to write stories based on prompts.", "Using a language model to translate between two languages.", "Using a language model to predict the next word in a sentence.", "Using a language model to summarize large amounts of text."], "complexity": 2}, {"id": 123, "context": "Technically an autoregressive model is a model that predicts a value at time t based on a linear function of the previous values at times t 2, and so on. Although language models are not linear (since they have many layers of non-linearities), we loosely refer to this generation technique as autoregressive generation since the word generated at each time step is conditioned on the word selected by the network from the previous step. Fig. 8.9 illustrates this approach. In this figure, the details of the RNN`s hidden layers and recurrent connections are hidden within the blue block. This simple architecture underlies state-of-the-art approaches to applications such as machine translation, summarization, and question answering. The key to these approaches is to prime the generation component with an appropriate context. That is, instead of simply using <s> to get things started we can provide a richer task-appropriate context; for translation the context is the sentence in the source language; for summarization it`s the long text we want to summarize. ", "Bloom_type": "application", "question": "What does the phrase 'prime the generation component' mean in the context of autoregressive models?", "options": ["It refers to giving initial conditions to the model.", "It means to start generating new data.", "It involves feeding random inputs to the model.", "It denotes stopping the current generation."], "complexity": 2}, {"id": 124, "context": "By contrast, encoder-decoder models are used especially for tasks like machine translation, where the input sequence and output sequence can have different lengths ", "Bloom_type": "application", "question": "What is an example of using encoder-decoder models for a task with variable-length inputs?", "options": ["Translating between languages", "Recognizing handwritten digits", "Summarizing news articles", "Generating poetry"], "complexity": 2}, {"id": 125, "context": "Encoder-decoder networks, sometimes called sequence-to-sequence networks, are models capable of generating contextually appropriate, arbitrary length, output sequences given an input sequence. Encoder-decoder networks have been applied to a very wide range of applications including summarization, question answering, and dialogue, but they are particularly popular for machine translation. ", "Bloom_type": "application", "question": "What is the primary function of encoder-decoder networks?", "options": ["To translate between languages", "To encode textual data only", "To decode textual data only", "To summarize large amounts of text"], "complexity": 2}, {"id": 126, "context": "We only have to make one slight change to turn this language model with autoregressive generation into an encoder-decoder model that is a translation model that can translate from a source text in one language to a target text in a second: add a sentence separation marker at the end of the source text, and then simply concatenate the target text. ", "Bloom_type": "application", "question": "What is the first step in translating from a source text in one language to a target text in another?", "options": ["Add a sentence separation marker at the end of the source text", "Concatenate the target text", "Identify the sentence structure of the source text", "Choose the appropriate translation for each sentence"], "complexity": 2}, {"id": 127, "context": "Transformer-based language models are complex, and so the details will unfold over the next 5 chapters. In the next sections we`ll introduce multi-head attention, the rest of the transformer block, and the input encoding and language modeling head components. Chapter 10 discusses how language models are pretrained, and how tokens are generated via sampling. Chapter 11 introduces masked language modeling and the BERT family of bidirectional transformer encoder models. Chapter 12 shows how to prompt LLMs to perform NLP tasks by giving instructions and demonstrations, and how to align the model with human preferences. Chapter 13 will introduce machine translation with the encoder-decoder architecture. ", "Bloom_type": "application", "question": "What is the first step in developing a machine translation system using an encoder-decoder architecture?", "options": ["Building the entire encoder-decoder model including both encoder and decoder parts.", "Designing the neural network layers for the decoder part only.", "Training the model on large datasets of source and target languages.", "Implementing the backpropagation algorithm to adjust weights during training."], "complexity": 2}, {"id": 128, "context": "able performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots. ", "Bloom_type": "application", "question": "What is the primary benefit of using transformers in NLP?", "options": ["Transformers excel at understanding and generating human-like text.", "They can only perform numerical calculations.", "Transformers are less accurate than traditional models.", "Transformers require more data to train."], "complexity": 2}, {"id": 129, "context": "Large language models are mainly trained on text scraped from the web, augmented by more carefully curated data. Because these training corpora are so large, they are likely to contain many natural examples that can be helpful for NLP tasks, such as question and answer pairs (for example from FAQ lists), translations of sentences between various languages, documents together with their summaries, and so on. ", "Bloom_type": "application", "question": "What is an effective way to improve translation quality?", "options": ["Combine both machine and human-generated translations for better accuracy.", "Only rely on machine translation tools.", "Use only human-translated texts for reference.", "Focus solely on translating existing content without creating new material."], "complexity": 2}, {"id": 130, "context": "Large pretrained neural language models exhibit many of the potential harms discussed in Chapter 4 and Chapter 6. Many of these harms become realized when pretrained language models are used for any downstream task, particularly those involving text generation, whether question answering, machine translation, or in assistive technologies like writing aids or web search query completion, or predictive typing for email (Olteanu et al., 2020). ", "Bloom_type": "application", "question": "What is an example of how large pretrained neural language models can lead to unintended consequences?", "options": ["They can improve accuracy in specific tasks but harm overall fairness.", "They can cause grammatical errors in written documents.", "They can produce biased results due to training data biases.", "They can enhance user experience with personalized recommendations."], "complexity": 2}, {"id": 131, "context": " Many NLP taskssuch as question answering, summarization, sentiment, and machine translationcan be cast as tasks of word prediction and hence addressed with Large language models. ", "Bloom_type": "application", "question": "What is a common approach used for addressing large language models in various natural language processing (NLP) tasks?", "options": ["Machine Translation", "Translation", "Summarization", "Sentiment analysis"], "complexity": 2}, {"id": 132, "context": "N-gram language models were very widely used over the next 30 years and more, across a wide variety of NLP tasks like speech recognition and machine translations, often as one of multiple components of the model. The contexts for these n-gram models grew longer, with 5-gram models used quite commonly by very efficient LM toolkits (Stolcke, 2002; Heafield, 2011). ", "Bloom_type": "application", "question": "What is the primary purpose of using N-gram language models?", "options": ["To improve the accuracy of natural language processing tasks such as translation.", "To reduce the computational complexity of training large language models.", "To increase the efficiency of neural network architectures.", "To enhance the speed of data transmission."], "complexity": 2}, {"id": 133, "context": "If we want to solve general tasks like summarization or translation, we don`t want to have to create a new prompt each time we do the task. Instead the first step in prompting is to design one or more templates: task-specific prompting text along with slots for the particular input that is being processed. ", "Bloom_type": "application", "question": "What should be done before creating prompts for solving general tasks?", "options": ["Design templates based on specific inputs.", "Create new prompts for every task.", "Use existing templates for all tasks.", "Develop a common template for various tasks."], "complexity": 2}, {"id": 134, "context": "But more generally, the best way to select demonstrations from the training set is programmatically: choosing the set of demonstrations that most increases task performance of the prompt on a test set. Task performance for sentiment analysis or multiple-choice question answering can be measured in accuracy; for machine translation with chrF, and for summarization via Rouge. Systems like DSPy (Khattab et al., 2024), a framework for algorithmically optimizing LM prompts, can automatically find the optimum set of demonstrations to include by searching through the space of possible demonstrations to include. We`ll return to automatic prompt optimization in Section 12.5. ", "Bloom_type": "application", "question": "What method should be used to optimize the selection of demonstrations for improving task performance?", "options": ["Use an automated system to search for optimal demonstrations", "Manually choose each demonstration", "Randomly select demonstrations", "Ignore the task performance and focus on other metrics"], "complexity": 2}, {"id": 135, "context": "Instruction tuning (short for instruction finetuning, and sometimes even shortened to instruct tuning) is a method for making an LLM better at following instructions. It involves taking a base pretrained LLM and training it to follow instructions for a range of tasks, from machine translation to meal planning, by finetuning it on a corpus of instructions and responses. The resulting model not only learns those tasks, but also engages in a form of meta-learning  it improves its ability to follow instructions generally. ", "Bloom_type": "application", "question": "What is the main purpose of instruction tuning?", "options": ["To improve the LLM's performance in specific task domains such as translation.", "To make the LLM more efficient in processing large amounts of data.", "To enhance the LLM's capability to understand natural language.", "To increase the LLM's capacity to learn new languages."], "complexity": 2}, {"id": 136, "context": "Many huge instruction tuning datasets have been created, covering many tasks and languages. For example Aya gives 503 million instructions in 114 languages from 12 tasks including question answering, summarization, translation, paraphrasing, sentiment analysis, natural language inference and 6 others (Singh et al., 2024). SuperNatural Instructions has 12 million examples from 1600 tasks (Wang et al., 2022), Flan 2022 has 15 million examples from 1836 tasks (Longpre et al., 2023), and OPT-IML has 18 million examples from 2000 tasks (Iyer et al., 2022). ", "Bloom_type": "application", "question": "Which of the following is an example of a task covered by the instruction tuning datasets mentioned?", "options": ["Translation", "Image classification", "Object detection", "Video segmentation"], "complexity": 2}, {"id": 137, "context": "Developing high quality supervised training data in this way is time consuming and costly. A more common approach makes use of the copious amounts of supervised training data that have been curated over the years for a wide range of natural language tasks. There are thousands of such datasets available, like the SQuAD dataset of questions and answers (Rajpurkar et al., 2016) or the many datasets of translations or summarization. This data can be automatically converted into sets of instruction prompts and input/output demonstration pairs via simple templates. ", "Bloom_type": "application", "question": "What is an alternative method to develop high-quality supervised training data?", "options": ["Using pre-existing instruction prompts and input/output demonstrations", "Manual labeling of every possible scenario", "Collecting existing datasets directly", "Creating new datasets through complex algorithms"], "complexity": 2}, {"id": 138, "context": "Given access to labeled training data, candidate prompts can be scored based on execution accuracy (Honovich et al., 2023). In this approach, candidate prompts are combined with inputs sampled from the training data and passed to an LLM for decoding. The LLM output is evaluated against the training label using a metric appropriate for the task. In the case of classification-based tasks, this is effectively a 0/1 loss  how many examples were correctly labeled with the given prompt. Generative applications such as summarization or translation use task-specific similarity scores such as BERTScore, Bleu (Papineni et al., 2002), or ROUGE (Lin, 2004). ", "Bloom_type": "application", "question": "What is the primary method used to evaluate the performance of a generative model like a language model?", "options": ["Through a combination of both A) and C)", "Using a fixed set of predefined labels", "By comparing the generated output directly to human-written responses", "By measuring the similarity between the generated text and reference texts"], "complexity": 2}, {"id": 139, "context": "Language models are evaluated in many ways. we introduced some evaluations for in Section 10.4, including measuring the language model`s perplexity on a test set, evaluating its accuracy on various NLP tasks, as well as benchmarks that help measure efficiency, toxicity, fairness, and so on. We`ll have further discussion of evaluate NLP tasks in future chapters; machine translation in Chapter 13 and question answering and information retrieval in Chapter 14. ", "Bloom_type": "application", "question": "What is another way to evaluate a language model besides using perplexity?", "options": ["Benchmarking based on efficiency, toxicity, and fairness", "Using accuracy on specific NLP tasks", "Analyzing the model's performance through code execution speed", "Comparing the model's output against human translations"], "complexity": 2}, {"id": 140, "context": "In this second part of the book we introduce fundamental NLP applications: machine translation, information retrieval, question answering, dialogue systems, and speech recognition. ", "Bloom_type": "application", "question": "Which application is not directly related to natural language processing (NLP) as mentioned in the context?", "options": ["Information Retrieval", "Machine Translation", "Speech Recognition", "Dialogue Systems"], "complexity": 2}, {"id": 141, "context": "This chapter introduces machine translation (MT), the use of computers to translate from one language to another. ", "Bloom_type": "application", "question": "What is the first step in developing an MT system?", "options": ["Training the computer on large datasets of bilingual texts", "Selecting appropriate languages for translation", "Designing user interfaces for human translators", "Implementing basic algorithms for word substitution"], "complexity": 2}, {"id": 142, "context": "Machine translation in its present form therefore focuses on a number of very practical tasks. Perhaps the most common current use of machine translation is for information access. We might want to translate some instructions on the web, perhaps the recipe for a favorite dish, or the steps for putting together some furniture. Or we might want to read an article in a newspaper, or get information from an online resource like Wikipedia or a government webpage in some other language. MT for information access is probably one of the most common uses of NLP technology, and Google Translate alone (shown above) translates hundreds of billions of words a day between over 100 languages. Improvements in machine translation can thus help reduce what is often called the digital divide in information access: the fact that much more information is available in English and other languages spoken in wealthy countries. Web searches in English return much more information than searches in other languages, and online resources like Wikipedia are much larger in English and other higher-resourced languages. High-quality translation can help provide information to speakers of lower-resourced languages. ", "Bloom_type": "application", "question": "What is one of the primary applications of machine translation?", "options": ["Information retrieval", "Automated writing assistance", "Speech recognition", "Natural Language Processing"], "complexity": 2}, {"id": 143, "context": "Another common use of machine translation is to aid human translators. MT systems are routinely used to produce a draft translation that is fixed up in a post-editing phase by a human translator. This task is often called computer-aided translation or CAT. CAT is commonly used as part of localization: the task of adapting content or a product to a particular language community. ", "Bloom_type": "application", "question": "What does CAT stand for?", "options": ["Computer-Aided Translation", "Computer-Assisted Translation", "Computer-Advised Translation", "Computer-Analyzed Translation"], "complexity": 2}, {"id": 144, "context": "Fig. 13.2 shows examples of other word order differences. All of these word order differences between languages can cause problems for translation, requiring the system to do huge structural reorderings as it generates the output. ", "Bloom_type": "application", "question": "What is required when translating sentences with different word orders?", "options": ["The entire sentence structure needs to be reordered", "No changes needed", "Only minor adjustments are necessary", "Structural reordering is only necessary if the original language was English"], "complexity": 2}, {"id": 145, "context": "Of course we also need to translate the individual words from one language to another. For any translation, the appropriate word can vary depending on the context. The English source-language word bass, for example, can appear in Spanish as the fish lubina or the musical instrument bajo. German uses two distinct words for what in English would be called a wall: Wand for walls inside a building, and Mauer for walls outside a building. Where English uses the word brother for any male sibling, Chinese and many other languages have distinct words for older brother and younger brother (Mandarin gege and didi, respectively). In all these cases, translating bass, wall, or brother from English would require a kind of specialization, disambiguating the different uses of a word. For this reason the fields of MT and Word Sense Disambiguation (Appendix G) are closely linked. ", "Bloom_type": "application", "question": "What is required when translating words with different meanings?", "options": ["Specialize in each context to avoid confusion", "Use the same word across all contexts", "Ignore the differences and use the most common meaning", "Translate only the literal meaning"], "complexity": 2}, {"id": 146, "context": "Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. Large numbers of parallel corpora are available. Some are governmental; the Europarl corpus (Koehn, 2005), extracted from the proceedings of the European Parliament, contains between 400,000 and 2 million sentences each from 21 European languages. The United Nations Parallel Corpus contains on the order of 10 million sentences in the six official languages of the United Nations (Arabic, Chinese, English, French, Russian, Spanish) Ziemski et al. (2016). Other parallel corpora have been made from movie and TV subtitles, like the OpenSubtitles corpus (Lison and Tiedemann, 2016), or from general web text, like the ParaCrawl corpus of 223 million sentence pairs between 23 EU languages and English extracted from the CommonCrawl Banon et al. (2020). ", "Bloom_type": "application", "question": "Which type of parallel corpus is commonly used for training machine translation models?", "options": ["All of the above", "Governmental corpora", "Parallel corpora extracted from movies and TV subtitles", "Corpora created from general web text"], "complexity": 2}, {"id": 147, "context": " a cost function that takes a span of source sentences and a span of target sentences and returns a score measuring how likely these spans are to be translations. ", "Bloom_type": "application", "question": "What is the first step in developing an efficient translation model?", "options": ["Implement a scoring mechanism based on the similarity between source and target sentences.", "Define the vocabulary size for both source and target languages.", "Choose a machine learning algorithm suitable for translation tasks.", "Create a dataset with parallel source-target pairs."], "complexity": 2}, {"id": 148, "context": "Thus at the first step of decoding, we compute a softmax over the entire vocabulary, assigning a probability to each word. We then select the k-best options from this softmax output. These initial k outputs are the search frontier and these k initial words are called hypotheses. A hypothesis is an output sequence, a translation-sofar, together with its probability. ", "Bloom_type": "application", "question": "What does the phrase 'k-best options' refer to?", "options": ["The k best hypotheses generated during the decoding process", "The top k most probable translations", "The k highest probabilities among all possible translations", "The k least likely translations"], "complexity": 2}, {"id": 149, "context": "For MT we generally use beam widths k between 5 and 10, giving us k hypotheses at the end. We can pass all k to the downstream application with their respective scores, or if we just need a single translation we can pass the most probable hypothesis. ", "Bloom_type": "application", "question": "What is the primary difference when passing hypotheses through an MT system?", "options": ["Passing only the most probable hypothesis reduces computational load.", "Passing all hypotheses gives more accurate translations.", "Beam width determines the number of hypotheses passed to the next stage.", "Beam width affects the accuracy of the final translation."], "complexity": 2}, {"id": 150, "context": "The intuition of minimum Bayes risk is that instead of trying to choose the translation which is most probable, we choose the one that is likely to have the least error. For example, we might want our decoding algorithm to find the translation which has the highest score on some evaluation metric. For example in Section 13.6 we will introduce metrics like chrF or BERTScore that measure the goodness-of-fit between a candidate translation and a set of reference human translations. A translation that maximizes this score, especially with a hypothetically huge set of perfect human translations is likely to be a good one (have minimum risk) even if it is not the most probable translation by our particular probability estimator. ", "Bloom_type": "application", "question": "Which strategy for choosing a translation aims at minimizing the expected loss rather than maximizing the likelihood?", "options": ["Minimize the expected loss", "Maximize the likelihood of the translation", "Choose the most probable translation", "Select the translation with the highest evaluation score"], "complexity": 2}, {"id": 151, "context": "In practice, we don`t know the perfect set of translations for a given sentence. So the standard simplification used in MBR decoding algorithms is to instead choose the candidate translation which is most similar (by some measure of goodness-offit) with some set of candidate translations. We`re essentially approximating the enormous space of all possible translations U with a smaller set of possible candidate translations Y. ", "Bloom_type": "application", "question": "What is the main idea behind using a simplified set of candidate translations when dealing with a large set of potential translations?", "options": ["To reduce computational time and resources needed for processing", "To increase the complexity of the algorithm", "To ensure every single translation is considered equally important", "To make sure the translation is as accurate as possible"], "complexity": 2}, {"id": 152, "context": "Minimum Bayes risk decoding can also be used for other NLP tasks; indeed it was widely applied to speech recognition (Stolcke et al., 1997; Goel and Byrne, 2000) before being applied to machine translation (Kumar and Byrne, 2004), and has been shown to work well across many other generation tasks as well (e.g., summarization, dialogue, and image captioning (Suzgun et al., 2023a)). ", "Bloom_type": "application", "question": "What is an example of a task besides machine translation where minimum Bayes risk decoding has been successfully applied?", "options": ["Image captioning", "Speech recognition", "Dialogue", "Summarization"], "complexity": 2}, {"id": 153, "context": "For some languages, and especially for English, online resources are widely available. There are many large parallel corpora that contain translations between English and many languages. But the vast majority of the world`s languages do not have large parallel training texts available. An important ongoing research question is how to get good translation with lesser resourced languages. The resource problem can even be true for high resource languages when we need to translate into low resource domains (for example in a particular genre that happens to have very little bitext). ", "Bloom_type": "application", "question": "What strategy might researchers employ to improve translations for less resourceful languages?", "options": ["Develop techniques to leverage limited data effectively.", "Increase the number of parallel corpora available for all languages.", "Focus solely on translating into high-resource languages.", "Ignore the issue as it cannot be resolved."], "complexity": 2}, {"id": 154, "context": "The most common data augmentation technique for machine translation is called backtranslation. Backtranslation relies on the intuition that while parallel corpora may be limited for particular languages or domains, we can often find a large (or at least larger) monolingual corpus, to add to the smaller parallel corpora that are available. The algorithm makes use of monolingual corpora in the target language by creating synthetic bitexts. ", "Bloom_type": "application", "question": "What is the main difference between backtranslation and other data augmentation techniques?", "options": ["Backtranslation creates synthetic bitexts using monolingual corpora.", "Backtranslation uses only parallel corpora.", "Other data augmentation techniques rely solely on monolingual corpora.", "None of the above"], "complexity": 2}, {"id": 155, "context": "In backtranslation, our goal is to improve source-to-target MT, given a small parallel text (a bitext) in the source/target languages, and some monolingual data in the target language. We first use the bitext to train a MT system in the reverse direction: a target-to-source MT system . We then use it to translate the monolingual target data to the source language. Now we can add this synthetic bitext (natural target sentences, aligned with MT-produced source sentences) to our training data, and retrain our source-to-target MT model. For example suppose we want to translate from Navajo to English but only have a small Navajo-English bitext, although of course we can find lots of monolingual English data. We use the small bitext to build an MT engine going the other way (from English to Navajo). Once we translate the monolingual English text to Navajo, we can add this synthetic Navajo/English bitext to our training data. ", "Bloom_type": "application", "question": "What is the next step after building a target-to-source MT system using the bitext?", "options": ["Translate monolingual data to the target language using the built system", "Use the bitext to directly translate monolingual data", "Train another separate MT system for translation from source to target", "Add the translated monolingual data to the original training data"], "complexity": 2}, {"id": 156, "context": "Backtranslation has various parameters. One is how we generate the backtranslated data; we can run the decoder in greedy inference, or use beam search. Or we can do sampling, like the temperature sampling algorithm we saw in Chapter 9. Another parameter is the ratio of backtranslated data to natural bitext data; we can choose to upsample the bitext data (include multiple copies of each sentence). In general backtranslation works surprisingly well; one estimate suggests that a system trained on backtranslated text gets about 2/3 of the gain as would training on the same amount of natural bitext (Edunov et al., 2018). ", "Bloom_type": "application", "question": "Which method for generating backtranslated data involves running the decoder with greedy inference?", "options": ["Greedy inference", "Beam search", "Sampling", "Temperature sampling"], "complexity": 2}, {"id": 157, "context": "One advantage of a multilingual model is that they can improve the translation of lower-resourced languages by drawing on information from a similar language in the training data that happens to have more resources. Perhaps we don`t know the meaning of a word in Galician, but the word appears in the similar and higherresourced language Spanish. ", "Bloom_type": "application", "question": "What strategy might help a multilingual model understand less common words in a lesser-known language?", "options": ["Draw upon knowledge from a similar, more resource-rich language for reference.", "Ignore the unknown word entirely.", "Use machine learning algorithms to predict meanings based on patterns.", "Translate the entire sentence using the closest matching phrase."], "complexity": 2}, {"id": 158, "context": "The most accurate evaluations use human raters, such as online crowdworkers, to evaluate each translation along the two dimensions. For example, along the dimension of fluency, we can ask how intelligible, how clear, how readable, or how natural the MT output (the target text) is. We can give the raters a scale, for example, from 1 (totally unintelligible) to 5 (totally intelligible), or 1 to 100, and ask them to rate each sentence or paragraph of the MT output. ", "Bloom_type": "application", "question": "What method should be used to assess the quality of translations based on their readability?", "options": ["Assign a numerical score between 1 and 5 based on clarity.", "Use an absolute scale from 1 to 100.", "Rate the fluency using a subjective judgment.", "Evaluate the comprehensibility with a binary classification."], "complexity": 2}, {"id": 159, "context": "We can do the same thing to judge the second dimension, adequacy, using raters to assign scores on a scale. If we have bilingual raters, we can give them the source sentence and a proposed target sentence, and rate, on a 5-point or 100-point scale, how much of the information in the source was preserved in the target. If we only have monolingual raters but we have a good human translation of the source text, we can give the monolingual raters the human reference translation and a target machine translation and again rate how much information is preserved. An alternative is to do ranking: give the raters a pair of candidate translations, and ask them which one they prefer. ", "Bloom_type": "application", "question": "What method could be used to evaluate the quality of a translation?", "options": ["Ask the raters to compare two different translations side by side.", "Give the raters the original text and ask for their opinion.", "Provide the raters with a single translation and ask them to rate it.", "Have the raters translate the text themselves."], "complexity": 2}, {"id": 160, "context": "As discussed above, an alternative way of using human raters is to have them post-edit translations, taking the MT output and changing it minimally until they feel it represents a correct translation. The difference between their post-edited translations and the original MT output can then be used as a measure of quality. ", "Bloom_type": "application", "question": "What is the first step in evaluating the quality of machine-translated texts?", "options": ["Have human raters post-edit the translations", "Compare the translated text with the original text", "Analyze the accuracy of the machine translation", "Identify areas for improvement in the machine translation model"], "complexity": 2}, {"id": 161, "context": "The simplest and most robust metric for MT evaluation is called chrF, which stands for character F-score (Popovic, 2015). chrF (along with many other earlier related metrics like BLEU, METEOR, TER, and others) is based on a simple intuition derived from the pioneering work of Miller and Beebe-Center (1956): a good machine translation will tend to contain characters and words that occur in a human translation of the same sentence. Consider a test set from a parallel corpus, in which each source sentence has both a gold human target translation and a candidate MT translation we`d like to evaluate. The chrF metric ranks each MT target sentence by a function of the number of character n-gram overlaps with the human translation. ", "Bloom_type": "application", "question": "Which method should be used to calculate the chrF score?", "options": ["Calculate the overlap between all possible character n-grams of the MT target and the human translation.", "Count only the exact matches between the MT target and the human translation.", "Use a weighted average of the overlap scores of different character n-grams.", "Ignore the human translation and focus solely on the MT target."], "complexity": 2}, {"id": 162, "context": "There are various alternative overlap metrics. For example, before the development of chrF, it was common to use a word-based overlap metric called BLEU (for BiLingual Evaluation Understudy), that is purely precision-based rather than combining precision and recall (Papineni et al., 2002). The BLEU score for a corpus of candidate translation sentences is a function of the n-gram word precision over all the sentences combined with a brevity penalty computed over the corpus as a whole. ", "Bloom_type": "application", "question": "What type of overlap metric did Papineni et al. mention being used before the development of chrF?", "options": ["Word-based overlap metric", "Sentence-based overlap metric", "Character-based overlap metric", "Phrase-based overlap metric"], "complexity": 2}, {"id": 163, "context": "Because BLEU is a word-based metric, it is very sensitive to word tokenization, making it impossible to compare different systems if they rely on different tokenization standards, and doesn`t work as well in languages with complex morphology. Nonetheless, you will sometimes still see systems evaluated by BLEU, particularly for translation into English. In such cases it`s important to use packages that enforce standardization for tokenization like SACREBLEU (Post, 2018). ", "Bloom_type": "application", "question": "What should you do when evaluating a system that relies on different tokenization standards?", "options": ["Use a package that enforces standardization for tokenization", "Ignore the differences and evaluate directly", "Rely solely on the BLEU score", "Compare the results of different systems"], "complexity": 2}, {"id": 164, "context": "For example, in some situations we might have datasets that have human assessments of translation quality. Such datasets consists of tuples (x, x, r), where x = (x1, . . . , xn) is a reference translation, x = ( x1, . . . , xm) is a candidate machine translation, and r R is a human rating that expresses the quality of x with respect to x. Given such data, algorithms like COMET (Rei et al., 2020) BLEURT (Sellam et al., 2020) train a predictor on the human-labeled datasets, for example by passing x and x through a version of BERT (trained with extra pretraining, and then finetuned on the human-labeled sentences), followed by a linear layer that is trained to predict r. The output of such models correlates highly with human labels. ", "Bloom_type": "application", "question": "What method should be used first when training an algorithm like COMET?", "options": ["Passing x and x through a version of BERT", "Training a predictor on the human-labeled datasets", "Finetuning the model on the human-labeled sentences", "Using a linear layer to predict r"], "complexity": 2}, {"id": 165, "context": "Many ethical questions in MT require further research. One open problem is developing metrics for knowing what our systems don`t know. This is because MT systems can be used in urgent situations where human translators may be unavailable or delayed: in medical domains, to help translate when patients and doctors don`t speak the same language, or in legal domains, to help judges or lawyers communicate with witnesses or defendants. In order to do no harm`, systems need ways to assign confidence values to candidate translations, so they can abstain from giving incorrect translations that may cause harm. ", "Bloom_type": "application", "question": "What is an important step in ensuring that machine translation (MT) systems are not harmful?", "options": ["Assigning confidence levels to translations", "Developing metrics for translating objects", "Creating algorithms for human translators", "Training models on diverse datasets"], "complexity": 2}, {"id": 166, "context": "Machine translation is one of the most widely used applications of NLP, and the encoder-decoder model, first developed for MT is a key tool that has applications throughout NLP. ", "Bloom_type": "application", "question": "What was the initial application of the encoder-decoder model before it became popular in machine translation?", "options": ["Speech recognition", "Natural language understanding", "Sentiment analysis", "Machine learning"], "complexity": 2}, {"id": 167, "context": " Machine translation models are trained on a parallel corpus, sometimes called ", "Bloom_type": "application", "question": "What is another name for the type of data used to train machine translation models?", "options": ["Parallel dataset", "Translation set", "Corpus pair", "Interlingual sample"], "complexity": 2}, {"id": 168, "context": " Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.  MT is evaluated by measuring a translation`s adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used. ", "Bloom_type": "application", "question": "What is backtranslation?", "options": ["It involves translating sentences backward using a trained machine learning model.", "It is a method for improving machine translation quality.", "It refers to creating synthetic bitexts by running an MT engine forward.", "It is the process of evaluating machine translation systems solely through human feedback."], "complexity": 2}, {"id": 169, "context": "In the early years, the space of MT architectures spanned three general models. In direct translation, the system proceeds word-by-word through the sourcelanguage text, translating each word incrementally. Direct translation uses a large bilingual dictionary, each of whose entries is a small program with the job of translating one word. In transfer approaches, we first parse the input text and then apply rules to transform the source-language parse into a target language parse. We then generate the target language sentence from the parse tree. In interlingua approaches, we analyze the source language text into some abstract meaning representation, called an interlingua. We then generate into the target language from this interlingual representation. A common way to visualize these three early approaches was the Vauquois triangle shown in Fig. 13.13. The triangle shows the increasing depth of analysis required (on both the analysis and generation end) as we move from the direct approach through transfer approaches to interlingual approaches. In addition, it shows the decreasing amount of transfer knowledge needed as we move up the triangle, from huge amounts of transfer at the direct level (almost all knowledge is transfer knowledge for each word) through transfer (transfer rules only for parse trees or thematic roles) through interlingua (no specific transfer knowledge). We can view the encoder-decoder network as an interlingual approach, with attention acting as an integration of direct and transfer, allowing words or their representations to be directly accessed by the decoder. ", "Bloom_type": "application", "question": "Which type of machine translation architecture involves analyzing the source language text into an abstract meaning representation before generating the target language sentence?", "options": ["Interlingua approaches", "Direct translation", "Transfer approaches", "Encoder-Decoder networks"], "complexity": 2}, {"id": 170, "context": "By the turn of the century, most academic research on machine translation used statistical MT, either in the generative or discriminative mode. An extended version of the generative approach, called phrase-based translation was developed, based on inducing translations for phrase-pairs (Och 1998, Marcu and Wong 2002, Koehn et al. (2003), Och and Ney 2004, Deng and Byrne 2005, inter alia). ", "Bloom_type": "application", "question": "Which methodological development followed the original generative approach in phrase-based translation?", "options": ["Statistical MT", "Rule-based translation", "Hybrid approaches", "Dependency parsing"], "complexity": 2}, {"id": 171, "context": "Neural networks had been applied at various times to various aspects of machine translation; for example Schwenk et al. (2006) showed how to use neural language models to replace n-gram language models in a Spanish-English system based on IBM Model 4. The modern neural encoder-decoder approach was pioneered by Kalchbrenner and Blunsom (2013), who used a CNN encoder and an RNN decoder, and was first applied to MT by Bahdanau et al. (2015). The transformer encoderdecoder was proposed by Vaswani et al. (2017) (see the History section of Chapter 9). ", "Bloom_type": "application", "question": "What is the key difference between the traditional neural network-based approaches and the modern transformer architecture?", "options": ["Transformers use attention mechanisms which allow them to focus on different parts of the input sequence during processing.", "Traditional methods rely solely on LSTM units, while transformers use GRU units.", "Transformers are more computationally efficient than traditional methods.", "Transformer architectures require less data preprocessing compared to traditional methods."], "complexity": 2}, {"id": 172, "context": "The encoder-decoder architecture was applied to speech at about the same time by two different groups, in the Listen Attend and Spell system of Chan et al. (2016) and the attention-based encoder decoder architecture of Chorowski et al. (2014) and Bahdanau et al. (2016). By 2018 Transformers were included in this encoderdecoder architecture. Karita et al. (2019) is a nice comparison of RNNs vs Transformers in encoder-architectures for ASR, TTS, and speech-to-speech translation. ", "Bloom_type": "application", "question": "What architectural change led to the inclusion of transformers in encoder-decoder architectures?", "options": ["The implementation of transformer models", "The introduction of recurrent neural networks (RNNs)", "The development of convolutional neural networks (CNNs)", "The application of autoencoders"], "complexity": 2}, {"id": 173, "context": "Automatic parsing using dependency grammars was first introduced into computational linguistics by early work on machine translation at the RAND Corporation led by David Hays. This work on dependency parsing closely paralleled work on constituent parsing and made explicit use of grammars to guide the parsing process. After this early period, computational work on dependency parsing remained intermittent over the following decades. Notable implementations of dependency parsers for English during this period include Link Grammar (Sleator and Temperley, 1993), Constraint Grammar (Karlsson et al., 1995), and MINIPAR (Lin, 2003). ", "Bloom_type": "application", "question": "What is an example of a dependency parser used for English?", "options": ["Link Grammar", "Constraint Grammar", "MINIPAR", "Dependency Parsing"], "complexity": 2}, {"id": 174, "context": "The main reason computational systems use semantic roles is to act as a shallow meaning representation that can let us make simple inferences that aren`t possible from the pure surface string of words, or even from the parse tree. To extend the earlier examples, if a document says that Company A acquired Company B, we`d like to know that this answers the query Was Company B acquired? despite the fact that the two sentences have very different surface syntax. Similarly, this shallow semantics might act as a useful intermediate language in machine translation. ", "Bloom_type": "application", "question": "What is the primary purpose of using semantic roles in computational systems?", "options": ["To facilitate deeper understanding and inference beyond literal word meanings.", "To enhance the accuracy of natural language processing algorithms.", "To improve the efficiency of data storage and retrieval.", "To reduce the complexity of parsing complex sentence structures."], "complexity": 2}, {"id": 175, "context": "Coreference is an important component of natural language processing. A dialogue system that has just told the user There is a 2pm flight on United and a 4pm one on Cathay Pacific must know which flight the user means by I`ll take the second one. A question answering system that uses Wikipedia to answer a question about Marie Curie must know who she was in the sentence She was born in Warsaw. And a machine translation system translating from a language like Spanish, in which pronouns can be dropped, must use coreference from the previous sentence to decide whether the Spanish sentence Me encanta el conocimiento, dice.` should be translated as I love knowledge, he says`, or I love knowledge, she says`. Indeed, this example comes from an actual news article in El Pas about a female professor and was mistranslated as he in machine translation because of inaccurate coreference resolution (Schiebinger, 2013). ", "Bloom_type": "application", "question": "In a machine translation system, how does coreference help resolve ambiguity?", "options": ["It aids in understanding the meaning of pronouns.", "It helps identify synonyms.", "It assists in recognizing antonyms.", "It enables the detection of homophones."], "complexity": 2}]}}, "require_validation": [{"term": "seed word", "context": "Another common way to learn sentiment lexicons is to start from a set of seed words that define two poles of a semantic axis (words like good or bad), and then find ways to label each word w by its similarity to the two seed sets. Here we summarize two families of seed-based semi-supervised lexicon induction algorithms, axis-based and graph-based. ", "Bloom_type": "remember", "question_body": "{\n    'Question': 'In the process of learning sentiment lexicons, what are used as starting points?',\n    'Options':\n    {\r\n        'A)' 'Sentiment labels',\r\n        'B)' 'Seed words',\r\n        'C)' 'Lexical units',\r\n        'D)' 'Context clues'\r\n    },\r\n    'Correct answer': 'B)'\n}"}, {"term": "dialogue system", "context": "Another important ethical issue is privacy. Already in the first days of ELIZA, Weizenbaum pointed out the privacy implications of people`s revelations to the chatbot. The ubiquity of in-home dialogue systems means they may often overhear private information (Henderson et al., 2017). If a chatbot is human-like, users are also more likely to disclose private information, and less likely to worry about the harm of this disclosure (Ischen et al., 2019). In general, chatbots that are trained on transcripts of human-human or human-machine conversation must anonymize personally identifiable information. ", "Bloom_type": "remember", "question_body": "{\n    'Question': 'What does the term \"dialogue system\" refer to?',\n    'Options': \n    {\n        'A)': 'A computer program designed for communication with humans',\n        'B)': 'A type of software used for data encryption',\n        'C)': 'An artificial intelligence model for language translation',\n        'D)': 'A network protocol for secure online transactions'\n    },\n    'Correct answer': ''\n}"}, {"term": "neural network", "context": "Finally, most modern neural networks are built using computation graph formalisms that make it easy and natural to do gradient computation and parallelization on vector-based GPUs (Graphic Processing Units). PyTorch (Paszke et al., 2017) and TensorFlow (Abadi et al., 2015) are two of the most popular. The interested reader should consult a neural network textbook for further details; some suggestions are at the end of the chapter. ", "Bloom_type": "remember", "question_body": "{\n    'Question': 'Which two software libraries are widely used for building modern neural networks?',\n    'Options':\n    {\n        'A)': 'MATLAB and R'\n        'B)': 'Python and Java'\n        'C)': 'R and Python'\n        'D)': 'Java and C++'\n    },\n    'Correct answer': 'C)'\n}"}, {"term": "question answering", "context": "People need to know things. So pretty much as soon as there were computers we were asking them questions. Systems in the 1960s were answering questions about baseball statistics and scientific facts. Even fictional computers in the 1970s like Deep Thought, invented by Douglas Adams in The Hitchhiker`s Guide to the Galaxy, answered the Ultimate Question Of Life, The Universe, and Everything.1 And because so much knowledge is encoded in text, question answering (QA) systems were performing at human levels even before LLMs: IBM`s Watson system won the TV game-show Jeopardy! in 2011, surpassing humans at answering questions like: ", "Bloom_type": "remember", "question_body": "{\n    'Question': 'In what year did IBM's Watson system win the TV game show Jeopardy!',',\n    'Options': \n    {\n        'A)': '1985',\n        'B)': '1995',\n        'C)': '2005',\n        'D)': '2011'\n    },\n    'Correct answer': 'D)'\n}"}, {"term": "loss function", "context": "In an actual logistic regression, the parameter vector w is much longer than 1 or 2, since the input feature vector x can be quite long, and we need a weight wi for each xi. For each dimension/variable wi in w (plus the bias b), the gradient will have a component that tells us the slope with respect to that variable. In each dimension wi, we express the slope as a partial derivative  of the loss function. Essentially  wi we`re asking: How much would a small change in that variable wi influence the total loss function L? ", "Bloom_type": "remember", "question_body": "{\n    'Question': 'In logistic regression, why does the parameter vector \\( \\mathbf{w} \\) often become very large compared to 1 or 2?',\n    'Options': \n    {\n        'A)': 'Because the number of features \\( x_i \\)'s are typically larger than the number of parameters.',\n        'B)': 'To ensure all variables contribute equally to the model.',\n        'C)': 'To minimize the complexity of the model.',\n        'D)': 'To increase computational efficiency by reducing the size of the dataset.'\n    },\n    'Correct answer': 'A)'\n}"}, {"term": "coreference", "context": "Much of natural language processing is concerned with measuring how similar two strings are. For example in spelling correction, the user typed some erroneous stringlet`s say graffeand we want to know what the user meant. The user probably intended a word that is similar to graffe. Among candidate similar words, the word giraffe, which differs by only one letter from graffe, seems intuitively to be more similar than, say grail or graf, which differ in more letters. Another example comes from coreference, the task of deciding whether two strings such as the following refer to the same entity: ", "Bloom_type": "remember", "question_body": "{\n    'Question': 'In natural language processing, why is it important to measure similarity between strings?',\n    'Options': \n    {\n        'A)': 'To determine if a user's input was correctly spelled',\n        'B)': 'To identify synonyms for different words',\n        'C)': 'To find similar words based on their structure',\n        'D)': 'To understand the meaning behind pronouns'\n    },\n    'Correct answer': 'A)'\n}"}, {"term": "coreference", "context": "In an attempt to get the field of NLP to focus more on methods involving world knowledge and common-sense reasoning, Levesque (2011) proposed a challenge task called the Winograd Schema Challenge.8 The problems in the challenge task are coreference problems designed to be easily disambiguated by the human reader, but hopefully not solvable by simple techniques such as selectional restrictions, or other basic word association methods. ", "Bloom_type": "remember", "question_body": "{\n    'Question': 'In what type of challenges does Levesque propose using coreference problems?',\n    'Options':\n    {\r\n        'A)' 'Logical puzzles',\r\n        'B)' 'Mathematical equations',\r\n        'C)' 'Geographical maps',\r\n        'D)' 'NLP tasks'\r\n    },\r\n    'Correct answer': 'D)'\n}"}, {"term": "dialogue", "context": "The dialogue above is from ELIZA, an early natural language processing system that could carry on a limited conversation with a user by imitating the responses of a Rogerian psychotherapist (Weizenbaum, 1966). ELIZA is a surprisingly simple program that uses pattern matching to recognize phrases like I need X and translate them into suitable outputs like What would it mean to you if you got X?. This simple technique succeeds in this domain because ELIZA doesn`t actually need to know anything to mimic a Rogerian psychotherapist. As Weizenbaum notes, this is one of the few dialogue genres where listeners can act as if they know nothing of the world. ELIZA`s mimicry of human conversation was remarkably successful: many people who interacted with ELIZA came to believe that it really understood them and their problems, many continued to believe in ELIZA`s abilities even after the program`s operation was explained to them (Weizenbaum, 1976), and even today such chatbots are a fun diversion. ", "Bloom_type": "remember", "question_body": "{\n    'Question': 'What aspect of ELIZA's success does the author attribute to its ability to mimic human conversation?',\n    'Options': \n    {\n        'A)': 'The use of complex algorithms',\n        'B)': 'Its ability to understand real-world knowledge',\n        'C)': 'Pattern matching techniques',\n        'D)': 'Listeners` belief in its understanding'\n    },\n    'Correct answer': 'C)'\n}"}, {"term": "translation", "context": "In practice, we don`t know the perfect set of translations for a given sentence. So the standard simplification used in MBR decoding algorithms is to instead choose the candidate translation which is most similar (by some measure of goodness-offit) with some set of candidate translations. We`re essentially approximating the enormous space of all possible translations U with a smaller set of possible candidate translations Y. ", "Bloom_type": "remember", "question_body": "{\n    'Question': 'In the context provided, what does the phrase \"we're essentially approximating the enormous space of all possible translations U with a smaller set of possible candidate translations Y\" mean?',\n    'Options': \n    {\n        'A)': 'The translations are exactly the same as the original.',\n        'B)': 'The translations are simplified versions of the original.',\n        'C)': 'The translations cover every single possibility.',\n        'D)': 'The translations are chosen randomly.'\n    },\n    'Correct answer': 'B)'\n}"}]}
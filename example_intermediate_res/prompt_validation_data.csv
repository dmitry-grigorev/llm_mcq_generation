,Term,Context,Remember_0shot,Remember_1shot,Comprehension_0shot,Comprehension_fewshot,Application_0shot,Application_1shot
0,translation,Another common use of machine translation is to aid human translators. MT systems are routinely used to produce a draft translation that is fixed up in a post-editing phase by a human translator. This task is often called computer-aided translation or CAT. CAT is commonly used as part of localization: the task of adapting content or a product to a particular language community. ,"{
    'Question': 'What does CAT stand for?',
    'Options':
    {
        'A)': 'Computer-Assisted Translation',
        'B)': 'Content-Aided Translation',
        'C)': 'Creative-Assisted Translation',
        'D)': 'Customized-Assigned Translation'
    },
    'Correct answer': 'A) Computer-Assisted Translation'
}","{
    'Question': ""In what process do machine translation systems typically assist human translators?"", 
    'Options': 
    {
        'A)': ""Translation proofreading"",
        'B)': ""Computer-assisted translation (CAT)"",
        'C)': ""Post-editing"",
        'D)': ""Machine learning training""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What does CAT stand for?',
    'Options': 
    {
        'A)': 'Computer-Assisted Translation',
        'B)': 'Computer-Aided Translation',
        'C)': 'Computer-Assisted Technology',
        'D)': 'Computer-Aided Technology'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What does CAT stand for?"",
    'Options': 
    {
        'A)': ""Computer-Assisted Translation"",
        'B)': ""Computer-Aided Translation"",
        'C)': ""Computer-Assisted Translation"",
        'D)': ""Computer-Aided Translation""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What does CAT stand for?',
    'Options': 
    {
        'A)': 'Computer-Assisted Translation',
        'B)': 'Computer-Aided Translation',
        'C)': 'Computer-Assisted Technology',
        'D)': 'Computer-Aided Technology'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What does CAT stand for?"", 
    'Options': 
    {
        'A)': ""Computer-Assisted Translation"",
        'B)': ""Computer-Aided Translation"",
        'C)': ""Computer-Assisted Technology"",
        'D)': ""Computer-Aided Technology""
    },
    'Correct answer': 'B)'
}"
1,question answering,"Let`s begin by introducing the bidirectional transformer encoder that underlies models like BERT and its descendants like RoBERTa (Liu et al., 2019) or SpanBERT (Joshi et al., 2020). In Chapter 9 we introduced causal (left-to-right) transformers and in Chapter 10 saw how they can serve as the basis for language models that can be applied to autoregressive contextual generation problems like question answering or summarization. But this left-to-right nature of these models is also a limitation, because there are tasks for which it would be useful, when processing a token, to be able to peak at future tokens. This is especially true for sequence labeling tasks ","{
    'Question': 'In what way does the left-to-right nature of transformers limit their application?',
    'Options': 
    {
        'A)': 'It restricts their ability to process future tokens.',
        'B)': 'It makes them unsuitable for all types of sequence labeling tasks.',
        'C)': 'It prevents them from being used for autoregressive generation tasks.',
        'D)': 'It limits their effectiveness in handling long sequences.'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""In what type of task is it beneficial to peek at future tokens while processing a token?"",
    'Options': 
    {
        'A)': ""Sequence classification"",
        'B)': ""Machine translation"",
        'C)': ""Question answering"",
        'D)': ""Sentiment analysis""
    },
    'Correct answer': 'C)'
}","{
    'Question': 'What is an example of a task where being able to peek at future tokens could be beneficial?',
    'Options': 
    {
        'A)': 'Summarizing long documents',
        'B)': 'Generating questions from a set of answers',
        'C)': 'Predicting the next word in a sentence',
        'D)': 'Classifying the sentiment of a tweet',
        'E)': 'Recognizing named entities in a paragraph'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""Explain why the left-to-right nature of transformers might limit their usefulness for certain types of tasks?"",
    'Options': 
    {
        'A)': ""Because they cannot handle long sequences efficiently."",
        'B)': ""Because they do not allow peeking into future tokens during processing."",
        'C)': ""Because they require more computational resources than right-to-left approaches."",
        'D)': ""Both B and C are correct.""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What type of model does not have a left-to-right structure?',
    'Options': 
    {
        'A)': 'Transformer Encoder',
        'B)': 'Causal Transformer',
        'C)': 'Left-to-Right Transformer',
        'D)': 'Right-to-Left Transformer'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What is an example of a task where peeking at future tokens could be beneficial?"", 
    'Options': 
    {
        'A)': ""Summarizing long documents"",
        'B)': ""Generating questions based on previous answers"",
        'C)': ""Classifying sentences into categories"",
        'D)': ""Predicting the next word in a sentence""
    },
    'Correct answer': 'B)'
}"
2,coherence relation,"Coherence relations arose from the independent development of a number of scholars, including Hobbs (1979) idea that coherence relations play an inferential role for the hearer, and the investigations by Mann and Thompson (1987) of the discourse structure of large texts. Other approaches to coherence relations and their extraction include Segmented Discourse Representation Theory (SDRT) (Asher and Lascarides 2003, Baldridge et al. 2007) and the Linguistic Discourse Model (Polanyi 1988, Scha and Polanyi 1988, Polanyi et al. 2004). Wolf and Gibson (2005) argue that coherence structure includes crossed bracketings, which make it impossible to represent as a tree, and propose a graph representation instead. A compendium of over 350 relations that have been proposed in the literature can be found in Hovy (1990). ","{
    'Question': 'In what way did Hobbs contribute to the understanding of coherence relations?',
    'Options': 
    {
        'A)': 'He introduced the concept of coherence relations.',
        'B)': 'He developed a new method for extracting coherence relations.',
        'C)': 'He argued that coherence relations are essential for inference.',
        'D)': 'He focused on the structural analysis of small texts.'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""Which scholar first introduced the concept of coherence relations playing an inferential role for the hearer?"", 
    'Options': 
    {
        'A)': ""Hobbs (1979)"",
        'B)': ""Mann and Thompson (1987)"",
        'C)': ""Wolf and Gibson (2005)"",
        'D)': ""Asher and Lascarides (2003)""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What did Asher and Lascarides contribute to coherence relations?',
    'Options': 
    {
        'A)': 'They developed a new method for extracting coherence relations.',
        'B)': 'They introduced SDRT, a theory for representing coherence structures.',
        'C)': 'They proposed a graph-based approach to represent coherence relations.',
        'D)': 'They argued against the use of trees for representing coherence relations.'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What are some methods other than SDRT and LDModel used to extract coherence relations?"",
    'Options': 
    {
        'A)': ""LDModel and SDRT"",
        'B)': ""SDRT and Linguistic Discourse Model"",
        'C)': ""Linguistic Discourse Model and Segmented Discourse Representation Theory"",
        'D)': ""None of the above""
    },
    'Correct answer': 'C)'
}","{
    'Question': 'Which scholar first introduced the idea that coherence relations play an inferential role for the hearer?',
    'Options': 
    {
        'A)': 'Hobbs',
        'B)': 'Mann and Thompson',
        'C)': 'Wolf and Gibson',
        'D)': 'Asher and Lascarides'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""Which approach does not directly involve the development of coherence relations?"", 
    'Options': 
    {
        'A)': ""Hobbs' idea about coherence relations playing an inferential role"",
        'B)': ""Mann and Thompson's investigation of discourse structure"",
        'C)': ""Segmented Discourse Representation Theory (SDRT)"",
        'D)': ""Wolf and Gibson's argument on crossing bracketings""
    },
    'Correct answer': 'C)'
}"
3,question answering,"Some of the question datasets described above augment each question with passage(s) from which the answer can be extracted. These datasets were mainly created for an earlier QA task called reading comprehension in which a model is given a question and a document and is required to extract the answer from the given document. We sometimes call the task of question answering given one or more documents (for example via RAG), the open book QA task, while the task of answering directly from the LM with no retrieval component at all is the closed book QA task.5 Thus datasets like Natural Questions can be treated as open book if the solver uses each question`s attached document, or closed book if the documents are not used, while datasets like MMLU are solely closed book. ","{
    'Question': 'In what type of QA tasks do models need to extract answers from documents?',
    'Options': 
    {
        'A)': 'Closed book QA',
        'B)': 'Open book QA',
        'C)': 'Both types require extracting answers from documents',
        'D)': 'Neither type requires extracting answers from documents'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""In what type of dataset do models need to extract answers directly from the given document?"",
    'Options': 
    {
        'A)': ""Natural Questions"",
        'B)': ""MMLU"",
        'C)': ""Open Book QA Task"",
        'D)': ""Closed Book QA Task""
    },
    'Correct answer': 'D)'
}","{
    'Question': 'What does the term ""open book QA"" refer to?',
    'Options': 
    {
        'A)': 'The task of answering questions directly from a language model.',
        'B)': 'The task of extracting answers from passages using a question dataset.',
        'C)': 'The task of providing answers based on previous knowledge.',
        'D)': 'The task of creating new questions for testing models.'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What distinguishes the open book QA task from the closed book QA task?"",
    'Options': 
    {
        'A)': ""The open book task requires using external knowledge sources."",
        'B)': ""The open book task allows direct answers from language models."",
        'C)': ""The open book task involves retrieving information from documents."",
        'D)': ""The open book task does not use any external resources.""
    },
    'Correct answer': 'C)'
}","{
    'Question': 'What type of QA tasks involve using retrieved passages?',
    'Options': 
    {
        'A)': 'Closed book QA',
        'B)': 'Open book QA',
        'C)': 'Reading comprehension',
        'D)': 'Machine learning generation'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What type of QA task does the dataset Natural Questions belong to?"", 
    'Options': 
    {
        'A)': ""Closed book QA task"",
        'B)': ""Open book QA task"",
        'C)': ""Reading comprehension task"",
        'D)': ""None of the above""
    },
    'Correct answer': 'B)'
}"
4,dialogue system,"It is a common practice for dialogue systems to use further labeled data for finetuning. One function of this fine-tuning step is to improve the quality of the dialogue, training the system to produce responses that are sensible and interesting. Another function might be to improve safety, keeping a dialogue system from suggesting harmful actions (like financial fraud, medical harm, inciting hatred, or abusing the user or other people). ","{
    'Question': 'What is another function of fine-tuning a dialogue system besides improving its quality?',
    'Options': 
    {
        'A)': 'Improving efficiency',
        'B)': 'Enhancing security',
        'C)': 'Increasing accuracy',
        'D)': 'Boosting speed'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What is another function of fine-tuning in dialogue systems besides improving quality?"",
    'Options': 
    {
        'A)': ""Improving accuracy"",
        'B)': ""Enhancing security"",
        'C)': ""Increasing speed"",
        'D)': ""Boosting efficiency""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What two main functions does fine-tuning a dialogue system serve?',
    'Options': 
    {
        'A)': 'Improving accuracy and enhancing security',
        'B)': 'Enhancing creativity and improving efficiency',
        'C)': 'Increasing speed and reducing costs',
        'D)': 'Strengthening privacy and protecting users'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What two functions does the fine-tuning step serve for dialogue systems?"",
    'Options': 
    {
        'A)': ""Improving the quality of the dialogue and improving safety"",
        'B)': ""Improving accuracy and reducing latency"",
        'C)': ""Enhancing performance and increasing efficiency"",
        'D)': ""Increasing complexity and enhancing security""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What is another function of using further labeled data during the fine-tuning process of dialogue systems?',
    'Options': 
    {
        'A)': 'To increase the speed of response generation',
        'B)': 'To enhance the complexity of the dialogue',
        'C)': 'To reduce the accuracy of the responses',
        'D)': 'To prevent the system from making inappropriate suggestions'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""What is another function of fine-tuning in dialogue systems besides improving the quality?"", 
    'Options': 
    {
        'A)': ""To make the dialogue more complex."",
        'B)': ""To increase the speed of response generation."",
        'C)': ""To enhance the entertainment value of the dialogue."",
        'D)': ""To prevent the system from making mistakes.""
    },
    'Correct answer': 'D)'
}"
5,naive bayes,"Consider the task of spam detection, deciding if a particular piece of email is an example of spam (unsolicited bulk email)one of the first applications of naive Bayes to text classification (Sahami et al., 1998). ","{
    'Question': 'In which application did naive Bayes classifiers first appear?',
    'Options': 
    {
        'A)': 'Image recognition',
        'B)': 'Spam detection',
        'C)': 'Sentiment analysis',
        'D)': 'Speech recognition'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""Which algorithm was initially used for text classification tasks such as spam detection?"",
    'Options': 
    {
        'A)': ""K-means clustering"",
        'B)': ""Naive Bayes"",
        'C)': ""Support Vector Machines"",
        'D)': ""Decision Trees""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What was one of the earliest uses of Naive Bayes for text classification?',
    'Options': 
    {
        'A)': 'Spam detection',
        'B)': 'Sentiment analysis',
        'C)': 'Topic modeling',
        'D)': 'Named entity recognition'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What was one of the early applications of Naive Bayes algorithm?"", 
    'Options': 
    {
        'A)': ""Spam detection"",
        'B)': ""Stock market prediction"",
        'C)': ""Image recognition"",
        'D)': ""Music recommendation""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'Which algorithm was initially used for text classification tasks?',
    'Options': 
    {
        'A)': 'K-means',
        'B)': 'Naive Bayes',
        'C)': 'Decision Trees',
        'D)': 'Support Vector Machines'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What preprocessing step should be done before applying Naive Bayes for spam detection?"", 
    'Options': 
    {
        'A)': ""Remove all punctuation marks"",
        'B)': ""Convert all letters to lowercase"",
        'C)': ""Tokenize the emails into individual words"",
        'D)': ""Count the frequency of each word""
    },
    'Correct answer': 'C)'
}"
6,question answering,"In this chapter we`ll introduce the two most commonly used models. In the tf-idf model, an important baseline, the meaning of a word is defined by a simple function of the counts of nearby words. We will see that this method results in very long vectors that are sparse, i.e. mostly zeros (since most words simply never occur in the context of others). We`ll introduce the word2vec model family for constructing short, dense vectors that have useful semantic properties. We`ll also introduce the cosine, the standard way to use embeddings to compute semantic similarity, between two words, two sentences, or two documents, an important tool in practical applications like question answering, summarization, or automatic essay grading. ","{
    'Question': 'In the context provided, what does the word `question answering` refer to?',
    'Options': 
    {
        'A)': 'The process of finding answers to questions using machine learning algorithms',
        'B)': 'The act of asking questions to gather information',
        'C)': 'The study of how animals communicate through sounds',
        'D)': 'The practice of teaching students about different types of questions'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""Which model introduces short, dense vectors with useful semantic properties?"",
    'Options': 
    {
        'A)': ""TF-IDF model"",
        'B)': ""Word2Vec model"",
        'C)': ""Cosine model"",
        'D)': ""None of the above""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What is the main focus of the discussion regarding the two models introduced?',
    'Options': 
    {
        'A)': 'Explaining the mathematical foundations of both models',
        'B)': 'Comparing their performance on various tasks',
        'C)': 'Discussing the differences in vector length and density',
        'D)': 'Introducing techniques for computing semantic similarity using embeddings'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""What is the primary focus when using the Word2Vec model for constructing vectors?"",
    'Options': 
    {
        'A)': ""Creating long, sparse vectors"",
        'B)': ""Generating short, dense vectors"",
        'C)': ""Using cosine similarity exclusively"",
        'D)': ""Avoiding semantic analysis entirely""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'In which application area does the word2vec model family primarily find its use?',
    'Options': 
    {
        'A)': 'Natural language processing',
        'B)': 'Image recognition',
        'C)': 'Financial forecasting',
        'D)': 'Robotics',
    },
    'Correct answer': 'A)'
}","{
    'Question': ""Which model introduces the cosine as a measure for semantic similarity?"", 
    'Options': 
    {
        'A)': ""TF-IDF model"",
        'B)': ""Word2Vec model"",
        'C)': ""Cosine model"",
        'D)': ""Sentence2Vec model""
    },
    'Correct answer': 'B)'
}"
7,regular expression,able to look for expressions like 6 GHz or 500 GB or $999.99. Let`s work out some regular expressions for this task. ,"{
    'Question': 'What can a regular expression be used for?',
    'Options':
    {
        'A)': 'To find patterns in strings',
        'B)': 'To encrypt messages',
        'C)': 'To create new languages',
        'D)': 'To play games'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What can be used to find patterns such as 6 GHz, 500 GB, or $999.99 in strings of text?"",
    'Options': 
    {
        'A)': ""Regular expressions"",
        'B)': ""Syntax rules"",
        'C)': ""Pattern matching algorithms"",
        'D)': ""String comparison methods""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What is a regular expression used for?',
    'Options':
    {
        'A)': 'To find patterns in strings',
        'B)': 'To encrypt messages',
        'C)': 'To create new files',
        'D)': 'To delete data'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What are the common patterns found in strings like 6 GHz, 500 GB, and $999.99 when using regular expressions?"", 
    'Options': 
    {
        'A)': ""The use of special characters like \\d, \\s, and \\w"",
        'B)': ""The presence of numbers followed by specific units (e.g., GHz, GB)"",
        'C)': ""The inclusion of dollar signs ($) at the end of each string"",
        'D)': ""The combination of all three options A), B), and C)""
    },
    'Correct answer': 'D)'
}","{
    'Question': 'What is the purpose of using regular expressions?',
    'Options':
    {
        'A)': 'To find patterns in strings',
        'B)': 'To encrypt messages',
        'C)': 'To compress files',
        'D)': 'To delete files'
    },
    'Correct answer': 'A) To find patterns in strings'
}","{
    'Question': ""What is the first step in creating a regular expression to match numbers with decimal points?"", 
    'Options': 
    {
        'A)': ""Define the pattern for whole numbers only."",
        'B)': ""Identify the specific number format (e.g., 6GHz, 500GB)."",
        'C)': ""Specify the exact length of the number."",
        'D)': ""Choose the character set for digits.""
    },
    'Correct answer': 'B)'
}"
8,question answering," QUESTION ANSWERING, INFORMATION RETRIEVAL, AND RAG ","{
    'Question': 'In what field is the term ""question answering"" primarily applied?',
    'Options': 
    {
        'A)': 'Machine Learning',
        'B)': 'Natural Language Processing',
        'C)': 'Computer Vision',
        'D)': 'Robotics'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""In which field of computer science does the process of finding answers to questions fall under?"",
    'Options': 
    {
        'A)': ""Artificial Intelligence"",
        'B)': ""Natural Language Processing"",
        'C)': ""Machine Learning"",
        'D)': ""Information Retrieval""
    },
    'Correct answer': 'D)'
}","{
    'Question': 'What is the process of finding answers to questions?',
    'Options': 
    {
        'A)': 'Information retrieval',
        'B)': 'RAG',
        'C)': 'QUESTION ANSWERING',
        'D)': 'None of the above'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What are some common methods for answering questions?"", 
    'Options': 
    {
        'A)': ""QUESTION ANSWERING, INFORMATION RETRIEVAL, AND RAG"",
        'B)': ""SUMMARIZING THE CONTEXT"",
        'C)': ""REPHRASING QUESTIONS"",
        'D)': ""COMPARING DIFFERENT METHODS""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What is the process of finding answers to questions?',
    'Options': 
    {
        'A)': 'Developing search algorithms',
        'B)': 'Organizing data structures',
        'C)': 'Building retrieval systems',
        'D)': 'Planning query formulation'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What is the first step in organizing an effective question-answering system?"", 
    'Options': 
    {
        'A)': ""Develop a comprehensive database of all possible questions."",
        'B)': ""Organize the responses based on relevance and importance."",
        'C)': ""Train machine learning models to understand natural language."",
        'D)': ""Identify key terms and phrases in user queries.""
    },
    'Correct answer': 'D)'
}"
9,translation,"We only have to make one slight change to turn this language model with autoregressive generation into an encoder-decoder model that is a translation model that can translate from a source text in one language to a target text in a second: add a sentence separation marker at the end of the source text, and then simply concatenate the target text. ","{
    'Question': 'How does adding a sentence separation marker affect the translation process?',
    'Options': 
    {
        'A)': 'It simplifies the encoding phase.',
        'B)': 'It increases the complexity of the decoding phase.',
        'C)': 'It enhances the accuracy of the translation model.',
        'D)': 'It reduces the overall length of the translated text.'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""How does adding a sentence separation marker affect the process of translating from a source text in one language to a target text in another?"",
    'Options': 
    {
        'A)': ""It simplifies the encoding phase by reducing the number of tokens."",
        'B)': ""It increases the complexity of the translation task by requiring additional processing steps."",
        'C)': ""It has no effect on the translation process as it remains unchanged."",
        'D)': ""It decreases the overall length of the target text.""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'How does adding a sentence separation marker affect the translation process?',
    'Options': 
    {
        'A)': 'It makes the translation more accurate.',
        'B)': 'It simplifies the encoding step.',
        'C)': 'It increases the complexity of the decoding step.',
        'D)': 'It allows for direct concatenation between source and target texts.'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""What is the primary difference between an autoregressive model and a translation model?"",
    'Options': 
    {
        'A)': ""An autoregressive model generates output based on previous inputs, while a translation model does not."",
        'B)': ""An autoregressive model requires manual input for each word, whereas a translation model uses pre-trained models."",
        'C)': ""An autoregressive model translates entire sentences at once, while a translation model breaks down sentences into smaller parts."",
        'D)': ""An autoregressive model focuses on generating new content, while a translation model aims to maintain meaning.""
    },
    'Correct answer': 'D)'
}","{
    'Question': 'How does adding a sentence separation marker affect the translation process?',
    'Options': 
    {
        'A)': 'It increases the complexity of the model.',
        'B)': 'It simplifies the translation by breaking down sentences.',
        'C)': 'It enhances the accuracy of word-by-word translation.',
        'D)': 'It has no impact on the translation.'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What should we do first when converting our current model into a translation model?"", 
    'Options': 
    {
        'A)': ""Add a sentence separator at the end of the source text"",
        'B)': ""Concatenate the target text directly"",
        'C)': ""Remove the autoregressive generation feature"",
        'D)': ""Identify the sentence boundaries in the source text""
    },
    'Correct answer': 'A)'
}"
10,grammar,"The resurgence of n-gram language models came from Fred Jelinek and colleagues at the IBM Thomas J. Watson Research Center, who were influenced by Shannon, and James Baker at CMU, who was influenced by the prior, classified work of Leonard Baum and colleagues on these topics at labs like the US Institute for Defense Analyses (IDA) after they were declassified. Independently these two labs successfully used n-grams in their speech recognition systems at the same time (Baker 1975b, Jelinek et al. 1975, Baker 1975a, Bahl et al. 1983, Jelinek 1990). The terms language model and perplexity were first used for this technology by the IBM group. Jelinek and his colleagues used the term language model in a pretty modern way, to mean the entire set of linguistic influences on word sequence probabilities, including grammar, semantics, discourse, and even speaker characteristics, rather than just the particular n-gram model itself. ","{
    'Question': 'Who coined the term ""language model""?',
    'Options': 
    {
        'A)': 'Fred Jelinek',
        'B)': 'James Baker',
        'C)': 'Leonard Baum',
        'D)': 'The IBM group'
    },
    'Correct answer': 'D)'
}","Term: 'language model'

Context: Language models are statistical models that predict the probability of sequences of words based on large amounts of data. They play a crucial role in various natural language processing tasks such as machine translation, text generation, and sentiment analysis.

{
    'Question': ""What do language models primarily predict?"",
    'Options': 
    {
        'A)': ""The grammatical structure of sentences"",
        'B)': ""The semantic meaning of words"",
        'C)': ""The pronunciation of spoken words"",
        'D)': ""The likelihood of word sequences""
    },
    'Correct answer': 'D)'
}","{
    'Question': 'What did Jelinek and his colleagues use the term ""language model"" to refer to?',
    'Options': 
    {
        'A)': 'Just the specific n-gram model',
        'B)': 'The entire set of linguistic influences on word sequence probabilities',
        'C)': 'Only the particular n-gram model itself',
        'D)': 'None of the above',
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What did Jelinek and his colleagues use the term 'language model' to refer to?"",
    'Options': 
    {
        'A)': ""Just the specific n-gram model"",
        'B)': ""The entire set of linguistic influences on word sequence probabilities"",
        'C)': ""Only the grammatical aspects of language"",
        'D)': ""No clear definition was established""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What did Jelinek and his colleagues use the term ""language model"" to describe?',
    'Options': 
    {
        'A)': 'Just the particular n-gram model',
        'B)': 'The entire set of linguistic influences on word sequence probabilities',
        'C)': 'Only the grammatical rules of a language',
        'D)': 'The specific phonetic patterns in speech sounds',
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What did Jelinek and his colleagues primarily refer to when using the term 'language model'? "", 
    'Options': 
    {
        'A)': ""The specific n-gram model"",
        'B)': ""Only the grammatical rules"",
        'C)': ""The entire set of linguistic influences on word sequence probabilities"",
        'D)': ""The computational complexity of the model""
    },
    'Correct answer': 'C)'
}"
11,speech recognition,"Due to the difficulty of porting systems from one domain to another, attention shifted to machine learning approaches. Early supervised learning approaches to IE (Cardie 1993, Cardie 1994, Riloff 1993, Soderland et al. 1995, Huffman 1996) focused on automating the knowledge acquisition process, mainly for finite-state rule-based systems. Their success, and the earlier success of HMM-based speech recognition, led to the use of sequence labeling (HMMs: Bikel et al. 1997; MEMMs McCallum et al. 2000; CRFs: Lafferty et al. 2001), and a wide exploration of features (Zhou et al., 2005). Neural approaches followed from the pioneering results of Collobert et al. (2011), who applied a CRF on top of a convolutional net. ","{
    'Question': 'Which approach was primarily used for automating the knowledge acquisition process in early speech recognition systems?',
    'Options': 
    {
        'A)': 'Sequence Labeling',
        'B)': 'Neural Approaches',
        'C)': 'Convolutional Nets',
        'D)': 'HMM-Based Speech Recognition'
    },
    'Correct answer': 'A)'
}","Term:'machine learning'

Context: Machine learning has evolved significantly since its inception with early applications focusing on automating knowledge acquisition processes for finite-state rule-based systems. As technology advanced, neural networks emerged as powerful tools, particularly after Collobert et al.'s application of Conditional Random Fields (CRFs) atop Convolutional Networks.

{
    'Question': ""Which approach did not initially focus on automating the knowledge acquisition process but later became popular due to advancements in machine learning?"",
    'Options': 
    {
        'A)': ""Finite-state rule-based systems"",
        'B)': ""Neural networks"",
        'C)': ""Sequence labeling"",
        'D)': ""Convolutional nets""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What was the main focus of early supervised learning approaches to Information Extraction (IE)?',
    'Options': 
    {
        'A)': 'Automating the knowledge acquisition process',
        'B)': 'Improving accuracy of HMM-based speech recognition',
        'C)': 'Exploring new types of neural networks',
        'D)': 'Developing novel feature extraction techniques'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What was the primary focus of early supervised learning approaches to IE before the advent of neural networks?"",
    'Options': 
    {
        'A)': ""Automating the knowledge acquisition process"",
        'B)': ""Improving feature extraction techniques"",
        'C)': ""Enhancing computational efficiency"",
        'D)': ""Reducing training time""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What approach was used after early supervised learning methods due to their limitations?',
    'Options': 
    {
        'A)': 'Sequence labeling',
        'B)': 'Neural networks',
        'C)': 'Feature engineering',
        'D)': 'Rule-based systems'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What approach was used initially when developing speech recognition systems?"", 
    'Options': 
    {
        'A)': ""Sequence labeling with HMMs"",
        'B)': ""Neural networks directly"",
        'C)': ""Feature engineering only"",
        'D)': ""Supervised learning using HMMs""
    },
    'Correct answer': 'D)'
}"
12,neural network,"By around 1990 neural alternatives to the HMM/GMM architecture for ASR arose, based on a number of earlier experiments with neural networks for phoneme recognition and other speech tasks. Architectures included the time-delay neural network (TDNN)the first use of convolutional networks for speech (Waibel et al. 1989, Lang et al. 1990), RNNs (Robinson and Fallside, 1991), and the hybrid HMM/MLP architecture in which a feedforward neural network is trained as a phonetic classifier whose outputs are used as probability estimates for an HMM-based architecture (Morgan and Bourlard 1990, Bourlard and Morgan 1994, Morgan and Bourlard 1995). ","{
    'Question': 'Which type of neural network was first used for speech processing by Waibel et al. in 1989?',
    'Options':
    {
        'A)': 'Convolutional Neural Network',
        'B)': 'Recurrent Neural Network',
        'C)': 'Time-Delay Neural Network',
        'D)': 'Feedforward Neural Network'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""Which type of neural network was first applied to speech processing specifically for phoneme recognition?"", 
    'Options': 
    {
        'A)': ""Convolutional Neural Network"",
        'B)': ""Recurrent Neural Network"",
        'C)': ""Time-Delay Neural Network"",
        'D)': ""Hybrid HMM/MLP Architecture""
    },
    'Correct answer': 'C)'
}","{
    'Question': 'What was one of the early uses of neural networks for speech processing?',
    'Options': 
    {
        'A)': 'Time-Delay Neural Network',
        'B)': 'Convolutional Networks',
        'C)': 'Recurrent Neural Networks',
        'D)': 'Hybrid HMM/MLP Architecture'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What was one of the earliest uses of neural networks in speech processing?"",
    'Options': 
    {
        'A)': ""Time-Delay Neural Network"",
        'B)': ""Convolutional Networks"",
        'C)': ""Recurrent Neural Networks"",
        'D)': ""Hybrid HMM/MLP Architecture""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'Which type of neural network was first used for speech processing by Waibel et al. in 1989?',
    'Options':
    {
        'A)': 'Convolutional Neural Network',
        'B)': 'Recurrent Neural Network',
        'C)': 'Time-Delay Neural Network',
        'D)': 'Hybrid HMM/MLP Architecture'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""Which approach was not directly used in the development of modern ASR systems?"", 
    'Options': 
    {
        'A)': ""Time-Delay Neural Network (TDNN)"",
        'B)': ""Recurrent Neural Networks (RNNs)"",
        'C)': ""Hybrid HMM/MLP Architecture"",
        'D)': ""Hidden Markov Models (HMMs)""
    },
    'Correct answer': 'D)'
}"
13,coreference, The Winograd Schema Challenge problems are difficult coreference problems that seem to require world knowledge or sophisticated reasoning to solve.  Coreference systems exhibit gender bias which can be evaluated using datasets ,"{
    'Question': 'What is a characteristic of coreference systems?',
    'Options': 
    {
        'A)': 'They always provide accurate answers.',
        'B)': 'They rely on personal experience for solving problems.',
        'C)': 'They can evaluate gender bias through specific datasets.',
        'D)': 'They do not require any external knowledge.'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What type of problem does the Winograd Schema Challenge involve?"",
    'Options': 
    {
        'A)': ""Synthetic language generation"",
        'B)': ""Coreference resolution"",
        'C)': ""Natural language inference"",
        'D)': ""Sentiment analysis""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What does the Winograd Schema Challenge involve?',
    'Options':
    {
        'A)': 'It is a test of logical reasoning.',
        'B)': 'It involves solving puzzles with no world knowledge required.',
        'C)': 'It requires evaluating datasets for gender bias.',
        'D)': 'It focuses on coreference, where phrases refer to each other within sentences.'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""Explain how coreference systems might exhibit gender bias based on the Winograd Schema Challenge problems?"",
    'Options': 
    {
        'A)': ""Coreference systems may show gender bias due to their difficulty level, making it harder for certain genders."",
        'B)': ""Coreference systems could display gender bias because they rely heavily on world knowledge, potentially favoring one gender over another."",
        'C)': ""Gender bias in coreference systems arises from the dataset used for training, where data representation varies by gender."",
        'D)': ""Coreference systems do not exhibit gender bias; all genders perform equally well.""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'Which of these sentences is an example of a coreference problem?',
    'Options': 
    {
        'A)': 'The cat sat on the mat.',
        'B)': 'She saw the dog.',
        'C)': 'He ate the cake.',
        'D)': 'They went to the store.'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What is an example of a coreference problem?"", 
    'Options': 
    {
        'A)': ""The dog chased the cat."",
        'B)': ""John went to the store and bought milk."",
        'C)': ""She likes apples and bananas."",
        'D)': ""I am taller than my brother.""
    },
    'Correct answer': 'B)'
}"
14,semantic role,"A PropBank semantic role labeling would allow us to infer the commonality in the event structures of the following three examples, that is, that in each case Big Fruit Co. is the AGENT and the price of bananas is the THEME, despite the differing surface forms. ","{
    'Question': 'In the context provided, what does the term ""semantic role"" refer to?',
    'Options':
    {
        'A)': 'The grammatical function of a word in a sentence',
        'B)': 'The part-of-speech tag assigned to a word',
        'C)': 'The meaning of a word in relation to other words in a sentence',
        'D)': 'The syntactic structure of a sentence'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""In which example does Big Fruit Co. act as the agent?"",
    'Options': 
    {
        'A)': ""Big Fruit Co. buys bananas at $1 per pound."",
        'B)': ""Big Fruit Co. sells bananas for $2 per pound."",
        'C)': ""Big Fruit Co. imports bananas from South America."",
        'D)': ""Big Fruit Co. exports bananas to Europe.""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'In the examples provided, what does Big Fruit Co. play as?',
    'Options': 
    {
        'A)': 'THEME',
        'B)': 'AGENT',
        'C)': 'PREDICATE',
        'D)': 'OBJECT'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""Explain how semantic roles are utilized across different sentences?"",
    'Options': 
    {
        'A)': ""Semantic roles help identify the subject and object in sentence structure."",
        'B)': ""Semantic roles provide insight into the agent and theme within various contexts."",
        'C)': ""Semantic roles assist in understanding the cause-effect relationships between events."",
        'D)': ""Semantic roles reveal the participants involved in actions regardless of their specific roles.""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'In the examples provided, what does Big Fruit Co. play as?',
    'Options': 
    {
        'A)': 'AGENT',
        'B)': 'THEME',
        'C)': 'PREDICATE',
        'D)': 'OBJECT'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What method should we use to analyze the semantic roles in these events?"", 
    'Options': 
    {
        'A)': ""Use a rule-based approach"",
        'B)': ""Employ machine learning algorithms"",
        'C)': ""Apply statistical methods"",
        'D)': ""Utilize PropBank for semantic role labeling""
    },
    'Correct answer': 'D)'
}"
15,neural network,"For sequence classification we represent the entire input to be classified by a single vector. We can represent a sequence in various ways. One way is to take the sum or the mean of the last output vector from each token in the sequence. For BERT, we instead add a new unique token to the vocabulary called [CLS], and prepended it to the start of all input sequences, both during pretraining and encoding. The output vector in the final layer of the model for the [CLS] input represents the entire input sequence and serves as the input to a classifier head, a logistic regression or neural network classifier that makes the relevant decision. ","{
    'Question': 'In what manner do we classify sequences using neural networks?',
    'Options': 
    {
        'A)': 'By taking the average of vectors',
        'B)': 'By adding a special token at the beginning of the sequence',
        'C)': 'By calculating the sum of vectors',
        'D)': 'Both A) and C)'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""In which type of neural network does BERT use a special token at the beginning of each sequence?"",
    'Options': 
    {
        'A)': ""Convolutional Neural Network (CNN)"",
        'B)': ""Recurrent Neural Network (RNN)"",
        'C)': ""Transformer Neural Network (TNT)"",
        'D)': ""Bidirectional LSTM""
    },
    'Correct answer': 'C)'
}","{
    'Question': 'What does the output vector in the final layer of the BERT model represent?',
    'Options': 
    {
        'A)': 'The average of all tokens',
        'B)': 'The sum of all tokens',
        'C)': 'The entire input sequence',
        'D)': 'None of the above',
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What does the [CLS] token do in BERT?"",
    'Options': 
    {
        'A)': ""It adds a new unique token to the vocabulary."",
        'B)': ""It represents the entire input sequence by taking the sum of the last output vectors."",
        'C)': ""It represents the entire input sequence by taking the mean of the last output vectors."",
        'D)': ""It serves as the input to a classifier head.""
    },
    'Correct answer': 'D)'
}","{
    'Question': 'What does the output vector in the final layer of the model for the [CLS] input represent?',
    'Options': 
    {
        'A)': 'The average of all tokens',
        'B)': 'The sum of all tokens',
        'C)': 'The representation of the entire input sequence',
        'D)': 'The probability distribution over classes',
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What does the output vector from the [CLS] token in BERT serve?"", 
    'Options': 
    {
        'A)': ""It represents the average of all tokens."",
        'B)': ""It represents the sum of all tokens."",
        'C)': ""It represents the entire input sequence."",
        'D)': ""It represents only the first token.""
    },
    'Correct answer': 'C)'
}"
16,temporal,"Language is an inherently temporal phenomenon. Spoken language is a sequence of acoustic events over time, and we comprehend and produce both spoken and written language as a sequential input stream. The temporal nature of language is reflected in the metaphors we use; we talk of the flow of conversations, news feeds, and twitter streams, all of which emphasize that language is a sequence that unfolds in time. ","{
    'Question': 'In what way does language reflect its inherent temporality?',
    'Options': 
    {
        'A)': 'Language reflects its temporality through metaphors such as the flow of conversations, news feeds, and Twitter streams.',
        'B)': 'Language reflects its temporality by emphasizing the static nature of spoken and written language.',
        'C)': 'Language reflects its temporality through the rhythm of speech patterns.',
        'D)': 'Language reflects its temporality by focusing on the continuous evolution of vocabulary.'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""Which linguistic concept reflects the inherent temporality of language?"", 
    'Options': 
    {
        'A)': ""Semantic priming"",
        'B)': ""Phonological patterns"",
        'C)': ""Syntax rules"",
        'D)': ""Temporal metaphor""
    },
    'Correct answer': 'D)'
}","{
    'Question': 'What does the term ""temporal"" refer to in relation to language?',
    'Options': 
    {
        'A)': 'The duration of sound waves',
        'B)': 'The order in which events occur over time',
        'C)': 'The speed at which sounds travel through air',
        'D)': 'The frequency of speech patterns'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""Explain how the concept of temporality is reflected in our everyday language usage?"",
    'Options': 
    {
        'A)': ""We describe conversation flows using metaphors emphasizing linear progression."",
        'B)': ""Metaphors about news feeds and Twitter streams are unrelated to language's temporal aspect."",
        'C)': ""Language does not reflect any temporal concepts in its metaphors."",
        'D)': ""Temporal concepts are only relevant when discussing scientific theories.""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'How does the temporal nature of language manifest metaphorically?',
    'Options': 
    {
        'A)': 'We speak of the flow of conversations, news feeds, and Twitter streams.',
        'B)': 'We describe actions using past tense.',
        'C)': 'We refer to concepts with abstract nouns.',
        'D)': 'We use adjectives to modify verbs.'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What does the temporal aspect of language imply about how we perceive and interact with it?"", 
    'Options': 
    {
        'A)': ""Language is static and unchanging."",
        'B)': ""We can only understand language through written forms."",
        'C)': ""The meaning of language changes instantaneously upon each utterance."",
        'D)': ""Language comprehension involves processing sequences of events over time.""
    },
    'Correct answer': 'D)'
}"
17,dialogue act,"In the 1990s, machine learning models that had first been applied to natural language processing began to be applied to dialogue tasks like slot filling (Miller et al. 1994, Pieraccini et al. 1991). This period also saw lots of analytic work on the linguistic properties of dialogue acts and on machine-learning-based methods for their detection. (Sag and Liberman 1975, Hinkelman and Allen 1989, Nagata and Morimoto 1994, Goodwin 1996, Chu-Carroll 1998, Shriberg et al. 1998, Stolcke et al. 2000, Gravano et al. 2012. This work strongly informed the development of the dialogue-state model (Larsson and Traum, 2000). Dialogue state tracking quickly became an important problem for task-oriented dialogue, and there has been an influential annual evaluation of state-tracking algorithms (Williams et al., 2016). The turn of the century saw a line of work on applying reinforcement learning to dialogue, which first came out of AT&T and Bell Laboratories with work on MDP dialogue systems (Walker 2000, Levin et al. 2000, Singh et al. 2002) along with work on cue phrases, prosody, and rejection and confirmation. Reinforcement learning research turned quickly to the more sophisticated POMDP models (Roy et al. 2000, Lemon et al. 2006, Williams and Young 2007) applied to small slotfilling dialogue tasks. Neural reinforcement learning models have been used both for chatbot systems, for example simulating dialogues between two dialogue systems, rewarding good conversational properties like coherence and ease of answering (Li et al., 2016a), and for task-oriented dialogue (Williams et al., 2017). ","{
    'Question': 'In what decade did machine learning models begin to be applied to dialogue tasks such as slot filling?',
    'Options': 
    {
        'A)': 'The 1980s',
        'B)': 'The 1970s',
        'C)': 'The 1960s',
        'D)': 'The 1990s'
    },
    'Correct answer': 'D)'
}","Term: 'dialogue act'

Context: In the 1990s, machine learning models that had first been applied to natural language processing began to be applied to dialogue tasks like slot filling (Miller et al. 1994, Pieraccini et al. 1991). This period also saw lots of analytic work on the linguistic properties of dialogue acts and on machine-learning-based methods for their detection.

{
    'Question': ""Which type of machine learning was primarily used to detect dialogue acts during the 1990s?"",
    'Options': 
    {
        'A)': ""Supervised Learning"",
        'B)': ""Unsupervised Learning"",
        'C)': ""Reinforcement Learning"",
        'D)': ""Deep Learning""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What was one significant application of machine learning in dialogue tasks during the 1990s?',
    'Options': 
    {
        'A)': 'Developing dialogue-state models',
        'B)': 'Analyzing linguistic properties of dialogue acts',
        'C)': 'Creating reinforcement learning models',
        'D)': 'Implementing slot-filling algorithms'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What was one significant area of focus during the early applications of machine learning to dialogue tasks?"",
    'Options': 
    {
        'A)': ""Natural language generation"",
        'B)': ""Dialogue act analysis"",
        'C)': ""Machine learning algorithm optimization"",
        'D)': ""Task-oriented dialogue system design""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What was one significant application of machine learning in dialogue tasks during the 1990s?',
    'Options': 
    {
        'A)': 'Developing dialogue state tracking algorithms',
        'B)': 'Creating new languages for communication',
        'C)': 'Improving speech recognition accuracy',
        'D)': 'Enhancing image classification techniques'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What was a significant development during the early years of applying machine learning to dialogue tasks?"", 
    'Options': 
    {
        'A)': ""The creation of the dialogue-state model"",
        'B)': ""The introduction of reinforcement learning techniques"",
        'C)': ""The focus on analyzing linguistic properties of dialogue acts"",
        'D)': ""The improvement of slot-filling algorithms""
    },
    'Correct answer': 'C)'
}"
18,temporal,"This chapter introduces a deep learning architecture that offers an alternative way of representing time: recurrent neural networks (RNNs), and their variants like LSTMs. RNNs have a mechanism that deals directly with the sequential nature of language, allowing them to handle the temporal nature of language without the use of arbitrary fixed-sized windows. The recurrent network offers a new way to represent the prior context, in its recurrent connections, allowing the model`s decision to depend on information from hundreds of words in the past. We`ll see how to apply the model to the task of language modeling, to text classification tasks like sentiment analysis, and to sequence modeling tasks like part-of-speech tagging (a task we`ll return to in detail in Chapter 17). ","{
    'Question': 'What does the term ""temporal"" refer to in this context?',
    'Options':
    {
        'A)': 'The duration of events',
        'B)': 'The flow of time over space',
        'C)': 'The order of events in time',
        'D)': 'The size of data sets'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""Which type of neural network has a mechanism that deals directly with the sequential nature of language?"", 
    'Options': 
    {
        'A)': ""Convolutional Neural Network (CNN)"",
        'B)': ""Recurrent Neural Network (RNN)"",
        'C)': ""Long Short-Term Memory (LSTM) Network"",
        'D)': ""Autoencoder""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What does the term ""temporal"" refer to in this context?',
    'Options': 
    {
        'A)': 'The duration of events over time',
        'B)': 'The order of events in time',
        'C)': 'The frequency of events over time',
        'D)': 'The speed at which events occur'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What does the introduction of this chapter suggest about the representation of time in deep learning architectures?"",
    'Options': 
    {
        'A)': ""The chapter focuses solely on the representation of space."",
        'B)': ""The chapter emphasizes the importance of using fixed-sized windows for handling sequences."",
        'C)': ""The chapter highlights the benefits of using recurrent neural networks over other methods."",
        'D)': ""The chapter discusses the limitations of traditional models when dealing with temporal data.""
    },
    'Correct answer': 'C)'
}","{
    'Question': 'What does the term ""temporal"" refer to in this context?',
    'Options': 
    {
        'A)': 'The duration of events',
        'B)': 'The order of events',
        'C)': 'The frequency of events',
        'D)': 'The size of events'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What is the primary advantage of using RNNs over traditional methods for handling sequential data?"", 
    'Options': 
    {
        'A)': ""They can only work with binary sequences."",
        'B)': ""They require large amounts of memory."",
        'C)': ""They cannot capture long-term dependencies."",
        'D)': ""They are more flexible and capable of capturing complex patterns.""
    },
    'Correct answer': 'D)'
}"
19,neural network,"The field of neural networks declined after it was shown that a single perceptron unit was unable to model functions as simple as XOR (Minsky and Papert, 1969). While some small amount of work continued during the next two decades, a major revival for the field didn`t come until the 1980s, when practical tools for building deeper networks like error backpropagation became widespread (Rumelhart et al., 1986). During the 1980s a wide variety of neural network and related architectures were developed, particularly for applications in psychology and cognitive science (Rumelhart and McClelland 1986b, McClelland and Elman 1986, Rumelhart and McClelland 1986a, Elman 1990), for which the term connectionist or parallel distributed processing was often used (Feldman and Ballard 1982, Smolensky 1988). Many of the principles and techniques developed in this period are foundational to modern work, including the ideas of distributed representations (Hinton, 1986), recurrent networks (Elman, 1990), and the use of tensors for compositionality (Smolensky, 1990). ","{
    'Question': 'In what decade did a major revival for the field of neural networks occur?',
    'Options': 
    {
        'A)': '1950s',
        'B)': '1970s',
        'C)': '1980s',
        'D)': '1990s'
    },
    'Correct answer': 'C)'
}","Term: artificial intelligence.

Context: Artificial Intelligence (AI) has been a subject of intense research since its inception. Early AI systems were limited by their inability to understand natural language and lacked common sense reasoning. However, significant progress was made in the 1970s with the development of expert systems, which could solve complex problems based on pre-programmed knowledge. In the 1980s, machine learning emerged as a key area within AI, enabling computers to learn from data without being explicitly programmed. This led to breakthroughs such as the creation of deep learning models capable of recognizing patterns in vast datasets. Today, AI encompasses a broad range of technologies, including natural language processing, computer vision, robotics, and more, transforming various industries and enhancing human capabilities across many domains.

{
    'Question': ""Which decade saw a significant resurgence in the field of artificial intelligence due to the availability of practical tools for building deeper neural networks?"",
    'Options': 
    {
        'A)': ""1950s"",
        'B)': ""1960s"",
        'C)': ""1970s"",
        'D)': ""1980s""
    },
    'Correct answer': 'D","{
    'Question': 'What event marked a significant resurgence in the development of neural networks?',
    'Options': 
    {
        'A)': 'The invention of the perceptron',
        'B)': 'The publication of Minsky and Papert\'s book',
        'C)': 'The introduction of error backpropagation',
        'D)': 'The rise of connectionism and parallel distributed processing'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What event marked a significant resurgence in the development of neural networks?"",
    'Options': 
    {
        'A)': ""The publication of Minsky and Paperts book"",
        'B)': ""The introduction of error backpropagation"",
        'C)': ""The invention of the perceptron unit"",
        'D)': ""The decline of the field due to its inability to model complex functions""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What event marked the resurgence of interest in neural networks?',
    'Options': 
    {
        'A)': 'The development of deep learning algorithms',
        'B)': 'The introduction of artificial neurons',
        'C)': 'The publication of Minsky and Papert\'s book',
        'D)': 'The invention of the perceptron unit'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What is the main reason behind the decline of the field of neural networks before the 1980s?"", 
    'Options': 
    {
        'A)': ""The complexity of models required."",
        'B)': ""The lack of computational power."",
        'C)': ""The inability to handle complex functions."",
        'D)': ""The failure of early researchers.""
    },
    'Correct answer': 'C)'
}"
20,logistic regression,"Logistic regression has a number of advantages over naive Bayes. Naive Bayes has overly strong conditional independence assumptions. Consider two features which are strongly correlated; in fact, imagine that we just add the same feature f1 twice. Naive Bayes will treat both copies of f1 as if they were separate, multiplying them both in, overestimating the evidence. By contrast, logistic regression is much more robust to correlated features; if two features f1 and f2 are perfectly correlated, regression will simply assign part of the weight to w1 and part to w2. Thus when there are many correlated features, logistic regression will assign a more accurate probability than naive Bayes. So logistic regression generally works better on larger documents or datasets and is a common default. ","{
    'Question': 'Which method is less prone to overestimating the evidence due to correlated features?',
    'Options': 
    {
        'A)': 'Naive Bayes',
        'B)': 'Logistic Regression',
        'C)': 'Decision Trees',
        'D)': 'Support Vector Machines'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""Which algorithm is more robust to correlated features?"",
    'Options': 
    {
        'A)': ""Naive Bayes"",
        'B)': ""Logistic Regression"",
        'C)': ""Decision Trees"",
        'D)': ""K-Nearest Neighbors""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'How does logistic regression handle correlated features compared to naive Bayes?',
    'Options': 
    {
        'A)': 'Logistic regression assigns all weights equally.',
        'B)': 'Naive Bayes treats each copy of the feature as independent.',
        'C)': 'Logistic regression ignores the correlation between features.',
        'D)': 'Naive Bayes assumes perfect independence among features.'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""How does logistic regression compare to naive Bayes in handling correlated features?"",
    'Options': 
    {
        'A)': ""Naive Bayes assigns all weights equally to correlated features."",
        'B)': ""Logistic regression treats each copy of a correlated feature separately."",
        'C)': ""Logistic regression ignores correlated features entirely."",
        'D)': ""Both A and B are true.""
    },
    'Correct answer': 'D)'
}","{
    'Question': 'Which method is less prone to overestimating the evidence due to correlated features?',
    'Options': 
    {
        'A)': 'Naive Bayes',
        'B)': 'Logistic Regression',
        'C)': 'Decision Trees',
        'D)': 'K-Nearest Neighbors',
    },
    'Correct answer': 'B)'
}","{
    'Question': ""Which method does logistic regression employ to handle correlated features differently compared to naive Bayes?"", 
    'Options': 
    {
        'A)': ""Naive Bayes uses weighted sums for handling correlated features."",
        'B)': ""Logistic regression assigns weights directly based on correlation."",
        'C)': ""Naive Bayes treats all features equally regardless of correlation."",
        'D)': ""Logistic regression combines features using their mutual information.""
    },
    'Correct answer': 'B)'
}"
21,temporal expression,"Progress in this area continues to be stimulated by formal evaluations with shared benchmark datasets, including the Automatic Content Extraction (ACE) evaluations of 2000-2007 on named entity recognition, relation extraction, and temporal expressions1, the KBP (Knowledge Base Population) evaluations (Ji et al. 2010, Surdeanu 2013) of relation extraction tasks like slot filling (extracting attributes (slots`) like age, birthplace, and spouse for a given entity) and a series of SemEval workshops (Hendrickx et al., 2009). ","{
    'Question': 'In which year did the first KBP evaluation on relation extraction tasks start?',
    'Options': 
    {
        'A)': '2000',
        'B)': '2005',
        'C)': '2010',
        'D)': '2013'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""Which evaluation method involves evaluating the ability of systems to recognize temporal expressions?"",
    'Options': 
    {
        'A)': ""Automatic Content Extraction (ACE)"",
        'B)': ""KBP (Knowledge Base Population)"",
        'C)': ""SemEval workshops"",
        'D)': ""Named Entity Recognition""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What type of evaluation is mentioned as stimulating progress in the field?',
    'Options': 
    {
        'A)': 'Formal evaluations',
        'B)': 'Informal evaluations',
        'C)': 'Peer reviews',
        'D)': 'Expert opinions'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What are some examples of formal evaluations mentioned in the context involving temporal expressions?"",
    'Options': 
    {
        'A)': ""Automatic Content Extraction (ACE) evaluations from 2000-2007 on named entity recognition, relation extraction, and temporal expressions."",
        'B)': ""KBP (Knowledge Base Population) evaluations of relation extraction tasks like slot filling (extracting attributes like age, birthplace, and spouse for a given entity) and a series of SemEval workshops."",
        'C)': ""Both A) and B)"",
        'D)': ""Neither A) nor B)""
    },
    'Correct answer': 'C)'
}","{
    'Question': 'What type of evaluation is mentioned as stimulating progress in the field?',
    'Options': 
    {
        'A)': 'Formal evaluations',
        'B)': 'Informal evaluations',
        'C)': 'Peer reviews',
        'D)': 'Expert opinions'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What is an example of a temporal expression used in knowledge base population?"", 
    'Options': 
    {
        'A)': ""Age"",
        'B)': ""Birthplace"",
        'C)': ""Spouse"",
        'D)': ""All of the above""
    },
    'Correct answer': 'D)'
}"
22,loss function," Minimizing this loss function is a convex optimization problem, and iterative ","{
    'Question': 'In machine learning, what does minimizing the loss function help achieve?',
    'Options': 
    {
        'A)': 'It helps reduce computational complexity.',
        'B)': 'It ensures the model parameters converge to their optimal values.',
        'C)': 'It guarantees the solution will always be unique.',
        'D)': 'It increases the accuracy of predictions.'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""In machine learning, what does minimizing the loss function help achieve?"",
    'Options': 
    {
        'A)': ""Improving model accuracy"",
        'B)': ""Increasing computational speed"",
        'C)': ""Enhancing data collection efficiency"",
        'D)': ""Optimizing parameter values""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What does minimizing the loss function involve?',
    'Options': {
        'A)': 'It involves finding the minimum value of the loss function.',
        'B)': 'It requires maximizing the loss function.',
        'C)': 'It necessitates using gradient descent methods.',
        'D)': 'It demands applying Newton-Raphson method.'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What does minimizing the loss function involve?"", 
    'Options': 
    {
        'A)': ""Finding the minimum value of the function"",
        'B)': ""Determining the maximum value of the function"",
        'C)': ""Calculating the average value of the function"",
        'D)': ""Identifying the root cause of the function""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What process involves minimizing the loss function to solve a convex optimization problem?',
    'Options': 
    {
        'A)': 'Developing an algorithm',
        'B)': 'Organizing data',
        'C)': 'Building models',
        'D)': 'Planning experiments'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What should be done first when minimizing the loss function for a convex optimization problem?"", 
    'Options': 
    {
        'A)': ""Find the global minimum directly"",
        'B)': ""Start with an initial guess and iteratively improve it"",
        'C)': ""Use gradient descent method only"",
        'D)': ""Ensure the loss function is differentiable""
    },
    'Correct answer': 'B)'
}"
23,loss function,"3. An objective function that we want to optimize for learning, usually involving minimizing a loss function corresponding to error on training examples. We will introduce the cross-entropy loss function. ","{
    'Question': 'In machine learning, what is the purpose of an objective function?',
    'Options': 
    {
        'A)': 'To measure the accuracy of predictions',
        'B)': 'To minimize the loss function',
        'C)': 'To maximize the model complexity',
        'D)': 'To calculate the gradient of weights'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""In machine learning, what does an objective function typically aim to minimize?"",
    'Options': 
    {
        'A)': ""Accuracy of predictions"",
        'B)': ""Loss function value"",
        'C)': ""Training time"",
        'D)': ""Data size""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What is an example of an objective function used in machine learning?',
    'Options': {
        'A)': 'Mean Squared Error',
        'B)': 'Cross-Entropy Loss Function',
        'C)': 'Hinge Loss Function',
        'D)': 'Logistic Regression Cost Function'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What does the loss function typically aim to minimize?"",
    'Options': 
    {
        'A)': ""Error on validation examples"",
        'B)': ""Accuracy on test data"",
        'C)': ""Loss on training examples"",
        'D)': ""Precision on prediction results""
    },
    'Correct answer': 'C)'
}","{
    'Question': 'What is the purpose of using a loss function during the learning process?',
    'Options': 
    {
        'A)': 'To increase accuracy',
        'B)': 'To minimize error',
        'C)': 'To reduce complexity',
        'D)': 'To maximize speed'
    },
    'Correct answer': 'B)'
}","```json
{
    'Question': ""What is the first step in using the cross-entropy loss function?"",
    'Options': 
    {
        'A)': ""Calculate the derivative of the loss function with respect to the model parameters."",
        'B)': ""Compute the sum of all elements in the output vector."",
        'C)': ""Multiply the input data by the weights."",
        'D)': ""Initialize the model parameters randomly.""
    },
    'Correct answer': 'D)'
}
```"
24,lexicon,In Chapter 4 we introduced the naive Bayes algorithm for sentiment analysis. The lexicons we have focused on throughout the chapter so far can be used in a number of ways to improve sentiment detection. ,"{
    'Question': 'In sentiment analysis, what is the role of lexicons?',
    'Options': 
    {
        'A)': 'They are used to detect sarcasm.',
        'B)': 'They help identify positive or negative words.',
        'C)': 'They provide background knowledge about language use.',
        'D)': 'They predict future trends in social media.'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""In sentiment analysis, what are the lexicons used for?"",
    'Options': 
    {
        'A)': ""To classify the type of data"",
        'B)': ""To detect the presence of emotions"",
        'C)': ""To enhance the accuracy of sentiment analysis"",
        'D)': ""To increase the speed of processing""
    },
    'Correct answer': 'C)'
}","{
    'Question': 'What are the focus areas of the lexicons discussed in the previous sentences?',
    'Options':
    {
        'A)': 'Sentiment analysis',
        'B)': 'Natural language processing',
        'C)': 'Machine learning algorithms',
        'D)': 'Text classification techniques'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What are some applications of lexicons discussed in this chapter?"", 
    'Options': 
    {
        'A)': ""Sentiment analysis, spam filtering, language translation"",
        'B)': ""Language translation, spell checking, grammar correction"",
        'C)': ""Spam filtering, spell checking, sentiment analysis"",
        'D)': ""Grammar correction, sentiment analysis, language translation""
    },
    'Correct answer': 'C)'
}","{
    'Question': 'What is the focus of the discussion regarding lexicons in this chapter?',
    'Options': 
    {
        'A)': 'Improving sentiment detection',
        'B)': 'Developing algorithms',
        'C)': 'Organizing data',
        'D)': 'Building models'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What is an effective way to utilize the lexicons discussed in this chapter?"", 
    'Options': 
    {
        'A)': ""Use them directly as they are"",
        'B)': ""Combine them with other features for better accuracy"",
        'C)': ""Ignore them completely since they are not useful"",
        'D)': ""Develop a new model combining lexicons with machine learning techniques""
    },
    'Correct answer': 'D)'
}"
25,dialogue act," The dialogue-state architecture augments the GUS frame-and-slot architecture with richer representations and more sophisticated algorithms for keeping track of user`s dialogue acts, policies for generating its own dialogue acts, and a natural language component. ","{
    'Question': 'In the dialogue-state architecture, what is augmented by the GUS frame-and-slot architecture?',
    'Options': 
    {
        'A)': 'The GUS frame-and-slot architecture',
        'B)': 'Richer representations and more sophisticated algorithms',
        'C)': 'User`s dialogue acts',
        'D)': 'Natural language component'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""Which architectural framework enhances the tracking of user's dialogue acts by incorporating richer representations and advanced algorithms?"",
    'Options': 
    {
        'A)': ""GUS frame-and-slot architecture"",
        'B)': ""Dialogue-State Architecture"",
        'C)': ""Natural Language Processing"",
        'D)': ""Machine Learning Algorithms""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What does the dialogue state architecture add compared to the GUS frame-and-slot architecture?',
    'Options': 
    {
        'A)': 'It adds richer representations and more sophisticated algorithms.',
        'B)': 'It simplifies the representation and reduces complexity.',
        'C)': 'It enhances the user interface design.',
        'D)': 'It focuses solely on natural language processing.'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What does the dialogue-state architecture do by augmenting the GUS frame-and-slot architecture?"", 
    'Options': 
    {
        'A)': ""It simplifies the representation of user's dialogue acts."",
        'B)': ""It enhances the tracking of user's dialogue acts."",
        'C)': ""It reduces the complexity of generating dialogue acts."",
        'D)': ""It integrates a natural language processing module.""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What does the dialogue-state architecture add to the GUS frame-and-slot architecture?',
    'Options':
    {
        'A)': 'Richer representations',
        'B)': 'Sophisticated algorithms',
        'C)': 'User's dialogue acts',
        'D)': 'Natural language component'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What is an essential part of the dialogue state architecture?"", 
    'Options': 
    {
        'A)': ""The GUS frame-and-slot architecture"",
        'B)': ""Richer representations and more sophisticated algorithms"",
        'C)': ""User's dialogue acts"",
        'D)': ""Natural language component""
    },
    'Correct answer': 'C)'
}"
26,edit distance,The intuition of a dynamic programming problem is that a large problem can be solved by properly combining the solutions to various subproblems. Consider the shortest path of transformed words that represents the minimum edit distance between the strings intention and execution shown in Fig. 2.16. ,"{
    'Question': 'In the context of solving problems, what does the concept of breaking down a larger task into smaller parts help achieve?',
    'Options': 
    {
        'A)': 'Improving efficiency',
        'B)': 'Enhancing creativity',
        'C)': 'Increasing complexity',
        'D)': 'Reducing accuracy'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""In the context of string transformations, what does the edit distance represent?"",
    'Options': 
    {
        'A)': ""The total number of characters that need to be changed to transform one word into another."",
        'B)': ""The length of the longest common substring between two strings."",
        'C)': ""The sum of all individual character differences between two strings."",
        'D)': ""The maximum number of insertions required to make two strings equal.""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What does the concept of edit distance refer to?',
    'Options': {
        'A)': 'The number of characters that need to be changed, inserted, or deleted to transform one string into another.',
        'B)': 'The total length of all characters in both strings combined.',
        'C)': 'The sum of the lengths of two strings divided by their greatest common divisor.',
        'D)': 'The difference in alphabetical order between two strings.'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""How does the concept of edit distance relate to dynamic programming?"",
    'Options': 
    {
        'A)': ""Edit distance helps in finding the longest common subsequence."",
        'B)': ""Dynamic programming uses edit distance to find the shortest transformation sequence."",
        'C)': ""Edit distance is unrelated to dynamic programming."",
        'D)': ""Dynamic programming solves problems based on the maximum edit distance.""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'How does the concept of edit distance help solve the shortest path problem?',
    'Options': {
        'A)': 'It helps determine the longest common substring.',
        'B)': 'It aids in identifying the most similar transformation sequence.',
        'C)': 'It assists in finding the lexicographically smallest string.',
        'D)': 'It enables calculating the maximum number of transformations needed.'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What strategy should be used to find the optimal solution for transforming the string 'intention' into 'execution'? "", 
    'Options': 
    {
        'A)': ""Manually compare each character pair and calculate the edit distance directly."",
        'B)': ""Break down the transformation into smaller steps using dynamic programming."",
        'C)': ""Use an iterative approach to solve the problem step-by-step."",
        'D)': ""Start with the longest common prefix and work backwards.""
    },
    'Correct answer': 'B)'
}"
27,coreference,"The move from mention-pair to mention-ranking approaches was pioneered by Yang et al. (2003) and Iida et al. (2003) who proposed pairwise ranking methods, then extended by Denis and Baldridge (2008) who proposed to do ranking via a softmax over all prior mentions. The idea of doing mention detection, anaphoricity, and coreference jointly in a single end-to-end model grew out of the early proposal of Ng (2005b) to use a dummy antecedent for mention-ranking, allowing non-referential` to be a choice for coreference classifiers, Denis and Baldridge`s 2007 joint system combining anaphoricity classifier probabilities with coreference probabilities, the Denis and Baldridge (2008) ranking model, and the Rahman and Ng (2009) proposal to train the two models jointly with a single objective. ","{
    'Question': 'In what year did Denis and Baldridge propose their ranking model?',
    'Options': 
    {
        'A)': '1995',
        'B)': '2004',
        'C)': '2006',
        'D)': '2008'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""Who first introduced the concept of doing mention detection, anaphoricity, and coreference jointly in a single end-to-end model?"",
    'Options': 
    {
        'A)': ""Ng (2005a)"",
        'B)': ""Denis and Baldridge (2008)"",
        'C)': ""Rahman and Ng (2009)"",
        'D)': ""Yang et al. (2003)""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What did Denis and Baldridge propose to extend the pairwise ranking methods?',
    'Options': 
    {
        'A)':'mention-detection',
        'B)': 'anaphoricity',
        'C)': 'coreference',
        'D)':'mention-ranking'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""How did the concept of coreference evolve from its initial stages to more advanced techniques?"",
    'Options': 
    {
        'A)': ""Coreference evolved from being purely a classification task into a multi-task learning approach."",
        'B)': ""The focus shifted from pairwise ranking to individual mention detection."",
        'C)': ""Coreference became less relevant due to advancements in other NLP tasks."",
        'D)': ""Coreference remained unchanged throughout these developments.""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What did Denis and Baldridge propose to extend their previous work?',
    'Options':
    {
        'A)': 'Pairwise ranking methods',
        'B)': 'Softmax over all prior mentions',
        'C)': 'Mention detection, anaphoricity, and coreference',
        'D)': 'Dummy antecedent for mention-ranking'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What is the next step after proposing pairwise ranking methods?"", 
    'Options': 
    {
        'A)': ""Propose a new method for mention-detection"",
        'B)': ""Extend the pairwise ranking approach using a softmax function"",
        'C)': ""Combine anaphoricity and coreference classifications in a single model"",
        'D)': ""Train both anaphoricity and coreference classifiers separately""
    },
    'Correct answer': 'B)'
}"
28,question answering,"We can also cast more complex tasks as word prediction. Consider question answering, in which the system is given a question (for example a question with a simple factual answer) and must give a textual answer; we introduce this task in detail in Chapter 14. We can cast the task of question answering as word prediction by giving a language model a question and a token like A: suggesting that an answer should come next: ","{
    'Question': 'In question answering, what does the system need to do?',
    'Options': 
    {
        'A)': 'Generate a new question',
        'B)': 'Predict the next word in a sentence',
        'C)': 'Translate between languages',
        'D)': 'Answer a complex factual question'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""In question answering, what does it mean when a language model suggests an answer?"",
    'Options': 
    {
        'A)': ""It means the model has no idea about the answer."",
        'B)': ""It means the model predicts the next word in the sentence."",
        'C)': ""It means the model provides a detailed explanation of the question."",
        'D)': ""It means the model gives a specific answer based on the input.""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'How do you convert a question into word prediction for better understanding?',
    'Options': 
    {
        'A)': 'By increasing the complexity of questions',
        'B)': 'By providing more detailed answers',
        'C)': 'By using advanced algorithms for prediction',
        'D)': 'By structuring questions in a way that makes them easier to predict',
        'E)': 'By limiting the number of possible answers'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""How does question answering relate to word prediction?"", 
    'Options': 
    {
        'A)': ""Word prediction is irrelevant to question answering."",
        'B)': ""Question answering involves predicting answers based on questions."",
        'C)': ""Word prediction focuses on generating new sentences, while question answering doesn't."",
        'D)': ""Both are about predicting future events.""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'How do you convert a question into word prediction?',
    'Options': 
    {
        'A)': 'By adding a specific token',
        'B)': 'Using machine learning algorithms',
        'C)': 'Increasing the complexity of questions',
        'D)': 'Reducing the number of tokens',
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What is the first step in casting the task of question answering as word prediction?"", 
    'Options': 
    {
        'A)': ""Give a language model a question"",
        'B)': ""Provide a token like A:"",
        'C)': ""Suggest that an answer should come next"",
        'D)': ""Introduce the task in detail""
    },
    'Correct answer': 'A)'
}"
29,dialogue,"Modern speech synthesis has a wide variety of applications. TTS is used in conversational agents that conduct dialogues with people, plays a role in devices that read out loud for the blind or in games, and can be used to speak for sufferers of neurological disorders, such as the late astrophysicist Steven Hawking who, after he lost the use of his voice because of ALS, spoke by manipulating a TTS system. ","{
    'Question': 'In what application does TTS play a significant role?',
    'Options':
    {
        'A)': 'Conversational agents',
        'B)': 'Devices reading out loud for the blind',
        'C)': 'Games',
        'D)': 'Speaking for sufferers of neurological disorders'
    },
    'Correct answer': 'B) Devices reading out loud for the blind'
}","{
    'Question': ""In what application does TTS play a significant role besides those mentioned?"",
    'Options': 
    {
        'A)': ""Automated customer service"",
        'B)': ""Virtual reality experiences"",
        'C)': ""Medical training simulations"",
        'D)': ""Educational software""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What is an example of a modern application of TTS?',
    'Options': 
    {
        'A)': 'Automated translation',
        'B)': 'Speech recognition',
        'C)': 'Voice cloning',
        'D)': 'Text-to-speech conversion'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""What are some common uses of TTS in modern speech synthesis?"",
    'Options': 
    {
        'A)': ""Conversational agents conducting dialogues, reading aloud for the blind, speaking for those with neurological disorders"",
        'B)': ""Conversational agents conducting dialogues, playing music, reading aloud for the blind"",
        'C)': ""Reading aloud for the blind, playing music, speaking for those with neurological disorders"",
        'D)': ""Conversational agents conducting dialogues, playing music, speaking for those with neurological disorders, reading aloud for the blind""
    },
    'Correct answer': 'A)'
}","{
    'Question': '""TTS"" stands for what?',
    'Options': 
    {
        'A)': 'Text-to-Speech',
        'B)': 'Speech-to-Text',
        'C)': 'Text-to-Image',
        'D)': 'Voice-to-Voice'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""In which application does TTS play a crucial role?"", 
    'Options': 
    {
        'A)': ""Conversational agents conducting dialogues with people"",
        'B)': ""Devices reading out loud for the blind"",
        'C)': ""Games"",
        'D)': ""Speaking for sufferers of neurological disorders""
    },
    'Correct answer': 'A)'
}"
30,hidden,"A Markov chain is useful when we need to compute a probability for a sequence of observable events. In many cases, however, the events we are interested in are hidden: we don`t observe them directly. For example we don`t normally observe part-of-speech tags in a text. Rather, we see words, and must infer the tags from the word sequence. We call the tags hidden because they are not observed. ","{
    'Question': 'In what way do we refer to the tags as hidden?',
    'Options': 
    {
        'A)': 'Because they are always visible',
        'B)': 'Because they are never observed',
        'C)': 'Because they are directly observed',
        'D)': 'Because they are indirectly observed'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""In the context of computing probabilities for sequences of observable events, what do we refer to as 'hidden'? "", 
    'Options': 
    {
        'A)': ""Hidden Markov Models"",
        'B)': ""Observable events"",
        'C)': ""Part-of-speech tags"",
        'D)': ""Probability distributions""
    },
    'Correct answer': 'C)'
}","{
    'Question': 'What does it mean when an event is considered ""hidden"" in the context of computing probabilities?',
    'Options': 
    {
        'A)': 'The event can be directly observed.',
        'B)': 'The event is always visible in the data set.',
        'C)': 'The event is not directly observable but inferred from other observations.',
        'D)': 'The event has no impact on the computation of probabilities.'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""Explain why it is necessary to use a Markov chain when dealing with sequences of observable events?"",
    'Options': 
    {
        'A)': ""To calculate probabilities of observable events directly."",
        'B)': ""To handle sequences where some events are not observed."",
        'C)': ""To simplify complex data structures."",
        'D)': ""To increase computational speed.""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What does it mean if an event is considered hidden?',
    'Options': 
    {
        'A)': 'It can be easily observed.',
        'B)': 'It is always observed by humans.',
        'C)': 'It cannot be observed directly.',
        'D)': 'It is visible but hard to understand.'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What should be done with the hidden events in a Markov chain?"", 
    'Options': 
    {
        'A)': ""Ignore them completely"",
        'B)': ""Observe them directly"",
        'C)': ""Model their probabilities based on observed data"",
        'D)': ""Use machine learning algorithms to predict them""
    },
    'Correct answer': 'C)'
}"
31,question answering,"A second problem is that simply prompting a large language model doesn`t allow us to ask questions about proprietary data. A common use of question answering is about data like our personal email or medical records. Or a company may have internal documents that contain answers for customer service or internal use. Or legal firms need to ask questions about legal discovery from proprietary documents. Finally, static large language models also have problems with questions about rapidly changing information (like questions about something that happened last week) since LLMs won`t have up-to-date information from after their release data. For this reason the most common way to do question-answering with LLMs is retrieval-augmented generation or RAG, and that is the method we will focus on in this chapter. In RAG we use information retrieval (IR) techniques to retrieve documents that are likely to have information that might help answer the question. Then we use a large language model to generate an answer given these documents. Basing our answers on retrieved documents can solve some of the problems with using simple prompting to answer questions. First, it helps ensure that the answer is grounded in facts from some curated dataset. And the system can give the user the answer accompanied by the context of the passage or document the answer came from. This information can help users have confidence in the accuracy of the answer (or help them spot when it is wrong!). And these retrieval techniques can be used on any proprietary data we want, such as legal or medical data for those applications. ","{
    'Question': 'What is one benefit of using retrieval-augmented generation (RAG) over simple prompting for question answering?',
    'Options': 
    {
        'A)': 'It ensures the answer is based on real-time data.',
        'B)': 'It allows users to directly access proprietary data.',
        'C)': 'It guarantees the answer is accurate and comes with contextual information.',
        'D)': 'It eliminates the need for large language models.'
    },
    'Correct answer': 'C)'
}","Term: Retrieval-Augmented Generation (RAG)

Context: Retrieval-Augmented Generation (RAG) is a technique used to improve the performance of large language models (LLMs) in answering questions. It involves retrieving relevant documents from a database before generating an answer.

{
    'Question': ""What is Retrieval-Augmented Generation (RAG)?"", 
    'Options': 
    {
        'A)': ""It is a type of artificial intelligence."",
        'B)': ""It is a process of improving LLMs through prompt engineering."",
        'C)': ""It is a method of generating answers based solely on existing knowledge."",
        'D)': ""It is a technique combining LLMs with information retrieval methods.""
    },
    'Correct answer': 'D)'
}","{
    'Question': 'What is a common application of question answering?',
    'Options': 
    {
        'A)': 'Personalizing advertisements',
        'B)': 'Generating new ideas',
        'C)': 'Answering complex mathematical equations',
        'D)': 'Retrieving documents containing relevant information'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""What is one of the primary methods used to address questions involving rapidly changing information within large language models?"",
    'Options': 
    {
        'A)': ""Prompt-based generation"",
        'B)': ""Retrieval-augmented generation (RAG)"",
        'C)': ""Direct prompt completion"",
        'D)': ""Manual annotation of data""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What is a common application of question answering?',
    'Options': 
    {
        'A)': 'Personal emails',
        'B)': 'Medical records',
        'C)': 'Internal documents',
        'D)': 'Legal discovery',
        'E)': 'All of the above'
    },
    'Correct answer': 'E)'
}","{
    'Question': ""What is the primary benefit of using retrieval-augmented generation (RAG) over traditional prompt-based methods for question answering?"", 
    'Options': 
    {
        'A)': ""It ensures the answer is based on current events."",
        'B)': ""It allows for real-time updates to the answer."",
        'C)': ""It provides more accurate and reliable answers."",
        'D)': ""It eliminates the need for human intervention.""
    },
    'Correct answer': 'C)'
}"
32,neural network, The power of neural networks comes from the ability of early layers to learn ,"{
    'Question': 'What does the power of neural networks come from?', 
    'Options': 
    {
        'A)': 'The ability of late layers to learn',
        'B)': 'The complexity of their architecture',
        'C)': 'Their capacity for pattern recognition',
        'D)': 'The speed at which they process data'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""Which layer of a neural network primarily focuses on learning patterns and features from raw data?"",
    'Options': 
    {
        'A)': ""Input Layer"",
        'B)': ""Hidden Layers"",
        'C)': ""Output Layer"",
        'D)': ""Feedback Layer""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'How do early layers of a neural network contribute to its overall functionality?',
    'Options': 
    {
        'A)': 'They specialize in recognizing specific patterns.',
        'B)': 'They enhance the learning process by reducing computational complexity.',
        'C)': 'They improve accuracy by fine-tuning weights iteratively.',
        'D)': 'They increase the speed of data processing through parallel computations.'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What aspect of neural networks allows for their powerful learning capabilities?"",
    'Options': 
    {
        'A)': ""The use of deep architectures"",
        'B)': ""Their capacity to store large amounts of data"",
        'C)': ""The complexity of their connections"",
        'D)': ""The speed at which they process information""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'How do early layers of a neural network contribute to its overall learning capability?',
    'Options': {
        'A)': 'They specialize in recognizing specific patterns.',
        'B)': 'They enhance the accuracy of predictions made by later layers.',
        'C)': 'They reduce the complexity of input data before processing.',
        'D)': 'They increase the speed of computation during training.'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What is an essential step for developing a deep learning model?"", 
    'Options': 
    {
        'A)': ""Training the entire network at once"",
        'B)': ""Fine-tuning only the last layer"",
        'C)': ""Starting with simple architectures and gradually increasing complexity"",
        'D)': ""Using pre-trained models as a starting point""
    },
    'Correct answer': 'C)'
}"
33,temporal expression,"The goal of temporal analysis, is to link times to events and then fit all these events into a complete timeline. This ambitious task is the subject of considerable current research but solving it with a high level of accuracy is beyond the capabilities of current systems. A somewhat simpler, but still useful, task is to impose a partial ordering on the events and temporal expressions mentioned in a text. Such an ordering can provide many of the same benefits as a true timeline. An example of such a partial ordering is the determination that the fare increase by American Airlines came after the fare increase by United in our sample text. Determining such an ordering can be viewed as a binary relation detection and classification task. ","{
    'Question': 'In temporal analysis, what does imposing a partial ordering on events and temporal expressions primarily help achieve?',
    'Options': 
    {
        'A)': 'It helps solve the ambitious task of creating a complete timeline.',
        'B)': 'It provides more accurate event timelines than traditional methods.',
        'C)': 'It allows for easier comparison between different texts.',
        'D)': 'It enables researchers to focus solely on fare increases.'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What type of task involves imposing a partial ordering on events and temporal expressions in a text?"",
    'Options': 
    {
        'A)': ""Temporal analysis"",
        'B)': ""Binary relation detection and classification"",
        'C)': ""Synovial joint connection"",
        'D)': ""True timeline creation""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What does the goal of temporal analysis aim to achieve?',
    'Options': 
    {
        'A)': 'To identify the exact time when each event occurred',
        'B)': 'To create a detailed timeline of every event in a text',
        'C)': 'To classify different types of temporal expressions in a text',
        'D)': 'To determine the sequence of events based on their occurrence order'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""What is one simple task related to imposing a partial ordering on events and temporal expressions?"",
    'Options': 
    {
        'A)': ""Determining if a time event occurs before another"",
        'B)': ""Identifying the exact sequence of events"",
        'C)': ""Creating a detailed timeline for every text"",
        'D)': ""Predicting future events based on past ones""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'Which of the following best describes the process of imposing a partial ordering on events and temporal expressions?',
    'Options': 
    {
        'A)': 'Developing a comprehensive timeline',
        'B)': 'Building a complex system for event prediction',
        'C)': 'Classifying binary relations between events',
        'D)': 'Organizing data based on chronological order'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What is the first step in imposing a partial ordering on events and temporal expressions?"", 
    'Options': 
    {
        'A)': ""Identify the events and temporal expressions present in the text."",
        'B)': ""Classify each event based on its type."",
        'C)': ""Determine if there is a chronological relationship between two events."",
        'D)': ""Combine all identified events into a single timeline.""
    },
    'Correct answer': 'A)'
}"
34,translation,"Language models are evaluated in many ways. we introduced some evaluations for in Section 10.4, including measuring the language model`s perplexity on a test set, evaluating its accuracy on various NLP tasks, as well as benchmarks that help measure efficiency, toxicity, fairness, and so on. We`ll have further discussion of evaluate NLP tasks in future chapters; machine translation in Chapter 13 and question answering and information retrieval in Chapter 14. ","{
    'Question': 'In what chapter will they discuss how to evaluate NLP tasks?',
    'Options': 
    {
        'A)': 'Chapter 12',
        'B)': 'Chapter 13',
        'C)': 'Chapter 14',
        'D)': 'Chapter 15'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""Which evaluation method involves assessing the difficulty level of a language model on unseen data?"",
    'Options': 
    {
        'A)': ""Perplexity measurement"",
        'B)': ""Accuracy assessment"",
        'C)': ""Efficiency benchmarking"",
        'D)': ""Toxicity testing""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What is one way language models are evaluated?',
    'Options': 
    {
        'A)': 'Measuring the language model\'s perplexity on a training set',
        'B)': 'Evaluating its accuracy on various NLP tasks',
        'C)': 'Benchmarking based on efficiency, toxicity, and fairness',
        'D)': 'Discussing how to improve language models'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What evaluation method was discussed in relation to language models in this section?"",
    'Options': 
    {
        'A)': ""Perplexity measurement"",
        'B)': ""Accuracy assessment"",
        'C)': ""Efficiency benchmarking"",
        'D)': ""Toxicity analysis""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What type of evaluation is not mentioned in the context?',
    'Options': 
    {
        'A)': 'Measuring the language model`s perplexity on a test set',
        'B)': 'Evaluating its accuracy on various NLP tasks',
        'C)': 'Benchmarking for efficiency, toxicity, and fairness',
        'D)': 'Discussing how to evaluate NLP tasks in future chapters'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What is an effective way to improve the performance of a language model?"", 
    'Options': 
    {
        'A)': ""Only focus on increasing the size of the training dataset."",
        'B)': ""Ensure consistent evaluation metrics across different stages of development."",
        'C)': ""Regularly update the model with new data to maintain relevance."",
        'D)': ""Implement feedback loops to continuously refine the model.""
    },
    'Correct answer': 'D)'
}"
35,question answering," QUESTION ANSWERING, INFORMATION RETRIEVAL, AND RAG ","{
    'Question': 'In what field is the term ""question answering"" primarily used?',
    'Options': 
    {
        'A)': 'Computer Science',
        'B)': 'Mathematics',
        'C)': 'Physics',
        'D)': 'Chemistry'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""In which field of computer science does the process of finding answers to questions fall under?"",
    'Options': 
    {
        'A)': ""Machine Learning"",
        'B)': ""Natural Language Processing"",
        'C)': ""Information Retrieval"",
        'D)': ""Computer Vision""
    },
    'Correct answer': 'C)'
}","{
    'Question': 'What is the main focus of this topic?',
    'Options': 
    {
        'A)': 'Information retrieval',
        'B)': 'RAG',
        'C)': 'Question answering',
        'D)': 'Both A) and C)'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""What does the field of study encompass when it comes to understanding how questions are answered?"", 
    'Options': 
    {
        'A)': ""Information retrieval techniques only"",
        'B)': ""RAG specifically"",
        'C)': ""Both Information Retrieval and Question Answering"",
        'D)': ""None of the above""
    },
    'Correct answer': 'C)'
}","{
    'Question': 'What is the process of finding relevant information from a database?',
    'Options': 
    {
        'A)': 'Developing questions',
        'B)': 'Organizing answers',
        'C)': 'Building databases',
        'D)': 'Planning searches'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""What is the first step in organizing an effective question-answering system?"", 
    'Options': 
    {
        'A)': ""Collecting data for training models"",
        'B)': ""Designing algorithms for retrieval"",
        'C)': ""Implementing feedback mechanisms"",
        'D)': ""Developing user interfaces""
    },
    'Correct answer': 'A)'
}"
36,text classification,"There are a variety of sources covering the many kinds of text classification tasks. For sentiment analysis see Pang and Lee (2008), and Liu and Zhang (2012). Stamatatos (2009) surveys authorship attribute algorithms. On language identification see Jauhiainen et al. (2019); Jaech et al. (2016) is an important early neural system. The task of newswire indexing was often used as a test case for text classification algorithms, based on the Reuters-21578 collection of newswire articles. ","{
    'Question': 'Which source provides an overview of various text classification tasks?',
    'Options': 
    {
        'A)': 'Pang and Lee (2008)',
        'B)': 'Liu and Zhang (2012)',
        'C)': 'Stamatatos (2009)',
        'D)': 'Jaech et al. (2016)'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""Which source provides an overview of various types of text classification tasks?"",
    'Options': 
    {
        'A)': ""Pang and Lee (2008)"",
        'B)': ""Liu and Zhang (2012)"",
        'C)': ""Stamatatos (2009)"",
        'D)': ""Jaech et al. (2016)""
    },
    'Correct answer': 'C)'
}","{
    'Question': 'What type of algorithm does Jaech et al. (2016) develop?',
    'Options': 
    {
        'A)': 'Sentiment analysis',
        'B)': 'Authorship attribute algorithms',
        'C)': 'Newswire indexing',
        'D)': 'Language identification'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""What type of algorithm is most commonly used for sentiment analysis?"",
    'Options': 
    {
        'A)': ""Stomatatos (2009)"",
        'B)': ""Pang and Lee (2008)"",
        'C)': ""Jaech et al. (2016)"",
        'D)': ""Jauhiainen et al. (2019)""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'Which type of text classification involves identifying the source of written material?',
    'Options': 
    {
        'A)': 'Sentiment analysis',
        'B)': 'Authorship attribute algorithms',
        'C)': 'Language identification',
        'D)': 'Newswire indexing'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""Which method should be applied first when performing text classification?"", 
    'Options': 
    {
        'A)': ""Language Identification"",
        'B)': ""Newswire Indexing"",
        'C)': ""Sentiment Analysis"",
        'D)': ""Authorship Attribute Algorithms""
    },
    'Correct answer': 'C)'
}"
37,parse tree,"The main reason computational systems use semantic roles is to act as a shallow meaning representation that can let us make simple inferences that aren`t possible from the pure surface string of words, or even from the parse tree. To extend the earlier examples, if a document says that Company A acquired Company B, we`d like to know that this answers the query Was Company B acquired? despite the fact that the two sentences have very different surface syntax. Similarly, this shallow semantics might act as a useful intermediate language in machine translation. ","{
    'Question': 'In what way does using semantic roles help computational systems?',
    'Options':
    {
        'A)': 'It simplifies complex sentence structures.',
        'B)': 'It allows for deeper understanding of the sentence structure.',
        'C)': 'It reduces the need for machine translation processes.',
        'D)': 'It increases the complexity of sentence analysis.'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""In what way does the concept of parse trees help in understanding the relationship between sentences with different surface syntax but similar meanings?"",
    'Options': 
    {
        'A)': ""It simplifies complex sentence structures."",
        'B)': ""It provides a visual representation of word order."",
        'C)': ""It allows for deeper semantic analysis beyond just surface strings."",
        'D)': ""It eliminates the need for machine translation.""
    },
    'Correct answer': 'C)'
}","{
    'Question': 'What does the term ""parse tree"" refer to?',
    'Options': 
    {
        'A)': 'A diagram showing the syntactic structure of a sentence',
        'B)': 'The process of breaking down a sentence into its grammatical components',
        'C)': 'A method for translating languages between natural and artificial intelligence',
        'D)': 'An algorithm used by computers to understand human speech',
    },
    'Correct answer': 'A)'
}","{
    'Question': ""Explain how shallow semantics using parse trees help in making inferences about sentence meanings?"",
    'Options': 
    {
        'A)': ""Parse trees are irrelevant for understanding sentence meanings."",
        'B)': ""Shallow semantics using parse trees allow extracting deeper semantic structures beyond just surface syntax."",
        'C)': ""Shallow semantics using parse trees prevent any inference from being made."",
        'D)': ""Parse trees only show syntactic structure and do not contribute to semantic understanding.""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What does the parse tree help with?',
    'Options': 
    {
        'A)': 'It helps understand the deep structure of the sentence.',
        'B)': 'It aids in making complex inferences about the sentence.',
        'C)': 'It assists in translating languages between each other.',
        'D)': 'It simplifies the surface syntax for easier understanding.'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""What is the primary purpose of using semantic roles in computational systems?"", 
    'Options': 
    {
        'A)': ""To enhance the complexity of sentence structures"",
        'B)': ""To increase the depth of understanding beyond mere word-level analysis"",
        'C)': ""To improve the accuracy of natural language processing tasks"",
        'D)': ""To simplify the translation process for foreign languages""
    },
    'Correct answer': 'B)'
}"
38,translation,"Neural networks had been applied at various times to various aspects of machine translation; for example Schwenk et al. (2006) showed how to use neural language models to replace n-gram language models in a Spanish-English system based on IBM Model 4. The modern neural encoder-decoder approach was pioneered by Kalchbrenner and Blunsom (2013), who used a CNN encoder and an RNN decoder, and was first applied to MT by Bahdanau et al. (2015). The transformer encoderdecoder was proposed by Vaswani et al. (2017) (see the History section of Chapter 9). ","{
    'Question': 'In what year did the transformer encoder-decoder architecture first appear in the field of machine translation?',
    'Options': 
    {
        'A)': '2006',
        'B)': '2013',
        'C)': '2015',
        'D)': '2017'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""Which type of neural network was initially applied to machine translation using a CNN encoder and an RNN decoder?"", 
    'Options': 
    {
        'A)': ""Convolutional Neural Network (CNN)"",
        'B)': ""Recurrent Neural Network (RNN)"",
        'C)': ""Transformer EncoderDecoder"",
        'D)': ""None of the above""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What is the main difference between the traditional neural network-based approaches to machine translation and the modern transformer model?',
    'Options': 
    {
        'A)': 'The traditional methods use recurrent neural networks while the transformer uses convolutional neural networks.',
        'B)': 'The traditional methods rely on statistical models whereas the transformer uses deep learning techniques.',
        'C)': 'The traditional methods are more accurate than the transformer due to their simpler architecture.',
        'D)': 'The traditional methods require less computational resources than the transformer.'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What type of neural network architecture was initially used for machine translation before the transformer model?"", 
    'Options': 
    {
        'A)': ""RNN-based Encoder-Decoder"",
        'B)': ""CNN-based Encoder-Decoder"",
        'C)': ""Transformer Encoder-Decoder"",
        'D)': ""Both A and B""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What is the main difference between the traditional neural network-based approaches to machine translation and the modern transformer model?',
    'Options': 
    {
        'A)': 'The traditional methods use recurrent neural networks while the transformer uses convolutional neural networks.',
        'B)': 'The traditional methods rely on statistical models whereas the transformer uses deep learning techniques.',
        'C)': 'The traditional methods require more computational resources than the transformer.',
        'D)': 'The traditional methods are less accurate than the transformer.'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What is the next step after applying the transformer encoder-decoder model in machine translation?"", 
    'Options': 
    {
        'A)': ""Train the model using backpropagation through time (BPTT)"",
        'B)': ""Use it directly as-is without further modifications"",
        'C)': ""Integrate it with attention mechanisms to improve accuracy"",
        'D)': ""Implement beam search to find the most likely translation""
    },
    'Correct answer': 'C)'
}"
39,dependency,It turns out that finding the best dependency parse for S is equivalent to finding the maximum spanning tree over G. A spanning tree over a graph G is a subset of G that is a tree and covers all the vertices in G; a spanning tree over G that starts from the ROOT is a valid parse of S. A maximum spanning tree is the spanning tree with the highest score. Thus a maximum spanning tree of G emanating from the ROOT is the optimal dependency parse for the sentence. ,"{
    'Question': 'In the context, what does the spanning tree represent?',
    'Options': 
    {
        'A)': 'The set of edges connecting nodes',
        'B)': 'The sequence of words in the sentence',
        'C)': 'The structure of the entire graph',
        'D)': 'The shortest path between two nodes'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What does finding the best dependency parse for a sentence entail?"", 
    'Options': 
    {
        'A)': ""Identifying the most frequent word usage"",
        'B)': ""Determining the shortest path between two nodes"",
        'C)': ""Selecting the largest connected component in an undirected graph"",
        'D)': ""Finding the maximum spanning tree starting from the root node""
    },
    'Correct answer': 'D)'
}","{
    'Question': 'What does it mean when we say that finding the best dependency parse for S is equivalent to finding the maximum spanning tree over G?',
    'Options': 
    {
        'A)': 'It means that both problems are solved by using different algorithms.',
        'B)': 'It means that solving one problem helps solve the other, but not vice versa.',
        'C)': 'It means that these two concepts are unrelated.',
        'D)': 'It means that they are completely identical.'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What does it mean when we find the best dependency parse for S?"",
    'Options': 
    {
        'A)': ""It means finding the shortest path between two points."",
        'B)': ""It means identifying the most efficient route through a network."",
        'C)': ""It means determining the longest possible path in a graph."",
        'D)': ""It means creating a spanning tree that maximizes coverage.""
    },
    'Correct answer': 'D)'
}","{
    'Question': 'What does a spanning tree over a graph G represent?',
    'Options': 
    {
        'A)': 'A path connecting two nodes',
        'B)': 'The shortest distance between any two nodes',
        'C)': 'A connected subgraph that spans all nodes',
        'D)': 'A cycle formed by adding edges to the original graph'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What is the first step in determining the optimal dependency parse for a sentence?"", 
    'Options': 
    {
        'A)': ""Find the maximum spanning tree over the graph G."",
        'B)': ""Identify the root vertex in the graph G."",
        'C)': ""Calculate the score of each spanning tree over G."",
        'D)': ""Select the spanning tree with the highest score as the optimal parse.""
    },
    'Correct answer': 'B)'
}"
40,temporal expression,"20.3 A useful functionality in newer email and calendar applications is the ability to associate temporal expressions connected with events in email (doctor`s appointments, meeting planning, party invitations, etc.) with specific calendar entries. Collect a corpus of email containing temporal expressions related to event planning. How do these expressions compare to the kinds of expressions commonly found in news text that we`ve been discussing in this chapter? ","{
    'Question': 'What are temporal expressions used for in newer email and calendar applications?',
    'Options':
    {
        'A)': 'To organize personal schedules',
        'B)': 'To track financial transactions',
        'C)': 'To analyze social media trends',
        'D)': 'To predict weather patterns'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What is the main purpose of associating temporal expressions with calendar entries in newer email and calendar applications?"", 
    'Options': 
    {
        'A)': ""To enhance user experience by providing reminders"",
        'B)': ""To improve data accuracy"",
        'C)': ""To increase productivity"",
        'D)': ""To reduce clutter on the screen""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'How are temporal expressions used differently in emails compared to news articles?',
    'Options': 
    {
        'A)': 'Temporal expressions in emails are more complex than those in news articles.',
        'B)': 'Temporal expressions in emails are less formal than those in news articles.',
        'C)': 'Temporal expressions in emails are often ambiguous and require interpretation.',
        'D)': 'Temporal expressions in emails are typically shorter and more concise.'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""How do temporal expressions from emails differ from those in news articles?"",
    'Options': 
    {
        'A)': ""Temporal expressions in emails are more formal than those in news articles."",
        'B)': ""Temporal expressions in emails are less precise than those in news articles."",
        'C)': ""Temporal expressions in emails are more common in casual communication."",
        'D)': ""Temporal expressions in emails are typically longer than those in news articles.""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What does the author suggest about the comparison between temporal expressions used in emails for event planning and those found in news texts?',
    'Options': 
    {
        'A)': 'The expressions are identical.',
        'B)': 'The expressions differ significantly.',
        'C)': 'The expressions have no relation.',
        'D)': 'The expressions overlap but are not interchangeable.'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""What is an example of how temporal expressions can be used in email for event planning?"", 
    'Options': 
    {
        'A)': ""Saying goodbye at the end of the day."",
        'B)': ""Placing an order online."",
        'C)': ""Sending a birthday message."",
        'D)': ""Booking a flight ticket.""
    },
    'Correct answer': 'D)'
}"
41,translation,"Because BLEU is a word-based metric, it is very sensitive to word tokenization, making it impossible to compare different systems if they rely on different tokenization standards, and doesn`t work as well in languages with complex morphology. Nonetheless, you will sometimes still see systems evaluated by BLEU, particularly for translation into English. In such cases it`s important to use packages that enforce standardization for tokenization like SACREBLEU (Post, 2018). ","{
    'Question': 'Why is BLEU considered sensitive to word tokenization?',
    'Options':
    {
        'A)': 'Because it uses machine learning algorithms.',
        'B)': 'Because it relies on natural language processing techniques.',
        'C)': 'Because it evaluates sentence-level translations.',
        'D)': 'Because it cannot handle non-English texts.'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""Which of the following is not an example of a system evaluation method involving translation?"",
    'Options': 
    {
        'A)': ""Evaluation using BLEU score"",
        'B)': ""Evaluation based on morphological complexity"",
        'C)': ""Evaluation through sentence similarity"",
        'D)': ""Evaluation via lexical diversity""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What makes BLEU difficult to compare different systems?',
    'Options': 
    {
        'A)': 'It relies on different tokenization standards.',
        'B)': 'BLEU is not sensitive to word tokenization.',
        'C)': 'BLEU works better in languages with simple morphology.',
        'D)': 'BLEU cannot handle translations.'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""Explain why BLEU might not always be suitable for comparing translations between different systems?"",
    'Options': 
    {
        'A)': ""Because BLEU is too sensitive to word tokenization and does not handle morphological complexity well."",
        'B)': ""Because BLEU cannot differentiate between different language tokens."",
        'C)': ""Because BLEU evaluates only sentence-level metrics which are insufficient for translation quality assessment."",
        'D)': ""Both A) and C)""
    },
    'Correct answer': 'D)'
}","{
    'Question': 'Why might BLEU not be suitable for evaluating translations?',
    'Options': 
    {
        'A)': 'Because BLEU is too slow',
        'B)': 'Because BLEU does not consider morphological complexity',
        'C)': 'Because BLEU cannot handle non-English inputs',
        'D)': 'Because BLEU requires manual tokenization'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What should you do when evaluating translations into English using BLEU?"", 
    'Options': 
    {
        'A)': ""Use any package without enforcing standardization."",
        'B)': ""Ignore the sensitivity of BLEU to word tokenization."",
        'C)': ""Evaluate translations based solely on their meaning."",
        'D)': ""Use SACREBLEU to ensure consistent tokenization.""
    },
    'Correct answer': 'D)'
}"
42,word error,"Alignment Knowing the minimum edit distance is useful for algorithms like finding potential spelling error corrections. But the edit distance algorithm is important in another way; with a small change, it can also provide the minimum cost alignment between two strings. Aligning two strings is useful throughout speech and language processing. In speech recognition, minimum edit distance alignment is used to compute the word error rate (Chapter 16). Alignment plays a role in machine translation, in which sentences in a parallel corpus (a corpus with a text in two languages) need to be matched to each other. ","{
    'Question': 'In what application of string alignment does the minimum edit distance play a crucial role?',
    'Options':
    {
        'A)': 'Speech recognition',
        'B)': 'Machine learning',
        'C)': 'Data compression',
        'D)': 'Image segmentation'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""In what application of string alignment does the minimum edit distance play a crucial role?"", 
    'Options': 
    {
        'A)': ""Speech recognition"",
        'B)': ""Machine learning"",
        'C)': ""Data compression"",
        'D)': ""Text classification""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What does the minimum edit distance algorithm help find when applied to two strings?',
    'Options': 
    {
        'A)': 'The maximum similarity score',
        'B)': 'The shortest path length',
        'C)': 'The longest common subsequence',
        'D)': 'The optimal alignment cost'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""Explain how the concept of word error rate relates to the use of minimum edit distance in speech recognition?"",
    'Options': 
    {
        'A)': ""Word error rate measures the total number of errors made during transcription."",
        'B)': ""The minimum edit distance helps determine the most efficient way to align phonemes in speech signals."",
        'C)': ""Minimum edit distance is only used for correcting grammatical mistakes in written texts."",
        'D)': ""Alignment does not play any significant role in speech recognition.""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'In speech recognition, what metric is used to measure the accuracy of word errors?',
    'Options': 
    {
        'A)': 'Minimum edit distance',
        'B)': 'Jaccard similarity',
        'C)': 'Hamming distance',
        'D)': 'Levenshtein distance'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""In speech recognition, what is the purpose of using minimum edit distance alignment?"", 
    'Options': 
    {
        'A)': ""To find the most common word errors"",
        'B)': ""To calculate the overall sentence length"",
        'C)': ""To determine the best pronunciation of words"",
        'D)': ""To measure the similarity between spoken and written words""
    },
    'Correct answer': 'D)'
}"
43,lexicon,"Most of the semi-supervised methods we describe for extending sentiment dictionaries drew on the early idea that synonyms and antonyms tend to co-occur in the same sentence (Miller and Charles 1991, Justeson and Katz 1991, Riloff and Shepherd 1997). Other semi-supervised methods for learning cues to affective meaning rely on information extraction techniques, like the AutoSlog pattern extractors (Riloff and Wiebe, 2003). Graph based algorithms for sentiment were first suggested by Hatzivassiloglou and McKeown (1997), and graph propagation became a standard method (Zhu and Ghahramani 2002, Zhu et al. 2003, Zhou et al. 2004a, Velikovich et al. 2010). Crowdsourcing can also be used to improve precision by filtering the result of semi-supervised lexicon learning (Riloff and Shepherd 1997, Fast et al. 2016). ","{
    'Question': 'What did Miller and Charles draw upon in their work?',
    'Options': 
    {
        'A)': 'Synonyms and antonyms',
        'B)': 'Graph-based algorithms',
        'C)': 'Crowdsourced data',
        'D)': 'AutoSlog pattern extractors'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""Which method was not initially proposed for learning cues to affective meaning?"",
    'Options': 
    {
        'A)': ""Graph-based algorithms"",
        'B)': ""Crowdsourcing"",
        'C)': ""AutoSlog pattern extractors"",
        'D)': ""Semi-supervised methods""
    },
    'Correct answer': 'C)'
}","{
    'Question': 'What is one way to enhance the accuracy of sentiment dictionaries?',
    'Options': 
    {
        'A)': 'Using synonym and antonym co-occurrence patterns',
        'B)': 'Implementing graph-based algorithms',
        'C)': 'Employing crowdsourced data collection',
        'D)': 'Extracting AutoSlog patterns manually'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What are some common approaches to creating sentiment dictionaries mentioned in the context?"",
    'Options': 
    {
        'A)': ""Using synonym and antonym co-occurrence patterns"",
        'B)': ""Extracting information from graphs"",
        'C)': ""Crowdsourcing feedback"",
        'D)': ""Combining all of the above""
    },
    'Correct answer': 'D)'
}","{
    'Question': 'Which technique was not mentioned as being used for improving the precision of sentiment dictionaries?',
    'Options': 
    {
        'A)': 'Crowdsourcing',
        'B)': 'Graph-based algorithms',
        'C)': 'AutoSlog pattern extractors',
        'D)': 'Lexical analysis'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""What is an effective strategy for improving the accuracy of sentiment lexicons?"", 
    'Options': 
    {
        'A)': ""Use only auto-slog pattern extractors"",
        'B)': ""Implement crowdsourcing to filter results"",
        'C)': ""Focus solely on synonym and antonym co-occurrence patterns"",
        'D)': ""Combine both auto-slog pattern extractors and crowdsourcing""
    },
    'Correct answer': 'B)'
}"
44,translation,"N-gram language models were very widely used over the next 30 years and more, across a wide variety of NLP tasks like speech recognition and machine translations, often as one of multiple components of the model. The contexts for these n-gram models grew longer, with 5-gram models used quite commonly by very efficient LM toolkits (Stolcke, 2002; Heafield, 2011). ","{
    'Question': 'What was a common use of N-gram language models before the widespread adoption of neural networks?',
    'Options':
    {
        'A)': 'Speech recognition',
        'B)': 'Machine translation',
        'C)': 'Text summarization',
        'D)': 'Named entity recognition'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""Which type of translation involves moving from one form to another within the same language?"",
    'Options': 
    {
        'A)': ""Interlingual Translation"",
        'B)': ""Transliteration"",
        'C)': ""Intersemiotic Translation"",
        'D)': ""Intra-lingual Translation""
    },
    'Correct answer': 'D)'
}","{
    'Question': 'What was a common use of N-gram language models before their widespread adoption?',
    'Options': 
    {
        'A)': 'Speech recognition',
        'B)': 'Machine translation',
        'C)': 'Natural language understanding',
        'D)': 'Text generation'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What was the primary use of N-gram language models before their widespread adoption?"",
    'Options': 
    {
        'A)': ""Translation"",
        'B)': ""Speech recognition"",
        'C)': ""Machine learning"",
        'D)': ""Natural language processing""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'Which type of language model was most commonly used alongside N-gram models for various NLP tasks?',
    'Options': 
    {
        'A)': 'Transformer models',
        'B)': 'RNN models',
        'C)': 'LSTM models',
        'D)': 'GPT models'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What is the first step in developing an effective translation system?"", 
    'Options': 
    {
        'A)': ""Selecting appropriate N-grams"",
        'B)': ""Training large-scale neural networks"",
        'C)': ""Implementing parallel corpora"",
        'D)': ""Designing statistical models""
    },
    'Correct answer': 'D)'
}"
45,hidden layer,"The hidden layer from the previous time step provides a form of memory, or context, that encodes earlier processing and informs the decisions to be made at later points in time. Critically, this approach does not impose a fixed-length limit on this prior context; the context embodied in the previous hidden layer can include information extending back to the beginning of the sequence. ","{
    'Question': 'What does the hidden layer from the previous time step provide?',
    'Options':
    {
        'A)': 'It provides a form of memory',
        'B)': 'It provides a form of prediction',
        'C)': 'It provides a form of reinforcement learning',
        'D)': 'It provides a form of classification'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""In machine learning, what role does the hidden layer play in encoding and informing decision-making processes?"", 
    'Options': 
    {
        'A)': ""It stores data directly."",
        'B)': ""It predicts future outcomes."",
        'C)': ""It retains past inputs and influences current outputs."",
        'D)': ""It encrypts sensitive information.""
    },
    'Correct answer': 'C)'
}","{
    'Question': 'What is the role of the hidden layer in the model?',
    'Options': 
    {
        'A)': 'It stores all past data for future predictions.',
        'B)': 'It processes new inputs directly without any context.',
        'C)': 'It retains information from previous steps as context for decision-making.',
        'D)': 'It limits the amount of context available based on the length of the sequence.'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""Explain how the hidden layer from the previous time step contributes to encoding and informing decision-making processes?"",
    'Options': 
    {
        'A)': ""The hidden layer stores specific data for immediate use."",
        'B)': ""The hidden layer retains information about past events but has no influence on future decisions."",
        'C)': ""The hidden layer accumulates contextual information over time, influencing subsequent decisions."",
        'D)': ""The hidden layer only updates its content based on new incoming data.""
    },
    'Correct answer': 'C)'
}","{
    'Question': 'How does the hidden layer from the previous time step contribute to decision-making?',
    'Options': 
    {
        'A)': 'It enhances the speed of computation.',
        'B)': 'It increases the accuracy of predictions.',
        'C)': 'It allows for unlimited contextual information.',
        'D)': 'It simplifies the learning process.'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What is the primary benefit of using a non-fixed-length context for the hidden layers?"", 
    'Options': 
    {
        'A)': ""It simplifies the computational requirements."",
        'B)': ""It allows for more complex decision-making processes."",
        'C)': ""It ensures faster training times."",
        'D)': ""It enables longer-term dependencies within sequences.""
    },
    'Correct answer': 'D)'
}"
46,translation,"Developing high quality supervised training data in this way is time consuming and costly. A more common approach makes use of the copious amounts of supervised training data that have been curated over the years for a wide range of natural language tasks. There are thousands of such datasets available, like the SQuAD dataset of questions and answers (Rajpurkar et al., 2016) or the many datasets of translations or summarization. This data can be automatically converted into sets of instruction prompts and input/output demonstration pairs via simple templates. ","{
    'Question': 'What method does the response suggest for converting large amounts of existing training data into usable formats?',
    'Options':
    {
        'A)': 'Manual transcription',
        'B)': 'Template-based conversion',
        'C)': 'Machine learning algorithms',
        'D)': 'Data anonymization'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""Which type of translation data is commonly used as an alternative to manually curating large datasets?"",
    'Options': 
    {
        'A)': ""Translation"",
        'B)': ""Summarization"",
        'C)': ""Dialogue"",
        'D)': ""Classification""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What method is commonly used instead of manually creating training data?',
    'Options': 
    {
        'A)': 'Manual creation',
        'B)': 'Translation',
        'C)': 'Summarization',
        'D)': 'Instruction prompts'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What is another common method besides manual curating of datasets for developing high-quality supervised training data?"",
    'Options': 
    {
        'A)': ""Manual transcription of speech"",
        'B)': ""Translation from one language to another"",
        'C)': ""Creation of new languages"",
        'D)': ""Designing algorithms""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'How do they convert large amounts of existing data into usable format?',
    'Options': 
    {
        'A)': 'They manually edit each piece of data.',
        'B)': 'They create new datasets from scratch.',
        'C)': 'They use simple templates to automate the process.',
        'D)': 'They rely on expert translators to translate the data.'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What method should be used to convert large amounts of existing datasets into instruction prompts and input/output demonstrations?"", 
    'Options': 
    {
        'A)': ""Manual transcription of each dataset entry"",
        'B)': ""Use complex templates with advanced algorithms"",
        'C)': ""Develop custom templates based on specific needs"",
        'D)': ""Automatically generate using simple templates""
    },
    'Correct answer': 'D)'
}"
47,translation,"The encoder-decoder architecture was applied to speech at about the same time by two different groups, in the Listen Attend and Spell system of Chan et al. (2016) and the attention-based encoder decoder architecture of Chorowski et al. (2014) and Bahdanau et al. (2016). By 2018 Transformers were included in this encoderdecoder architecture. Karita et al. (2019) is a nice comparison of RNNs vs Transformers in encoder-architectures for ASR, TTS, and speech-to-speech translation. ","{
    'Question': 'In which year did the Transformer model become part of the encoder-decoder architecture?', 
    'Options': 
    {
        'A)': '2015',
        'B)': '2017',
        'C)': '2018',
        'D)': '2016'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""In which year did Karita et al. publish their comparison study on RNNs versus Transformers in encoder-architecture applications?"",
    'Options': 
    {
        'A)': ""2015"",
        'B)': ""2017"",
        'C)': ""2018"",
        'D)': ""2019""
    },
    'Correct answer': 'D)'
}","{
    'Question': 'What architectural approach did Karita et al. (2019) compare between RNNs and Transformers?',
    'Options': 
    {
        'A)': 'Encoder-decoder',
        'B)': 'Attention-based',
        'C)': 'Transformer',
        'D)': 'RNNs'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""What did Karita et al. compare between RNNs and Transformers in terms of their applications for speech processing?"",
    'Options': 
    {
        'A)': ""The accuracy of speech recognition systems"",
        'B)': ""The efficiency of speech synthesis processes"",
        'C)': ""The effectiveness of speech-to-speech translation tools"",
        'D)': ""The complexity of speech encoding algorithms""
    },
    'Correct answer': 'C)'
}","{
    'Question': 'Which group developed the Listen Attend and Spell system?',
    'Options': 
    {
        'A)': 'Karita et al.',
        'B)': 'Chan et al.',
        'C)': 'Chorowski et al.',
        'D)': 'Bahdanau et al.'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What architectural change led to the inclusion of transformers in encoder-decoder architectures?"", 
    'Options': 
    {
        'A)': ""The introduction of recurrent neural networks (RNNs)"",
        'B)': ""The development of convolutional neural networks (CNNs)"",
        'C)': ""The application of autoencoders"",
        'D)': ""The implementation of transformer models""
    },
    'Correct answer': 'D)'
}"
48,relation extraction,"There are five main classes of algorithms for relation extraction: handwritten patterns, supervised machine learning, semi-supervised (via bootstrapping or distant supervision), and unsupervised. We`ll introduce each of these in the next sections. ","{
    'Question': 'Which method is not mentioned as a class of algorithms for relation extraction?',
    'Options': 
    {
        'A)': 'handwritten patterns',
        'B)':'supervised machine learning',
        'C)':'semi-supervised (via bootstrapping or distant supervision)',
        'D)': 'unsupervised'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""Which type of algorithm is used for relation extraction when there is no labeled data available?"", 
    'Options': 
    {
        'A)': ""Handwritten patterns"",
        'B)': ""Supervised machine learning"",
        'C)': ""Semi-supervised (via bootstrapping or distant supervision)"",
        'D)': ""Unsupervised""
    },
    'Correct answer': 'D)'
}","{
    'Question': 'What is the first step in explaining how relation extraction works?',
    'Options': 
    {
        'A)': 'Discussing different types of algorithms',
        'B)': 'Introducing the concept of hand-written patterns',
        'C)': 'Explaining the process of bootstrapping or distant supervision',
        'D)': 'Summarizing the importance of unsupervised methods'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""Which type of algorithm is NOT typically used for relation extraction?"",
    'Options': 
    {
        'A)': ""Handwritten patterns"",
        'B)': ""Supervised machine learning"",
        'C)': ""Semi-supervised via bootstrapping or distant supervision"",
        'D)': ""Unsupervised""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'Which type of algorithm is used for relation extraction?',
    'Options': 
    {
        'A)': 'Handwritten patterns',
        'B)': 'Supervised machine learning',
        'C)': 'Semi-supervised via bootstrapping or distant supervision',
        'D)': 'Unsupervised'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""Which type of algorithm is used when there is no labeled data available?"", 
    'Options': 
    {
        'A)': ""Handwritten patterns"",
        'B)': ""Supervised machine learning"",
        'C)': ""Semi-supervised (via bootstrapping or distant supervision)"",
        'D)': ""Unsupervised""
    },
    'Correct answer': 'D)'
}"
49,speech recognition,"The best way to evaluate the performance of a language model is to embed it in an application and measure how much the application improves. Such end-to-end evaluation is called extrinsic evaluation. Extrinsic evaluation is the only way to know if a particular improvement in the language model (or any component) is really going to help the task at hand. Thus for evaluating n-gram language models that are a component of some task like speech recognition or machine translation, we can compare the performance of two candidate language models by running the speech recognizer or machine translator twice, once with each language model, and seeing which gives the more accurate transcription. ","{
    'Question': 'In speech recognition tasks, what method is used to determine if improvements in language models will benefit the specific task?',
    'Options': 
    {
        'A)': 'Comparing the accuracy of transcriptions between different language models',
        'B)': 'Running the speech recognizer twice but using the same language model',
        'C)': 'Measuring the speed of the speech recognizer',
        'D)': 'Analyzing the complexity of the speech data'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""In speech recognition tasks, what method is used to determine the effectiveness of a language model?"",
    'Options': 
    {
        'A)': ""Comparing the accuracy of transcriptions between different languages."",
        'B)': ""Running the speech recognizer twice with different language models."",
        'C)': ""Measuring the speed of the speech recognizer."",
        'D)': ""Analyzing the complexity of the speech input.""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What does extrinsic evaluation refer to?',
    'Options': 
    {
        'A)': 'Measuring the accuracy of a language model on its own',
        'B)': 'Comparing the performance of different components within a single application',
        'C)': 'Evaluating the effectiveness of a language model through real-world tasks',
        'D)': 'Determining the internal workings of a language model using simulations'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""Explain why extrinsic evaluation is crucial for assessing improvements in components of speech recognition systems?"",
    'Options': 
    {
        'A)': ""It allows direct comparison between different components."",
        'B)': ""It provides feedback on internal workings of the system."",
        'C)': ""It measures the overall accuracy of the entire system."",
        'D)': ""It eliminates the need for subjective human judgment.""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'How do you evaluate the performance of a language model in tasks such as speech recognition?',
    'Options': 
    {
        'A)': 'By measuring the accuracy of the output against human transcriptions.',
        'B)': 'By comparing the speed of the language model\'s processing time.',
        'C)': 'By using intrinsic evaluation methods within the language model itself.',
        'D)': 'By embedding the language model in an application and observing improvements.'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""What is the primary method used to determine the effectiveness of a language model in tasks such as speech recognition?"",
    'Options': 
    {
        'A)': ""Comparing the accuracy of transcriptions using different language models."",
        'B)': ""Analyzing the internal structure of the language model."",
        'C)': ""Testing the model on unrelated datasets."",
        'D)': ""Implementing the model in a real-world application.""
    },
    'Correct answer': 'A)'
}"
50,speech recognition, AUTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH ,"{
    'Question': 'What is a key benefit of automatic speech recognition technology?',
    'Options': 
    {
        'A)': 'It can improve accuracy for people with hearing impairments.',
        'B)': 'It allows users to type on their smartphones more easily.',
        'C)': 'It enables real-time translation between languages.',
        'D)': 'It enhances the quality of voice recordings.'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""Which technology involves converting spoken language into written text?"",
    'Options': 
    {
        'A)': ""Automatic speech recognition"",
        'B)': ""Text-to-speech conversion"",
        'C)': ""Voiceprint identification"",
        'D)': ""Speech synthesis""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What technology is used for converting spoken language into written text?',
    'Options': 
    {
        'A)': 'Speech Recognition',
        'B)': 'Text-to-Speech',
        'C)': 'Natural Language Processing',
        'D)': 'Machine Learning Algorithms'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What does speech recognition primarily focus on based on the response?"",
    'Options': 
    {
        'A)': ""Translating written texts into spoken language"",
        'B)': ""Converting spoken words into typed text"",
        'C)': ""Recognizing and interpreting human speech patterns"",
        'D)': ""Analyzing audio signals for emotional content""
    },
    'Correct answer': 'C)'
}","{
    'Question': 'How does speech recognition technology work?',
    'Options': 
    {
        'A)': 'It converts spoken language into written text.',
        'B)': 'It translates written text into spoken language.',
        'C)': 'It uses algorithms to identify and interpret sounds.',
        'D)': 'It records audio and then plays it back.'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What is the first step in converting speech to text?"", 
    'Options': 
    {
        'A)': ""Transcribe the audio file directly into text."",
        'B)': ""Use an automatic speech recognition system to convert spoken words into written text."",
        'C)': ""Record the speech using a microphone."",
        'D)': ""Convert the audio file into a digital format.""
    },
    'Correct answer': 'B)'
}"
51,translation,"For MT we generally use beam widths k between 5 and 10, giving us k hypotheses at the end. We can pass all k to the downstream application with their respective scores, or if we just need a single translation we can pass the most probable hypothesis. ","{
    'Question': 'In machine translation (MT), what is typically used for beam width \( k \) when generating translations?',
    'Options':
    {
        'A)': 'Beam width \( k \) is irrelevant as it does not affect the quality of the translation.',
        'B)': 'Beam width \( k \) determines the number of hypotheses generated by the model, which affects the diversity of the output.',
        'C)': 'Beam width \( k \) should always be set to 1 to ensure accuracy.',
        'D)': 'Beam width \( k \) has no impact on the final translation.'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""In machine translation (MT), what technique allows passing only one translation instead of all possible hypotheses?"",
    'Options': 
    {
        'A)': ""Beam search"",
        'B)': ""Back-translation"",
        'C)': ""Parallel processing"",
        'D)': ""Statistical models""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What is the purpose of using beam width \( k \) in machine translation?',
    'Options':
    {
        'A)': 'To reduce computational complexity',
        'B)': 'To increase the number of translations',
        'C)': 'To improve the accuracy of the translation',
        'D)': 'To decrease the time required for translation'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What are two common methods for passing translations through an MT system?"",
    'Options': 
    {
        'A)': ""Beam search and greedy decoding"",
        'B)': ""Beam search and backoff"",
        'C)': ""Greedy decoding and backoff"",
        'D)': ""Backoff and beam search""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What is the purpose of passing all \(k\) hypotheses to the downstream application?',
    'Options': 
    {
        'A)': 'To improve accuracy',
        'B)': 'To reduce computational cost',
        'C)': 'To increase processing speed',
        'D)': 'To enhance user experience'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What is the best approach for selecting only one translation when using beam search?"", 
    'Options': 
    {
        'A)': ""Use beam width of 1"",
        'B)': ""Pass all translations through the beam search"",
        'C)': ""Select the least probable hypothesis"",
        'D)': ""Choose the most probable hypothesis""
    },
    'Correct answer': 'D)'
}"
52,question answering,"able performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots. ","{
    'Question': 'In what way has the ability to perform various natural language tasks improved due to the learning process in pretraining?',
    'Options': 
    {
        'A)': 'It enhances their capacity to solve complex mathematical problems.',
        'B)': 'It allows them to understand and respond to questions more accurately.',
        'C)': 'It increases their speed in processing visual data.',
        'D)': 'It improves their proficiency in coding languages.'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What has been especially transformative for tasks requiring the production of text?"",
    'Options': 
    {
        'A)': ""Pretrained models"",
        'B)': ""Natural language processing algorithms"",
        'C)': ""Machine learning techniques"",
        'D)': ""Deep neural networks""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What has made these models particularly useful for certain natural language tasks?',
    'Options': 
    {
        'A)': 'Their ability to perform well on various types of questions',
        'B)': 'Their capacity to understand complex sentences',
        'C)': 'Their proficiency in mathematical calculations',
        'D)': 'Their skill at recognizing images'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What aspect of AI has seen significant advancements due to their ability to perform various natural language tasks?"",
    'Options': 
    {
        'A)': ""Speech recognition"",
        'B)': ""Image processing"",
        'C)': ""Natural Language Understanding (NLU)"",
        'D)': ""Robotics""
    },
    'Correct answer': 'C)'
}","{
    'Question': 'What aspect of AI is particularly enhanced by the use of question answering models?',
    'Options': 
    {
        'A)': 'Image recognition',
        'B)': 'Speech synthesis',
        'C)': 'Natural language understanding',
        'D)': 'Robotics development'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What is the primary benefit of using large language models in various NLP tasks?"", 
    'Options': 
    {
        'A)': ""They can perform complex mathematical calculations."",
        'B)': ""They enhance the accuracy of image recognition systems."",
        'C)': ""They improve the efficiency of data encryption processes."",
        'D)': ""They enable effective question answering across diverse domains.""
    },
    'Correct answer': 'D)'
}"
53,hidden,The score that results from this dot product is a scalar that reflects the degree of similarity between the two vectors. The vector of these scores across all the encoder hidden states gives us the relevance of each encoder state to the current step of the decoder. ,"{
    'Question': 'In natural language processing, what does the score resulting from the dot product indicate?',
    'Options': 
    {
        'A)': 'The magnitude of the vectors',
        'B)': 'The angle between the vectors',
        'C)': 'The degree of similarity between the two vectors',
        'D)': 'The distance between the vectors'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""In the context of vector analysis, what does the hidden component represent?"",
    'Options': 
    {
        'A)': ""Magnitude of the vector"",
        'B)': ""Angle between vectors"",
        'C)': ""Direction of the vector"",
        'D)': ""Sum of vector components""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What does the score result from the dot product represent?',
    'Options': 
    {
        'A)': 'The magnitude of the vectors',
        'B)': 'The angle between the vectors',
        'C)': 'The degree of similarity between the vectors',
        'D)': 'The direction of the vectors'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What does the score resulting from the dot product reflect about the relationship between two vectors?"",
    'Options': 
    {
        'A)': ""The magnitude of the vectors"",
        'B)': ""The angle between the vectors"",
        'C)': ""The direction of the vectors"",
        'D)': ""The similarity between the vectors""
    },
    'Correct answer': 'D)'
}","{
    'Question': 'What does the score represent?',
    'Options': 
    {
        'A)': 'The magnitude of the vectors',
        'B)': 'The angle between the vectors',
        'C)': 'The degree of similarity between the vectors',
        'D)': 'The direction of the vectors'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What should we do with the hidden states obtained from the encoders?"", 
    'Options': 
    {
        'A)': ""Store them separately for future reference."",
        'B)': ""Use them immediately as they are."",
        'C)': ""Combine them using some method before feeding it back to the decoder."",
        'D)': ""Transform them through an activation function before passing them on.""
    },
    'Correct answer': 'C)'
}"
54,neural network,"While theoretically interesting, the difficulty with training RNNs and managing context over long sequences impeded progress on practical applications. This situation changed with the introduction of LSTMs in Hochreiter and Schmidhuber (1997) and Gers et al. (2000). Impressive performance gains were demonstrated on tasks at the boundary of signal processing and language processing including phoneme recognition (Graves and Schmidhuber, 2005), handwriting recognition (Graves et al., 2007) and most significantly speech recognition (Graves et al., 2013). Interest in applying neural networks to practical NLP problems surged with the work of Collobert and Weston (2008) and Collobert et al. (2011). These efforts made use of learned word embeddings, convolutional networks, and end-to-end training. ","{
    'Question': 'What was a significant improvement in neural network application for natural language processing mentioned in the response?',
    'Options': 
    {
        'A)': 'Speech recognition',
        'B)': 'Handwriting recognition',
        'C)': 'Phoneme recognition',
        'D)': 'All of the above'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""Which paper introduced Long Short-Term Memory (LSTM) units as an improvement over previous recurrent neural network architectures?"",
    'Options': 
    {
        'A)': ""Hochreiter and Schmidhuber (1997)"",
        'B)': ""Gers et al. (2000)"",
        'C)': ""Collobert and Weston (2008)"",
        'D)': ""Graves et al. (2013)""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What was one significant application of neural networks in natural language processing after the introduction of LSTM models?',
    'Options': 
    {
        'A)': 'Improving image recognition',
        'B)': 'Enhancing music composition',
        'C)': 'Speech synthesis',
        'D)': 'Automated theorem proving'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What was one significant breakthrough in the development of neural networks for natural language processing after the initial challenges were overcome?"",
    'Options': 
    {
        'A)': ""The introduction of LSTM units by Hochreiter and Schmidhuber (1997)"",
        'B)': ""The creation of Convolutional Neural Networks (CNNs) by Graves and Schmidhuber (2005)"",
        'C)': ""The use of recurrent neural networks (RNNs) by Gers et al. (2000)"",
        'D)': ""The implementation of deep learning techniques by Collobert and Weston (2008)""
    },
    'Correct answer': 'D)'
}","{
    'Question': 'What was one significant application of neural networks in natural language processing after their initial theoretical interest?',
    'Options': 
    {
        'A)': 'Developed more efficient algorithms for solving linear equations',
        'B)': 'Introduced the first successful deep learning model',
        'C)': 'Created the first neural network capable of image classification',
        'D)': 'Applied neural networks to improve speech recognition systems'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""What was a significant factor contributing to the resurgence of interest in using neural networks for natural language processing?"", 
    'Options': 
    {
        'A)': ""Theoretical advancements in machine learning algorithms"",
        'B)': ""Improved computational power available"",
        'C)': ""Increased availability of large datasets"",
        'D)': ""Development of specialized architectures such as LSTM""
    },
    'Correct answer': 'D)'
}"
55,coherence," Modern neural representation-learning coherence models, beginning with Li et al. (2014), draw on the intuitions of these early unsupervised models for learning sentence representations and measuring how they change between neighboring sentences. But the new models also draw on the idea pioneered by Barzilay and Lapata (2005) of self-supervision. That is, unlike say coherence relation models, which train on hand-labeled representations for RST or PDTB, these models are trained to distinguish natural discourses from unnatural discourses formed by scrambling the order of sentences, thus using representation learning to discover the features that matter for at least the ordering aspect of coherence. ","{
    'Question': 'What do modern neural representation-learning coherence models use?',
    'Options':
    {
        'A)': 'Hand-labeled representations',
        'B)': 'Unsupervised models',
        'C)': 'Self-supervision',
        'D)': 'Sentence scrambles'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""Which model draws on the intuition of early unsupervised models for learning sentence representations?"",
    'Options': 
    {
        'A)': ""Coherence Relation Models"",
        'B)': ""Self-Supervision Models"",
        'C)': ""Sentence Embedding Models"",
        'D)': ""Discourse Representation Theory Models""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What distinguishes modern neural representation-learning coherence models from earlier unsupervised models?',
    'Options': 
    {
        'A)': 'They use hand-labeled data for training.',
        'B)': 'They focus solely on the ordering aspect of coherence.',
        'C)': 'They rely on self-supervision through scrambled sentence orders.',
        'D)': 'They incorporate semantic similarity measures.'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""How do modern neural representation-learning coherence models differ from earlier unsupervised models in their approach to discovering coherence?"",
    'Options': 
    {
        'A)': ""They use hand-labeled representations for RST or PDTB."",
        'B)': ""They focus solely on distinguishing natural discourses."",
        'C)': ""They rely on self-supervision through scrambled sentence order."",
        'D)': ""They incorporate both supervised and unsupervised methods.""
    },
    'Correct answer': 'C)'
}","{
    'Question': 'What distinguishes modern neural representation-learning coherence models from earlier unsupervised models?',
    'Options': 
    {
        'A)': 'They use hand-labeled data for training.',
        'B)': 'They focus solely on the ordering aspect of coherence.',
        'C)': 'They rely on self-supervision through scrambled sentence orders.',
        'D)': 'They incorporate semantic similarity measures.'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What strategy does modern neural representation-learning coherence models employ to improve their understanding of discourse coherence?"", 
    'Options': 
    {
        'A)': ""They rely solely on supervised learning."",
        'B)': ""They focus on training on labeled data for specific tasks."",
        'C)': ""They use self-supervision techniques to learn relevant features."",
        'D)': ""They ignore the ordering aspect of coherence.""
    },
    'Correct answer': 'C)'
}"
56,edit distance,"Edit distance gives us a way to quantify these intuitions about string similarity. More formally, the minimum edit distance between two strings is defined as the minimum number of editing operations (operations like insertion, deletion, substitution) needed to transform one string into another. ","{
    'Question': 'What does the edit distance measure?',
    'Options': 
    {
        'A)': 'The length of the longest common subsequence',
        'B)': 'The total number of characters in both strings combined',
        'C)': 'The maximum difference in character frequency between two strings',
        'D)': 'The minimum number of editing operations required to transform one string into another'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""In computer science, what does the term 'edit distance' measure?"",
    'Options': 
    {
        'A)': ""The time it takes to execute a program"",
        'B)': ""The length of a string"",
        'C)': ""The difference in characters between two strings"",
        'D)': ""The complexity of sorting algorithms""
    },
    'Correct answer': 'C)'
}","{
    'Question': 'What does the edit distance measure?',
    'Options': 
    {
        'A)': 'The maximum number of characters that can be inserted, deleted, or substituted',
        'B)': 'The total length of both strings combined',
        'C)': 'The average number of characters that need to be changed',
        'D)': 'The exact number of characters that are identical in both strings'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What does the edit distance measure?"", 
    'Options': 
    {
        'A)': ""The maximum number of characters that can be inserted, deleted, or substituted to make two strings equal."",
        'B)': ""The total number of unique characters present in both strings."",
        'C)': ""The average length of the two strings combined."",
        'D)': ""The difference in frequency of each character across the two strings.""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What does the minimum edit distance measure?',
    'Options': 
    {
        'A)': 'The maximum number of characters that need to be changed',
        'B)': 'The total length of both strings combined',
        'C)': 'The difference in frequency of each character',
        'D)': 'The minimum number of operations required to change one string into another'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""What is the first step in calculating the edit distance between two strings?"", 
    'Options': 
    {
        'A)': ""Identify all possible transformations"",
        'B)': ""Count the total number of characters in both strings"",
        'C)': ""Choose the most frequent character in each string"",
        'D)': ""Select the shortest string for comparison""
    },
    'Correct answer': 'B)'
}"
57,semantic role,"The combination of rich linguistic annotation and corpus-based approach instantiated in FrameNet and PropBank led to a revival of automatic approaches to semantic role labeling, first on FrameNet (Gildea and Jurafsky, 2000) and then on PropBank data (Gildea and Palmer, 2002, inter alia). The problem first addressed in the 1970s by handwritten rules was thus now generally recast as one of supervised machine learning enabled by large and consistent databases. Many popular features used for role labeling are defined in Gildea and Jurafsky (2002), Surdeanu et al. (2003), Xue and Palmer (2004), Pradhan et al. (2005), Che et al. (2009), and Zhao et al. (2009). The use of dependency rather than constituency parses was introduced in the CoNLL-2008 shared task (Surdeanu et al., 2008). For surveys see Palmer et al. (2010) and M`arquez et al. (2008). ","{
    'Question': 'In what year did the problem of semantic role labeling first receive attention?',
    'Options': 
    {
        'A)': '1960',
        'B)': '1970',
        'C)': '1980',
        'D)': '1990'
    },
    'Correct answer': 'B)'
}","Term: 'dependency parsing'

{
    'Question': ""Which type of parse was introduced in the CoNLL-2008 shared task?"",
    'Options': 
    {
        'A)': ""Constituency parse"",
        'B)': ""Dependency parse"",
        'C)': ""Syntactic parse"",
        'D)': ""Semantic parse""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What is the main focus of this passage?',
    'Options': 
    {
        'A)': 'The history of semantic role labeling',
        'B)': 'The development of automatic semantic role labeling techniques',
        'C)': 'The challenges faced in semantic role labeling',
        'D)': 'The impact of FrameNet and PropBank on semantic role labeling',
        'E)': 'The evolution of semantic role labeling from hand-written rules to machine learning'
    },
    'Correct answer': 'E)'
}","{
    'Question': ""What methodological shift occurred in the field of semantic role labeling due to advancements in technology?"",
    'Options': 
    {
        'A)': ""The transition from manual rule-based systems to automated machine learning techniques"",
        'B)': ""The move from constituency parsing to dependency parsing"",
        'C)': ""The change from corpora-based methods to purely statistical models"",
        'D)': ""The shift from annotated datasets to unlabeled data sets""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What method did researchers primarily use to address the problem of semantic role labeling in the 1970s?',
    'Options': 
    {
        'A)': 'Handwritten rules',
        'B)': 'Rule induction algorithms',
        'C)': 'Machine learning techniques',
        'D)': 'Dependency parsing methods'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What is the primary method used for developing semantic role labeling techniques?"", 
    'Options': 
    {
        'A)': ""Manual rule-based methods"",
        'B)': ""Supervised machine learning using large databases"",
        'C)': ""Dependency parsing instead of constituency parsing"",
        'D)': ""Combination of both manual rule-based methods and supervised machine learning""
    },
    'Correct answer': 'B)'
}"
58,coherence,"Fig. 24.11 describes the architecture of the model f , which takes a sentence pair and returns a score, higher scores for more coherent pairs. Given an input sentence pair s and t, the model computes sentence embeddings s and t (using any sentence embeddings algorithm), and then concatenates four features of the pair: (1) t; (3) the absolute value the concatenation of the two vectors (2) their difference s t. These are passed of their difference through a one-layer feedforward network to output the coherence score. ","{
    'Question': 'In the described model, what is the primary purpose of computing sentence embeddings?',
    'Options':
    {
        'A)': 'To determine the sentiment of sentences',
        'B)': 'To calculate the length of sentences',
        'C)': 'To measure the similarity between sentences',
        'D)': 'To find synonyms within sentences'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""Which feature does not contribute to calculating the coherence score in the described model?"", 
    'Options': 
    {
        'A)': ""t"",
        'B)': ""the absolute value of the concatenation of the two vectors"",
        'C)': ""their difference s t"",
        'D)': ""a one-layer feedforward network""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What is the first step in computing the coherence score using the described method?',
    'Options': 
    {
        'A)': 'Compute sentence embeddings',
        'B)': 'Concatenate the absolute values of the two vectors',
        'C)': 'Pass the concatenated features through a one-layer feedforward network',
        'D)': 'Calculate the difference between the two vectors'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""Explain how the model computes the coherence score between two sentences using the architecture described in Fig. 24.11?"",
    'Options': 
    {
        'A)': ""The model first calculates the sentence embeddings of both sentences, then compares these embeddings directly."",
        'B)': ""The model uses the absolute values of the concatenated vectors to compute the coherence score."",
        'C)': ""The model combines the embeddings by taking the sum of the absolute values of the differences."",
        'D)': ""The model applies a one-layer feedforward network to the concatenation of the embeddings.""
    },
    'Correct answer': 'D)'
}","{
    'Question': 'What is the first step in building the model described in the context?',
    'Options': 
    {
        'A)': 'Developing the architecture',
        'B)': 'Organizing the data',
        'C)': 'Building the neural network',
        'D)': 'Planning the experiment design'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What is the first step in building a system that measures coherence between sentences?"", 
    'Options': 
    {
        'A)': ""Compute sentence embeddings using any method"",
        'B)': ""Concatenate features based on the absolute value of the vector differences"",
        'C)': ""Pass the concatenated features through a one-layer feedforward network"",
        'D)': ""Combine all steps mentioned above""
    },
    'Correct answer': 'A)'
}"
59,similar word,"Either the PPMI model or the tf-idf model can be used to compute word similarity, for tasks like finding word paraphrases, tracking changes in word meaning, or automatically discovering meanings of words in different corpora. For example, we can find the 10 most similar words to any target word w by computing the cosines between w and each of the V ","{
    'Question': 'Which method is used to calculate word similarity?',
    'Options': 
    {
        'A)': 'PPMI model',
        'B)': 'tf-idf model',
        'C)': 'WordNet',
        'D)': 'Latent Semantic Analysis'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""Which method involves comparing vectors to determine how similar two words are?"",
    'Options': 
    {
        'A)': ""TF-IDF Model"",
        'B)': ""PPMI Model"",
        'C)': ""Word Embeddings"",
        'D)': ""Cosine Similarity""
    },
    'Correct answer': 'D)'
}","{
    'Question': 'How do you determine the similarity between two words?',
    'Options': 
    {
        'A)': 'By calculating the cosine between the frequency vectors of both words.',
        'B)': 'By comparing the length of the word stems only.',
        'C)': 'By measuring the distance between the word embeddings using Euclidean distance.',
        'D)': 'Both A) and C) are correct methods.'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""Which method involves calculating cosine similarities between a target word and all other words in a corpus to identify its top 10 closest synonyms?"",
    'Options': 
    {
        'A)': ""PPMI model"",
        'B)': ""tf-idf model"",
        'C)': ""WordNet"",
        'D)': ""Latent Semantic Analysis (LSA)""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'Which method is commonly used to calculate word similarity?',
    'Options': 
    {
        'A)': 'PPMI model',
        'B)': 'tf-idf model',
        'C)': 'WordNet',
        'D)': 'Latent Semantic Analysis (LSA)'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""Which method is commonly used to determine the similarity between two words?"", 
    'Options': 
    {
        'A)': ""PPMI model only"",
        'B)': ""tf-idf model only"",
        'C)': ""Both PPMI model and tf-idf model are used"",
        'D)': ""None of the above""
    },
    'Correct answer': 'C)'
}"
60,translation,"Large language models are mainly trained on text scraped from the web, augmented by more carefully curated data. Because these training corpora are so large, they are likely to contain many natural examples that can be helpful for NLP tasks, such as question and answer pairs (for example from FAQ lists), translations of sentences between various languages, documents together with their summaries, and so on. ","{
    'Question': 'What is a key benefit of using large language models for Natural Language Processing (NLP) tasks?',
    'Options': 
    {
        'A)': 'They can translate entire books into different languages.',
        'B)': 'They can automatically summarize long documents.',
        'C)': 'They can create detailed FAQs based on user questions.',
        'D)': 'They can learn from vast amounts of internet text.'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""Which type of translation is commonly used in large language models for improving NLP tasks?"", 
    'Options': 
    {
        'A)': ""Machine Translation"",
        'B)': ""Parallel Text Translation"",
        'C)': ""Automatic Translation"",
        'D)': ""Human-Computer Translation""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What type of data is primarily used to train large language models?',
    'Options': 
    {
        'A)': 'Only text scraped from the web',
        'B)': 'Text scraped from the web and carefully curated data',
        'C)': 'Text scraped from the web only',
        'D)': 'Carefully curated data only'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What type of examples are most commonly found in large language model training corpora?"",
    'Options': 
    {
        'A)': ""Natural examples from web scraping"",
        'B)': ""Specific task-oriented examples"",
        'C)': ""Synthetic data generated by algorithms"",
        'D)': ""Examples from academic papers only""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What type of data is primarily used to train large language models?',
    'Options': 
    {
        'A)': 'Only textual data',
        'B)': 'Textual data and images',
        'C)': 'Textual data and audio',
        'D)': 'Textual data and video'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What is an effective way to prepare a dataset for translation tasks?"", 
    'Options': 
    {
        'A)': ""Collect only high-quality texts for translation."",
        'B)': ""Ensure all texts are translated before using them."",
        'C)': ""Use existing datasets directly without modification."",
        'D)': ""Create diverse subsets of the corpus to represent different linguistic features.""
    },
    'Correct answer': 'D)'
}"
61,parse tree,"We say the string a flight can be derived from the non-terminal NP. Thus, a CFG can be used to generate a set of strings. This sequence of rule expansions is called a derivation of the string of words. It is common to represent a derivation by a parse tree (commonly shown inverted with the root at the top). Figure 18.1 shows the tree representation of this derivation. ","{
    'Question': 'In the context provided, what does the term ""parse tree"" refer to?',
    'Options':
    {
        'A)': 'A visual representation of a sentence structure',
        'B)': 'The process of generating sentences using a grammar',
        'C)': 'A method for expanding rules into strings',
        'D)': 'An algorithm for solving grammatical problems'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""In the context of generating strings using a CFG, what does a parse tree represent?"", 
    'Options': 
    {
        'A)': ""An alphabetical order of rules"",
        'B)': ""A visual representation of the grammar structure"",
        'C)': ""A list of terminal symbols"",
        'D)': ""A sequence of random numbers""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What does a parse tree show?',
    'Options': 
    {
        'A)': 'The order in which rules are applied',
        'B)': 'The structure of the grammar',
        'C)': 'The meaning of the sentence',
        'D)': 'The number of derivations possible'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What does the phrase 'parse tree' refer to in the context?"",
    'Options': 
    {
        'A)': ""A diagram showing how a sentence is constructed using grammar rules."",
        'B)': ""The actual words forming the sentence."",
        'C)': ""A method for generating new sentences based on existing ones."",
        'D)': ""A visual representation of the steps taken during the parsing process.""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What does a parse tree show?',
    'Options':
    {
        'A)': 'The order in which rules are applied',
        'B)': 'The structure of the grammar',
        'C)': 'The meaning of the sentence',
        'D)': 'The number of derivations possible',
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What does the parse tree help visualize?"", 
    'Options': 
    {
        'A)': ""The structure of the grammar rules"",
        'B)': ""The syntax of the sentence"",
        'C)': ""The flow of data through the system"",
        'D)': ""The steps involved in generating the string""
    },
    'Correct answer': 'D)'
}"
62,dialogue," Dialogue systems are a kind of human-computer interaction, and general HCI principles apply in their design, including the role of the user, simulations such as Wizard-of-Oz systems, and the importance of iterative design and testing on real users. ","{
    'Question': 'In dialogue systems, what is an important principle for designing interactions with humans?',
    'Options': 
    {
        'A)': 'Using only natural language processing',
        'B)': 'Focusing solely on computer algorithms',
        'C)': 'Adapting to user feedback during development',
        'D)': 'Ignoring user preferences'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""In dialogue systems, what principle ensures that the system learns from real user interactions?"", 
    'Options': 
    {
        'A)': ""User feedback loops"",
        'B)': ""Automated testing tools"",
        'C)': ""Simulation models"",
        'D)': ""Virtual reality environments""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What principle is crucial for designing dialogue systems?',
    'Options': 
    {
        'A)': 'User-centered design',
        'B)': 'Wizard-of-Oz simulation',
        'C)': 'Iterative testing with real users',
        'D)': 'General HCI principles'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""What aspect of dialogue systems is most closely aligned with general Human-Computer Interaction (HCI) principles?"", 
    'Options': 
    {
        'A)': ""The simulation of user interactions through Wizard-of-Oz techniques"",
        'B)': ""Iterative design and testing with real users"",
        'C)': ""The use of natural language processing for understanding user inputs"",
        'D)': ""The implementation of complex algorithms for conversation flow""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'In designing dialogue systems, which principle is NOT directly applicable?',
    'Options': 
    {
        'A)': 'Developing clear and concise language',
        'B)': 'Using Wizard-of-Oz simulations for user experience',
        'C)': 'Iterating designs with real users',
        'D)': 'Following general HCI principles'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What is an important aspect of designing dialogue systems?"", 
    'Options': 
    {
        'A)': ""Using only natural language processing techniques"",
        'B)': ""Focusing solely on visual interfaces for better understanding"",
        'C)': ""Incorporating user feedback through iterative testing"",
        'D)': ""Designing without considering usability""
    },
    'Correct answer': 'C)'
}"
63,hidden,"The third innovation of this period was the rise of the HMM. Hidden Markov models seem to have been applied to speech independently at two laboratories around 1972. One application arose from the work of statisticians, in particular Baum and colleagues at the Institute for Defense Analyses in Princeton who applied HMMs to various prediction problems (Baum and Petrie 1966, Baum and Eagon 1967). James Baker learned of this work and applied the algorithm to speech processing (Baker, 1975a) during his graduate work at CMU. Independently, Frederick Jelinek and collaborators (drawing from their research in information-theoretical models influenced by the work of Shannon (1948)) applied HMMs to speech at the IBM Thomas J. Watson Research Center (Jelinek et al., 1975). One early difference was the decoding algorithm; Baker`s DRAGON system used Viterbi (dynamic programming) decoding, while the IBM system applied Jelinek`s stack decoding algorithm (Jelinek, 1969). Baker then joined the IBM group for a brief time before founding the speech-recognition company Dragon Systems. ","{
    'Question': 'In what year did James Baker learn about the work on HMMs?',
    'Options': 
    {
        'A)': '1966',
        'B)': '1967',
        'C)': '1972',
        'D)': '1975'
    },
    'Correct answer': 'C)'
}","Term: hidden Markov model.

{
    'Question': ""Which laboratory first applied hidden Markov models to speech processing?"",
    'Options': 
    {
        'A)': ""James Baker"",
        'B)': ""Frederick Jelinek"",
        'C)': ""Institute for Defense Analyses"",
        'D)': ""Princeton University""
    },
    'Correct answer': 'C)'
}","{
    'Question': 'What is the main focus of the passage regarding the hidden Markov model?',
    'Options': 
    {
        'A)': 'The history of its development',
        'B)': 'Its applications in speech recognition',
        'C)': 'The differences between its algorithms',
        'D)': 'The impact on statistical modeling',
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What were some key differences between the approaches taken by the two groups when applying hidden Markov models to speech processing?"",
    'Options': 
    {
        'A)': ""The IBM group used dynamic programming decoding, while the CMU group employed stack decoding."",
        'B)': ""James Baker and Frederick Jelinek had different backgrounds influencing their approach."",
        'C)': ""One group focused more on statistical analysis, while the other emphasized information theory."",
        'D)': ""Both groups developed similar algorithms but differed in implementation details.""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'Which laboratory did not use HMMs for speech processing?',
    'Options': 
    {
        'A)': 'The Institute for Defense Analyses',
        'B)': 'CMU',
        'C)': 'IBM Thomas J. Watson Research Center',
        'D)': 'Frederick Jelinek\'s lab'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""Which method did James Baker use for speech processing?"", 
    'Options': 
    {
        'A)': ""Viterbi decoding"",
        'B)': ""Stack decoding"",
        'C)': ""Dynamic Programming"",
        'D)': ""None of the above""
    },
    'Correct answer': 'A)'
}"
64,neural network,"The earliest disambiguation algorithms for parsing were based on probabilistic context-free grammars, first worked out by Booth (1969) and Salomaa (1969); see Appendix C for more history. Neural methods were first applied to parsing at around the same time as statistical parsing methods were developed (Henderson, 1994). In the earliest work neural networks were used to estimate some of the probabilities for statistical constituency parsers (Henderson, 2003, 2004; Emami and Jelinek, 2005) . The next decades saw a wide variety of neural parsing algorithms, including recursive neural architectures (Socher et al., 2011, 2013), encoder-decoder models (Vinyals et al., 2015; Choe and Charniak, 2016), and the idea of focusing on spans (Cross and Huang, 2016). For more on the span-based self-attention approach we describe in this chapter see Stern et al. (2017), Gaddy et al. (2018), Kitaev and Klein (2018), and Kitaev et al. (2019). See Chapter 20 for the parallel history of neural dependency parsing. ","{
    'Question': 'When did neural methods begin being applied to parsing?',
    'Options': 
    {
        'A)': 'Around the same time as statistical parsing methods',
        'B)': 'In the early 1960s',
        'C)': 'After the development of statistical parsing methods',
        'D)': 'The 1990s'
    },
    'Correct answer': 'A)'
}","Term: neural network

Context: Neural networks are computational systems modeled after the human brain, designed to process data through layers of interconnected nodes.

{
    'Question': ""Which type of algorithm was first applied to parsing using neural networks?"",
    'Options': 
    {
        'A)': ""Probabilistic context-free grammars"",
        'B)': ""Statistical parsing methods"",
        'C)': ""Recursive neural architectures"",
        'D)': ""Encoder-decoder models""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What was one of the early applications of neural networks in parsing?',
    'Options': 
    {
        'A)': 'Estimating probabilities for statistical constituency parsers',
        'B)': 'Developing statistical parsing methods',
        'C)': 'Creating recursive neural architectures',
        'D)': 'Focusing on spans'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What was one of the early applications of neural networks in parsing?"",
    'Options': 
    {
        'A)': ""Estimating probabilities for statistical constituency parsers"",
        'B)': ""Creating recursive neural architectures"",
        'C)': ""Focusing on spans"",
        'D)': ""Encoding and decoding sentences""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'Which method was not mentioned in the passage?',
    'Options': 
    {
        'A)': 'Recursive neural architectures',
        'B)': 'Encoder-decoder models',
        'C)': 'Span-based self-attention approach',
        'D)': 'Neural parsing algorithms'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What was the initial application of neural networks in parsing?"", 
    'Options': 
    {
        'A)': ""To predict sentence structure"",
        'B)': ""To analyze word frequencies"",
        'C)': ""To classify sentences into categories"",
        'D)': ""To translate languages""
    },
    'Correct answer': 'A)'
}"
65,translation,"Thus at the first step of decoding, we compute a softmax over the entire vocabulary, assigning a probability to each word. We then select the k-best options from this softmax output. These initial k outputs are the search frontier and these k initial words are called hypotheses. A hypothesis is an output sequence, a translation-sofar, together with its probability. ","{
    'Question': 'In the process of decoding, what does the softmax function primarily determine?',
    'Options': 
    {
        'A)': 'The overall likelihood of all possible translations',
        'B)': 'The most probable next word in the sentence',
        'C)': 'The number of hypotheses generated for each word',
        'D)': 'The length of the current hypothesis'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""In the process of decoding, what does the softmax function assign probabilities to?"",
    'Options': 
    {
        'A)': ""Words"",
        'B)': ""Phrases"",
        'C)': ""Sentences"",
        'D)': ""Transliterations""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What do we call the initial k outputs after computing a softmax over the entire vocabulary?',
    'Options': 
    {
        'A)': 'Translation',
        'B)': 'Hypotheses',
        'C)': 'Search Frontier',
        'D)': 'Decoding Process'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What does the phrase 'k-best options' refer to in the context of translation?"",
    'Options': 
    {
        'A)': ""The top k most probable translations based on their probabilities"",
        'B)': ""The k highest scoring hypotheses generated by the decoder"",
        'C)': ""The k best possible sentences in the target language"",
        'D)': ""The k shortest translations found during the decoding process""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What do we call the initial k outputs after computing a softmax over the entire vocabulary?',
    'Options': 
    {
        'A)': 'Hypotheses',
        'B)': 'Translations',
        'C)': 'Decodings',
        'D)': 'Search frontiers'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What is the next step after computing the softmax probabilities?"", 
    'Options': 
    {
        'A)': ""Select the top-k hypotheses based on their probabilities."",
        'B)': ""Compute the cross-entropy loss for all hypotheses."",
        'C)': ""Update the softmax probabilities using backpropagation."",
        'D)': ""Generate the final translation based on the selected hypotheses.""
    },
    'Correct answer': 'A)'
}"
66,hidden layer,"Feedforward layer The feedforward layer is a fully-connected 2-layer network, i.e., one hidden layer, two weight matrices, as introduced in Chapter 7. The weights are the same for each token position i , but are different from layer to layer. It is common to make the dimensionality dff of the hidden layer of the feedforward network be larger than the model dimensionality d. (For example in the original transformer model, d = 512 and dff = 2048.) ","{
    'Question': 'In the context of the feedforward layer described, what does the term ""hidden layer"" refer to?',
    'Options': 
    {
        'A)': 'The first layer before the input layer',
        'B)': 'The second layer after the input layer',
        'C)': 'The third layer after the input layer',
        'D)': 'The fourth layer after the input layer'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""In a feedforward neural network, what does the hidden layer refer to?"", 
    'Options': 
    {
        'A)': ""The input layer"",
        'B)': ""The output layer"",
        'C)': ""The first layer before the activation function"",
        'D)': ""The second layer after the activation function""
    },
    'Correct answer': 'C)'
}","{
    'Question': 'What does the term ""hidden layer"" refer to in the context?',
    'Options': 
    {
        'A)': 'The second layer of neurons in a neural network',
        'B)': 'The part of the neural network where data flows through without any changes',
        'C)': 'The final output layer of a neural network',
        'D)': 'A layer within a neural network that processes input before passing it on to other layers'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""What distinguishes the feedforward layer from other layers in neural networks?"",
    'Options': 
    {
        'A)': ""It has only one hidden layer."",
        'B)': ""It uses a single weight matrix per token position."",
        'C)': ""Its dimensions are always smaller than those of the input data."",
        'D)': ""It increases the complexity of the model by adding more parameters.""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'In a feedforward layer with a hidden layer, what does the term ""hidden layer"" refer to?',
    'Options': 
    {
        'A)': 'The first layer before the input layer',
        'B)': 'The second layer after the input layer',
        'C)': 'The third layer between the first and second layers',
        'D)': 'The fourth layer after the second layer'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What should be done with the dimensions of the hidden layer compared to the model dimensionality?"", 
    'Options': 
    {
        'A)': ""Make the dimensionality of the hidden layer smaller than the model dimensionality."",
        'B)': ""Keep the dimensionality of the hidden layer equal to the model dimensionality."",
        'C)': ""Increase the dimensionality of the hidden layer beyond the model dimensionality."",
        'D)': ""Decrease the dimensionality of the hidden layer below the model dimensionality.""
    },
    'Correct answer': 'C)'
}"
67,loss function,"While humans produce the best evaluations of machine translation output, running a human evaluation can be time consuming and expensive. For this reason automatic metrics are often used as temporary proxies. Automatic metrics are less accurate than human evaluation, but can help test potential system improvements, and even be used as an automatic loss function for training. In this section we introduce two families of such metrics, those based on character- or word-overlap and those based on embedding similarity. ","{
    'Question': 'What is an example of an automatic metric mentioned in the context?',
    'Options': 
    {
        'A)': 'Human evaluation',
        'B)': 'Character-overlap metrics',
        'C)': 'Word-similarity metrics',
        'D)': 'Embedding-based metrics'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""Which type of metric is commonly used as a proxy for human evaluation in machine translation systems?"",
    'Options': 
    {
        'A)': ""Character-overlap metrics"",
        'B)': ""Word-overlap metrics"",
        'C)': ""Embedding-similarity metrics"",
        'D)': ""Both A) and B)""
    },
    'Correct answer': 'D)'
}","{
    'Question': 'What is the purpose of using automatic metrics as a proxy for human evaluation?',
    'Options': 
    {
        'A)': 'To replace human evaluation completely',
        'B)': 'To provide more accurate results than human evaluation',
        'C)': 'To speed up the testing process',
        'D)': 'To reduce the cost of human evaluation'
    },
    'Correct answer': 'C)',
    'Verb': 'explain'
}","{
    'Question': ""Explain how automatic metrics compare to human evaluation in terms of accuracy?"",
    'Options': 
    {
        'A)': ""Automatic metrics are more accurate than human evaluation."",
        'B)': ""Human evaluation is always more accurate than automatic metrics."",
        'C)': ""Both automatic metrics and human evaluation have their own strengths and weaknesses."",
        'D)': ""Automatic metrics are less accurate than human evaluation.""
    },
    'Correct answer': 'C)'
}","{
    'Question': 'What is the purpose of using automatic metrics as a proxy for human evaluation?',
    'Options': 
    {
        'A)': 'To increase the accuracy of machine translations',
        'B)': 'To reduce the cost of human evaluations',
        'C)': 'To improve the quality of training data',
        'D)': 'To develop new language models faster'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""Which type of metric is commonly used as an automatic loss function for training?"", 
    'Options': 
    {
        'A)': ""Metrics based on character-overlap"",
        'B)': ""Metrics based on word-overlap"",
        'C)': ""Metrics based on embedding-similarity"",
        'D)': ""Metrics based on sentence-overlap""
    },
    'Correct answer': 'C)'
}"
68,naive bayes,"Let`s walk through an example of training and testing naive Bayes with add-one smoothing. We`ll use a sentiment analysis domain with the two classes positive (+) and negative (-), and take the following miniature training and test documents simplified from actual movie reviews. ","{
    'Question': 'In the context of training and testing naive Bayes, what is the purpose of using add-one smoothing?',
    'Options':
    {
        'A)': 'To increase the accuracy of predictions by penalizing rare events',
        'B)': 'To reduce computational complexity during model training',
        'C)': 'To enhance the interpretability of the model',
        'D)': 'To improve the performance on unseen data'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""In the context of naive Bayes, what technique is used to handle unseen data points?"",
    'Options': 
    {
        'A)': ""K-means clustering"",
        'B)': ""Bayesian inference"",
        'C)': ""Add-one smoothing"",
        'D)': ""Naive assumption""
    },
    'Correct answer': 'C)'
}","{
    'Question': 'What is Naive Bayes?',
    'Options': 
    {
        'A)': 'Naive Bayes is a machine learning algorithm used for classification tasks.',
        'B)': 'Naive Bayes is a type of neural network used for image recognition.',
        'C)': 'Naive Bayes is a statistical method used for regression analysis.',
        'D)': 'Naive Bayes is a technique used for clustering data points.'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What does the naive Bayes algorithm do when it comes to sentiment analysis?"",
    'Options': 
    {
        'A)': ""Classifies movies into positive or negative based on their content."",
        'B)': ""Predicts whether a review will be positive or negative using probability."",
        'C)': ""Determines if a document contains spam or not."",
        'D)': ""Calculates the average rating of all movies.""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'In the context of training and testing naive Bayes using add-one smoothing for sentiment analysis on movie reviews, what is the primary purpose of this technique?',
    'Options': 
    {
        'A)': 'To increase the accuracy of classifying positive vs. negative sentiments',
        'B)': 'To reduce the computational complexity of model building',
        'C)': 'To enhance the precision of predictions by adjusting probabilities',
        'D)': 'To improve the recall rate of the model'
    },
    'Correct answer': 'A)'
}","```python
{
    'Question': ""What is the first step in applying Naive Bayes for sentiment analysis?"", 
    'Options': 
    {
        'A)': ""Count the frequency of each word in both classes separately."",
        'B)': ""Calculate the probability of each class using the prior probabilities."",
        'C)': ""Add +1 to all counts before calculating conditional probabilities."",
        'D)': ""Combine the frequencies of positive and negative words.""
    },
    'Correct answer': 'C)'
}
```"
69,regular expression,"Kleene 1951; 1956 first defined regular expressions and the finite automaton, based on the McCulloch-Pitts neuron. Ken Thompson was one of the first to build regular expressions compilers into editors for text searching (Thompson, 1968). His editor ed included a command g/regular expression/p, or Global Regular Expression Print, which later became the Unix grep utility. ","{
    'Question': 'Who among the following is credited with defining regular expressions?',
    'Options':
    {
        'A)': 'Kleene',
        'B)': 'McCulloch-Pitts',
        'C)': 'Ken Thompson',
        'D)': 'The context does not provide this information'
    },
    'Correct answer': 'C) Ken Thompson'
}","{
    'Question': ""Who was one of the first to build regular expressions compilers into editors for text searching?"", 
    'Options': 
    {
        'A)': ""Kleene"",
        'B)': ""McCulloch-Pitts"",
        'C)': ""Ken Thompson"",
        'D)': ""Thompson, 1968""
    },
    'Correct answer': 'C)'
}","{
    'Question': 'Who among the following is credited with being one of the first to incorporate regular expressions into an editor?',
    'Options': 
    {
        'A)': 'Kleene',
        'B)': 'McCulloch-Pitts',
        'C)': 'Ken Thompson',
        'D)': 'Thompson, 1968'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What did Ken Thompson do with regular expressions before they were widely recognized?"",
    'Options': 
    {
        'A)': ""Defined regular expressions"",
        'B)': ""Built regular expressions compilers into editors"",
        'C)': ""Invented the McCulloch-Pitts neuron"",
        'D)': ""Created the Unix grep utility""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'Who among the following is credited with developing regular expressions?',
    'Options': 
    {
        'A)': 'Kleene',
        'B)': 'McCulloch-Pitts',
        'C)': 'Ken Thompson',
        'D)': 'Thompson, 1968'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What is an essential step in developing a regular expression?"", 
    'Options': 
    {
        'A)': ""Identify the type of pattern to match"",
        'B)': ""Choose a programming language"",
        'C)': ""Compile the regular expression"",
        'D)': ""Test the regular expression against various inputs""
    },
    'Correct answer': 'C)'
}"
70,temporal expression,"Next, we discuss event extraction, the task of finding events in which these entities participate, like, in our sample text, the fare increases by United and American and the reporting events said and cite. Events are also situated in time, occurring at a particular date or time, and events can be related temporally, happening before or after or simultaneously with each other. We`ll need to recognize temporal expressions like Friday, Thursday or two days from now and times such as 3:30 P.M., and normalize them onto specific calendar dates or times. We`ll need to link Friday to the time of United`s announcement, Thursday to the previous day`s fare increase, and we`ll need to produce a timeline in which United`s announcement follows the fare increase and American`s announcement follows both of those events. ","{
    'Question': 'In the discussion about event extraction, what is one way to identify temporal expressions?',
    'Options':
    {
        'A)': 'Identifying patterns in the order of actions',
        'B)': 'Recognizing common phrases indicating time',
        'C)': 'Analyzing the sequence of events',
        'D)': 'Determining the duration of events'
    },
    'Correct answer': 'B) Recognizing common phrases indicating time'
}","{
    'Question': ""In the context of recognizing temporal expressions, what does it mean when an event occurs at a particular date or time?"",
    'Options': 
    {
        'A)': ""It means the event happens on a specific day or time."",
        'B)': ""It means the event happens randomly without any pattern."",
        'C)': ""It means the event happens in different locations."",
        'D)': ""It means the event happens during different seasons.""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What is an example of a temporal expression mentioned in the context?',
    'Options': 
    {
        'A)': 'Friday',
        'B)': 'Wednesday',
        'C)': 'Two days ago',
        'D)': '12:00 PM',
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What type of temporal expressions will be identified during the event extraction process?"",
    'Options': 
    {
        'A)': ""Dates and times"",
        'B)': ""Entities only"",
        'C)': ""Events only"",
        'D)': ""Temporal relationships between events""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What is an example of a temporal expression used in the given text?',
    'Options': 
    {
        'A)': 'United and American',
        'B)': 'fare increases',
        'C)': 'Friday',
        'D)': 'Wednesday'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What is the first step in recognizing temporal expressions in a text?"", 
    'Options': 
    {
        'A)': ""Identify all occurrences of time-related phrases"",
        'B)': ""Find out when the events occur relative to each other"",
        'C)': ""Map the relationships between different events"",
        'D)': ""Normalize time expressions to specific calendar dates""
    },
    'Correct answer': 'A)'
}"
71,question answering,"Meanwhile, the information-retrieval paradigm for question answering was influenced by the rise of the web in the 1990s. The U.S. government-sponsored TREC (Text REtrieval Conference) evaluations, run annually since 1992, provide a testbed for evaluating information-retrieval tasks and techniques (Voorhees and Harman, 2005). TREC added an influential QA track in 1999, which led to a wide variety of factoid and non-factoid systems competing in annual evaluations. ","{
    'Question': 'In what year did the U.S. government-sponsored TREC Text Retrieval Conferences start?',
    'Options': 
    {
        'A)': '1987',
        'B)': '1990',
        'C)': '1992',
        'D)': '1994'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""In what year did the U.S. government-sponsored TREC Text Retrieval Conferences start?"", 
    'Options': 
    {
        'A)': ""1987"",
        'B)': ""1993"",
        'C)': ""1996"",
        'D)': ""1999""
    },
    'Correct answer': 'D)'
}","{
    'Question': 'What significant event influenced the development of question answering?',
    'Options': 
    {
        'A)': 'The invention of the internet',
        'B)': 'The launch of the World Wide Web',
        'C)': 'The creation of Google',
        'D)': 'The establishment of TREC'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""What significant development in technology influenced the field of question answering during the 1990s?"",
    'Options': 
    {
        'A)': ""The rise of the internet"",
        'B)': ""The invention of smartphones"",
        'C)': ""The launch of social media platforms"",
        'D)': ""The emergence of cloud computing""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What did the U.S. government-sponsored TREC evaluations contribute to?',
    'Options': 
    {
        'A)': 'Developed new algorithms',
        'B)': 'Introduced the concept of question answering',
        'C)': 'Created a standardized evaluation framework',
        'D)': 'Enhanced the performance of existing retrieval systems'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What is the primary focus of the TREC evaluations?"", 
    'Options': 
    {
        'A)': ""Testing retrieval methods only"",
        'B)': ""Evaluating information-retrieval tasks and techniques"",
        'C)': ""Comparing different types of questions"",
        'D)': ""Assessing machine translation models""
    },
    'Correct answer': 'B)'
}"
72,text classification,"Up to now we have been describing text classification tasks with only two classes. But lots of classification tasks in language processing have more than two classes. For sentiment analysis we generally have 3 classes (positive, negative, neutral) and even more classes are common for tasks like part-of-speech tagging, word sense disambiguation, semantic role labeling, emotion detection, and so on. Luckily the naive Bayes algorithm is already a multi-class classification algorithm. ","{
    'Question': 'In what type of task do we often use three classes instead of just positive, negative, and neutral?',
    'Options':
    {
        'A)': 'Sentiment Analysis',
        'B)': 'Part-of-Speech Tagging',
        'C)': 'Word Sense Disambiguation',
        'D)': 'Semantic Role Labeling'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""In which type of classification task do we often encounter more than two classes?"",
    'Options': 
    {
        'A)': ""Sentiment analysis"",
        'B)': ""Part-of-speech tagging"",
        'C)': ""Word sense disambiguation"",
        'D)': ""Emotion detection""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What does the term ""text classification"" refer to?',
    'Options': 
    {
        'A)': 'The process of categorizing texts into predefined categories based on their content.',
        'B)': 'The task of identifying the author of a text.',
        'C)': 'The method of translating one language into another.',
        'D)': 'The study of how to classify sentences into grammatical structures.'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What is the primary difference between binary and multi-class text classification?"",
    'Options': 
    {
        'A)': ""Binary text classification requires less computational resources."",
        'B)': ""Multi-class text classification can handle more complex data sets."",
        'C)': ""Binary text classification algorithms are easier to implement."",
        'D)': ""Multi-class text classification cannot use Naive Bayes.""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'In which type of classification task do you typically find three or more classes?',
    'Options': 
    {
        'A)': 'Sentiment analysis',
        'B)': 'Part-of-speech tagging',
        'C)': 'Word sense disambiguation',
        'D)': 'Emotion detection'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What is an example of how to extend the naive Bayes algorithm beyond binary classification?"", 
    'Options': 
    {
        'A)': ""Use it directly for three-class sentiment analysis."",
        'B)': ""Extend it to handle four classes using a different approach."",
        'C)': ""Implement it as a classifier for five classes."",
        'D)': ""Train it on datasets with at least six classes.""
    },
    'Correct answer': 'A)'
}"
73,hidden,"Fig. 8.17 shows an English source text (the green witch arrived), a sentence separator token (<s>, and a Spanish target text (llego la bruja verde). To translate a source text, we run it through the network performing forward inference to generate hidden states until we get to the end of the source. Then we begin autoregressive generation, asking for a word in the context of the hidden layer from the end of the source input as well as the end-of-sentence marker. Subsequent words are conditioned on the previous hidden state and the embedding for the last word generated. ","{
    'Question': 'In translation using the network, what is the purpose of running the source text through the network?',
    'Options':
    {
        'A)': 'To identify hidden characters',
        'B)': 'To extract hidden states',
        'C)': 'To find hidden meanings',
        'D)': 'To locate hidden objects'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""In translation, what does the process of generating hidden states refer to?"",
    'Options': 
    {
        'A)': ""Forward inference"",
        'B)': ""Backward decoding"",
        'C)': ""Contextualization"",
        'D)': ""Hidden Markov Model""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What does the process of translating a source text involve?',
    'Options': 
    {
        'A)': 'Running the source text through a neural network',
        'B)': 'Identifying the meaning of each word individually',
        'C)': 'Breaking down sentences into smaller parts',
        'D)': 'Generating new words based on existing ones'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""What does the phrase 'hidden states' refer to in the context of translation?"",
    'Options': 
    {
        'A)': ""The actual meaning of the source text"",
        'B)': ""The intermediate representations during the translation process"",
        'C)': ""The final output after all translations are done"",
        'D)': ""The original language before translation""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What is the purpose of running the source text through the network?',
    'Options': 
    {
        'A)': 'To identify synonyms',
        'B)': 'To perform forward inference',
        'C)': 'To detect plagiarism',
        'D)': 'To find cultural references'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What is the first step in translating an English source text using the method described?"", 
    'Options': 
    {
        'A)': ""Run the source text through the network performing forward inference"",
        'B)': ""Begin autoregressive generation with the end-of-sentence marker"",
        'C)': ""Generate a word based on the last hidden state and the last word generated"",
        'D)': ""Identify the end of the source text""
    },
    'Correct answer': 'A)'
}"
74,coherence relation," Modern neural representation-learning coherence models, beginning with Li et al. (2014), draw on the intuitions of these early unsupervised models for learning sentence representations and measuring how they change between neighboring sentences. But the new models also draw on the idea pioneered by Barzilay and Lapata (2005) of self-supervision. That is, unlike say coherence relation models, which train on hand-labeled representations for RST or PDTB, these models are trained to distinguish natural discourses from unnatural discourses formed by scrambling the order of sentences, thus using representation learning to discover the features that matter for at least the ordering aspect of coherence. ","{
    'Question': 'In what way do modern neural representation-learning coherence models differ from earlier unsupervised models?',
    'Options':
    {
        'A)': 'They use hand-labeled data for training.',
        'B)': 'They focus solely on syntactic coherence.',
        'C)': 'They rely on self-supervision through scrambled sentence orders.',
        'D)': 'They require explicit labeling of discourse types.'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""Which model uses representation learning to measure changes between neighboring sentences?"",
    'Options': 
    {
        'A)': ""Coherence Relation Models"",
        'B)': ""Unsupervised Models"",
        'C)': ""Self-Supervised Models"",
        'D)': ""Sentence Representation Learning Models""
    },
    'Correct answer': 'C)'
}","{
    'Question': 'What distinguishes modern neural representation-learning coherence models from earlier unsupervised models?',
    'Options': 
    {
        'A)': 'They use hand-labeled representations for RST or PDTB.',
        'B)': 'They focus solely on the ordering aspect of coherence.',
        'C)': 'They rely on self-supervision through scrambled sentence orders.',
        'D)': 'They incorporate semantic similarity measures.'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""How do modern neural representation-learning coherence models differ from earlier unsupervised models in their approach to discovering coherence?"", 
    'Options': 
    {
        'A)': ""They use hand-labeled data for training."",
        'B)': ""They focus solely on the ordering aspect of coherence."",
        'C)': ""They rely on self-supervision through scrambled sentence orderings."",
        'D)': ""They ignore the relationship between sentences.""
    },
    'Correct answer': 'C)'
}","{
    'Question': 'How do modern neural representation-learning coherence models differ from earlier unsupervised models?',
    'Options': 
    {
        'A)': 'They use hand-labeled representations for RST or PDTB.',
        'B)': 'They focus solely on discovering the features that matter for the ordering aspect of coherence.',
        'C)': 'They learn sentence representations without supervision.',
        'D)': 'They measure changes in sentence representations over time.'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What technique does modern neural representation-learning coherence models use to measure coherence?"", 
    'Options': 
    {
        'A)': ""Coherence relation models"",
        'B)': ""Self-supervision techniques"",
        'C)': ""Hand-labeled representations"",
        'D)': ""Sentence scramblings""
    },
    'Correct answer': 'B)'
}"
75,neural network,"For logistic regression we can initialize gradient descent with all the weights and biases having the value 0. In neural networks, by contrast, we need to initialize the weights with small random numbers. It`s also helpful to normalize the input values to have 0 mean and unit variance. ","{
    'Question': 'In neural networks, why is it important to initialize weights with small random numbers?',
    'Options': 
    {
        'A)': 'To ensure the activation function works correctly',
        'B)': 'To prevent overfitting of the model',
        'C)': 'To make sure the gradients are positive during backpropagation',
        'D)': 'To reduce computational complexity'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""In neural networks, what technique is used to initialize the weights with small random numbers?"",
    'Options': 
    {
        'A)': ""Gradient descent"",
        'B)': ""Backpropagation"",
        'C)': ""Normalization"",
        'D)': ""Activation function""
    },
    'Correct answer': 'C)'
}","{
    'Question': 'Why is it important to initialize weights in neural networks with small random numbers?',
    'Options': 
    {
        'A)': 'To ensure the model converges faster',
        'B)': 'To prevent overfitting',
        'C)': 'To make the training process more efficient',
        'D)': 'To improve the accuracy of predictions'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""How do we typically initialize the weights in neural networks compared to other models?"",
    'Options': 
    {
        'A)': ""We initialize all weights and biases to zero."",
        'B)': ""We initialize all weights and biases randomly but uniformly between -1 and 1."",
        'C)': ""We initialize all weights and biases to their expected values based on data distribution."",
        'D)': ""We normalize the inputs before feeding them into the model.""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'In neural networks, what is important for initializing the weights?',
    'Options': 
    {
        'A)': 'All weights and biases should be initialized to zero.',
        'B)': 'Weights should be initialized randomly with small values.',
        'C)': 'Input values should not be normalized.',
        'D)': 'Normalization of input values is unnecessary.'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What is an important step when initializing weights for a neural network?"", 
    'Options': 
    {
        'A)': ""Set all weights and biases to zero."",
        'B)': ""Initialize weights randomly with small numbers."",
        'C)': ""Normalize the input values to have a mean of 0 and standard deviation of 1."",
        'D)': ""Both A) and C)""
    },
    'Correct answer': 'D)'
}"
76,relation extraction,"The text tells us, for example, that Tim Wagner is a spokesman for American Airlines, that United is a unit of UAL Corp., and that American is a unit of AMR. These binary relations are instances of more generic relations such as part-of or employs that are fairly frequent in news-style texts. Figure 20.1 lists the 17 relations used in the ACE relation extraction evaluations and Fig. 20.2 shows some sample relations. We might also extract more domain-specific relations such as the notion of an airline route. For example from this text we can conclude that United has routes to Chicago, Dallas, Denver, and San Francisco. ","{
    'Question': 'What type of text does the passage describe?',
    'Options': 
    {
        'A)': 'Science fiction',
        'B)': 'Historical document',
        'C)': 'Technical report',
        'D)': 'News article'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""Which type of relation is not mentioned in the text about Tim Wagner, American Airlines, United, and AMR?"",
    'Options': 
    {
        'A)': ""Employs"",
        'B)': ""Part-of"",
        'C)': ""Route"",
        'D)': ""Synovial ball and socket joint""
    },
    'Correct answer': 'D)'
}","{
    'Question': 'What does the text provide examples of?',
    'Options': 
    {
        'A)': 'binary relations',
        'B)': 'domain-specific relations',
        'C)': 'generic relations',
        'D)': 'news-style relations'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What type of relations does the text mention as examples of more frequent relations in news-style texts?"",
    'Options': 
    {
        'A)': ""binary relations"",
        'B)': ""domain-specific relations"",
        'C)': ""generic relations"",
        'D)': ""all of the above""
    },
    'Correct answer': 'D)'
}","{
    'Question': 'What type of task does the passage describe?',
    'Options': 
    {
        'A)': 'Developing a machine learning model',
        'B)': 'Organizing data into categories',
        'C)': 'Building a database',
        'D)': 'Planning research methodology'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What is the first step in extracting the most common relations mentioned in the text?"", 
    'Options': 
    {
        'A)': ""Identify all the binary relations present in the text."",
        'B)': ""Find specific examples of domain-specific relations."",
        'C)': ""Extract the names of airlines and their units."",
        'D)': ""Analyze the structure of the text to identify parts of speech.""
    },
    'Correct answer': 'A)'
}"
77,target word,"For training a binary classifier we also need negative examples. In fact skipgram with negative sampling (SGNS) uses more negative examples than positive examples (with the ratio between them set by a parameter k). So for each of these (w, cpos) training instances we`ll create k negative samples, each consisting of the target w plus a noise word` cneg. A noise word is a random word from the lexicon, constrained not to be the target word w. The right above shows the setting where k = 2, so we`ll have 2 negative examples in the negative training set for each positive example w, cpos. ","{
    'Question': 'In SGNS, how many negative examples are created for each positive example?',
    'Options': 
    {
        'A)': '1',
        'B)': '2',
        'C)': '3',
        'D)': '4'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""In SGNS, how many negative examples are created for each positive example?"",
    'Options': 
    {
        'A)': ""1"",
        'B)': ""2"",
        'C)': ""3"",
        'D)': ""4""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'In SGNS, how many negative examples are created for each positive example?',
    'Options': 
    {
        'A)': '1',
        'B)': '2',
        'C)': '3',
        'D)': '4'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""Explain how SGNS creates negative examples for training a binary classifier?"",
    'Options': 
    {
        'A)': ""By randomly selecting any word from the lexicon other than the target word w."",
        'B)': ""By creating k negative examples for each positive example using a noise word cneg which is a random word from the lexicon but different from the target word w."",
        'C)': ""By only using the target word w in the negative training set."",
        'D)': ""By combining the target word w with another word from the lexicon.""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'In SGNS, how many negative examples are created for each positive example?',
    'Options': 
    {
        'A)': '1',
        'B)': '2',
        'C)': '3',
        'D)': '4'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""In SGNS, how many negative examples are created for each positive example?"", 
    'Options': 
    {
        'A)': ""k=3"",
        'B)': ""k=4"",
        'C)': ""k=5"",
        'D)': ""k=6""
    },
    'Correct answer': 'A)'
}"
78,hidden layer,"Instead, a modern neural network is a network of small computing units, each of which takes a vector of input values and produces a single output value. In this chapter we introduce the neural net applied to classification. The architecture we introduce is called a feedforward network because the computation proceeds iteratively from one layer of units to the next. The use of modern neural nets is often called deep learning, because modern networks are often deep (have many layers). Neural networks share much of the same mathematics as logistic regression. But neural networks are a more powerful classifier than logistic regression, and indeed a minimal neural network (technically one with a single hidden layer`) can be shown to learn any function. ","{
    'Question': 'In a neural network, what does the term ""hidden layer"" refer to?',
    'Options': 
    {
        'A)': 'Layers between the input and output layers where computations take place',
        'B)': 'The first layer before the input layer',
        'C)': 'The final layer after all computations',
        'D)': 'The only layer used for data input'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""Which type of neural network is characterized by having only one hidden layer?"",
    'Options': 
    {
        'A)': ""Convolutional neural network"",
        'B)': ""Recurrent neural network"",
        'C)': ""Feedforward neural network"",
        'D)': ""Deep neural network""
    },
    'Correct answer': 'C)'
}","{
    'Question': 'What does the term ""hidden layer"" refer to in the context?',
    'Options': 
    {
        'A)': 'Layers between the input and output layers where computations occur.',
        'B)': 'The first layer of neurons before the input layer.',
        'C)': 'The final layer of neurons after all other layers have been processed.',
        'D)': 'The only layer used for processing data.'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What does the introduction of a hidden layer in a neural network signify?"",
    'Options': 
    {
        'A)': ""It signifies the end of the development phase."",
        'B)': ""It indicates the beginning of a new era in machine learning."",
        'C)': ""It represents the addition of an extra layer for better accuracy."",
        'D)': ""It means the network will only work for simple problems.""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What does the term ""hidden layer"" refer to in the context?',
    'Options': 
    {
        'A)': 'Layers between the input and output layers where computations occur.',
        'B)': 'The first layer of neurons before the activation function is applied.',
        'C)': 'The final layer of neurons used for prediction or decision-making.',
        'D)': 'The initial layer of neurons receiving raw data inputs.'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""Which method should be used to develop a neural network for classification?"", 
    'Options': 
    {
        'A)': ""Use only the first layer of neurons"",
        'B)': ""Start with a simple feedforward network structure"",
        'C)': ""Build a deep network with multiple hidden layers"",
        'D)': ""Combine all layers together at once""
    },
    'Correct answer': 'C)'
}"
79,coreference," Some NPs are not referring expressions, such as pleonastic it in It is raining.  Many corpora have human-labeled coreference annotations that can be used for supervised learning, including OntoNotes for English, Chinese, and Arabic, ARRAU for English, and AnCora for Spanish and Catalan. ","{
    'Question': 'What do some NP refer to?',
    'Options': 
    {
        'A)': 'It',
        'B)': 'Pleonastic it',
        'C)': 'Human-labeled coreference annotations',
        'D)': 'Corpora'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""Which corpus contains labeled coreference annotations?"", 
    'Options': 
    {
        'A)': ""AnCora"",
        'B)': ""OntoNotes"",
        'C)': ""ARRAU"",
        'D)': ""None of the above""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What do we call phrases where one noun phrase refers back to another?',
    'Options': 
    {
        'A)': 'Pleonastic it',
        'B)': 'Referential expression',
        'C)': 'Coreference annotation',
        'D)': 'Supervised learning'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""Explain how coreference works in language processing?"",
    'Options': 
    {
        'A)': ""Coreference refers to the repetition of pronouns within sentences."",
        'B)': ""Coreference involves identifying and resolving references between different parts of a sentence."",
        'C)': ""Coreference means using synonyms instead of specific nouns."",
        'D)': ""Coreference is about changing the tense of verbs.""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What do some NP referents lack?',
    'Options': 
    {
        'A)': 'Coreference',
        'B)': 'Pleonastic it',
        'C)': 'Supervised learning',
        'D)': 'Human labeling'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""Which of the following best describes how coreference relationships are identified?"", 
    'Options': 
    {
        'A)': ""Coreference relationships are automatically detected by machine learning algorithms."",
        'B)': ""Coreference relationships are manually labeled by linguists."",
        'C)': ""Coreference relationships are inferred based on syntactic dependencies."",
        'D)': ""Coreference relationships are annotated using linguistic resources.""
    },
    'Correct answer': 'D)'
}"
80,word meaning,"Surely we can do better than this! After all, we`ll want a model of word meaning to do all sorts of things for us. It should tell us that some words have similar meanings (cat is similar to dog), others are antonyms (cold is the opposite of hot), some have positive connotations (happy) while others have negative connotations (sad). It should represent the fact that the meanings of buy, sell, and pay offer differing perspectives on the same underlying purchasing event. (If I buy something from you, you`ve probably sold it to me, and I likely paid you.) More generally, a model of word meaning should allow us to draw inferences to address meaning-related tasks like question-answering or dialogue. ","{
    'Question': 'What does the speaker suggest about models of word meaning?',
    'Options':
    {
        'A)': 'They will only focus on synonyms.',
        'B)': 'They will ignore the differences between words.',
        'C)': 'They will help with understanding complex sentences.',
        'D)': 'They will replace human translators.'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What does a model of word meaning help us do?"", 
    'Options': 
    {
        'A)': ""It helps us understand synonyms."",
        'B)': ""It allows us to predict future events."",
        'C)': ""It enables us to perform complex mathematical calculations."",
        'D)': ""It aids in answering questions about word meanings.""
    },
    'Correct answer': 'D)'
}","{
    'Question': 'What does a model of word meaning help us do?',
    'Options': 
    {
        'A)': 'Draw conclusions about the speaker\'s mood',
        'B)': 'Predict future events based on past patterns',
        'C)': 'Analyze the emotional tone of a sentence',
        'D)': 'Understand the relationship between different words with similar meanings'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""What does a model of word meaning help us do?"", 
    'Options': 
    {
        'A)': ""Draw inferences about the relationships between different words"",
        'B)': ""Predict future events based on past data"",
        'C)': ""Create new words by combining existing ones"",
        'D)': ""Identify synonyms and antonyms within a sentence""
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What does the term 'word meaning' refer to?"",
    'Options': 
    {
        'A)': ""The pronunciation of a word."",
        'B)': ""The sound made when saying a word."",
        'C)': ""The definition or significance attached to a word by speakers and writers."",
        'D)': ""The spelling of a word.""
    },
    'Correct answer': ""C)""
}","{
    'Question': ""What is an essential component needed to develop a model of word meaning?"", 
    'Options': 
    {
        'A)': ""Collecting large datasets of texts."",
        'B)': ""Creating algorithms that can identify synonyms."",
        'C)': ""Designing systems capable of understanding sentence structures."",
        'D)': ""Building models that can infer relationships between different meanings.""
    },
    'Correct answer': 'D)'
}"
81,target word,"Nonetheless it is useful to have intrinsic evaluations. The most common metric is to test their performance on similarity, computing the correlation between an algorithm`s word similarity scores and word similarity ratings assigned by humans. WordSim-353 (Finkelstein et al., 2002) is a commonly used set of ratings from 0 to 10 for 353 noun pairs; for example (plane, car) had an average score of 5.77. SimLex-999 (Hill et al., 2015) is a more complex dataset that quantifies similarity (cup, mug) rather than relatedness (cup, coffee), and includes concrete and abstract adjective, noun and verb pairs. The TOEFL dataset is a set of 80 questions, each consisting of a target word with 4 additional word choices; the task is to choose which is the correct synonym, as in the example: Levied is closest in meaning to: imposed, believed, requested, correlated (Landauer and Dumais, 1997). All of these datasets present words without context. ","{
    'Question': 'In the context of evaluating algorithms, what does the term ""target word"" refer to?',
    'Options': 
    {
        'A)': 'The word being tested for its similarity',
        'B)': 'The human evaluator who rates word similarities',
        'C)': 'The machine learning model predicting word similarity',
        'D)': 'The dataset containing all possible synonyms'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""Which dataset provides a set of ratings from 0 to 10 for 353 noun pairs?"",
    'Options': 
    {
        'A)': ""WordSim-353"",
        'B)': ""SimLex-999"",
        'C)': ""TOEFL"",
        'D)': ""None of the above""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What type of evaluation is primarily used to assess the performance of algorithms?',
    'Options': 
    {
        'A)': 'Intrinsic evaluations',
        'B)': 'Extrinsic evaluations',
        'C)': 'Contextual evaluations',
        'D)': 'Semantic evaluations'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What are some examples of datasets used for testing word similarity metrics?"",
    'Options': 
    {
        'A)': ""WordSim-353, SimLex-999, TOEFL"",
        'B)': ""WordSim-353, SimLex-999, TOEFL, other datasets"",
        'C)': ""WordSim-353, SimLex-999, other datasets"",
        'D)': ""TOEFL, other datasets""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'Which dataset provides a set of ratings from 0 to 10 for 353 noun pairs?',
    'Options': 
    {
        'A)': 'WordSim-353',
        'B)': 'SimLex-999',
        'C)': 'TOEFL',
        'D)': 'None of the above'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""Which method should be applied first when evaluating the performance of algorithms on word similarity?"", 
    'Options': 
    {
        'A)': ""Compute the correlation between word similarity scores and human ratings"",
        'B)': ""Use the word similarity ratings directly"",
        'C)': ""Identify the target word among the four options"",
        'D)': ""Choose the correct synonym using the TOEFL dataset""
    },
    'Correct answer': 'A)'
}"
82,naive bayes,"While standard naive Bayes text classification can work well for sentiment analysis, some small changes are generally employed that improve performance. ","{
    'Question': 'In sentiment analysis using naive Bayes, what is a common modification made?',
    'Options': 
    {
        'A)': 'Increasing the number of features',
        'B)': 'Using a different algorithm instead of naive Bayes',
        'C)': 'Applying more sophisticated feature selection techniques',
        'D)': 'Decreasing the size of the training dataset'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What type of machine learning algorithm is often modified for better sentiment analysis using standard naive Bayes techniques?"",
    'Options': 
    {
        'A)': ""Support Vector Machines (SVM)"",
        'B)': ""K-Nearest Neighbors (KNN)"",
        'C)': ""Decision Trees"",
        'D)': ""Random Forests""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What is a common modification made to naive Bayes for better sentiment analysis?',
    'Options': 
    {
        'A)': 'Increasing the number of features',
        'B)': 'Using more complex models instead of naive Bayes',
        'C)': 'Applying regularization techniques',
        'D)': 'Decreasing the size of training data'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What type of improvements are typically made to naive Bayes for better sentiment analysis performance?"",
    'Options': 
    {
        'A)': ""Adding more features to the model"",
        'B)': ""Increasing the complexity of the algorithm"",
        'C)': ""Using different algorithms instead"",
        'D)': ""Applying specific techniques tailored for sentiment analysis""
    },
    'Correct answer': 'D)'
}","{
    'Question': 'Which of the following is an improvement often made to naive Bayes for better sentiment analysis?',
    'Options': 
    {
        'A)': 'Increasing the number of features',
        'B)': 'Using more complex models instead of naive Bayes',
        'C)': 'Applying smoothing techniques to handle zero probabilities',
        'D)': 'Decreasing the sample size for training data'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What is an example of a small change made to naive Bayes for better sentiment analysis?"", 
    'Options': 
    {
        'A)': ""Using a different dataset entirely"",
        'B)': ""Increasing the number of features used in the model"",
        'C)': ""Applying more complex machine learning algorithms instead"",
        'D)': ""Implementing techniques such as Laplace smoothing""
    },
    'Correct answer': 'D)'
}"
83,dependency,"Treebanks play a critical role in the development and evaluation of dependency parsers. They are used for training parsers, they act as the gold labels for evaluating parsers, and they also provide useful information for corpus linguistics studies. ","{
    'Question': 'What do treebanks primarily serve in relation to dependency parsing?',
    'Options':
    {
        'A)': 'To train machine learning models',
        'B)': 'As a source of linguistic data',
        'C)': 'For improving parser accuracy',
        'D)': 'To enhance corpus linguistics research'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""In what way do treebanks contribute to the field of natural language processing?"", 
    'Options': 
    {
        'A)': ""They help in improving the accuracy of speech recognition systems."",
        'B)': ""They assist in creating more efficient neural networks."",
        'C)': ""They facilitate the creation of better dependency parsers."",
        'D)': ""They enhance the understanding of grammatical structures.""
    },
    'Correct answer': 'C)'
}","{
    'Question': 'What is the primary purpose of using treebanks in the field of natural language processing?',
    'Options': 
    {
        'A)': 'To train machine learning models',
        'B)': 'To evaluate parser performance',
        'C)': 'To improve computational efficiency',
        'D)': 'To enhance data privacy'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""How do treebanks contribute to the field of natural language processing?"", 
    'Options': 
    {
        'A)': ""They help train models by providing examples."",
        'B)': ""They serve as reference texts for grammatical analysis."",
        'C)': ""They offer insights into linguistic structures through annotated data."",
        'D)': ""They facilitate the creation of new languages.""
    },
    'Correct answer': 'C)'
}","{
    'Question': 'What is the primary purpose of using treebanks in the context of dependency parsing?',
    'Options': 
    {
        'A)': 'To develop new languages',
        'B)': 'To evaluate parser performance',
        'C)': 'To create grammatical rules',
        'D)': 'To translate between languages'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What is the primary purpose of using treebanks in the field of natural language processing?"", 
    'Options': 
    {
        'A)': ""To develop new languages"",
        'B)': ""To evaluate parser performance"",
        'C)': ""To create grammatical rules"",
        'D)': ""To improve pronunciation""
    },
    'Correct answer': 'B)'
}"
84,translation," Machine translation models are trained on a parallel corpus, sometimes called ","{
    'Question': 'What is often referred to as a parallel corpus for machine translation models?',
    'Options': 
    {
        'A)': 'Training data',
        'B)': 'Translation pairs',
        'C)': 'Parallel sentences',
        'D)': 'Corpus of texts'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""In machine translation models, what process involves moving from one language to another based on statistical probabilities?"",
    'Options': 
    {
        'A)': ""Translation"",
        'B)': ""Interpretation"",
        'C)': ""Rephrasing"",
        'D)': ""Comprehension""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What is referred to as a parallel corpus when training machine translation models?',
    'Options': 
    {
        'A)': 'Training data',
        'B)': 'Reference texts',
        'C)': 'Source language sentences',
        'D)': 'Target language sentences'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What type of data do machine translation models typically use for training?"",
    'Options': 
    {
        'A)': ""Parallel corpora"",
        'B)': ""Untranslated texts"",
        'C)': ""Audio recordings"",
        'D)': ""Video clips""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What is often referred to as a parallel corpus used for training machine translation models?',
    'Options': 
    {
        'A)': 'Parallel data set',
        'B)': 'Translation pair',
        'C)': 'Interlingual dictionary',
        'D)': 'Synthetic language model'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What is an example of a parallel corpus used for training machine translation models?"", 
    'Options': 
    {
        'A)': ""A single language sentence"",
        'B)': ""Two different languages sentences side by side"",
        'C)': ""A dictionary of vocabulary terms"",
        'D)': ""Audio recordings of spoken languages""
    },
    'Correct answer': 'B)'
}"
85,semantic role,"Semantic role labeling (sometimes shortened as SRL) is the task of automatically finding the semantic roles of each argument of each predicate in a sentence. Current approaches to semantic role labeling are based on supervised machine learning, often using the FrameNet and PropBank resources to specify what counts as a predicate, define the set of roles used in the task, and provide training and test sets. ","{
    'Question': 'In which way does semantic role labeling differ from other types of natural language processing?',
    'Options': 
    {
        'A)': 'It involves identifying the grammatical structure of sentences.',
        'B)': 'It focuses on understanding the meaning behind words.',
        'C)': 'It uses unsupervised machine learning techniques.',
        'D)': 'It requires manual annotation by human linguists.'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""Which approach is commonly used for semantic role labeling?"", 
    'Options': 
    {
        'A)': ""Rule-based methods"",
        'B)': ""Supervised machine learning"",
        'C)': ""Unsupervised learning"",
        'D)': ""Hybrid method combining rule-based and supervised techniques""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What does semantic role labeling involve?',
    'Options': 
    {
        'A)': 'It involves manually assigning roles to arguments.',
        'B)': 'It involves identifying the grammatical parts of speech.',
        'C)': 'It involves predicting the next word in a sentence.',
        'D)': 'It involves classifying predicates into different categories.'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""What does semantic role labeling involve?"",
    'Options': 
    {
        'A)': ""Automatically identifying the grammatical functions of verbs in sentences."",
        'B)': ""Determining the relationships between nouns and their antecedents."",
        'C)': ""Finding the semantic roles of arguments within predicates."",
        'D)': ""Classifying adjectives into different categories.""
    },
    'Correct answer': 'C)'
}","{
    'Question': 'What does semantic role labeling involve?',
    'Options': 
    {
        'A)': 'It involves manually identifying the roles of arguments in sentences.',
        'B)': 'It involves creating new predicates for the task.',
        'C)': 'It involves developing algorithms to find the roles of arguments in sentences.',
        'D)': 'It involves organizing data into categories.'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What is the first step in developing an automated system for semantic role labeling?"", 
    'Options': 
    {
        'A)': ""Collecting annotated data"",
        'B)': ""Training the machine learning model"",
        'C)': ""Identifying predicates in sentences"",
        'D)': ""Selecting appropriate resources""
    },
    'Correct answer': 'C)'
}"
86,translation,"Fig. 13.2 shows examples of other word order differences. All of these word order differences between languages can cause problems for translation, requiring the system to do huge structural reorderings as it generates the output. ","{
    'Question': 'What causes problems for translation?',
    'Options': 
    {
        'A)': 'The use of different sentence structures',
        'B)': 'The complexity of grammar rules',
        'C)': 'The variation in vocabulary usage',
        'D)': 'All of the above'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""Which concept explains why translating from one language to another requires significant restructuring?"",
    'Options': 
    {
        'A)': ""Translation errors"",
        'B)': ""Word order variations"",
        'C)': ""Syntax changes"",
        'D)': ""Grammar inconsistencies""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What problem does having different word orders between languages create for translation?',
    'Options': 
    {
        'A)': 'It makes translation easier by providing more options.',
        'B)': 'It simplifies the process of generating the output.',
        'C)': 'It requires the system to perform large structural reordering during translation.',
        'D)': 'It decreases the need for human translators.'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What problem does the context suggest might arise during translation due to language-specific word orders?"",
    'Options': 
    {
        'A)': ""Inconsistent grammar structures"",
        'B)': ""Difficulty in maintaining sentence structure"",
        'C)': ""Translation software overload"",
        'D)': ""Loss of cultural nuances""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'Which of the following is NOT an example of word order differences?',
    'Options': 
    {
        'A)': 'Japanese',
        'B)': 'English',
        'C)': 'Spanish',
        'D)': 'French'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What is required when translating sentences with different word orders?"", 
    'Options': 
    {
        'A)': ""Ignore the original sentence structure completely."",
        'B)': ""Keep the original sentence structure intact."",
        'C)': ""Translate each part of speech separately."",
        'D)': ""Reorder the translated parts based on target language syntax.""
    },
    'Correct answer': 'D)'
}"
87,question answering,"Chatbots are systems that can carry on extended conversations with the goal of mimicking the unstructured conversations or chats` characteristic of informal humanhuman interaction. While early systems like ELIZA (Weizenbaum, 1966) or PARRY (Colby et al., 1971) had theoretical goals like testing theories of psychological counseling, for most of the last 50 years chatbots have been designed for entertainment. That changed with the recent rise of neural chatbots like ChatGPT, which incorporate solutions to NLP tasks like question answering, writing tools, or machine translation into a conversational interface. A conversation with ChatGPT is shown in Fig. 15.12. In this section we describe neural chatbot architectures and datasets. ","{
    'Question': 'What type of interactions do early chatbots primarily aim to mimic?',
    'Options': 
    {
        'A)': 'Structured conversations',
        'B)': 'Formal business communication',
        'C)': 'Unstructured human-human interaction',
        'D)': 'Technical support inquiries'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What type of system was initially developed to test theories of psychological counseling?"",
    'Options': 
    {
        'A)': ""ELIZA"",
        'B)': ""PARRY"",
        'C)': ""Chatbots"",
        'D)': ""Neural chatbots""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What is the primary purpose of chatbots?',
    'Options': 
    {
        'A)': 'To provide entertainment',
        'B)': 'To mimic human-human interactions',
        'C)': 'To solve complex mathematical problems',
        'D)': 'To translate languages between different cultures'
    },
    'Correct answer': 'B)',
    'Verb': 'explain'
}","{
    'Question': ""What aspect of chatbots has evolved significantly over time due to advancements in technology?"",
    'Options': 
    {
        'A)': ""Theoretical goals of psychological counseling"",
        'B)': ""Entertainment features"",
        'C)': ""NLP task solutions"",
        'D)': ""Conversational interfaces""
    },
    'Correct answer': 'C)'
}","{
    'Question': 'What is the primary purpose of chatbots?',
    'Options': 
    {
        'A)': 'To provide entertainment',
        'B)': 'To mimic human-human interactions',
        'C)': 'To test theories of psychological counseling',
        'D)': 'To translate languages'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What is the primary function of a chatbot?"", 
    'Options': 
    {
        'A)': ""To provide structured responses only."",
        'B)': ""To mimic human-human interactions through natural language processing."",
        'C)': ""To focus solely on entertainment purposes."",
        'D)': ""To analyze user data for personalized recommendations.""
    },
    'Correct answer': 'B)'
}"
88,naive bayes,"Many kinds of machine learning algorithms are used to build classifiers. This chapter introduces naive Bayes; the following one introduces logistic regression. These exemplify two ways of doing classification. Generative classifiers like naive Bayes build a model of how a class could generate some input data. Given an observation, they return the class most likely to have generated the observation. Discriminative classifiers like logistic regression instead learn what features from the input are most useful to discriminate between the different possible classes. While discriminative systems are often more accurate and hence more commonly used, generative classifiers still have a role. ","{
    'Question': 'Which type of classifier is based on the assumption that all features are independent of each other?',
    'Options': 
    {
        'A)': 'Logistic Regression',
        'B)': 'Support Vector Machines',
        'C)': 'Decision Trees',
        'D)': 'Naive Bayes'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""Which algorithm builds a model of how a class could generate some input data?"",
    'Options': 
    {
        'A)': ""Naive Bayes"",
        'B)': ""Logistic Regression"",
        'C)': ""Support Vector Machines"",
        'D)': ""K-Nearest Neighbors""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What is a key difference between naive Bayes and logistic regression?',
    'Options': 
    {
        'A)': 'Naive Bayes builds models based on observed data, while logistic regression focuses on feature selection.',
        'B)': 'Naive Bayes uses discriminative methods, whereas logistic regression employs generative approaches.',
        'C)': 'Logistic regression generates probabilities for each class, unlike naive Bayes which predicts the most probable class.',
        'D)': 'Naive Bayes requires labeled training data, but logistic regression does not.'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""Which type of classifier does naive Bayes belong to?"",
    'Options': 
    {
        'A)': ""Discriminative Classifier"",
        'B)': ""Generative Classifier"",
        'C)': ""Both Discriminative and Generative Classifier"",
        'D)': ""Neither Discriminative nor Generative Classifier""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'Which type of classifier builds a model of how a class generates some input data?',
    'Options': 
    {
        'A)': 'Logistic Regression',
        'B)': 'Naive Bayes',
        'C)': 'Support Vector Machines',
        'D)': 'Decision Trees'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""Which type of classifier is primarily concerned with modeling the probability distribution of each class?"",
    'Options': 
    {
        'A)': ""Discriminative classifiers only"",
        'B)': ""Generative classifiers only"",
        'C)': ""Both types equally"",
        'D)': ""Neither type applies""
    },
    'Correct answer': 'B)'
}"
89,dialogue system,"In the 1990s, machine learning models that had first been applied to natural language processing began to be applied to dialogue tasks like slot filling (Miller et al. 1994, Pieraccini et al. 1991). This period also saw lots of analytic work on the linguistic properties of dialogue acts and on machine-learning-based methods for their detection. (Sag and Liberman 1975, Hinkelman and Allen 1989, Nagata and Morimoto 1994, Goodwin 1996, Chu-Carroll 1998, Shriberg et al. 1998, Stolcke et al. 2000, Gravano et al. 2012. This work strongly informed the development of the dialogue-state model (Larsson and Traum, 2000). Dialogue state tracking quickly became an important problem for task-oriented dialogue, and there has been an influential annual evaluation of state-tracking algorithms (Williams et al., 2016). The turn of the century saw a line of work on applying reinforcement learning to dialogue, which first came out of AT&T and Bell Laboratories with work on MDP dialogue systems (Walker 2000, Levin et al. 2000, Singh et al. 2002) along with work on cue phrases, prosody, and rejection and confirmation. Reinforcement learning research turned quickly to the more sophisticated POMDP models (Roy et al. 2000, Lemon et al. 2006, Williams and Young 2007) applied to small slotfilling dialogue tasks. Neural reinforcement learning models have been used both for chatbot systems, for example simulating dialogues between two dialogue systems, rewarding good conversational properties like coherence and ease of answering (Li et al., 2016a), and for task-oriented dialogue (Williams et al., 2017). ","{
    'Question': 'In what decade did machine learning models begin to be applied to dialogue tasks?',
    'Options': 
    {
        'A)': 'The 1980s',
        'B)': 'The 1970s',
        'C)': 'The 1960s',
        'D)': 'The 1990s'
    },
    'Correct answer': 'D)'
}","Term:'machine learning'

Context: Machine learning involves training algorithms to learn from data.

{
    'Question': ""Which field does machine learning belong to?"", 
    'Options': 
    {
        'A)': ""Artificial Intelligence"",
        'B)': ""Computer Science"",
        'C)': ""Mathematics"",
        'D)': ""Physics""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What was one significant application of machine learning in dialogue systems during the 1990s?',
    'Options': 
    {
        'A)': 'Natural language generation',
        'B)': 'Dialogue act analysis',
        'C)': 'Reinforcement learning',
        'D)': 'Slot filling'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""What was one of the key developments in the early applications of machine learning to dialogue systems in the 1990s?"",
    'Options': 
    {
        'A)': ""The use of neural networks for speech recognition"",
        'B)': ""The introduction of reinforcement learning techniques"",
        'C)': ""The focus on developing natural language understanding"",
        'D)': ""The shift towards end-to-end dialogue generation""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What was one significant application of machine learning in dialogue systems during the 1990s?',
    'Options': 
    {
        'A)': 'Developed dialogue state tracking algorithms',
        'B)': 'Applied reinforcement learning to dialogue',
        'C)': 'Informed the creation of neural networks',
        'D)': 'Enhanced the accuracy of slot-filling techniques'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What was a significant development in the application of machine learning to dialogue systems during the early 2000s?"", 
    'Options': 
    {
        'A)': ""The introduction of neural networks"",
        'B)': ""The focus on reinforcement learning"",
        'C)': ""The increase in dialogue state tracking"",
        'D)': ""The shift towards POMDP models""
    },
    'Correct answer': 'B)'
}"
90,target word,"To measure similarity between two target words v and w, we need a metric that takes two vectors (of the same dimensionality, either both with words as dimensions, ) and gives hence of length | a measure of their similarity. By far the most common similarity metric is the cosine of the angle between the vectors. ","{
    'Question': 'In measuring the similarity between two target words v and w, what does the cosine of the angle between the vectors represent?',
    'Options': 
    {
        'A)': 'The absolute difference between the words',
        'B)': 'The sum of the lengths of the vectors',
        'C)': 'The ratio of the magnitudes of the vectors',
        'D)': 'The product of the vectors'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""Which vector operation measures the similarity between two vectors based on the cosine of the angle between them?"",
    'Options': 
    {
        'A)': ""Dot product"",
        'B)': ""Cross product"",
        'C)': ""Magnitude"",
        'D)': ""Angle""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What does the cosine of the angle between two vectors represent?',
    'Options': 
    {
        'A)': 'The sum of the products of corresponding elements',
        'B)': 'The ratio of the magnitudes of the vectors',
        'C)': 'The difference between the vectors',
        'D)': 'The similarity measure based on vector angles'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""What is the most commonly used similarity metric for measuring the similarity between two target words?"",
    'Options': 
    {
        'A)': ""Euclidean distance"",
        'B)': ""Cosine similarity"",
        'C)': ""Jaccard index"",
        'D)': ""Hamming distance""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What does the cosine of the angle between two vectors represent?',
    'Options': 
    {
        'A)': 'The sum of the products of corresponding elements',
        'B)': 'The difference between the maximum and minimum values',
        'C)': 'The ratio of the magnitudes of the vectors',
        'D)': 'The similarity between the vectors based on their angles'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""What is commonly used to quantify the similarity between two target words?"",
    'Options': 
    {
        'A)': ""The sum of the absolute values of the elements in the vector"",
        'B)': ""The product of the corresponding elements in the vectors"",
        'C)': ""The dot product of the vectors divided by the product of their magnitudes"",
        'D)': ""The ratio of the lengths of the vectors""
    },
    'Correct answer': 'C)'
}"
91,dialogue,"By around 2010 the GUS architecture finally began to be widely used commercially in dialogue systems on phones like Apple`s SIRI (Bellegarda, 2013) and other digital assistants. ","{
    'Question': 'In what year did the GUS architecture start being used commercially for dialogue systems?',
    'Options': 
    {
        'A)': '2005',
        'B)': '2008',
        'C)': '2010',
        'D)': '2012'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""Which technology started to be widely used commercially in dialogue systems starting around 2010?"", 
    'Options': 
    {
        'A)': ""Natural language processing"",
        'B)': ""Machine learning algorithms"",
        'C)': ""Speech recognition software"",
        'D)': ""Gus architecture""
    },
    'Correct answer': 'D)'
}","{
    'Question': 'What did the GUS architecture start being used for by around 2010?',
    'Options': 
    {
        'A)': 'Social media platforms',
        'B)': 'Video conferencing tools',
        'C)': 'Voice recognition software',
        'D)': 'Text messaging applications'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What was one of the key applications for the GUS architecture by around 2010?"", 
    'Options': 
    {
        'A)': ""Improving phone call quality"",
        'B)': ""Enhancing user interaction through natural language processing"",
        'C)': ""Increasing battery life of devices"",
        'D)': ""Reducing data usage during calls""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What was the primary application of the GUS architecture by 2010?',
    'Options': 
    {
        'A)': 'Developing new software for gaming consoles',
        'B)': 'Creating interactive educational tools for children',
        'C)': 'Building voice recognition technology for smartphones',
        'D)': 'Organizing international trade agreements between countries'
    },
    'Correct answer': 'C)'
}","```json
{
    'Question': ""What is the first step in developing an effective dialogue system?"", 
    'Options': 
    {
        'A)': ""Designing the user interface for interaction"",
        'B)': ""Selecting appropriate natural language processing techniques"",
        'C)': ""Building a prototype of the dialogue engine"",
        'D)': ""Collecting and analyzing user data""
    },
    'Correct answer': 'D)'
}
```"
92,word meaning,"We also introduced finetuning in the prior chapter. Here we describe a new kind of finetuning, in which we take the transformer network learned by these pretrained models, add a neural net classifier after the top layer of the network, and train it on some additional labeled data to perform some downstream task like named entity tagging or natural language inference. As before, the intuition is that the pretraining phase learns a language model that instantiates rich representations of word meaning, that thus enables the model to more easily learn (be finetuned to`) the requirements of a downstream language understanding task. This aspect of the pretrain-finetune paradigm is an instance of what is called transfer learning in machine learning: the method of acquiring knowledge from one task or domain, and then applying it (transferring it) to solve a new task. ","{
    'Question': 'In the described finetuning process, why is the pre-trained transformer network important for performing downstream tasks?',
    'Options': 
    {
        'A)': 'To increase computational efficiency',
        'B)': 'To enhance the accuracy of word meanings',
        'C)': 'To improve the speed of training',
        'D)': 'To reduce the need for labeled data'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""In the context of pretraining transformers with a neural net classifier, how does this approach enable better performance in downstream tasks?"",
    'Options': 
    {
        'A)': ""By directly improving the accuracy of the transformer model."",
        'B)': ""By enhancing the ability of the model to generalize across different domains."",
        'C)': ""By allowing the model to focus solely on the specific task at hand."",
        'D)': ""By enabling the model to learn richer representations of word meanings.""
    },
    'Correct answer': 'D)'
}","{
    'Question': 'What does the new kind of finetuning described involve?',
    'Options': 
    {
        'A)': 'Training a transformer network with a neural net classifier',
        'B)': 'Fine-tuning a transformer network using labeled data for a specific task',
        'C)': 'Using pre-trained models to improve the performance of a language understanding task',
        'D)': 'Applying knowledge acquired from one task to another task through transfer learning',
        'E)': 'None of the above'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What does the addition of a neural net classifier after the top layer of the transformer network enable?"",
    'Options': 
    {
        'A)': ""The ability to fine-tune the model for specific tasks"",
        'B)': ""The enhancement of the existing language model"",
        'C)': ""The reduction of computational resources needed"",
        'D)': ""The improvement of the model's accuracy on unseen data""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What does adding a neural net classifier after the top layer of the transformer network enable?',
    'Options': 
    {
        'A)': 'To fine-tune the existing model for better performance',
        'B)': 'To directly improve the accuracy of the downstream task',
        'C)': 'To enhance the representation of word meanings through additional training',
        'D)': 'To reduce the complexity of the model architecture'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What does adding a neural net classifier after the top layer of the transformer network enable?"", 
    'Options': 
    {
        'A)': ""It allows for fine-tuning the entire model."",
        'B)': ""It enhances the model's capacity to understand complex linguistic patterns."",
        'C)': ""It simplifies the training process significantly."",
        'D)': ""It improves the model's performance on downstream tasks.""
    },
    'Correct answer': 'D)'
}"
93,grammar,"The Hobbs algorithm9 is a tree-search algorithm that was the first in a long series of syntax-based methods for identifying reference robustly in naturally occurring text. The input to the Hobbs algorithm is a pronoun to be resolved, together with a syntactic (constituency) parse of the sentences up to and including the current sentence. The details of the algorithm depend on the grammar used, but can be understood from a simplified version due to Kehler et al. (2004) that just searches through the list of NPs in the current and prior sentences. This simplified Hobbs algorithm searches NPs in the following order: (i) in the current sentence from right-to-left, starting with the first NP to the left of the pronoun, (ii) in the previous sentence from left-to-right, (iii) in two sentences prior from left-to-right, and (iv) in ","{
    'Question': 'In which order does the simplified Hobbs algorithm search for NPs?',
    'Options': 
    {
        'A)': 'Left-to-right, Right-to-left, Two sentences prior, Previous sentence',
        'B)': 'Right-to-left, Left-to-right, Two sentences prior, Previous sentence',
        'C)': 'Previous sentence, Two sentences prior, Left-to-right, Right-to-left',
        'D)': 'Two sentences prior, Previous sentence, Left-to-right, Right-to-left'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""In which part of the sentence does the Hobbs algorithm start its search when resolving a pronoun?"",
    'Options': 
    {
        'A)': ""At the beginning of the current sentence"",
        'B)': ""At the end of the current sentence"",
        'C)': ""At the beginning of the previous sentence"",
        'D)': ""At the end of the previous sentence""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What does the Hobbs algorithm use as its input?',
    'Options': 
    {
        'A)': 'The pronouns only',
        'B)': 'The syntactic parses only',
        'C)': 'Both the pronouns and syntactic parses',
        'D)': 'Neither the pronouns nor syntactic parses'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What does the simplified Hobbs algorithm do when searching for a pronoun?"",
    'Options': 
    {
        'A)': ""It starts by looking at all NPs in the current sentence from right-to-left."",
        'B)': ""It looks at all NPs in the previous sentence from left-to-right."",
        'C)': ""It skips over NPs in the next two sentences before checking the current one."",
        'D)': ""It checks every NP in the entire document.""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What does the Hobbs algorithm use as its input?',
    'Options': 
    {
        'A)': 'The pronoun to be resolved',
        'B)': 'A syntactic parse of the entire document',
        'C)': 'Both A and B',
        'D)': 'None of the above'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What method should be applied to identify the grammatical structure needed for resolving a pronoun?"", 
    'Options': 
    {
        'A)': ""Kehler et al.'s simplified version of the Hobbs algorithm"",
        'B)': ""The original Hobbs algorithm itself"",
        'C)': ""Parsing the entire document recursively"",
        'D)': ""Using a pre-existing parser based on the specified grammar""
    },
    'Correct answer': 'D)'
}"
94,regular expression,"We`ll then turn to a set of tasks collectively called text normalization, in which regular expressions play an important part. Normalizing text means converting it to a more convenient, standard form. For example, most of what we are going to do with language relies on first separating out or tokenizing words or word parts from running text, the task of tokenization. English words are often separated from each other by whitespace, but whitespace is not always sufficient. New York and rock `n` roll are sometimes treated as large words despite the fact that they contain spaces, while sometimes we`ll need to separate I`m into the two words I and am. For processing tweets or texts we`ll need to tokenize emoticons like :) or hashtags ","{
    'Question': 'In text normalization, why does regular expression play an important role?',
    'Options': 
    {
        'A)': 'To identify patterns in text',
        'B)': 'To convert text to binary code',
        'C)': 'To remove all punctuation marks',
        'D)': 'To increase text length'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""In the process of normalizing text, what tool plays a crucial role?"", 
    'Options': 
    {
        'A)': ""Regular expressions"",
        'B)': ""Natural Language Processing (NLP) algorithms"",
        'C)': ""Machine learning models"",
        'D)': ""Speech recognition software""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What does normalizing text mean?',
    'Options': 
    {
        'A)': 'Converting text into a simpler format for easier reading',
        'B)': 'Removing all punctuation marks from the text',
        'C)': 'Changing the case of all letters in the text',
        'D)': 'Separating words or word parts from running text using regular expressions'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""What does the term 'text normalization' involve?"",
    'Options': 
    {
        'A)': ""Separating out or tokenizing words or word parts from running text"",
        'B)': ""Converting text to a more convenient, standard form"",
        'C)': ""Processing tweets or texts using regular expressions"",
        'D)': ""Both A) and B)""
    },
    'Correct answer': 'D)'
}","{
    'Question': 'What does normalizing text mean?',
    'Options': 
    {
        'A)': 'Converting text to its original form',
        'B)': 'Removing all punctuation marks from text',
        'C)': 'Storing text in a database',
        'D)': 'Transforming text into a standardized format for easier processing'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""What is the primary purpose of using regular expressions in text normalization?"", 
    'Options': 
    {
        'A)': ""To identify and replace specific patterns within text"",
        'B)': ""To convert all uppercase letters to lowercase"",
        'C)': ""To remove all punctuation marks from the text"",
        'D)': ""To increase the speed of data entry""
    },
    'Correct answer': 'A)'
}"
95,relation extraction,"The great advantage of unsupervised relation extraction is its ability to handle a huge number of relations without having to specify them in advance. The disadvantage is the need to map all the strings into some canonical form for adding to databases or knowledge graphs. Current methods focus heavily on relations expressed with verbs, and so will miss many relations that are expressed nominally. ","{
    'Question': 'What is one significant benefit of using unsupervised relation extraction?',
    'Options': 
    {
        'A)': 'It can easily identify all possible relations.',
        'B)': 'It requires less computational resources than supervised methods.',
        'C)': 'It handles a large number of relations more efficiently than supervised methods.',
        'D)': 'It automatically maps strings into canonical forms.'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What is an example of a relation that current methods may miss due to being expressed nominally?"",
    'Options': 
    {
        'A)': ""The relationship between two people who share a birthday."",
        'B)': ""The relationship between two cities based on their geographical proximity."",
        'C)': ""The relationship between a book and its author."",
        'D)': ""The relationship between a person and their profession.""
    },
    'Correct answer': 'A)'
}","{
    'Question': 'What is one of the main advantages of using unsupervised relation extraction?',
    'Options': 
    {
        'A)': 'It can easily identify complex relationships.',
        'B)': 'It requires less computational resources than supervised methods.',
        'C)': 'It handles a large number of relations efficiently without specifying them beforehand.',
        'D)': 'It automatically maps all strings into a common format.'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""Explain how unsupervised relation extraction differs from supervised methods?"",
    'Options': 
    {
        'A)': ""Unsupervised methods require more data than supervised methods."",
        'B)': ""Supervised methods can automatically learn the mapping between strings and their canonical forms."",
        'C)': ""Supervised methods are better at handling large numbers of relations."",
        'D)': ""Unsupervised methods cannot handle relations expressed nominally.""
    },
    'Correct answer': 'D)'
}","{
    'Question': 'Which statement best describes the main challenge faced by current unsupervised relation extraction methods?',
    'Options': 
    {
        'A)': 'They struggle to identify relations involving abstract concepts.',
        'B)': 'They require extensive manual labeling of data.',
        'C)': 'They cannot process large volumes of textual data efficiently.',
        'D)': 'They often overlook nominal expressions due to their focus on verb-based relations.'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""What challenge does unsupervised relation extraction face when dealing with nominal expressions?"", 
    'Options': 
    {
        'A)': ""It cannot handle large numbers of relations."",
        'B)': ""It requires specifying relations beforehand."",
        'C)': ""It maps strings into canonical forms."",
        'D)': ""It misses many relations expressed nominally.""
    },
    'Correct answer': 'D)'
}"
96,lexicon,"The Computational Grammar Coder (CGC) of Klein and Simmons (1963) had three components: a lexicon, a morphological analyzer, and a context disambiguator. The small 1500-word lexicon listed only function words and other irregular words. The morphological analyzer used inflectional and derivational suffixes to assign part-of-speech classes. These were run over words to produce candidate parts of speech which were then disambiguated by a set of 500 context rules by relying on surrounding islands of unambiguous words. For example, one rule said that between an ARTICLE and a VERB, the only allowable sequences were ADJ-NOUN, NOUNADVERB, or NOUN-NOUN. The TAGGIT tagger (Greene and Rubin, 1971) used the same architecture as Klein and Simmons (1963), with a bigger dictionary and more tags (87). TAGGIT was applied to the Brown corpus and, according to Francis and Kucera (1982, p. 9), accurately tagged 77% of the corpus; the remainder of the Brown corpus was then tagged by hand. All these early algorithms were based on a two-stage architecture in which a dictionary was first used to assign each word a set of potential parts of speech, and then lists of handwritten disambiguation rules winnowed the set down to a single part of speech per word. ","{
    'Question': 'What did the CGC use for its lexical data?',
    'Options': 
    {
        'A)': 'The Brown Corpus',
        'B)': 'A large database of English sentences',
        'C)': 'An online encyclopedia',
        'D)': 'A smaller but more detailed lexicon'
    },
    'Correct answer': 'D)'
}","Term: computational grammar coder.

Context: The Computational Grammar Coder (CGC) of Klein and Simmons (1963) had three components: a lexicon, a morphological analyzer, and a context disambiguator. The small 1500-word lexicon listed only function words and other irregular words. The morphological analyzer used inflectional and derivational suffixes to assign part-of-speech classes. These were run over words to produce candidate parts of speech which were then disambiguated by a set of 500 context rules by relying on surrounding islands of unambiguous words. For example, one rule said that between an ARTICLE and a VERB, the only allowable sequences were ADJ-NOUN, NOUN-ADVERB, or NOUN-NOUN. The TAGGIT tagger (Greene and Rubin, 1971) used the same architecture as Klein and Simmons (1963), with a bigger dictionary and more tags (87). TAGGIT was applied to the Brown corpus and, according to Francis and Kucera (1982, p. 9), accurately tagged 77% of the corpus; the remainder of the Brown corpus","{
    'Question': 'What did Klein and Simmons use for their lexicon?',
    'Options': 
    {
        'A)': 'A large database',
        'B)': 'A smaller but comprehensive list',
        'C)': 'An extensive collection of texts',
        'D)': 'A detailed glossary'
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What was the primary difference between the CGC and the TAGGIT taggers?"",
    'Options': 
    {
        'A)': ""The CGC used a smaller lexicon than the TAGGIT."",
        'B)': ""The TAGGIT employed more context-based rules for disambiguation."",
        'C)': ""The CGC relied solely on dictionaries for part-of-speech tagging."",
        'D)': ""The TAGGIT utilized fewer types of tags compared to the CGC.""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What did the CGC use for its lexical data?',
    'Options': 
    {
        'A)': 'A large database',
        'B)': 'An online encyclopedia',
        'C)': 'The Brown Corpus',
        'D)': 'A smaller lexicon'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""What is the primary component of the Computational Grammar Coder (CGC) developed by Klein and Simmons?"", 
    'Options': 
    {
        'A)': ""The morphological analyzer"",
        'B)': ""The context disambiguator"",
        'C)': ""The TAGGIT tagger"",
        'D)': ""The lexicon""
    },
    'Correct answer': 'D)'
}"
97,dialogue,"The goal of text-to-speech (TTS) systems is to map from strings of letters to waveforms, a technology that`s important for a variety of applications from dialogue systems to games to education. ","{
    'Question': 'What are TTS systems primarily used for?',
    'Options': 
    {
        'A)': 'Mapping from waveforms to strings of letters',
        'B)': 'Creating visual representations of text',
        'C)': 'Translating languages between different scripts',
        'D)': 'Generating audio output based on input text'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""In what application are TTS systems most commonly used?"", 
    'Options': 
    {
        'A)': ""Medical diagnostics"",
        'B)': ""Educational content creation"",
        'C)': ""Weather forecasting"",
        'D)': ""Financial market analysis""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What are TTS systems primarily used for?',
    'Options': 
    {
        'A)': 'Mapping from numbers to symbols',
        'B)': 'Translating languages into speech',
        'C)': 'Creating visual aids',
        'D)': 'Processing images',
    },
    'Correct answer': 'B)'
}","{
    'Question': ""What are some potential applications of TTS systems beyond just dialogue?"", 
    'Options': 
    {
        'A)': ""Games and education"",
        'B)': ""Medical records and financial reports"",
        'C)': ""Weather forecasts and sports scores"",
        'D)': ""All of the above""
    },
    'Correct answer': 'D)'
}","{
    'Question': 'What are TTS systems primarily used for?',
    'Options': 
    {
        'A)': 'Mapping from waveforms to strings of letters',
        'B)': 'Creating visual representations of text',
        'C)': 'Converting speech into written language',
        'D)': 'Translating languages between different scripts'
    },
    'Correct answer': 'A)'
}","{
    'Question': ""What is the first step in creating a dialogue system?"", 
    'Options': 
    {
        'A)': ""Translating speech into text"",
        'B)': ""Processing audio signals"",
        'C)': ""Mapping letters to waveforms"",
        'D)': ""Generating voice output""
    },
    'Correct answer': 'C)'
}"
98,discourse coherence,"Training models to predict longer contexts than just consecutive pairs of sentences can result in even stronger discourse representations. For example a Transformer language model trained with a contrastive sentence objective to predict text up to a distance of 2 sentences improves performance on various discourse coherence tasks (Iter et al., 2020). ","{
    'Question': 'What does training models to predict longer contexts improve?',
    'Options': 
    {
        'A)': 'Model accuracy',
        'B)': 'Sentence length',
        'C)': 'Discourse coherence',
        'D)': 'Word count'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What type of task does the study by Iter et al. focus on improving?"", 
    'Options': 
    {
        'A)': ""Long-term memory recall"",
        'B)': ""Discourse coherence prediction"",
        'C)': ""Sentence pair similarity"",
        'D)': ""Text generation accuracy""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'What does training models to predict longer contexts improve?',
    'Options': 
    {
        'A)': 'Word embeddings',
        'B)': 'Sentiment analysis',
        'C)': 'Discourse coherence',
        'D)': 'Machine translation'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""Explain how training models for longer context prediction enhances discourse representation?"",
    'Options': 
    {
        'A)': ""It allows models to focus solely on immediate sentence pair predictions."",
        'B)': ""It enables models to improve their understanding of complex discourse structures beyond simple sentence-level analysis."",
        'C)': ""It restricts models to predicting only the next word in a sequence, thus simplifying discourse analysis."",
        'D)': ""It decreases the overall accuracy of discourse-related tasks.""
    },
    'Correct answer': 'B)'
}","{
    'Question': 'How does training models to predict longer contexts affect their performance on discourse coherence tasks?',
    'Options': 
    {
        'A)': 'It decreases performance by making predictions too early.',
        'B)': 'It has no effect as long as the model is well-trained.',
        'C)': 'It enhances performance by capturing more contextual information.',
        'D)': 'It reduces the overall length of coherent discourse.'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What is an effective strategy for improving discourse coherence in models?"", 
    'Options': 
    {
        'A)': ""Increase the training dataset size only."",
        'B)': ""Use larger sentence distances in the prediction task."",
        'C)': ""Focus solely on predicting consecutive sentences."",
        'D)': ""Both B) and C) are equally important.""
    },
    'Correct answer': 'D)'
}"
99,lexicon,"In the next sections we introduce basic theories of emotion, show how sentiment lexicons are a special case of emotion lexicons, and mention some useful lexicons. We then survey three ways for building lexicons: human labeling, semi-supervised, and supervised. Finally, we talk about how to detect affect toward a particular entity, and introduce connotation frames. ","{
    'Question': 'What is discussed in the last section?',
    'Options': 
    {
        'A)': 'Theories of emotion',
        'B)': 'Sentiment lexicons',
        'C)': 'Building lexicons',
        'D)': 'Detecting affect towards entities'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""Which type of lexicon involves humans labeling data?"",
    'Options': 
    {
        'A)': ""Semi-supervised lexicon"",
        'B)': ""Supervised lexicon"",
        'C)': ""Unsupervised lexicon"",
        'D)': ""Human-labeled lexicon""
    },
    'Correct answer': 'D)'
}","{
    'Question': 'What is discussed in the last section?',
    'Options': 
    {
        'A)': 'Human labeling',
        'B)': 'Semi-supervised',
        'C)': 'Supervised',
        'D)': 'Detecting affect towards a particular entity'
    },
    'Correct answer': 'D)'
}","{
    'Question': ""What does the introduction of basic theories of emotion and sentiment lexicons suggest?"", 
    'Options': 
    {
        'A)': ""It introduces new methods for detecting emotions."",
        'B)': ""It explains the importance of using sentiment lexicons."",
        'C)': ""It highlights the role of human labeling in creating lexicons."",
        'D)': ""It discusses various approaches to constructing lexicons.""
    },
    'Correct answer': 'D)'
}","{
    'Question': 'What is the main focus of the passage?',
    'Options': 
    {
        'A)': 'Introducing theories of emotion',
        'B)': 'Discussing sentiment lexicons as a type of emotion lexicons',
        'C)': 'Exploring methods for creating lexicons',
        'D)': 'Analyzing affect towards entities',
        'E)': 'Examining connotation frames'
    },
    'Correct answer': 'C)'
}","{
    'Question': ""What is the first step in creating an emotion lexicon?"", 
    'Options': 
    {
        'A)': ""Identify entities with positive connotations"",
        'B)': ""Develop algorithms for detecting affect towards entities"",
        'C)': ""Collect data through human labeling"",
        'D)': ""Choose between different methods for building lexicons""
    },
    'Correct answer': 'C)'
}"

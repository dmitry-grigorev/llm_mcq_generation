{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11875339,"sourceType":"datasetVersion","datasetId":7463240}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nfrom datasets import Dataset\nfrom transformers.pipelines.pt_utils import KeyDataset\nfrom datasets import IterableDataset\nfrom tqdm import tqdm\nimport json\nimport random\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-20T08:48:21.884526Z","iopub.execute_input":"2025-05-20T08:48:21.884772Z","iopub.status.idle":"2025-05-20T08:48:49.874840Z","shell.execute_reply.started":"2025-05-20T08:48:21.884753Z","shell.execute_reply":"2025-05-20T08:48:49.874202Z"}},"outputs":[{"name":"stderr","text":"2025-05-20 08:48:34.912892: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747730915.146521      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747730915.213247      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# LLM as judge generation assessment\n\nIn this notebook some randomly chosen MCQ questions that were generated by LLM are assessed for compliance with the prompt that was used for the generation to instruct model.","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T08:48:49.876127Z","iopub.execute_input":"2025-05-20T08:48:49.876733Z","iopub.status.idle":"2025-05-20T08:48:49.890024Z","shell.execute_reply.started":"2025-05-20T08:48:49.876712Z","shell.execute_reply":"2025-05-20T08:48:49.889187Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"\n# Set your HF token & username as environment variables\nos.environ[\"HF_TOKEN\"] = user_secrets.get_secret(\"HF_TOKEN\")\n# Replace with your username)\nos.environ[\"HF_USERNAME\"] = \"dimvsometimes\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T08:48:49.890772Z","iopub.execute_input":"2025-05-20T08:48:49.890976Z","iopub.status.idle":"2025-05-20T08:48:50.065728Z","shell.execute_reply.started":"2025-05-20T08:48:49.890954Z","shell.execute_reply":"2025-05-20T08:48:50.065036Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Judge model\nGoogle's Gemma-3 is used as one of strong models","metadata":{}},{"cell_type":"code","source":"torch.random.manual_seed(0)\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-3-4b-it\",\n    device_map=\"cuda\",\n    #device_map='cpu',\n    torch_dtype=\"auto\",\n    trust_remote_code=True,\n)\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-4b-it\", padding_side='left')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T08:48:50.067135Z","iopub.execute_input":"2025-05-20T08:48:50.067572Z","iopub.status.idle":"2025-05-20T08:49:24.166496Z","shell.execute_reply.started":"2025-05-20T08:48:50.067555Z","shell.execute_reply":"2025-05-20T08:49:24.165876Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1649d91a91ac404a89c345ff689439cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/90.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22918982c7534cae98e09966343b5a5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4341263c588a4a3e99014d0f116dd1f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f06ba2e6e0ac43e6a2fb634835d8ca8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.64G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bbfdc3429f74ead9c4e71efa6c0c9ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5269b831a3b8416fadc0e6946d56c978"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71f41cb18c5f4ed98ff3766e3af3f42c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5a263fcbc604ede8ee9329eff6410ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1de28e8cfaef48348bbee0c7c757e5f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8995b3c6f8cd476182bbd18ac811f03d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2aa6b9fda2db4da987efb1af1e725dfc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f772fd753ad0427082d5cb0c4b15806d"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"text = lambda msg: tokenizer.apply_chat_template(\n    msg,\n    tokenize=False,\n    add_generation_prompt=True\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T08:49:24.167165Z","iopub.execute_input":"2025-05-20T08:49:24.167361Z","iopub.status.idle":"2025-05-20T08:49:24.171118Z","shell.execute_reply.started":"2025-05-20T08:49:24.167344Z","shell.execute_reply":"2025-05-20T08:49:24.170357Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"generation_args = {\n    \"max_new_tokens\": 250,\n    \"return_full_text\": False,\n    \"temperature\": 0.2,\n    \"do_sample\": True,\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T08:49:24.171821Z","iopub.execute_input":"2025-05-20T08:49:24.172109Z","iopub.status.idle":"2025-05-20T08:49:29.173496Z","shell.execute_reply.started":"2025-05-20T08:49:24.172087Z","shell.execute_reply":"2025-05-20T08:49:29.172594Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## The question set taken for prompt alignment assessment\nEach pair of term and context that were randomly chosen got 6 generated questions:\n- for each type of Bloom's taxonomy of questions considered: Remember type (the lowest one), Comprehension type and Application type\n - MCQ were generated with 0-shot prompt and one/few-shot prompt","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport ast\ntest_data = pd.read_csv(\"/kaggle/input/mcq-validation/prompt_validation_data.csv\", index_col=None).iloc[:, 1:]\ntest_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T08:49:29.175162Z","iopub.execute_input":"2025-05-20T08:49:29.175473Z","iopub.status.idle":"2025-05-20T08:49:30.042179Z","shell.execute_reply.started":"2025-05-20T08:49:29.175443Z","shell.execute_reply":"2025-05-20T08:49:30.041272Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                   Term                                            Context  \\\n0           translation  Another common use of machine translation is t...   \n1    question answering  Let`s begin by introducing the bidirectional t...   \n2    coherence relation  Coherence relations arose from the independent...   \n3    question answering  Some of the question datasets described above ...   \n4       dialogue system  It is a common practice for dialogue systems t...   \n..                  ...                                                ...   \n95  relation extraction  The great advantage of unsupervised relation e...   \n96              lexicon  The Computational Grammar Coder (CGC) of Klein...   \n97             dialogue  The goal of text-to-speech (TTS) systems is to...   \n98  discourse coherence  Training models to predict longer contexts tha...   \n99              lexicon  In the next sections we introduce basic theori...   \n\n                                       Remember_0shot  \\\n0   {\\n    'Question': 'What does CAT stand for?',...   \n1   {\\n    'Question': 'In what way does the left-...   \n2   {\\n    'Question': 'In what way did Hobbs cont...   \n3   {\\n    'Question': 'In what type of QA tasks d...   \n4   {\\n    'Question': 'What is another function o...   \n..                                                ...   \n95  {\\n    'Question': 'What is one significant be...   \n96  {\\n    'Question': 'What did the CGC use for i...   \n97  {\\n    'Question': 'What are TTS systems prima...   \n98  {\\n    'Question': 'What does training models ...   \n99  {\\n    'Question': 'What is discussed in the l...   \n\n                                       Remember_1shot  \\\n0   {\\n    'Question': \"In what process do machine...   \n1   {\\n    'Question': \"In what type of task is it...   \n2   {\\n    'Question': \"Which scholar first introd...   \n3   {\\n    'Question': \"In what type of dataset do...   \n4   {\\n    'Question': \"What is another function o...   \n..                                                ...   \n95  {\\n    'Question': \"What is an example of a re...   \n96  Term: computational grammar coder.\\n\\nContext:...   \n97  {\\n    'Question': \"In what application are TT...   \n98  {\\n    'Question': \"What type of task does the...   \n99  {\\n    'Question': \"Which type of lexicon invo...   \n\n                                  Comprehension_0shot  \\\n0   {\\n    'Question': 'What does CAT stand for?',...   \n1   {\\n    'Question': 'What is an example of a ta...   \n2   {\\n    'Question': 'What did Asher and Lascari...   \n3   {\\n    'Question': 'What does the term \"open b...   \n4   {\\n    'Question': 'What two main functions do...   \n..                                                ...   \n95  {\\n    'Question': 'What is one of the main ad...   \n96  {\\n    'Question': 'What did Klein and Simmons...   \n97  {\\n    'Question': 'What are TTS systems prima...   \n98  {\\n    'Question': 'What does training models ...   \n99  {\\n    'Question': 'What is discussed in the l...   \n\n                                Comprehension_fewshot  \\\n0   {\\n    'Question': \"What does CAT stand for?\",...   \n1   {\\n    'Question': \"Explain why the left-to-ri...   \n2   {\\n    'Question': \"What are some methods othe...   \n3   {\\n    'Question': \"What distinguishes the ope...   \n4   {\\n    'Question': \"What two functions does th...   \n..                                                ...   \n95  {\\n    'Question': \"Explain how unsupervised r...   \n96  {\\n    'Question': \"What was the primary diffe...   \n97  {\\n    'Question': \"What are some potential ap...   \n98  {\\n    'Question': \"Explain how training model...   \n99  {\\n    'Question': \"What does the introduction...   \n\n                                    Application_0shot  \\\n0   {\\n    'Question': 'What does CAT stand for?',...   \n1   {\\n    'Question': 'What type of model does no...   \n2   {\\n    'Question': 'Which scholar first introd...   \n3   {\\n    'Question': 'What type of QA tasks invo...   \n4   {\\n    'Question': 'What is another function o...   \n..                                                ...   \n95  {\\n    'Question': 'Which statement best descr...   \n96  {\\n    'Question': 'What did the CGC use for i...   \n97  {\\n    'Question': 'What are TTS systems prima...   \n98  {\\n    'Question': 'How does training models t...   \n99  {\\n    'Question': 'What is the main focus of ...   \n\n                                    Application_1shot  \n0   {\\n    'Question': \"What does CAT stand for?\",...  \n1   {\\n    'Question': \"What is an example of a ta...  \n2   {\\n    'Question': \"Which approach does not di...  \n3   {\\n    'Question': \"What type of QA task does ...  \n4   {\\n    'Question': \"What is another function o...  \n..                                                ...  \n95  {\\n    'Question': \"What challenge does unsupe...  \n96  {\\n    'Question': \"What is the primary compon...  \n97  {\\n    'Question': \"What is the first step in ...  \n98  {\\n    'Question': \"What is an effective strat...  \n99  {\\n    'Question': \"What is the first step in ...  \n\n[100 rows x 8 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Term</th>\n      <th>Context</th>\n      <th>Remember_0shot</th>\n      <th>Remember_1shot</th>\n      <th>Comprehension_0shot</th>\n      <th>Comprehension_fewshot</th>\n      <th>Application_0shot</th>\n      <th>Application_1shot</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>translation</td>\n      <td>Another common use of machine translation is t...</td>\n      <td>{\\n    'Question': 'What does CAT stand for?',...</td>\n      <td>{\\n    'Question': \"In what process do machine...</td>\n      <td>{\\n    'Question': 'What does CAT stand for?',...</td>\n      <td>{\\n    'Question': \"What does CAT stand for?\",...</td>\n      <td>{\\n    'Question': 'What does CAT stand for?',...</td>\n      <td>{\\n    'Question': \"What does CAT stand for?\",...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>question answering</td>\n      <td>Let`s begin by introducing the bidirectional t...</td>\n      <td>{\\n    'Question': 'In what way does the left-...</td>\n      <td>{\\n    'Question': \"In what type of task is it...</td>\n      <td>{\\n    'Question': 'What is an example of a ta...</td>\n      <td>{\\n    'Question': \"Explain why the left-to-ri...</td>\n      <td>{\\n    'Question': 'What type of model does no...</td>\n      <td>{\\n    'Question': \"What is an example of a ta...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>coherence relation</td>\n      <td>Coherence relations arose from the independent...</td>\n      <td>{\\n    'Question': 'In what way did Hobbs cont...</td>\n      <td>{\\n    'Question': \"Which scholar first introd...</td>\n      <td>{\\n    'Question': 'What did Asher and Lascari...</td>\n      <td>{\\n    'Question': \"What are some methods othe...</td>\n      <td>{\\n    'Question': 'Which scholar first introd...</td>\n      <td>{\\n    'Question': \"Which approach does not di...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>question answering</td>\n      <td>Some of the question datasets described above ...</td>\n      <td>{\\n    'Question': 'In what type of QA tasks d...</td>\n      <td>{\\n    'Question': \"In what type of dataset do...</td>\n      <td>{\\n    'Question': 'What does the term \"open b...</td>\n      <td>{\\n    'Question': \"What distinguishes the ope...</td>\n      <td>{\\n    'Question': 'What type of QA tasks invo...</td>\n      <td>{\\n    'Question': \"What type of QA task does ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>dialogue system</td>\n      <td>It is a common practice for dialogue systems t...</td>\n      <td>{\\n    'Question': 'What is another function o...</td>\n      <td>{\\n    'Question': \"What is another function o...</td>\n      <td>{\\n    'Question': 'What two main functions do...</td>\n      <td>{\\n    'Question': \"What two functions does th...</td>\n      <td>{\\n    'Question': 'What is another function o...</td>\n      <td>{\\n    'Question': \"What is another function o...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>relation extraction</td>\n      <td>The great advantage of unsupervised relation e...</td>\n      <td>{\\n    'Question': 'What is one significant be...</td>\n      <td>{\\n    'Question': \"What is an example of a re...</td>\n      <td>{\\n    'Question': 'What is one of the main ad...</td>\n      <td>{\\n    'Question': \"Explain how unsupervised r...</td>\n      <td>{\\n    'Question': 'Which statement best descr...</td>\n      <td>{\\n    'Question': \"What challenge does unsupe...</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>lexicon</td>\n      <td>The Computational Grammar Coder (CGC) of Klein...</td>\n      <td>{\\n    'Question': 'What did the CGC use for i...</td>\n      <td>Term: computational grammar coder.\\n\\nContext:...</td>\n      <td>{\\n    'Question': 'What did Klein and Simmons...</td>\n      <td>{\\n    'Question': \"What was the primary diffe...</td>\n      <td>{\\n    'Question': 'What did the CGC use for i...</td>\n      <td>{\\n    'Question': \"What is the primary compon...</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>dialogue</td>\n      <td>The goal of text-to-speech (TTS) systems is to...</td>\n      <td>{\\n    'Question': 'What are TTS systems prima...</td>\n      <td>{\\n    'Question': \"In what application are TT...</td>\n      <td>{\\n    'Question': 'What are TTS systems prima...</td>\n      <td>{\\n    'Question': \"What are some potential ap...</td>\n      <td>{\\n    'Question': 'What are TTS systems prima...</td>\n      <td>{\\n    'Question': \"What is the first step in ...</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>discourse coherence</td>\n      <td>Training models to predict longer contexts tha...</td>\n      <td>{\\n    'Question': 'What does training models ...</td>\n      <td>{\\n    'Question': \"What type of task does the...</td>\n      <td>{\\n    'Question': 'What does training models ...</td>\n      <td>{\\n    'Question': \"Explain how training model...</td>\n      <td>{\\n    'Question': 'How does training models t...</td>\n      <td>{\\n    'Question': \"What is an effective strat...</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>lexicon</td>\n      <td>In the next sections we introduce basic theori...</td>\n      <td>{\\n    'Question': 'What is discussed in the l...</td>\n      <td>{\\n    'Question': \"Which type of lexicon invo...</td>\n      <td>{\\n    'Question': 'What is discussed in the l...</td>\n      <td>{\\n    'Question': \"What does the introduction...</td>\n      <td>{\\n    'Question': 'What is the main focus of ...</td>\n      <td>{\\n    'Question': \"What is the first step in ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 8 columns</p>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"batch_size = 5\npipe = pipeline(\n    \"text-generation\",\n  \n    model=model,\n    tokenizer=tokenizer,\n    batch_size=batch_size\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T08:52:13.624258Z","iopub.execute_input":"2025-05-20T08:52:13.624978Z","iopub.status.idle":"2025-05-20T08:52:13.629729Z","shell.execute_reply.started":"2025-05-20T08:52:13.624954Z","shell.execute_reply":"2025-05-20T08:52:13.629081Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## The judge prompt for verdicts","metadata":{}},{"cell_type":"code","source":"message_expl = lambda q: [\n    {\"role\": \"system\", \"content\": \"\"\"\n    For the provided list of prompt instructions, determine whether each instruction has been followed in the LLM actual output.\n    Please generate a list of JSON with two keys: `verdict` and `reason`.\n    The 'verdict' key should STRICTLY be either a 'yes' or 'no'. Only answer 'yes' if the instruction COMPLETELY follows the instruction, and 'no' otherwise.\n    You should be EXTRA STRICT AND CAREFUL when giving a 'yes'.\n    The 'reason' is the reason for the verdict.\n    Provide a 'reason' ONLY if the answer is 'no'.\n    The provided prompt instructions are the instructions to be followed in the prompt, which you have no access to.\n    IMPORTANT: Please make sure to only return in JSON format, with the 'verdicts' key mapping to a list of JSON objects.\n    \"\"\"\n     },\n    {\"role\": \"user\", \"content\": \"\"\"\n    Example input: For MT beam widths k give us k hypotheses at the end. We can pass all k to the downstream application with their respective scores, or if we just need a single translation we can pass the most probable hypothesis.\nExample term: 'Hydrology'.\nExample answer:  Beam width k determines the number of hypotheses generated by the model, which affects the diversity of the output.\nExample actual output: In machine translation (MT), what is typically used for beam width k when generating translations?\nExample prompt instructions: [\"Question in LLM Actual Output is based on the information available only from Input.\", \"Answer is completely wrong to the question in actual output accoring to Input\", \"LLM Actual Output does not completely contain, spoil or provide Answer explicitly.\", \"Question in actual output is related to the term.\"]\nExample JSON:\n{\n    \"verdicts\": [\n        {\n            \"verdict\": \"yes\"\n        },\n        {\n            \"verdict\": \"no\",\n            \"reason\": \"The answer corresponds to Input and answers the question in the LLM output overall.\"\n        },\n        {\n            \"verdict\": \"yes\",\n        },\n        {\n            \"verdict\": \"no\",\n            \"reason\": \"The LLM output is not related to the term 'Hydrology' and is related to another topic.\"\n        }\n    ]  \n}\nThe number of 'verdicts' MUST BE STRICTLY EQUAL to the number of prompt instructions.\n\n    \"\"\"+q}\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T08:50:46.746141Z","iopub.execute_input":"2025-05-20T08:50:46.746424Z","iopub.status.idle":"2025-05-20T08:50:46.751461Z","shell.execute_reply.started":"2025-05-20T08:50:46.746404Z","shell.execute_reply":"2025-05-20T08:50:46.750698Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Criteria for assessment\n\nBelow the criteria that judge will make its verdicts based on are:\n- Context correspondence\n- Term correspondence\n- Answer correctness\n- Bloom correspondence (assessed separately a bit further)\n\nThe criteria are based on what's passed in MCQ generation prompt to agent so it's expected that generated questions follow the required prompt instructions","metadata":{}},{"cell_type":"code","source":"response_cols = ['Remember_0shot', 'Remember_1shot',\n       'Comprehension_0shot', 'Comprehension_fewshot', 'Application_0shot',\n       'Application_1shot']\n\nmetrics = ['Context correspondence', 'Term correspondence', 'Answer correctness']\npositive_response=[\"yes\", \"yes\", \"no\"]\n\n\ninstructions=['Question in LLM Actual Output is based on the information available only from Input.', \n              \"Question in LLM Actual Output is related to the term according to Input.\", \n            'LLM Actual Output does not completely contain, spoil or provide content of Answer explicitly.']\n\nmetrics_by_prompt = {}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T08:52:17.192516Z","iopub.execute_input":"2025-05-20T08:52:17.192757Z","iopub.status.idle":"2025-05-20T08:52:17.196875Z","shell.execute_reply.started":"2025-05-20T08:52:17.192741Z","shell.execute_reply":"2025-05-20T08:52:17.196274Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"for col in response_cols:\n    \n    cnt_not_dict = test_data[~test_data[col].str.contains('{\\n')].shape[0]\n    print(f'{col}: {cnt_not_dict}') # only one time LLM failed to generate dictionary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T08:52:18.595837Z","iopub.execute_input":"2025-05-20T08:52:18.596574Z","iopub.status.idle":"2025-05-20T08:52:18.603978Z","shell.execute_reply.started":"2025-05-20T08:52:18.596548Z","shell.execute_reply":"2025-05-20T08:52:18.603156Z"}},"outputs":[{"name":"stdout","text":"Remember_0shot: 0\nRemember_1shot: 1\nComprehension_0shot: 0\nComprehension_fewshot: 0\nApplication_0shot: 0\nApplication_1shot: 0\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"test_data\n\nfor col in [response_cols[5]]:\n    metrics_res = {m: 0 for m in metrics}\n\n    subdataset = test_data[test_data[col].str.contains('{\\n')]\n    \n    def gen():\n        for i, row in subdataset.iterrows():\n            \n            \n            if '{\\n' not in row[col]:\n                continue\n            if row[col].count('{') != row[col].count('}'):\n                continue\n            start_i = row[col].find('{')\n            end_i = row[col].rfind('}')\n      \n            llm_dict = row[col][start_i:end_i+1].replace(\"'User's dialogue acts'\", \"'Users dialogue acts'\")\n            llm_answer = ast.literal_eval(llm_dict)\n            question = llm_answer['Question']\n            answer = llm_answer['Options'][llm_answer['Correct answer'][:2]].replace(\"User's dialogue acts\", \"User\\'s dialogue acts\")\n            \n            yield {\"Message\": text(message_expl(\n                        f\"\"\"Prompt Instructions:\n                    [{', '.join(f'\"{instr}\"' for instr in instructions)}]\n                    \n                    Input:\n                    {row['Context']}\n                    \n                    Term:\n                    '{row['Term']}'\n                    \n                    Answer: \n                    {answer}\n                    \n                    LLM Actual Output:\n                    {question}\n                    \n                    JSON output:\n                    \"\"\"))}\n\n    \n    dataset = Dataset.from_generator(gen)\n    #j=0\n    corr_cnt = 0\n    \n    for j, out in tqdm(enumerate(pipe(KeyDataset(dataset, 'Message'), **generation_args)),\n        total=test_data.shape[0]):\n        response = out[0]['generated_text']\n        response_dict = ast.literal_eval(response[8:-3])['verdicts']\n\n        for verd, metric, pos_resp in zip(response_dict, metrics, positive_response):\n            if verd['verdict']==pos_resp:\n                metrics_res[metric]+=1\n        corr_cnt+=1\n    metrics_res['total'] = corr_cnt\n    metrics_by_prompt[col] = metrics_res","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T10:58:52.737570Z","iopub.execute_input":"2025-05-20T10:58:52.737823Z","iopub.status.idle":"2025-05-20T11:18:12.712252Z","shell.execute_reply.started":"2025-05-20T10:58:52.737804Z","shell.execute_reply":"2025-05-20T11:18:12.711688Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4142f790fa44ebdbee16ed10f5afc2d"}},"metadata":{}},{"name":"stderr","text":"100%|██████████| 100/100 [19:19<00:00, 11.60s/it]\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"metrics_by_prompt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T11:18:17.397430Z","iopub.execute_input":"2025-05-20T11:18:17.397966Z","iopub.status.idle":"2025-05-20T11:18:17.402583Z","shell.execute_reply.started":"2025-05-20T11:18:17.397946Z","shell.execute_reply":"2025-05-20T11:18:17.402043Z"}},"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"{'Remember_0shot': {'Context correspondence': 95,\n  'Term correspondence': 97,\n  'Answer correctness': 95,\n  'total': 100},\n 'Remember_1shot': {'Context correspondence': 93,\n  'Term correspondence': 95,\n  'Answer correctness': 98,\n  'total': 98},\n 'Comprehension_0shot': {'Context correspondence': 98,\n  'Term correspondence': 99,\n  'Answer correctness': 96,\n  'total': 100},\n 'Comprehension_fewshot': {'Context correspondence': 98,\n  'Term correspondence': 100,\n  'Answer correctness': 97,\n  'total': 100},\n 'Application_0shot': {'Context correspondence': 95,\n  'Term correspondence': 96,\n  'Answer correctness': 96,\n  'total': 100},\n 'Application_1shot': {'Context correspondence': 96,\n  'Term correspondence': 97,\n  'Answer correctness': 97,\n  'total': 100}}"},"metadata":{}}],"execution_count":54},{"cell_type":"code","source":"df_metrics = pd.DataFrame(columns=['Prompt']+metrics)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T09:12:33.167458Z","iopub.execute_input":"2025-05-20T09:12:33.167908Z","iopub.status.idle":"2025-05-20T09:12:33.172338Z","shell.execute_reply.started":"2025-05-20T09:12:33.167886Z","shell.execute_reply":"2025-05-20T09:12:33.171582Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"key = 'Application_1shot'\ni=5\ndf_metrics.loc[i, :] = [key]+[round(metrics_by_prompt[key][m]/metrics_by_prompt[key]['total']*100, 2) for m in metrics]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T11:18:30.587558Z","iopub.execute_input":"2025-05-20T11:18:30.588130Z","iopub.status.idle":"2025-05-20T11:18:30.592774Z","shell.execute_reply.started":"2025-05-20T11:18:30.588109Z","shell.execute_reply":"2025-05-20T11:18:30.592054Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"df_metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T11:18:33.666293Z","iopub.execute_input":"2025-05-20T11:18:33.666790Z","iopub.status.idle":"2025-05-20T11:18:33.674354Z","shell.execute_reply.started":"2025-05-20T11:18:33.666766Z","shell.execute_reply":"2025-05-20T11:18:33.673776Z"}},"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"                  Prompt Context correspondence Term correspondence  \\\n0         Remember_0shot                   95.0                97.0   \n1         Remember_1shot                   94.9               96.94   \n2    Comprehension_0shot                   98.0                99.0   \n3  Comprehension_fewshot                   98.0               100.0   \n4      Application_0shot                   95.0                96.0   \n5      Application_1shot                   96.0                97.0   \n\n  Answer correctness  \n0               95.0  \n1              100.0  \n2               96.0  \n3               97.0  \n4               96.0  \n5               97.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Prompt</th>\n      <th>Context correspondence</th>\n      <th>Term correspondence</th>\n      <th>Answer correctness</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Remember_0shot</td>\n      <td>95.0</td>\n      <td>97.0</td>\n      <td>95.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Remember_1shot</td>\n      <td>94.9</td>\n      <td>96.94</td>\n      <td>100.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Comprehension_0shot</td>\n      <td>98.0</td>\n      <td>99.0</td>\n      <td>96.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Comprehension_fewshot</td>\n      <td>98.0</td>\n      <td>100.0</td>\n      <td>97.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Application_0shot</td>\n      <td>95.0</td>\n      <td>96.0</td>\n      <td>96.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Application_1shot</td>\n      <td>96.0</td>\n      <td>97.0</td>\n      <td>97.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":56},{"cell_type":"code","source":"df_metrics.to_csv('prompt_judge_res.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T11:18:54.436308Z","iopub.execute_input":"2025-05-20T11:18:54.436553Z","iopub.status.idle":"2025-05-20T11:18:54.447300Z","shell.execute_reply.started":"2025-05-20T11:18:54.436537Z","shell.execute_reply":"2025-05-20T11:18:54.446663Z"}},"outputs":[],"execution_count":57},{"cell_type":"markdown","source":"Bloom correspondence is measured based on the indicator-words (specific words that are related to each type of questions in the taxonomy) that were used in the generation prompt.","metadata":{}},{"cell_type":"code","source":"bloom_instr = {'Remember': 'Question in LLM Actual Output includes question words like \"What\",\"Why\" or \"How\".',\n                'Comprehension': 'Question in LLM Actual Output includes a word like \"explain\", \"summarize\", \"rephrase\", \"compare\" or their synonyms',\n               'Application': 'Question in LLM Actual Output includes a verb like \"develop\", \"organize\", \"build\", \"plan\" or their synonyms'\n                ''}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T11:25:16.372650Z","iopub.execute_input":"2025-05-20T11:25:16.373141Z","iopub.status.idle":"2025-05-20T11:25:16.376815Z","shell.execute_reply.started":"2025-05-20T11:25:16.373119Z","shell.execute_reply":"2025-05-20T11:25:16.376243Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"#bloom_taxonomy_correspondence\nbloom_metric='Bloom correspondence'\n\nfor col in [response_cols[5]]:\n    metrics_res = {m: 0 for m in [bloom_metric]}\n    bloom_instr_str = [bloom_instr[col.split('_')[0]]]\n\n    subdataset = test_data[test_data[col].str.contains('{\\n')]\n    \n    def gen():\n        for i, row in subdataset.iterrows():\n            \n            if '{\\n' not in row[col]:\n                continue\n            if row[col].count('{') != row[col].count('}'):\n                continue\n            start_i = row[col].find('{')\n            end_i = row[col].rfind('}')\n\n            llm_dict = row[col][start_i:end_i+1].replace(\"'User's dialogue acts'\", \"'Users dialogue acts'\")\n            llm_answer = ast.literal_eval(llm_dict)\n            question = llm_answer['Question']\n            answer = llm_answer['Options'][llm_answer['Correct answer'][:2]].replace(\"User's dialogue acts\", \"User\\'s dialogue acts\")\n           \n            yield {\"Message\": text(message_expl(\n                        f\"\"\"Prompt Instructions:\n                    [{', '.join(f'\"{instr}\"' for instr in instructions)}]\n                    \n                    Input:\n                    {row['Context']}\n                    \n                    Term:\n                    '{row['Term']}'\n                    \n                    Answer: \n                    {answer}\n                    \n                    LLM Actual Output:\n                    {question}\n                    \n                    JSON output:\n                    \"\"\"))}\n\n    \n    dataset = Dataset.from_generator(gen)\n \n    corr_cnt = 0\n    \n    for j, out in tqdm(enumerate(pipe(KeyDataset(dataset, 'Message'), **generation_args)),\n        total=test_data.shape[0]):\n        response = out[0]['generated_text']\n        response_dict = ast.literal_eval(response[8:-3])['verdicts']\n\n        for verd, metric, pos_resp in zip(response_dict, [bloom_metric], ['yes']):\n            if verd['verdict']==pos_resp:\n                metrics_res[metric]+=1\n        corr_cnt+=1\n    metrics_res['total'] = corr_cnt\n    metrics_by_prompt[col][bloom_metric] = metrics_res[bloom_metric]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T13:39:29.280578Z","iopub.execute_input":"2025-05-20T13:39:29.280899Z","iopub.status.idle":"2025-05-20T13:58:57.530850Z","shell.execute_reply.started":"2025-05-20T13:39:29.280881Z","shell.execute_reply":"2025-05-20T13:58:57.530140Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 100/100 [19:27<00:00, 11.68s/it]\n","output_type":"stream"}],"execution_count":76},{"cell_type":"code","source":"metrics_by_prompt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T13:59:41.154458Z","iopub.execute_input":"2025-05-20T13:59:41.154724Z","iopub.status.idle":"2025-05-20T13:59:41.160984Z","shell.execute_reply.started":"2025-05-20T13:59:41.154706Z","shell.execute_reply":"2025-05-20T13:59:41.159864Z"}},"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"{'Remember_0shot': {'Context correspondence': 95,\n  'Term correspondence': 97,\n  'Answer correctness': 95,\n  'total': 100,\n  'Bloom correspondence': 95},\n 'Remember_1shot': {'Context correspondence': 93,\n  'Term correspondence': 95,\n  'Answer correctness': 98,\n  'total': 98,\n  'Bloom correspondence': 93},\n 'Comprehension_0shot': {'Context correspondence': 98,\n  'Term correspondence': 99,\n  'Answer correctness': 96,\n  'total': 100,\n  'Bloom correspondence': 98},\n 'Comprehension_fewshot': {'Context correspondence': 98,\n  'Term correspondence': 100,\n  'Answer correctness': 97,\n  'total': 100,\n  'Bloom correspondence': 98},\n 'Application_0shot': {'Context correspondence': 95,\n  'Term correspondence': 96,\n  'Answer correctness': 96,\n  'total': 100,\n  'Bloom correspondence': 95},\n 'Application_1shot': {'Context correspondence': 96,\n  'Term correspondence': 97,\n  'Answer correctness': 97,\n  'total': 100,\n  'Bloom correspondence': 96}}"},"metadata":{}}],"execution_count":77},{"cell_type":"code","source":"df_metrics[bloom_metric] = [round(metrics_by_prompt[col][bloom_metric]/metrics_by_prompt[col]['total']*100, 2) for col in response_cols]\ndf_metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T14:01:51.106799Z","iopub.execute_input":"2025-05-20T14:01:51.107308Z","iopub.status.idle":"2025-05-20T14:01:51.116784Z","shell.execute_reply.started":"2025-05-20T14:01:51.107288Z","shell.execute_reply":"2025-05-20T14:01:51.116228Z"}},"outputs":[{"execution_count":78,"output_type":"execute_result","data":{"text/plain":"                  Prompt Context correspondence Term correspondence  \\\n0         Remember_0shot                   95.0                97.0   \n1         Remember_1shot                   94.9               96.94   \n2    Comprehension_0shot                   98.0                99.0   \n3  Comprehension_fewshot                   98.0               100.0   \n4      Application_0shot                   95.0                96.0   \n5      Application_1shot                   96.0                97.0   \n\n  Answer correctness  Bloom correspondence  \n0               95.0                  95.0  \n1              100.0                  94.9  \n2               96.0                  98.0  \n3               97.0                  98.0  \n4               96.0                  95.0  \n5               97.0                  96.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Prompt</th>\n      <th>Context correspondence</th>\n      <th>Term correspondence</th>\n      <th>Answer correctness</th>\n      <th>Bloom correspondence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Remember_0shot</td>\n      <td>95.0</td>\n      <td>97.0</td>\n      <td>95.0</td>\n      <td>95.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Remember_1shot</td>\n      <td>94.9</td>\n      <td>96.94</td>\n      <td>100.0</td>\n      <td>94.9</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Comprehension_0shot</td>\n      <td>98.0</td>\n      <td>99.0</td>\n      <td>96.0</td>\n      <td>98.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Comprehension_fewshot</td>\n      <td>98.0</td>\n      <td>100.0</td>\n      <td>97.0</td>\n      <td>98.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Application_0shot</td>\n      <td>95.0</td>\n      <td>96.0</td>\n      <td>96.0</td>\n      <td>95.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Application_1shot</td>\n      <td>96.0</td>\n      <td>97.0</td>\n      <td>97.0</td>\n      <td>96.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":78},{"cell_type":"code","source":"df_metrics.to_csv('prompt_judge_res.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T14:02:23.863383Z","iopub.execute_input":"2025-05-20T14:02:23.863903Z","iopub.status.idle":"2025-05-20T14:02:23.868440Z","shell.execute_reply.started":"2025-05-20T14:02:23.863882Z","shell.execute_reply":"2025-05-20T14:02:23.867791Z"}},"outputs":[],"execution_count":79},{"cell_type":"markdown","source":"# Evaluation prompt reference:\nAdjusted prompt of **deepeval** framework for prompt alignment assessment","metadata":{}},{"cell_type":"code","source":"!pip install deepeval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T10:01:20.346047Z","iopub.execute_input":"2025-05-10T10:01:20.346333Z","iopub.status.idle":"2025-05-10T10:01:44.037979Z","shell.execute_reply.started":"2025-05-10T10:01:20.346311Z","shell.execute_reply":"2025-05-10T10:01:44.037171Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting deepeval\n  Downloading deepeval-2.8.8-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from deepeval) (3.11.16)\nCollecting anthropic<0.50.0,>=0.49.0 (from deepeval)\n  Downloading anthropic-0.49.0-py3-none-any.whl.metadata (24 kB)\nCollecting google-genai<2.0.0,>=1.9.0 (from deepeval)\n  Downloading google_genai-1.14.0-py3-none-any.whl.metadata (33 kB)\nRequirement already satisfied: grpcio<2.0.0,>=1.67.1 in /usr/local/lib/python3.11/dist-packages (from deepeval) (1.70.0)\nRequirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from deepeval) (1.6.0)\nCollecting ollama (from deepeval)\n  Downloading ollama-0.4.8-py3-none-any.whl.metadata (4.7 kB)\nRequirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (from deepeval) (1.61.1)\nCollecting opentelemetry-api<2.0.0,>=1.24.0 (from deepeval)\n  Downloading opentelemetry_api-1.33.0-py3-none-any.whl.metadata (1.6 kB)\nCollecting opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0 (from deepeval)\n  Downloading opentelemetry_exporter_otlp_proto_grpc-1.33.0-py3-none-any.whl.metadata (2.5 kB)\nCollecting opentelemetry-sdk<2.0.0,>=1.24.0 (from deepeval)\n  Downloading opentelemetry_sdk-1.33.0-py3-none-any.whl.metadata (1.6 kB)\nCollecting portalocker (from deepeval)\n  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\nCollecting posthog<4.0.0,>=3.23.0 (from deepeval)\n  Downloading posthog-3.25.0-py2.py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (from deepeval) (8.3.4)\nCollecting pytest-asyncio (from deepeval)\n  Downloading pytest_asyncio-0.26.0-py3-none-any.whl.metadata (4.0 kB)\nCollecting pytest-repeat (from deepeval)\n  Downloading pytest_repeat-0.9.4-py3-none-any.whl.metadata (4.9 kB)\nCollecting pytest-rerunfailures<13.0,>=12.0 (from deepeval)\n  Downloading pytest_rerunfailures-12.0-py3-none-any.whl.metadata (18 kB)\nCollecting pytest-xdist (from deepeval)\n  Downloading pytest_xdist-3.6.1-py3-none-any.whl.metadata (4.3 kB)\nRequirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from deepeval) (2.32.3)\nCollecting rich<14.0.0,>=13.6.0 (from deepeval)\n  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: sentry-sdk in /usr/local/lib/python3.11/dist-packages (from deepeval) (2.21.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from deepeval) (75.1.0)\nRequirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from deepeval) (0.9.0)\nRequirement already satisfied: tenacity<=9.0.0 in /usr/local/lib/python3.11/dist-packages (from deepeval) (9.0.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from deepeval) (4.67.1)\nRequirement already satisfied: typer in /usr/local/lib/python3.11/dist-packages (from deepeval) (0.15.1)\nRequirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from deepeval) (0.45.1)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.50.0,>=0.49.0->deepeval) (3.7.1)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.50.0,>=0.49.0->deepeval) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.50.0,>=0.49.0->deepeval) (0.28.1)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.50.0,>=0.49.0->deepeval) (0.8.2)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.50.0,>=0.49.0->deepeval) (2.11.3)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic<0.50.0,>=0.49.0->deepeval) (1.3.1)\nRequirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.50.0,>=0.49.0->deepeval) (4.13.1)\nCollecting anyio<5,>=3.5.0 (from anthropic<0.50.0,>=0.49.0->deepeval)\n  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\nRequirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (2.27.0)\nRequirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (14.2)\nRequirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<2.0.0,>=1.24.0->deepeval) (1.2.18)\nRequirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<2.0.0,>=1.24.0->deepeval) (8.6.1)\nRequirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (1.67.0)\nCollecting opentelemetry-exporter-otlp-proto-common==1.33.0 (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval)\n  Downloading opentelemetry_exporter_otlp_proto_common-1.33.0-py3-none-any.whl.metadata (1.9 kB)\nCollecting opentelemetry-proto==1.33.0 (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval)\n  Downloading opentelemetry_proto-1.33.0-py3-none-any.whl.metadata (2.4 kB)\nCollecting protobuf<6.0,>=5.0 (from opentelemetry-proto==1.33.0->opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval)\n  Downloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\nCollecting opentelemetry-semantic-conventions==0.54b0 (from opentelemetry-sdk<2.0.0,>=1.24.0->deepeval)\n  Downloading opentelemetry_semantic_conventions-0.54b0-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from posthog<4.0.0,>=3.23.0->deepeval) (1.17.0)\nCollecting monotonic>=1.5 (from posthog<4.0.0,>=3.23.0->deepeval)\n  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\nCollecting backoff>=1.10.0 (from posthog<4.0.0,>=3.23.0->deepeval)\n  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: python-dateutil>2.1 in /usr/local/lib/python3.11/dist-packages (from posthog<4.0.0,>=3.23.0->deepeval) (2.9.0.post0)\nRequirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.11/dist-packages (from pytest-rerunfailures<13.0,>=12.0->deepeval) (24.2)\nRequirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest->deepeval) (2.0.0)\nRequirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest->deepeval) (1.5.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->deepeval) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->deepeval) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->deepeval) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->deepeval) (2025.1.31)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.6.0->deepeval) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.6.0->deepeval) (2.19.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->deepeval) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->deepeval) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->deepeval) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->deepeval) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->deepeval) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->deepeval) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->deepeval) (1.19.0)\nCollecting execnet>=2.1 (from pytest-xdist->deepeval)\n  Downloading execnet-2.1.1-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer->deepeval) (8.1.8)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer->deepeval) (1.5.4)\nRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api<2.0.0,>=1.24.0->deepeval) (1.17.2)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval) (5.5.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval) (0.4.1)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval) (4.9)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->anthropic<0.50.0,>=0.49.0->deepeval) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic<0.50.0,>=0.49.0->deepeval) (0.14.0)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api<2.0.0,>=1.24.0->deepeval) (3.21.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.6.0->deepeval) (0.1.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic<0.50.0,>=0.49.0->deepeval) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic<0.50.0,>=0.49.0->deepeval) (2.33.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic<0.50.0,>=0.49.0->deepeval) (0.4.0)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval) (0.6.1)\nDownloading deepeval-2.8.8-py3-none-any.whl (526 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.7/526.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading anthropic-0.49.0-py3-none-any.whl (243 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading google_genai-1.14.0-py3-none-any.whl (168 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.9/168.9 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_api-1.33.0-py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.33.0-py3-none-any.whl (18 kB)\nDownloading opentelemetry_exporter_otlp_proto_common-1.33.0-py3-none-any.whl (18 kB)\nDownloading opentelemetry_proto-1.33.0-py3-none-any.whl (55 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_sdk-1.33.0-py3-none-any.whl (118 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_semantic_conventions-0.54b0-py3-none-any.whl (194 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.9/194.9 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading posthog-3.25.0-py2.py3-none-any.whl (89 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pytest_rerunfailures-12.0-py3-none-any.whl (12 kB)\nDownloading rich-13.9.4-py3-none-any.whl (242 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ollama-0.4.8-py3-none-any.whl (13 kB)\nDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\nDownloading pytest_asyncio-0.26.0-py3-none-any.whl (19 kB)\nDownloading pytest_repeat-0.9.4-py3-none-any.whl (4.2 kB)\nDownloading pytest_xdist-3.6.1-py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading anyio-4.9.0-py3-none-any.whl (100 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\nDownloading execnet-2.1.1-py3-none-any.whl (40 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nDownloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: monotonic, protobuf, portalocker, execnet, backoff, anyio, rich, pytest-xdist, pytest-rerunfailures, pytest-repeat, pytest-asyncio, posthog, opentelemetry-proto, opentelemetry-api, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, ollama, google-genai, anthropic, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, deepeval\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n  Attempting uninstall: anyio\n    Found existing installation: anyio 3.7.1\n    Uninstalling anyio-3.7.1:\n      Successfully uninstalled anyio-3.7.1\n  Attempting uninstall: rich\n    Found existing installation: rich 14.0.0\n    Uninstalling rich-14.0.0:\n      Successfully uninstalled rich-14.0.0\n  Attempting uninstall: opentelemetry-api\n    Found existing installation: opentelemetry-api 1.16.0\n    Uninstalling opentelemetry-api-1.16.0:\n      Successfully uninstalled opentelemetry-api-1.16.0\n  Attempting uninstall: opentelemetry-semantic-conventions\n    Found existing installation: opentelemetry-semantic-conventions 0.37b0\n    Uninstalling opentelemetry-semantic-conventions-0.37b0:\n      Successfully uninstalled opentelemetry-semantic-conventions-0.37b0\n  Attempting uninstall: google-genai\n    Found existing installation: google-genai 0.8.0\n    Uninstalling google-genai-0.8.0:\n      Successfully uninstalled google-genai-0.8.0\n  Attempting uninstall: opentelemetry-sdk\n    Found existing installation: opentelemetry-sdk 1.16.0\n    Uninstalling opentelemetry-sdk-1.16.0:\n      Successfully uninstalled opentelemetry-sdk-1.16.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\ngoogle-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\npandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nibis-framework 9.2.0 requires toolz<1,>=0.11, but you have toolz 1.0.0 which is incompatible.\ngoogle-cloud-bigtable 2.28.1 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed anthropic-0.49.0 anyio-4.9.0 backoff-2.2.1 deepeval-2.8.8 execnet-2.1.1 google-genai-1.14.0 monotonic-1.6 ollama-0.4.8 opentelemetry-api-1.33.0 opentelemetry-exporter-otlp-proto-common-1.33.0 opentelemetry-exporter-otlp-proto-grpc-1.33.0 opentelemetry-proto-1.33.0 opentelemetry-sdk-1.33.0 opentelemetry-semantic-conventions-0.54b0 portalocker-3.1.1 posthog-3.25.0 protobuf-5.29.4 pytest-asyncio-0.26.0 pytest-repeat-0.9.4 pytest-rerunfailures-12.0 pytest-xdist-3.6.1 rich-13.9.4\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from deepeval.models.base_model import DeepEvalBaseLLM\n\nclass JudgeLLM(DeepEvalBaseLLM):\n    def __init__(\n        self,\n        model\n    ):\n        self.model = model\n        self.gen_pars = {\n            \"max_new_tokens\": 250,\n            \"return_full_text\": False,\n            \"temperature\": 0.01,\n            \"do_sample\": True,\n        }\n\n    def load_model(self):\n        return self.model\n\n    def generate(self, prompt: str) -> str:\n        chat_model = self.load_model()\n        model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n        generated_ids = model.generate(**model_inputs, max_new_tokens=100, temperature=0.01)\n        tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n        gen_text = tokenizer.batch_decode(generated_ids[:, model_inputs['input_ids'][0].shape[0]:])[0]\n        print(prompt, gen_text)\n        return gen_text\n\n    async def a_generate(self, prompt: str) -> str:\n        chat_model = self.load_model()\n        model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n        generated_ids = model.generate(**model_inputs, max_new_tokens=100, temperature=0.01)\n        tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n        gen_text = tokenizer.batch_decode(generated_ids[:, model_inputs['input_ids'][0].shape[0]:])[0]\n        print(prompt, gen_text)\n        return gen_text\n\n    def get_model_name(self):\n        return \"Qwen2.5\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:02:20.367912Z","iopub.execute_input":"2025-05-10T11:02:20.368600Z","iopub.status.idle":"2025-05-10T11:02:20.375212Z","shell.execute_reply.started":"2025-05-10T11:02:20.368577Z","shell.execute_reply":"2025-05-10T11:02:20.374556Z"}},"outputs":[],"execution_count":109},{"cell_type":"code","source":"judge_model = JudgeLLM(model=model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from deepeval import evaluate\nfrom deepeval.metrics import HallucinationMetric\nfrom deepeval.test_case import LLMTestCase\n\n\ncontext=[\"Alignment Knowing the minimum edit distance is useful for algorithms like finding potential spelling error corrections. But the edit distance algorithm is important in another way; with a small change, it can also provide the minimum cost alignment between two strings. Aligning two strings is useful throughout speech and language processing. In speech recognition, minimum edit distance alignment is used to compute the word error rate (Chapter 16). Alignment plays a role in machine translation, in which sentences in a parallel corpus (a corpus with a text in two languages) need to be matched to each other. \"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T10:54:35.014820Z","iopub.execute_input":"2025-05-10T10:54:35.015108Z","iopub.status.idle":"2025-05-10T10:54:35.019421Z","shell.execute_reply.started":"2025-05-10T10:54:35.015088Z","shell.execute_reply":"2025-05-10T10:54:35.018840Z"}},"outputs":[],"execution_count":99},{"cell_type":"code","source":"from deepeval import evaluate\nfrom deepeval.metrics import PromptAlignmentMetric\nfrom deepeval.test_case import LLMTestCase\n\nmetric = PromptAlignmentMetric(\n    async_mode=False,\n    prompt_instructions=[\"\"\"Using the information in the context and given term generate question\n    related to the term according to context. \n    in question's text and starts with \"What\",\"Why\" or \"How\"\"\"],\n    model=judge_model,\n    include_reason=True,\n)\ntest_case = LLMTestCase(\n    input=\"Alignment Knowing the minimum edit distance is useful for algorithms like finding potential spelling error corrections. But the edit distance algorithm is important in another way; with a small change, it can also provide the minimum cost alignment between two strings. Aligning two strings is useful throughout speech and language processing. In speech recognition, minimum edit distance alignment is used to compute the word error rate (Chapter 16). Alignment plays a role in machine translation, in which sentences in a parallel corpus (a corpus with a text in two languages) need to be matched to each other. \",\n\n    actual_output=\"In speech recognition, what is the purpose of using minimum edit distance alignment?\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T10:54:35.284736Z","iopub.execute_input":"2025-05-10T10:54:35.285026Z","iopub.status.idle":"2025-05-10T10:54:35.289600Z","shell.execute_reply.started":"2025-05-10T10:54:35.285008Z","shell.execute_reply":"2025-05-10T10:54:35.288954Z"}},"outputs":[],"execution_count":100},{"cell_type":"code","source":"evaluate(test_cases=[test_case], metrics=[metric])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T11:03:06.735359Z","iopub.execute_input":"2025-05-10T11:03:06.736048Z","iopub.status.idle":"2025-05-10T11:03:09.435267Z","shell.execute_reply.started":"2025-05-10T11:03:06.736021Z","shell.execute_reply":"2025-05-10T11:03:09.434374Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"✨ You're running DeepEval's latest \u001b[38;2;106;0;255mPrompt Alignment Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing Qwen2.\u001b[0m\u001b[1;38;2;55;65;81m5\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Prompt Alignment Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using Qwen2.</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">5</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n</pre>\n"},"metadata":{}},{"name":"stderr","text":"Evaluating 1 test case(s) in parallel: |          |  0% (0/1) [Time Taken: 00:02, ?test case/s]","output_type":"stream"},{"name":"stdout","text":"For the provided list of prompt instructions, determine whether each instruction has been followed in the LLM actual output.\nPlease generate a list of JSON with two keys: `verdict` and `reason`.\nThe 'verdict' key should STRICTLY be either a 'yes' or 'no'. Only answer 'yes' if the instruction COMPLETELY follows the instruction, and 'no' otherwise.\nYou should be EXTRA STRICT AND CAREFUL when giving a 'yes'.\nThe 'reason' is the reason for the verdict.\nProvide a 'reason' ONLY if the answer is 'no'. \nThe provided prompt instructions are the instructions to be followed in the prompt, which you have no access to.\n\n**\nIMPORTANT: Please make sure to only return in JSON format, with the 'verdicts' key mapping to a list of JSON objects.\nExample input: What number is the stars of the sky?\nExample actual output: HEY THERE! I think what you meant is \"What is the number of stars in the sky\", but unfortunately I don't know the answer to it.\nExample prompt instructions: [\"Answer the input in a well-mannered fashion.\", \"Do not correct user of any grammatical errors.\", \"Respond in all upper case\"]\nExample JSON:\n{\n    \"verdicts\": [\n        {\n            \"verdict\": \"yes\"\n        },\n        {\n            \"verdict\": \"no\",\n            \"reason\": \"The LLM corrected the user when the user used the wrong grammar in asking about the number of stars in the sky.\"\n        },\n        {\n            \"verdict\": \"no\",\n            \"reason\": \"The LLM only made 'HEY THERE' uppercase, which does not follow the instruction of making everything uppercase completely.\"\n        }\n    ]  \n}\n\nSince you are going to generate a verdict for each instruction, the number of 'verdicts' SHOULD BE STRICTLY EQUAL to the number of prompt instructions.\n**          \n\nPrompt Instructions:\n['Using the information in the context and given term generate question\\n    related to the term according to context. \\n    in question\\'s text and starts with \"What\",\"Why\" or \"How']\n\nInput:\nAlignment Knowing the minimum edit distance is useful for algorithms like finding potential spelling error corrections. But the edit distance algorithm is important in another way; with a small change, it can also provide the minimum cost alignment between two strings. Aligning two strings is useful throughout speech and language processing. In speech recognition, minimum edit distance alignment is used to compute the word error rate (Chapter 16). Alignment plays a role in machine translation, in which sentences in a parallel corpus (a corpus with a text in two languages) need to be matched to each other. \n\nLLM Actual Output:\nIn speech recognition, what is the purpose of using minimum edit distance alignment?\n\nJSON:\n ```json\n[\n    {\n        \"verdict\": \"no\",\n        \"reason\": \"The LLM's generated question does not start with 'What', 'Why', or 'How' as specified in the prompt instructions.\"\n    }\n]\n``\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/deepeval/metrics/prompt_alignment/prompt_alignment.py\u001b[0m in \u001b[0;36m_a_generate_verdicts\u001b[0;34m(self, input, actual_output)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                 res: Verdicts = await self.model.a_generate(\n\u001b[0m\u001b[1;32m    192\u001b[0m                     \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVerdicts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: JudgeLLM.a_generate() got an unexpected keyword argument 'schema'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1602517860.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_cases\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_case\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/deepeval/evaluate/evaluate.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(test_cases, metrics, hyperparameters, goldens, observed_callback, identifier, async_config, display_config, cache_config, error_config)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0masync_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_async\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0mloop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_or_create_event_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m                 test_results = loop.run_until_complete(\n\u001b[0m\u001b[1;32m    267\u001b[0m                     a_execute_test_cases(\n\u001b[1;32m    268\u001b[0m                         \u001b[0mtest_cases\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 raise RuntimeError(\n\u001b[1;32m     97\u001b[0m                     'Event loop stopped before Future completed.')\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/asyncio/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__log_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_must_cancel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/deepeval/evaluate/execute.py\u001b[0m in \u001b[0;36ma_execute_test_cases\u001b[0;34m(test_cases, metrics, ignore_errors, skip_on_missing_params, use_cache, show_indicator, throttle_value, max_concurrent, save_to_disk, verbose_mode, identifier, test_run_manager, _use_bar_indicator, _is_assert_test)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m                     \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthrottle_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m             \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtest_case\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_cases\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/asyncio/tasks.py\u001b[0m in \u001b[0;36m__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__wakeup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m             \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0;31m# This may also be a cancellation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_must_cancel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/deepeval/evaluate/execute.py\u001b[0m in \u001b[0;36mexecute_with_semaphore\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mexecute_with_semaphore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0msemaphore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0mglobal_test_run_cache_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_write_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_to_disk\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/deepeval/evaluate/execute.py\u001b[0m in \u001b[0;36ma_execute_llm_test_cases\u001b[0;34m(metrics, test_case, test_run_manager, test_results, count, test_run, ignore_errors, skip_on_missing_params, use_cache, show_indicator, _use_bar_indicator, _is_assert_test, pbar)\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[0mnew_cached_test_case\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCachedTestCase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCachedTestCase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0mtest_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m     await measure_metrics_with_indicator(\n\u001b[0m\u001b[1;32m    615\u001b[0m         \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0mtest_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_case\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/deepeval/metrics/indicator.py\u001b[0m in \u001b[0;36mmeasure_metrics_with_indicator\u001b[0;34m(metrics, test_case, cached_test_case, ignore_errors, skip_on_missing_params, show_indicator, pbar_eval)\u001b[0m\n\u001b[1;32m    205\u001b[0m                 )\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/asyncio/tasks.py\u001b[0m in \u001b[0;36m__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__wakeup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m             \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0;31m# This may also be a cancellation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    275\u001b[0m                 \u001b[0;31m# We use the `send` method directly, because coroutines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/deepeval/metrics/indicator.py\u001b[0m in \u001b[0;36msafe_a_measure\u001b[0;34m(metric, tc, ignore_errors, skip_on_missing_params, pbar_eval)\u001b[0m\n\u001b[1;32m    216\u001b[0m ):\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0;32mawait\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_measure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_show_indicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpbar_eval\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mpbar_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/deepeval/metrics/prompt_alignment/prompt_alignment.py\u001b[0m in \u001b[0;36ma_measure\u001b[0;34m(self, test_case, _show_indicator)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_show_indicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_show_indicator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         ):\n\u001b[0;32m--> 100\u001b[0;31m             self.verdicts: Verdicts = await self._a_generate_verdicts(\n\u001b[0m\u001b[1;32m    101\u001b[0m                 \u001b[0mtest_case\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_case\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactual_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/deepeval/metrics/prompt_alignment/prompt_alignment.py\u001b[0m in \u001b[0;36m_a_generate_verdicts\u001b[0;34m(self, input, actual_output)\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrimAndLoadJson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 return [\n\u001b[0;32m--> 199\u001b[0;31m                     \u001b[0mPromptAlignmentVerdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"verdicts\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 ]\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'verdicts'"],"ename":"KeyError","evalue":"'verdicts'","output_type":"error"}],"execution_count":111},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"##### ","metadata":{}}]}